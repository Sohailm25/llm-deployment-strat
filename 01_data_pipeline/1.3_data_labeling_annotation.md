# Document 1.3: Data Labeling & Annotation Guide

**Document ID:** 1.3
**Version:** 1.0
**Last Updated:** December 2025
**Author:** LLM Platform Team
**Status:** Complete
**Priority:** P1

> **Navigation** | [← 1.2 Data Cleaning](1.2_data_cleaning_preprocessing.md) | [1.4 Data Versioning →](1.4_data_versioning_lineage.md)
>
> | | |
> |---|---|
> | **Prerequisites** | [1.1 Data Collection](1.1_data_collection_sourcing.md), [1.2 Data Cleaning](1.2_data_cleaning_preprocessing.md) |
> | **Related** | [3.1 Supervised Fine-Tuning](../03_fine_tuning/3.1_supervised_fine_tuning.md) &#124; [4.1 RLHF](../04_alignment_safety/4.1_rlhf_guide.md) |
> | **Next** | [1.4 Data Versioning & Lineage](1.4_data_versioning_lineage.md) |

---

## Executive Summary

High-quality labeled data is the foundation of supervised fine-tuning (SFT), reinforcement learning from human feedback (RLHF), and evaluation datasets. This guide provides comprehensive coverage of annotation task design, annotator management, platform selection, and quality assurance for LLM data annotation.

Key takeaways:
- Clear annotation guidelines with examples significantly improve consistency
- Inter-annotator agreement metrics (≥0.80) are essential for quality datasets
- Preference labeling (pairwise comparisons) simplifies subjective evaluation in RLHF
- Calibration and ongoing training prevent annotator drift
- Quality control requires golden set validation and continuous monitoring

---

## Table of Contents

1. [Prerequisites](#prerequisites)
2. [Annotation Task Design](#annotation-task-design)
3. [Annotator Management](#annotator-management)
4. [Annotation Platforms](#annotation-platforms)
5. [Annotation Workflows](#annotation-workflows)
6. [Specialized Annotation Types](#specialized-annotation-types)
7. [Quality Assurance](#quality-assurance)
8. [Implementation Guide](#implementation-guide)
9. [Code Examples](#code-examples)
10. [Configuration Templates](#configuration-templates)
11. [Troubleshooting](#troubleshooting)
12. [Appendices](#appendices)
13. [Glossary](#glossary)
14. [References](#references)

---

## Prerequisites

### Required Knowledge
- Understanding of machine learning evaluation concepts
- Basic statistics (inter-annotator agreement metrics)
- Familiarity with LLM training objectives (SFT, RLHF)
- Understanding of annotation bias and quality control

### Required Tools & Access
```bash
# Annotation platform CLIs
pip install label-studio
pip install argilla

# Quality metrics
pip install scikit-learn
pip install krippendorff
pip install nltk

# Data processing
pip install pandas numpy
pip install pyarrow
```

### Recommended Reading
- [Doc 3.1: Supervised Fine-Tuning Guide]
- [Doc 4.1: RLHF Guide]
- [Doc 5.4: Human Evaluation Protocol Guide]

---

## Annotation Task Design

### Task Type Taxonomy

| Task Type | Description | Example | Complexity |
|-----------|-------------|---------|------------|
| **Classification** | Assign category label | Sentiment, topic, toxicity | Low |
| **Extraction** | Identify spans in text | NER, key phrases, citations | Medium |
| **Generation** | Create new text | Responses, summaries, edits | High |
| **Ranking** | Order items by preference | Response quality ranking | Medium |
| **Pairwise Comparison** | Choose between two options | A vs B preference | Low-Medium |
| **Rating** | Assign numerical score | Quality 1-5, helpfulness 1-7 | Medium |
| **Correction** | Fix errors in output | Grammar, factuality, formatting | High |

### Instruction Writing Best Practices

```markdown
## Annotation Guideline Template

### Task Overview
[Brief description of the annotation task and its purpose]

### Input Format
- You will see: [describe what annotator will see]
- Context provided: [describe any context given]
- Length range: [typical input length]

### Your Task
[Clear, unambiguous description of what to do]

### Label Definitions
For each label, provide:
1. **Definition**: What this label means
2. **When to use**: Clear criteria for applying this label
3. **When NOT to use**: Explicit exclusions
4. **Examples**: 2-3 annotated examples per label

### Decision Flowchart
[If task is complex, provide step-by-step decision process]

### Edge Cases
[Document how to handle ambiguous situations]

### Common Mistakes
[List frequent errors to avoid]

### Examples

#### Example 1: [Typical case]
**Input**: [Input text]
**Correct Label**: [Label]
**Explanation**: [Why this label is correct]

#### Example 2: [Edge case]
**Input**: [Input text]
**Correct Label**: [Label]
**Explanation**: [Why this is the right decision]

#### Example 3: [What NOT to do]
**Input**: [Input text]
**Incorrect Label**: [Wrong label]
**Correct Label**: [Right label]
**Explanation**: [Why the first choice is wrong]
```

### Label Schema Design

```python
"""
Label schema design patterns for different annotation tasks.
"""

from dataclasses import dataclass, field
from typing import List, Dict, Optional, Any
from enum import Enum
import json


# Flat classification schema
class SentimentLabel(Enum):
    """Simple flat label schema for sentiment classification."""
    POSITIVE = "positive"
    NEGATIVE = "negative"
    NEUTRAL = "neutral"
    MIXED = "mixed"


# Hierarchical schema
@dataclass
class HierarchicalLabel:
    """Hierarchical label schema for complex taxonomies."""
    category: str
    subcategory: str
    confidence: float = 1.0

    def __str__(self):
        return f"{self.category}/{self.subcategory}"


# Multi-label schema
@dataclass
class MultiLabelAnnotation:
    """Multi-label annotation for items that can have multiple labels."""
    labels: List[str]
    primary_label: Optional[str] = None

    def validate(self, allowed_labels: List[str]) -> bool:
        """Validate all labels are in allowed set."""
        return all(label in allowed_labels for label in self.labels)


# Structured annotation schema
@dataclass
class ResponseQualityAnnotation:
    """
    Structured annotation schema for LLM response quality.
    Used in RLHF and evaluation tasks.
    """
    # Overall assessment
    overall_quality: int  # 1-5 scale
    preferred_response: Optional[str] = None  # For A/B comparisons

    # Dimension scores
    helpfulness: int = 0  # 1-5
    accuracy: int = 0  # 1-5
    coherence: int = 0  # 1-5
    safety: int = 0  # 1-5

    # Binary flags
    contains_hallucination: bool = False
    contains_harmful_content: bool = False
    follows_instructions: bool = True

    # Free text
    explanation: str = ""
    suggested_improvement: str = ""

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for storage."""
        return {
            "overall_quality": self.overall_quality,
            "preferred_response": self.preferred_response,
            "dimensions": {
                "helpfulness": self.helpfulness,
                "accuracy": self.accuracy,
                "coherence": self.coherence,
                "safety": self.safety
            },
            "flags": {
                "hallucination": self.contains_hallucination,
                "harmful": self.contains_harmful_content,
                "follows_instructions": self.follows_instructions
            },
            "explanation": self.explanation,
            "suggested_improvement": self.suggested_improvement
        }


# Span annotation schema
@dataclass
class SpanAnnotation:
    """Schema for span-based annotations (NER, extraction)."""
    text: str
    start: int
    end: int
    label: str
    confidence: float = 1.0
    attributes: Dict[str, Any] = field(default_factory=dict)

    def validate(self, source_text: str) -> bool:
        """Validate span matches source text."""
        return source_text[self.start:self.end] == self.text


# Preference annotation schema for RLHF
@dataclass
class PreferenceAnnotation:
    """
    Schema for preference annotations used in RLHF.
    Supports pairwise comparison and ranking.
    """
    prompt: str
    response_a: str
    response_b: str

    # Preference (1 = A wins, 2 = B wins, 0 = tie)
    preference: int

    # Confidence in decision
    confidence: str  # "low", "medium", "high"

    # Optional dimension-level preferences
    dimension_preferences: Dict[str, int] = field(default_factory=dict)

    # Explanation for decision
    reasoning: str = ""

    def validate(self) -> bool:
        """Validate preference annotation."""
        if self.preference not in [0, 1, 2]:
            return False
        if self.confidence not in ["low", "medium", "high"]:
            return False
        return True


def create_label_schema(task_type: str, config: Dict) -> Any:
    """
    Factory function to create label schema based on task type.

    Args:
        task_type: Type of annotation task
        config: Task-specific configuration

    Returns:
        Appropriate schema class
    """
    schemas = {
        "classification": SentimentLabel,
        "quality": ResponseQualityAnnotation,
        "span": SpanAnnotation,
        "preference": PreferenceAnnotation,
    }

    return schemas.get(task_type)
```

### Edge Case Documentation

```markdown
## Edge Case Handling Guide

### Ambiguous Cases

#### Case 1: Multiple Valid Labels
**Situation**: Text could reasonably fall into multiple categories
**Decision Rule**:
1. If confidence difference > 20%, choose higher confidence
2. If confidence similar, choose based on primary purpose of text
3. If still unclear, mark as "needs_review" and document reasoning

#### Case 2: Insufficient Context
**Situation**: Cannot determine correct label without more context
**Decision Rule**:
1. Check if context is truly necessary (avoid unnecessary escalation)
2. If necessary, mark as "insufficient_context"
3. Document what additional context would help

#### Case 3: Out of Scope Content
**Situation**: Input doesn't fit any defined category
**Decision Rule**:
1. First verify input is valid (not corrupted/truncated)
2. If valid but doesn't fit, mark as "out_of_scope"
3. Suggest potential new category if pattern repeats

#### Case 4: Conflicting Signals
**Situation**: Different parts of text suggest different labels
**Decision Rule**:
1. Identify the dominant signal (>60% of content)
2. If no dominant signal, mark as "mixed"
3. Document the conflicting signals in notes

### Special Handling

#### Sarcasm and Irony
- Consider literal vs intended meaning
- Default to intended meaning if detectable
- Mark uncertainty if ambiguous

#### Code Mixed with Text
- Treat code blocks separately if task allows
- Focus on natural language portions for text tasks
- Note presence of code in metadata

#### Non-English Content
- Follow language-specific guidelines if available
- Mark for specialist review if outside your expertise
- Don't guess - escalate if uncertain
```

---

## Annotator Management

### Recruitment and Qualification

```python
"""
Annotator qualification and screening system.
"""

from dataclasses import dataclass
from typing import List, Dict, Optional
from enum import Enum
import json


class ExpertiseLevel(Enum):
    """Annotator expertise levels."""
    NOVICE = "novice"
    INTERMEDIATE = "intermediate"
    EXPERT = "expert"
    SPECIALIST = "specialist"


@dataclass
class AnnotatorProfile:
    """Profile for tracking annotator qualifications and performance."""
    annotator_id: str
    name: str
    email: str

    # Qualifications
    expertise_areas: List[str]
    expertise_level: ExpertiseLevel
    languages: List[str]
    certifications: List[str]

    # Performance metrics
    total_annotations: int = 0
    accuracy_score: float = 0.0
    agreement_score: float = 0.0
    speed_percentile: float = 50.0

    # Status
    is_active: bool = True
    qualified_tasks: List[str] = None

    def __post_init__(self):
        if self.qualified_tasks is None:
            self.qualified_tasks = []


class QualificationTest:
    """
    Qualification test system for new annotators.
    """

    def __init__(
        self,
        task_type: str,
        golden_set: List[Dict],
        passing_threshold: float = 0.85
    ):
        """
        Args:
            task_type: Type of annotation task
            golden_set: List of items with ground truth labels
            passing_threshold: Minimum accuracy to pass (0-1)
        """
        self.task_type = task_type
        self.golden_set = golden_set
        self.passing_threshold = passing_threshold

    def administer_test(self, annotator: AnnotatorProfile) -> Dict:
        """
        Administer qualification test.

        Returns:
            Test results including pass/fail and detailed scores
        """
        # In practice, this would integrate with annotation platform
        # Here we show the structure of results

        results = {
            "annotator_id": annotator.annotator_id,
            "task_type": self.task_type,
            "total_questions": len(self.golden_set),
            "correct_answers": 0,
            "incorrect_answers": [],
            "accuracy": 0.0,
            "passed": False,
            "time_taken_minutes": 0,
            "feedback": []
        }

        return results

    def evaluate_submission(
        self,
        submission: List[Dict],
        ground_truth: List[Dict]
    ) -> Dict:
        """
        Evaluate test submission against ground truth.

        Args:
            submission: Annotator's answers
            ground_truth: Correct answers

        Returns:
            Evaluation results
        """
        correct = 0
        incorrect = []

        for sub, truth in zip(submission, ground_truth):
            if sub["label"] == truth["label"]:
                correct += 1
            else:
                incorrect.append({
                    "item_id": sub["item_id"],
                    "submitted": sub["label"],
                    "expected": truth["label"],
                    "explanation": truth.get("explanation", "")
                })

        accuracy = correct / len(ground_truth) if ground_truth else 0

        return {
            "correct": correct,
            "total": len(ground_truth),
            "accuracy": accuracy,
            "passed": accuracy >= self.passing_threshold,
            "incorrect_items": incorrect
        }


class AnnotatorTraining:
    """
    Training and onboarding system for annotators.
    """

    def __init__(self, task_type: str):
        self.task_type = task_type
        self.modules = []

    def add_module(
        self,
        name: str,
        content: str,
        examples: List[Dict],
        quiz: List[Dict] = None
    ):
        """Add training module."""
        self.modules.append({
            "name": name,
            "content": content,
            "examples": examples,
            "quiz": quiz or []
        })

    def generate_curriculum(self) -> Dict:
        """Generate training curriculum structure."""
        return {
            "task_type": self.task_type,
            "modules": [
                {
                    "name": m["name"],
                    "num_examples": len(m["examples"]),
                    "has_quiz": len(m["quiz"]) > 0
                }
                for m in self.modules
            ],
            "total_modules": len(self.modules),
            "estimated_time_hours": len(self.modules) * 0.5
        }


# Example training curriculum for RLHF preference annotation
RLHF_TRAINING_CURRICULUM = """
# RLHF Preference Annotation Training

## Module 1: Introduction to RLHF
- What is reinforcement learning from human feedback?
- Why human preferences matter for AI alignment
- Your role as an annotator

## Module 2: Understanding the Task
- Reading prompts carefully
- Evaluating response pairs
- Making preference decisions

## Module 3: Quality Dimensions
- Helpfulness: Does it answer the question?
- Accuracy: Is the information correct?
- Safety: Does it avoid harmful content?
- Coherence: Is it well-written and clear?

## Module 4: Making Decisions
- When responses are clearly different
- When responses are similar
- When to mark ties
- Confidence levels

## Module 5: Common Pitfalls
- Length bias: Longer ≠ better
- Verbosity bias: More detail ≠ more helpful
- Position bias: A and B equally likely to be better
- Surface features vs actual quality

## Module 6: Practice Exercises
- 20 practice comparisons with feedback
- Discussion of edge cases
- Final calibration quiz

## Module 7: Quality Standards
- Expected agreement rate: ≥80%
- Annotation speed: 2-5 minutes per comparison
- When to skip or escalate
"""
```

### Quality Control Metrics

```python
"""
Inter-annotator agreement and quality metrics.
"""

import numpy as np
from typing import List, Dict, Tuple
from collections import defaultdict
import warnings


def cohens_kappa(
    annotations1: List[Any],
    annotations2: List[Any],
    labels: List[Any] = None
) -> float:
    """
    Calculate Cohen's Kappa for two annotators.

    Args:
        annotations1: First annotator's labels
        annotations2: Second annotator's labels
        labels: List of possible labels (auto-detected if None)

    Returns:
        Cohen's Kappa coefficient (-1 to 1)
    """
    from sklearn.metrics import cohen_kappa_score

    return cohen_kappa_score(annotations1, annotations2)


def fleiss_kappa(annotations_matrix: np.ndarray) -> float:
    """
    Calculate Fleiss' Kappa for multiple annotators.

    Args:
        annotations_matrix: Matrix where rows are items, columns are
                          counts of each category assignment

    Returns:
        Fleiss' Kappa coefficient
    """
    n_items, n_categories = annotations_matrix.shape
    n_annotators = annotations_matrix.sum(axis=1)[0]

    # P_i: proportion of agreement for each item
    p_i = (np.sum(annotations_matrix ** 2, axis=1) - n_annotators) / \
          (n_annotators * (n_annotators - 1))

    # P_bar: mean of P_i
    p_bar = np.mean(p_i)

    # P_j: proportion of assignments to each category
    p_j = np.sum(annotations_matrix, axis=0) / (n_items * n_annotators)

    # P_e: expected agreement by chance
    p_e = np.sum(p_j ** 2)

    if p_e == 1:
        return 1.0

    kappa = (p_bar - p_e) / (1 - p_e)
    return kappa


def krippendorff_alpha(
    reliability_data: List[List],
    level_of_measurement: str = "nominal"
) -> float:
    """
    Calculate Krippendorff's Alpha for any number of annotators.

    Args:
        reliability_data: List of annotator responses
                         (each inner list is one annotator's ratings)
        level_of_measurement: "nominal", "ordinal", "interval", or "ratio"

    Returns:
        Krippendorff's Alpha coefficient
    """
    try:
        import krippendorff
        return krippendorff.alpha(
            reliability_data=reliability_data,
            level_of_measurement=level_of_measurement
        )
    except ImportError:
        warnings.warn("krippendorff package not installed, using sklearn")
        # Fall back to simpler metric
        return cohens_kappa(reliability_data[0], reliability_data[1])


class AnnotationQualityTracker:
    """
    Track annotation quality metrics over time.
    """

    def __init__(self, task_id: str):
        self.task_id = task_id
        self.annotations = defaultdict(list)  # annotator_id -> annotations
        self.golden_results = {}  # annotator_id -> accuracy on golden set
        self.agreement_history = []

    def add_annotation(
        self,
        annotator_id: str,
        item_id: str,
        label: Any,
        metadata: Dict = None
    ):
        """Record an annotation."""
        self.annotations[annotator_id].append({
            "item_id": item_id,
            "label": label,
            "metadata": metadata or {}
        })

    def add_golden_result(
        self,
        annotator_id: str,
        correct: int,
        total: int
    ):
        """Record golden set performance."""
        self.golden_results[annotator_id] = {
            "correct": correct,
            "total": total,
            "accuracy": correct / total if total > 0 else 0
        }

    def calculate_pairwise_agreement(self) -> Dict[Tuple[str, str], float]:
        """Calculate agreement between all pairs of annotators."""
        annotator_ids = list(self.annotations.keys())
        agreements = {}

        for i, ann1 in enumerate(annotator_ids):
            for ann2 in annotator_ids[i+1:]:
                # Find common items
                items1 = {a["item_id"]: a["label"]
                         for a in self.annotations[ann1]}
                items2 = {a["item_id"]: a["label"]
                         for a in self.annotations[ann2]}

                common_items = set(items1.keys()) & set(items2.keys())

                if len(common_items) < 10:
                    continue

                labels1 = [items1[item] for item in common_items]
                labels2 = [items2[item] for item in common_items]

                kappa = cohens_kappa(labels1, labels2)
                agreements[(ann1, ann2)] = kappa

        return agreements

    def get_annotator_stats(self, annotator_id: str) -> Dict:
        """Get statistics for a specific annotator."""
        annotations = self.annotations.get(annotator_id, [])

        return {
            "annotator_id": annotator_id,
            "total_annotations": len(annotations),
            "golden_accuracy": self.golden_results.get(annotator_id, {}).get("accuracy"),
            "items_annotated": len(set(a["item_id"] for a in annotations))
        }

    def generate_quality_report(self) -> Dict:
        """Generate comprehensive quality report."""
        pairwise = self.calculate_pairwise_agreement()

        return {
            "task_id": self.task_id,
            "total_annotators": len(self.annotations),
            "total_annotations": sum(
                len(anns) for anns in self.annotations.values()
            ),
            "pairwise_agreements": {
                f"{k[0]}-{k[1]}": v for k, v in pairwise.items()
            },
            "mean_pairwise_agreement": np.mean(list(pairwise.values())) if pairwise else 0,
            "golden_set_performance": self.golden_results,
            "annotator_stats": {
                ann_id: self.get_annotator_stats(ann_id)
                for ann_id in self.annotations.keys()
            }
        }


# Quality thresholds and interpretation
AGREEMENT_THRESHOLDS = {
    "poor": (0.0, 0.20),
    "fair": (0.21, 0.40),
    "moderate": (0.41, 0.60),
    "substantial": (0.61, 0.80),
    "almost_perfect": (0.81, 1.00)
}


def interpret_kappa(kappa: float) -> str:
    """Interpret Kappa score according to Landis & Koch (1977)."""
    for level, (low, high) in AGREEMENT_THRESHOLDS.items():
        if low <= kappa <= high:
            return level
    return "invalid"
```

### Compensation Guidelines

```markdown
## Fair Compensation Guidelines for Data Annotation

### Principles
1. **Living wage**: Annotators should earn at least local living wage
2. **Skill-based**: Complex tasks requiring expertise pay more
3. **Quality incentives**: Bonuses for high-quality work
4. **Transparency**: Clear payment structure communicated upfront

### Rate Calculation Framework

#### Base Rate Factors
| Factor | Consideration |
|--------|---------------|
| Task complexity | Simple (1x) to Expert (4x) |
| Required expertise | General (1x) to Specialist (2x) |
| Language | English (1x), Other (1.2-1.5x) |
| Time sensitivity | Standard (1x), Urgent (1.5x) |

#### Example Rate Structure (USD, 2025)

##### Classification Tasks
- Simple binary: $0.03-0.05 per item
- Multi-class: $0.05-0.10 per item
- Multi-label: $0.08-0.15 per item

##### Extraction Tasks
- Named entity: $0.10-0.20 per item
- Complex extraction: $0.20-0.50 per item

##### Generation/Correction Tasks
- Response editing: $0.50-2.00 per item
- Full response writing: $2.00-5.00 per item

##### RLHF Tasks
- Pairwise preference: $0.20-0.50 per comparison
- Multi-turn dialogue evaluation: $1.00-3.00 per conversation
- Expert domain review: $5.00-20.00 per item

### Quality Bonuses
- Agreement rate > 90%: +10% bonus
- Golden set accuracy > 95%: +15% bonus
- Top 10% performer: +20% bonus

### Minimum Standards
- Target hourly rate: $15-25/hour (adjust for location)
- Track actual annotation time
- Regular rate reviews
```

---

## Annotation Platforms

### Platform Comparison Matrix

| Platform | Best For | Pricing | Self-Hosted | RLHF Support |
|----------|----------|---------|-------------|--------------|
| **Scale AI** | Enterprise, high volume | $$$$$ | No | Yes |
| **Labelbox** | General purpose, integrations | $$$$ | No | Yes |
| **Label Studio** | Open source, customization | Free | Yes | Limited |
| **Argilla** | LLM/NLP tasks | Free | Yes | Yes |
| **Surge AI** | Quality-focused | $$$$ | No | Yes |
| **Prodigy** | Active learning | $$ | Yes | Limited |
| **Amazon SageMaker Ground Truth** | AWS integration | $$$ | No | Limited |

### Label Studio Configuration

```python
"""
Label Studio configuration for LLM annotation tasks.
"""

# Classification task configuration
CLASSIFICATION_CONFIG = """
<View>
  <Header value="Classify the sentiment of the following text"/>
  <Text name="text" value="$text"/>

  <Choices name="sentiment" toName="text" choice="single">
    <Choice value="positive" hint="Text expresses positive sentiment"/>
    <Choice value="negative" hint="Text expresses negative sentiment"/>
    <Choice value="neutral" hint="Text is neutral or factual"/>
    <Choice value="mixed" hint="Text contains both positive and negative"/>
  </Choices>

  <Header value="Confidence"/>
  <Rating name="confidence" toName="text" maxRating="5"/>

  <Header value="Notes (optional)"/>
  <TextArea name="notes" toName="text" rows="3"
            placeholder="Any additional observations..."/>
</View>
"""

# RLHF preference task configuration
RLHF_PREFERENCE_CONFIG = """
<View>
  <Style>
    .response-box {
      border: 1px solid #ddd;
      padding: 15px;
      margin: 10px 0;
      border-radius: 8px;
      background: #f9f9f9;
    }
    .prompt-box {
      background: #e8f4fd;
      padding: 15px;
      border-radius: 8px;
      margin-bottom: 20px;
    }
  </Style>

  <View className="prompt-box">
    <Header value="User Prompt"/>
    <Text name="prompt" value="$prompt"/>
  </View>

  <View style="display: flex; gap: 20px;">
    <View className="response-box" style="flex: 1;">
      <Header value="Response A"/>
      <Text name="response_a" value="$response_a"/>
    </View>

    <View className="response-box" style="flex: 1;">
      <Header value="Response B"/>
      <Text name="response_b" value="$response_b"/>
    </View>
  </View>

  <Header value="Which response is better overall?"/>
  <Choices name="preference" toName="prompt" choice="single" required="true">
    <Choice value="A" hint="Response A is clearly better"/>
    <Choice value="B" hint="Response B is clearly better"/>
    <Choice value="tie" hint="Both responses are equally good/bad"/>
  </Choices>

  <Header value="Rate each dimension (1=Poor, 5=Excellent)"/>

  <View style="display: grid; grid-template-columns: 1fr 1fr; gap: 10px;">
    <View>
      <Text name="help_label" value="Helpfulness - Response A"/>
      <Rating name="helpfulness_a" toName="prompt" maxRating="5"/>
    </View>
    <View>
      <Text name="help_label_b" value="Helpfulness - Response B"/>
      <Rating name="helpfulness_b" toName="prompt" maxRating="5"/>
    </View>

    <View>
      <Text name="acc_label" value="Accuracy - Response A"/>
      <Rating name="accuracy_a" toName="prompt" maxRating="5"/>
    </View>
    <View>
      <Text name="acc_label_b" value="Accuracy - Response B"/>
      <Rating name="accuracy_b" toName="prompt" maxRating="5"/>
    </View>

    <View>
      <Text name="safe_label" value="Safety - Response A"/>
      <Rating name="safety_a" toName="prompt" maxRating="5"/>
    </View>
    <View>
      <Text name="safe_label_b" value="Safety - Response B"/>
      <Rating name="safety_b" toName="prompt" maxRating="5"/>
    </View>
  </View>

  <Header value="Confidence in your choice"/>
  <Choices name="confidence" toName="prompt" choice="single" required="true">
    <Choice value="high" hint="I'm very confident"/>
    <Choice value="medium" hint="Somewhat confident"/>
    <Choice value="low" hint="This was difficult to decide"/>
  </Choices>

  <Header value="Explain your decision"/>
  <TextArea name="reasoning" toName="prompt" rows="3" required="true"
            placeholder="Why did you prefer this response?"/>
</View>
"""

# Response quality annotation configuration
QUALITY_ANNOTATION_CONFIG = """
<View>
  <Header value="Evaluate the AI response quality"/>

  <View className="prompt-box">
    <Header value="User Query"/>
    <Text name="query" value="$query"/>
  </View>

  <View className="response-box">
    <Header value="AI Response"/>
    <Text name="response" value="$response"/>
  </View>

  <Header value="Quality Dimensions"/>

  <View style="margin: 10px 0;">
    <Text name="help_label" value="Helpfulness - Does it address the query?"/>
    <Rating name="helpfulness" toName="query" maxRating="5"/>
  </View>

  <View style="margin: 10px 0;">
    <Text name="acc_label" value="Accuracy - Is the information correct?"/>
    <Rating name="accuracy" toName="query" maxRating="5"/>
  </View>

  <View style="margin: 10px 0;">
    <Text name="coh_label" value="Coherence - Is it well-written?"/>
    <Rating name="coherence" toName="query" maxRating="5"/>
  </View>

  <View style="margin: 10px 0;">
    <Text name="safe_label" value="Safety - Does it avoid harmful content?"/>
    <Rating name="safety" toName="query" maxRating="5"/>
  </View>

  <Header value="Issues Detected"/>
  <Choices name="issues" toName="query" choice="multiple">
    <Choice value="hallucination" hint="Contains false or made-up information"/>
    <Choice value="incomplete" hint="Doesn't fully answer the question"/>
    <Choice value="off_topic" hint="Strays from the original question"/>
    <Choice value="harmful" hint="Contains potentially harmful content"/>
    <Choice value="biased" hint="Shows unfair bias"/>
    <Choice value="formatting" hint="Poor formatting or structure"/>
    <Choice value="none" hint="No issues detected"/>
  </Choices>

  <Header value="Additional Notes"/>
  <TextArea name="notes" toName="query" rows="3"
            placeholder="Any specific issues or suggestions..."/>
</View>
"""


def setup_label_studio_project(
    project_name: str,
    config_xml: str,
    api_url: str = "http://localhost:8080",
    api_key: str = None
) -> Dict:
    """
    Create Label Studio project with configuration.

    Args:
        project_name: Name for the project
        config_xml: Labeling interface XML configuration
        api_url: Label Studio API URL
        api_key: API authentication key

    Returns:
        Project details
    """
    import requests

    headers = {"Authorization": f"Token {api_key}"} if api_key else {}

    # Create project
    response = requests.post(
        f"{api_url}/api/projects",
        json={
            "title": project_name,
            "label_config": config_xml
        },
        headers=headers
    )

    return response.json()
```

### Argilla Configuration for LLM Tasks

```python
"""
Argilla configuration for LLM annotation tasks.
"""

import argilla as rg


def setup_argilla_workspace():
    """Set up Argilla workspace for LLM annotation."""

    # Initialize connection
    rg.init(
        api_url="http://localhost:6900",
        api_key="admin.apikey"
    )


def create_feedback_dataset(name: str) -> rg.FeedbackDataset:
    """
    Create Argilla FeedbackDataset for RLHF-style annotation.
    """

    dataset = rg.FeedbackDataset(
        fields=[
            rg.TextField(name="prompt", title="User Prompt"),
            rg.TextField(name="response_a", title="Response A"),
            rg.TextField(name="response_b", title="Response B"),
        ],
        questions=[
            rg.RatingQuestion(
                name="preference",
                title="Which response is better?",
                description="1 = Response A, 2 = Tie, 3 = Response B",
                values=[1, 2, 3],
                required=True
            ),
            rg.RatingQuestion(
                name="helpfulness_a",
                title="Helpfulness of Response A",
                values=[1, 2, 3, 4, 5],
                required=True
            ),
            rg.RatingQuestion(
                name="helpfulness_b",
                title="Helpfulness of Response B",
                values=[1, 2, 3, 4, 5],
                required=True
            ),
            rg.RatingQuestion(
                name="accuracy_a",
                title="Accuracy of Response A",
                values=[1, 2, 3, 4, 5],
                required=True
            ),
            rg.RatingQuestion(
                name="accuracy_b",
                title="Accuracy of Response B",
                values=[1, 2, 3, 4, 5],
                required=True
            ),
            rg.TextQuestion(
                name="reasoning",
                title="Explain your preference",
                required=True
            ),
            rg.LabelQuestion(
                name="confidence",
                title="How confident are you?",
                labels=["Low", "Medium", "High"],
                required=True
            )
        ],
        guidelines="""
        ## Task: Compare AI Responses

        You will see a user prompt and two AI responses. Your job is to:

        1. **Read the prompt carefully** - Understand what the user is asking
        2. **Read both responses** - Consider helpfulness, accuracy, and clarity
        3. **Choose the better response** - Or mark as tie if equally good/bad
        4. **Rate each dimension** - Score helpfulness and accuracy separately
        5. **Explain your reasoning** - Why did you prefer one over the other?

        ### Quality Criteria
        - **Helpfulness**: Does it answer the question completely?
        - **Accuracy**: Is the information factually correct?
        - **Safety**: Does it avoid harmful or inappropriate content?

        ### Common Pitfalls to Avoid
        - Don't prefer responses just because they're longer
        - Don't let minor style differences influence major quality judgments
        - Focus on substance over surface features
        """
    )

    return dataset


def add_records_to_dataset(
    dataset: rg.FeedbackDataset,
    data: list[dict]
) -> None:
    """
    Add records to Argilla dataset.

    Args:
        dataset: Argilla FeedbackDataset
        data: List of dicts with prompt, response_a, response_b
    """
    records = [
        rg.FeedbackRecord(
            fields={
                "prompt": item["prompt"],
                "response_a": item["response_a"],
                "response_b": item["response_b"]
            }
        )
        for item in data
    ]

    dataset.add_records(records)


def export_annotations(
    dataset: rg.FeedbackDataset,
    output_path: str
) -> None:
    """Export annotations to HuggingFace format."""
    # Export to HuggingFace dataset format
    hf_dataset = dataset.format_as("datasets")
    hf_dataset.save_to_disk(output_path)
```

---

## Annotation Workflows

### Single Annotator vs Consensus

```python
"""
Annotation workflow configurations.
"""

from dataclasses import dataclass
from typing import List, Dict, Any, Optional
from enum import Enum


class WorkflowType(Enum):
    """Types of annotation workflows."""
    SINGLE = "single"  # One annotator per item
    DUAL = "dual"  # Two annotators, adjudication if disagree
    CONSENSUS = "consensus"  # Multiple annotators, majority vote
    EXPERT_REVIEW = "expert_review"  # Junior annotators + expert review


@dataclass
class WorkflowConfig:
    """Configuration for annotation workflow."""
    workflow_type: WorkflowType
    annotators_per_item: int
    agreement_threshold: float = 0.8
    require_expert_review: bool = False
    adjudication_rules: Dict = None

    def __post_init__(self):
        if self.adjudication_rules is None:
            self.adjudication_rules = {
                "tie_breaker": "expert",
                "high_disagreement_action": "review",
                "max_adjudication_rounds": 2
            }


class AnnotationWorkflow:
    """
    Manage annotation workflow with multiple annotators.
    """

    def __init__(self, config: WorkflowConfig):
        self.config = config
        self.annotations = {}  # item_id -> list of annotations
        self.final_labels = {}  # item_id -> resolved label
        self.pending_adjudication = []

    def submit_annotation(
        self,
        item_id: str,
        annotator_id: str,
        label: Any,
        metadata: Dict = None
    ):
        """Submit an annotation for an item."""
        if item_id not in self.annotations:
            self.annotations[item_id] = []

        self.annotations[item_id].append({
            "annotator_id": annotator_id,
            "label": label,
            "metadata": metadata or {}
        })

        # Check if we have enough annotations to resolve
        if len(self.annotations[item_id]) >= self.config.annotators_per_item:
            self._try_resolve(item_id)

    def _try_resolve(self, item_id: str):
        """Try to resolve final label for item."""
        item_annotations = self.annotations[item_id]
        labels = [a["label"] for a in item_annotations]

        if self.config.workflow_type == WorkflowType.SINGLE:
            # Single annotator - use their label
            self.final_labels[item_id] = labels[0]

        elif self.config.workflow_type == WorkflowType.DUAL:
            # Two annotators - check agreement
            if labels[0] == labels[1]:
                self.final_labels[item_id] = labels[0]
            else:
                # Need adjudication
                self.pending_adjudication.append(item_id)

        elif self.config.workflow_type == WorkflowType.CONSENSUS:
            # Multiple annotators - use majority vote
            from collections import Counter
            counts = Counter(labels)
            most_common, count = counts.most_common(1)[0]

            agreement_rate = count / len(labels)
            if agreement_rate >= self.config.agreement_threshold:
                self.final_labels[item_id] = most_common
            else:
                self.pending_adjudication.append(item_id)

    def adjudicate(
        self,
        item_id: str,
        expert_id: str,
        final_label: Any,
        reasoning: str = ""
    ):
        """Expert adjudication for disputed items."""
        if item_id in self.pending_adjudication:
            self.pending_adjudication.remove(item_id)

        self.final_labels[item_id] = final_label
        self.annotations[item_id].append({
            "annotator_id": expert_id,
            "label": final_label,
            "metadata": {
                "is_adjudication": True,
                "reasoning": reasoning
            }
        })

    def get_status(self) -> Dict:
        """Get workflow status."""
        return {
            "total_items": len(self.annotations),
            "resolved": len(self.final_labels),
            "pending_adjudication": len(self.pending_adjudication),
            "in_progress": len(self.annotations) - len(self.final_labels) - len(self.pending_adjudication)
        }


# Workflow templates for common scenarios
WORKFLOW_TEMPLATES = {
    "quick_labeling": WorkflowConfig(
        workflow_type=WorkflowType.SINGLE,
        annotators_per_item=1,
    ),
    "quality_focused": WorkflowConfig(
        workflow_type=WorkflowType.DUAL,
        annotators_per_item=2,
        agreement_threshold=1.0,  # Must agree
    ),
    "high_reliability": WorkflowConfig(
        workflow_type=WorkflowType.CONSENSUS,
        annotators_per_item=3,
        agreement_threshold=0.67,  # 2 of 3 agree
    ),
    "expert_reviewed": WorkflowConfig(
        workflow_type=WorkflowType.EXPERT_REVIEW,
        annotators_per_item=2,
        require_expert_review=True,
        agreement_threshold=0.8,
    ),
}
```

### Active Learning Integration

```python
"""
Active learning for efficient annotation.
"""

import numpy as np
from typing import List, Dict, Callable
from dataclasses import dataclass


@dataclass
class ActiveLearningConfig:
    """Configuration for active learning annotation."""
    initial_sample_size: int = 100
    batch_size: int = 50
    uncertainty_threshold: float = 0.3
    diversity_weight: float = 0.3
    selection_strategy: str = "uncertainty"  # "uncertainty", "diversity", "hybrid"


class ActiveLearningSelector:
    """
    Select samples for annotation using active learning strategies.
    """

    def __init__(self, config: ActiveLearningConfig):
        self.config = config
        self.annotated_indices = set()
        self.model = None  # Trained model for uncertainty estimation

    def select_initial_batch(
        self,
        corpus: List[str],
        strategy: str = "random"
    ) -> List[int]:
        """
        Select initial batch for annotation.

        Args:
            corpus: List of texts
            strategy: "random" or "diverse"

        Returns:
            Indices of selected items
        """
        if strategy == "random":
            indices = np.random.choice(
                len(corpus),
                size=min(self.config.initial_sample_size, len(corpus)),
                replace=False
            )
        elif strategy == "diverse":
            indices = self._diverse_selection(corpus, self.config.initial_sample_size)

        self.annotated_indices.update(indices)
        return list(indices)

    def select_next_batch(
        self,
        corpus: List[str],
        predictions: np.ndarray = None,
        embeddings: np.ndarray = None
    ) -> List[int]:
        """
        Select next batch using active learning.

        Args:
            corpus: Full corpus
            predictions: Model predictions (probabilities)
            embeddings: Text embeddings for diversity sampling

        Returns:
            Indices of selected items
        """
        # Get unannotated indices
        unannotated = [
            i for i in range(len(corpus))
            if i not in self.annotated_indices
        ]

        if not unannotated:
            return []

        if self.config.selection_strategy == "uncertainty":
            selected = self._uncertainty_sampling(
                unannotated, predictions
            )
        elif self.config.selection_strategy == "diversity":
            selected = self._diversity_sampling(
                unannotated, embeddings
            )
        else:  # hybrid
            selected = self._hybrid_sampling(
                unannotated, predictions, embeddings
            )

        self.annotated_indices.update(selected)
        return selected

    def _uncertainty_sampling(
        self,
        indices: List[int],
        predictions: np.ndarray
    ) -> List[int]:
        """Select samples with highest uncertainty."""
        if predictions is None:
            return np.random.choice(
                indices,
                size=min(self.config.batch_size, len(indices)),
                replace=False
            ).tolist()

        # Calculate uncertainty (entropy or margin)
        uncertainties = []
        for idx in indices:
            probs = predictions[idx]
            # Entropy
            entropy = -np.sum(probs * np.log(probs + 1e-10))
            uncertainties.append((idx, entropy))

        # Sort by uncertainty (descending)
        uncertainties.sort(key=lambda x: x[1], reverse=True)

        # Select top uncertain samples
        selected = [x[0] for x in uncertainties[:self.config.batch_size]]
        return selected

    def _diversity_sampling(
        self,
        indices: List[int],
        embeddings: np.ndarray
    ) -> List[int]:
        """Select diverse samples using k-means++."""
        if embeddings is None:
            return np.random.choice(
                indices,
                size=min(self.config.batch_size, len(indices)),
                replace=False
            ).tolist()

        # k-means++ initialization for diversity
        selected = []
        available = list(indices)

        # First point randomly
        first_idx = np.random.choice(available)
        selected.append(first_idx)
        available.remove(first_idx)

        # Remaining points
        while len(selected) < self.config.batch_size and available:
            # Calculate distances to nearest selected point
            distances = []
            for idx in available:
                min_dist = min(
                    np.linalg.norm(embeddings[idx] - embeddings[s])
                    for s in selected
                )
                distances.append(min_dist)

            # Select farthest point
            max_idx = np.argmax(distances)
            selected.append(available[max_idx])
            available.pop(max_idx)

        return selected

    def _hybrid_sampling(
        self,
        indices: List[int],
        predictions: np.ndarray,
        embeddings: np.ndarray
    ) -> List[int]:
        """Combine uncertainty and diversity sampling."""
        # Get uncertainty scores
        uncertainties = {}
        if predictions is not None:
            for idx in indices:
                probs = predictions[idx]
                entropy = -np.sum(probs * np.log(probs + 1e-10))
                uncertainties[idx] = entropy
        else:
            uncertainties = {idx: 0.5 for idx in indices}

        # Normalize uncertainties
        max_unc = max(uncertainties.values()) if uncertainties else 1
        uncertainties = {k: v / max_unc for k, v in uncertainties.items()}

        # Combined scoring with diversity
        selected = []
        available = list(indices)

        while len(selected) < self.config.batch_size and available:
            scores = []
            for idx in available:
                # Uncertainty score
                unc_score = uncertainties[idx]

                # Diversity score (distance to selected)
                if selected and embeddings is not None:
                    min_dist = min(
                        np.linalg.norm(embeddings[idx] - embeddings[s])
                        for s in selected
                    )
                    # Normalize roughly
                    div_score = min(min_dist / 10, 1)
                else:
                    div_score = 1

                # Combined score
                combined = (1 - self.config.diversity_weight) * unc_score + \
                          self.config.diversity_weight * div_score
                scores.append((idx, combined))

            # Select highest scoring
            scores.sort(key=lambda x: x[1], reverse=True)
            best_idx = scores[0][0]
            selected.append(best_idx)
            available.remove(best_idx)

        return selected
```

---

## Specialized Annotation Types

### RLHF Preference Data

```python
"""
RLHF preference data annotation and processing.
"""

from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple
import json


@dataclass
class PreferenceExample:
    """Single preference comparison example."""
    prompt: str
    chosen: str  # Preferred response
    rejected: str  # Non-preferred response

    # Optional metadata
    chosen_model: Optional[str] = None
    rejected_model: Optional[str] = None
    annotator_id: Optional[str] = None
    confidence: Optional[str] = None
    dimensions: Optional[Dict[str, int]] = None


class PreferenceDataProcessor:
    """
    Process preference annotations for RLHF training.
    """

    def __init__(self):
        self.examples = []

    def add_annotation(
        self,
        prompt: str,
        response_a: str,
        response_b: str,
        preference: int,  # 1 = A, 2 = B, 0 = tie
        metadata: Dict = None
    ) -> Optional[PreferenceExample]:
        """
        Add preference annotation and convert to training format.

        Args:
            prompt: User prompt
            response_a: First response
            response_b: Second response
            preference: 1 for A, 2 for B, 0 for tie
            metadata: Additional metadata

        Returns:
            PreferenceExample if valid preference (not tie)
        """
        if preference == 0:
            # Skip ties for training
            return None

        if preference == 1:
            chosen, rejected = response_a, response_b
        else:
            chosen, rejected = response_b, response_a

        example = PreferenceExample(
            prompt=prompt,
            chosen=chosen,
            rejected=rejected,
            annotator_id=metadata.get("annotator_id") if metadata else None,
            confidence=metadata.get("confidence") if metadata else None,
            dimensions=metadata.get("dimensions") if metadata else None
        )

        self.examples.append(example)
        return example

    def export_dpo_format(self, output_path: str) -> None:
        """
        Export in DPO training format.

        Format: {"prompt": str, "chosen": str, "rejected": str}
        """
        with open(output_path, "w") as f:
            for example in self.examples:
                f.write(json.dumps({
                    "prompt": example.prompt,
                    "chosen": example.chosen,
                    "rejected": example.rejected
                }) + "\n")

    def export_reward_model_format(self, output_path: str) -> None:
        """
        Export in reward model training format.

        Creates pairs of (prompt + response, label) where label is 1 for
        chosen and 0 for rejected.
        """
        with open(output_path, "w") as f:
            for example in self.examples:
                # Chosen example (label = 1)
                f.write(json.dumps({
                    "text": f"{example.prompt}\n\n{example.chosen}",
                    "label": 1
                }) + "\n")

                # Rejected example (label = 0)
                f.write(json.dumps({
                    "text": f"{example.prompt}\n\n{example.rejected}",
                    "label": 0
                }) + "\n")

    def export_hf_format(self) -> List[Dict]:
        """Export in HuggingFace datasets format."""
        return [
            {
                "prompt": ex.prompt,
                "chosen": ex.chosen,
                "rejected": ex.rejected
            }
            for ex in self.examples
        ]

    def get_statistics(self) -> Dict:
        """Get statistics about collected preferences."""
        return {
            "total_examples": len(self.examples),
            "unique_prompts": len(set(ex.prompt for ex in self.examples)),
            "with_confidence": sum(
                1 for ex in self.examples if ex.confidence is not None
            ),
            "confidence_distribution": self._get_confidence_dist(),
            "avg_chosen_length": sum(
                len(ex.chosen.split()) for ex in self.examples
            ) / len(self.examples) if self.examples else 0,
            "avg_rejected_length": sum(
                len(ex.rejected.split()) for ex in self.examples
            ) / len(self.examples) if self.examples else 0,
        }

    def _get_confidence_dist(self) -> Dict[str, int]:
        """Get distribution of confidence levels."""
        from collections import Counter
        confidences = [
            ex.confidence for ex in self.examples
            if ex.confidence is not None
        ]
        return dict(Counter(confidences))


# Ranking annotation (Plackett-Luce)
class RankingDataProcessor:
    """
    Process ranking annotations for RLHF.
    Converts rankings to pairwise preferences.
    """

    def __init__(self):
        self.rankings = []

    def add_ranking(
        self,
        prompt: str,
        responses: List[str],
        ranking: List[int]  # [0, 2, 1] means responses[0] > responses[2] > responses[1]
    ) -> None:
        """Add a ranking annotation."""
        self.rankings.append({
            "prompt": prompt,
            "responses": responses,
            "ranking": ranking
        })

    def to_pairwise(self) -> List[PreferenceExample]:
        """Convert rankings to pairwise preferences."""
        examples = []

        for r in self.rankings:
            prompt = r["prompt"]
            responses = r["responses"]
            ranking = r["ranking"]

            # Generate all pairs from ranking
            for i, rank_i in enumerate(ranking):
                for j, rank_j in enumerate(ranking):
                    if i >= j:
                        continue

                    # Lower rank number = better
                    if rank_i < rank_j:
                        chosen, rejected = responses[i], responses[j]
                    else:
                        chosen, rejected = responses[j], responses[i]

                    examples.append(PreferenceExample(
                        prompt=prompt,
                        chosen=chosen,
                        rejected=rejected
                    ))

        return examples
```

### Conversation Data Annotation

```python
"""
Multi-turn conversation annotation.
"""

from dataclasses import dataclass, field
from typing import List, Dict, Optional


@dataclass
class ConversationTurn:
    """Single turn in a conversation."""
    role: str  # "user", "assistant", "system"
    content: str
    turn_number: int

    # Annotations
    quality_score: Optional[int] = None
    issues: List[str] = field(default_factory=list)
    needs_improvement: bool = False
    improved_version: Optional[str] = None


@dataclass
class ConversationAnnotation:
    """Annotation for a full conversation."""
    conversation_id: str
    turns: List[ConversationTurn]

    # Overall annotations
    overall_quality: int = 0  # 1-5
    conversation_flow: int = 0  # 1-5
    goal_completion: bool = False
    issues_summary: str = ""

    # Turn-level issues
    problematic_turns: List[int] = field(default_factory=list)


class ConversationAnnotator:
    """
    Tools for annotating multi-turn conversations.
    """

    # Label Studio config for conversation annotation
    LABEL_STUDIO_CONFIG = """
    <View>
      <Header value="Conversation Annotation"/>

      <Paragraphs name="conversation" value="$turns"
                  nameKey="role" textKey="content"
                  layout="dialogue"/>

      <Header value="Rate each response (click on turn to annotate)"/>

      <Labels name="turn_quality" toName="conversation">
        <Label value="excellent" background="green"/>
        <Label value="good" background="lightgreen"/>
        <Label value="acceptable" background="yellow"/>
        <Label value="poor" background="orange"/>
        <Label value="unacceptable" background="red"/>
      </Labels>

      <Header value="Flag issues with responses"/>
      <Choices name="turn_issues" toName="conversation" choice="multiple">
        <Choice value="hallucination"/>
        <Choice value="off_topic"/>
        <Choice value="unsafe"/>
        <Choice value="unhelpful"/>
        <Choice value="inconsistent"/>
      </Choices>

      <Header value="Overall Conversation Quality"/>
      <Rating name="overall_quality" toName="conversation" maxRating="5"/>

      <Header value="Did the assistant successfully help the user?"/>
      <Choices name="goal_completion" toName="conversation" choice="single">
        <Choice value="yes"/>
        <Choice value="partially"/>
        <Choice value="no"/>
      </Choices>
    </View>
    """

    def create_annotation_batch(
        self,
        conversations: List[List[Dict]]
    ) -> List[Dict]:
        """
        Prepare conversations for annotation.

        Args:
            conversations: List of conversations, each a list of
                          {"role": str, "content": str}

        Returns:
            Formatted annotation batch
        """
        batch = []
        for i, conv in enumerate(conversations):
            batch.append({
                "conversation_id": f"conv_{i}",
                "turns": conv
            })
        return batch

    def process_annotations(
        self,
        annotations: List[Dict]
    ) -> List[ConversationAnnotation]:
        """Process raw annotations into structured format."""
        processed = []

        for ann in annotations:
            turns = []
            for i, turn in enumerate(ann["turns"]):
                turns.append(ConversationTurn(
                    role=turn["role"],
                    content=turn["content"],
                    turn_number=i,
                    quality_score=ann.get("turn_quality", {}).get(str(i)),
                    issues=ann.get("turn_issues", {}).get(str(i), [])
                ))

            processed.append(ConversationAnnotation(
                conversation_id=ann["conversation_id"],
                turns=turns,
                overall_quality=ann.get("overall_quality", 0),
                goal_completion=ann.get("goal_completion") == "yes"
            ))

        return processed
```

### Safety and Toxicity Labeling

```python
"""
Safety and toxicity annotation for LLM content.
"""

from dataclasses import dataclass
from typing import List, Dict, Optional
from enum import Enum


class HarmCategory(Enum):
    """Categories of potential harm."""
    VIOLENCE = "violence"
    HATE_SPEECH = "hate_speech"
    SEXUAL = "sexual_content"
    SELF_HARM = "self_harm"
    DANGEROUS = "dangerous_activities"
    ILLEGAL = "illegal_activities"
    PRIVACY = "privacy_violation"
    DECEPTION = "deception"
    HARASSMENT = "harassment"


class SeverityLevel(Enum):
    """Severity levels for harmful content."""
    NONE = 0
    LOW = 1
    MEDIUM = 2
    HIGH = 3
    CRITICAL = 4


@dataclass
class SafetyAnnotation:
    """Safety annotation for a piece of content."""
    content_id: str
    text: str

    # Overall safety
    is_safe: bool
    overall_severity: SeverityLevel

    # Category-level labels
    harm_categories: Dict[HarmCategory, SeverityLevel]

    # Context
    context_dependent: bool = False
    context_notes: str = ""

    # Refusal appropriateness (for assistant responses)
    should_refuse: bool = False
    refusal_appropriate: Optional[bool] = None

    # Annotator info
    annotator_id: str = ""
    confidence: str = "medium"


class SafetyAnnotationConfig:
    """Configuration for safety annotation task."""

    GUIDELINES = """
    # Safety Annotation Guidelines

    ## Task
    Evaluate content for potential harm across multiple categories.

    ## Severity Levels
    - **None (0)**: Content is completely safe
    - **Low (1)**: Minor concerns, generally acceptable
    - **Medium (2)**: Noticeable issues, may need filtering
    - **High (3)**: Significant harm potential, should be blocked
    - **Critical (4)**: Severe harm potential, immediate escalation

    ## Categories

    ### Violence
    - None: No violent content
    - Low: Mild conflict, sports violence, historical violence (educational)
    - Medium: Graphic descriptions without glorification
    - High: Glorification of violence, detailed instructions
    - Critical: Instructions for mass violence, terrorism

    ### Hate Speech
    - None: No discriminatory content
    - Low: Insensitive but not hateful
    - Medium: Stereotyping, microaggressions
    - High: Direct attacks on protected groups
    - Critical: Calls for discrimination/violence against groups

    [Continue for each category...]

    ## Context Matters
    - Educational contexts may reduce severity
    - Fictional contexts are generally more permissible
    - Professional contexts (medical, legal) have different standards
    - User intent affects interpretation

    ## When Uncertain
    - Err on the side of caution
    - Flag for expert review
    - Document your reasoning
    """

    LABEL_STUDIO_CONFIG = """
    <View>
      <Header value="Safety Annotation"/>

      <Text name="content" value="$text"/>

      <Header value="Is this content safe?"/>
      <Choices name="is_safe" toName="content" choice="single" required="true">
        <Choice value="safe"/>
        <Choice value="unsafe"/>
        <Choice value="borderline"/>
      </Choices>

      <Header value="If unsafe, select all applicable harm categories"/>
      <Choices name="harm_categories" toName="content" choice="multiple">
        <Choice value="violence"/>
        <Choice value="hate_speech"/>
        <Choice value="sexual_content"/>
        <Choice value="self_harm"/>
        <Choice value="dangerous_activities"/>
        <Choice value="illegal_activities"/>
        <Choice value="privacy_violation"/>
        <Choice value="deception"/>
        <Choice value="harassment"/>
      </Choices>

      <Header value="Overall severity"/>
      <Rating name="severity" toName="content" maxRating="5"/>

      <Header value="Is the harm context-dependent?"/>
      <Choices name="context_dependent" toName="content" choice="single">
        <Choice value="yes"/>
        <Choice value="no"/>
      </Choices>

      <Header value="If context-dependent, explain"/>
      <TextArea name="context_notes" toName="content" rows="2"/>

      <Header value="Confidence in assessment"/>
      <Choices name="confidence" toName="content" choice="single" required="true">
        <Choice value="high"/>
        <Choice value="medium"/>
        <Choice value="low"/>
      </Choices>
    </View>
    """
```

---

## Quality Assurance

### Golden Set Validation

```python
"""
Golden set validation for annotation quality.
"""

from dataclasses import dataclass
from typing import List, Dict, Set
import random


@dataclass
class GoldenSetItem:
    """Item in golden set with verified ground truth."""
    item_id: str
    content: Dict
    ground_truth: any
    explanation: str
    difficulty: str  # "easy", "medium", "hard"


class GoldenSetManager:
    """
    Manage golden set for annotator validation.
    """

    def __init__(
        self,
        golden_items: List[GoldenSetItem],
        insertion_rate: float = 0.1
    ):
        """
        Args:
            golden_items: List of golden set items with ground truth
            insertion_rate: Fraction of golden items to insert in batches
        """
        self.golden_items = {item.item_id: item for item in golden_items}
        self.insertion_rate = insertion_rate
        self.annotator_results: Dict[str, List[Dict]] = {}

    def create_batch_with_golden(
        self,
        regular_items: List[Dict],
        batch_size: int = 50
    ) -> List[Dict]:
        """
        Create annotation batch with inserted golden items.

        Args:
            regular_items: Regular items to annotate
            batch_size: Size of batch

        Returns:
            Mixed batch with golden items
        """
        n_golden = int(batch_size * self.insertion_rate)
        n_regular = batch_size - n_golden

        # Sample items
        regular_sample = random.sample(
            regular_items,
            min(n_regular, len(regular_items))
        )

        golden_sample = random.sample(
            list(self.golden_items.values()),
            min(n_golden, len(self.golden_items))
        )

        # Combine and shuffle
        batch = regular_sample + [
            {"item_id": g.item_id, **g.content, "_is_golden": True}
            for g in golden_sample
        ]
        random.shuffle(batch)

        return batch

    def evaluate_annotator(
        self,
        annotator_id: str,
        annotations: List[Dict]
    ) -> Dict:
        """
        Evaluate annotator performance on golden items.

        Args:
            annotator_id: ID of annotator
            annotations: Annotator's submissions

        Returns:
            Performance metrics
        """
        golden_annotations = [
            a for a in annotations
            if a.get("item_id") in self.golden_items
        ]

        if not golden_annotations:
            return {"error": "No golden items found in annotations"}

        correct = 0
        incorrect = []
        by_difficulty = {"easy": [], "medium": [], "hard": []}

        for ann in golden_annotations:
            item_id = ann["item_id"]
            golden = self.golden_items[item_id]

            is_correct = ann["label"] == golden.ground_truth
            if is_correct:
                correct += 1

            by_difficulty[golden.difficulty].append(is_correct)

            if not is_correct:
                incorrect.append({
                    "item_id": item_id,
                    "submitted": ann["label"],
                    "expected": golden.ground_truth,
                    "explanation": golden.explanation,
                    "difficulty": golden.difficulty
                })

        accuracy = correct / len(golden_annotations)

        result = {
            "annotator_id": annotator_id,
            "total_golden": len(golden_annotations),
            "correct": correct,
            "accuracy": accuracy,
            "passed": accuracy >= 0.85,  # 85% threshold
            "incorrect_items": incorrect,
            "accuracy_by_difficulty": {
                diff: sum(results) / len(results) if results else None
                for diff, results in by_difficulty.items()
            }
        }

        # Store for tracking
        if annotator_id not in self.annotator_results:
            self.annotator_results[annotator_id] = []
        self.annotator_results[annotator_id].append(result)

        return result

    def get_annotator_trend(self, annotator_id: str) -> Dict:
        """Get performance trend for annotator."""
        results = self.annotator_results.get(annotator_id, [])
        if not results:
            return {"error": "No results for annotator"}

        accuracies = [r["accuracy"] for r in results]
        return {
            "annotator_id": annotator_id,
            "num_evaluations": len(results),
            "average_accuracy": sum(accuracies) / len(accuracies),
            "trend": "improving" if len(accuracies) > 1 and accuracies[-1] > accuracies[0] else "stable",
            "latest_accuracy": accuracies[-1]
        }
```

### Systematic Bias Detection

```python
"""
Detect systematic biases in annotations.
"""

from typing import List, Dict
import numpy as np
from collections import Counter, defaultdict


class BiasDetector:
    """
    Detect systematic biases in annotation data.
    """

    def __init__(self):
        self.annotations = []
        self.annotator_labels = defaultdict(list)

    def add_annotation(
        self,
        annotator_id: str,
        item_id: str,
        label: any,
        metadata: Dict = None
    ):
        """Record an annotation."""
        self.annotations.append({
            "annotator_id": annotator_id,
            "item_id": item_id,
            "label": label,
            "metadata": metadata or {}
        })
        self.annotator_labels[annotator_id].append(label)

    def detect_label_bias(self) -> Dict[str, Dict]:
        """
        Detect if annotators have systematic label preferences.

        Returns:
            Per-annotator label distributions and bias metrics
        """
        results = {}

        # Get overall label distribution
        all_labels = [a["label"] for a in self.annotations]
        overall_dist = Counter(all_labels)
        total = len(all_labels)
        overall_props = {k: v/total for k, v in overall_dist.items()}

        for annotator_id, labels in self.annotator_labels.items():
            annotator_dist = Counter(labels)
            annotator_total = len(labels)
            annotator_props = {k: v/annotator_total for k, v in annotator_dist.items()}

            # Calculate deviation from overall
            deviations = {}
            for label in overall_props:
                expected = overall_props[label]
                actual = annotator_props.get(label, 0)
                deviations[label] = actual - expected

            # Chi-square test for significant deviation
            from scipy import stats
            observed = [annotator_dist.get(l, 0) for l in overall_props]
            expected = [overall_props[l] * annotator_total for l in overall_props]
            chi2, p_value = stats.chisquare(observed, expected)

            results[annotator_id] = {
                "label_distribution": dict(annotator_dist),
                "proportions": annotator_props,
                "deviations_from_overall": deviations,
                "chi_square": chi2,
                "p_value": p_value,
                "has_significant_bias": p_value < 0.05
            }

        return results

    def detect_length_bias(self) -> Dict[str, float]:
        """
        Detect if annotators prefer longer/shorter responses.
        Relevant for preference annotation tasks.
        """
        # Assumes metadata contains "chosen_length" and "rejected_length"
        annotator_length_diffs = defaultdict(list)

        for ann in self.annotations:
            if "chosen_length" in ann["metadata"]:
                diff = ann["metadata"]["chosen_length"] - \
                       ann["metadata"]["rejected_length"]
                annotator_length_diffs[ann["annotator_id"]].append(diff)

        results = {}
        for annotator_id, diffs in annotator_length_diffs.items():
            avg_diff = np.mean(diffs)
            # Positive = prefers longer, negative = prefers shorter
            results[annotator_id] = {
                "avg_length_difference": avg_diff,
                "bias_direction": "longer" if avg_diff > 50 else (
                    "shorter" if avg_diff < -50 else "neutral"
                ),
                "std_dev": np.std(diffs)
            }

        return results

    def detect_position_bias(self) -> Dict[str, Dict]:
        """
        Detect if annotators prefer options in certain positions.
        Relevant for A/B preference tasks.
        """
        # Assumes label is "A" or "B" (or 1/2)
        position_counts = defaultdict(lambda: {"A": 0, "B": 0})

        for ann in self.annotations:
            label = ann["label"]
            if label in ["A", "B", 1, 2]:
                position = "A" if label in ["A", 1] else "B"
                position_counts[ann["annotator_id"]][position] += 1

        results = {}
        for annotator_id, counts in position_counts.items():
            total = counts["A"] + counts["B"]
            if total == 0:
                continue

            a_ratio = counts["A"] / total
            # Should be ~0.5 if no bias
            bias_score = abs(a_ratio - 0.5) * 2  # 0 = no bias, 1 = complete bias

            results[annotator_id] = {
                "position_A_ratio": a_ratio,
                "position_B_ratio": 1 - a_ratio,
                "bias_score": bias_score,
                "has_position_bias": bias_score > 0.2  # >60/40 split
            }

        return results

    def generate_bias_report(self) -> Dict:
        """Generate comprehensive bias report."""
        return {
            "total_annotations": len(self.annotations),
            "num_annotators": len(self.annotator_labels),
            "label_bias": self.detect_label_bias(),
            "length_bias": self.detect_length_bias(),
            "position_bias": self.detect_position_bias()
        }
```

---

## Implementation Guide

### Complete Annotation Pipeline

```python
"""
Complete annotation pipeline implementation.
"""

import json
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class AnnotationPipeline:
    """
    End-to-end annotation pipeline.
    """

    def __init__(
        self,
        task_config: Dict,
        output_dir: str,
        platform: str = "label_studio"
    ):
        self.task_config = task_config
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.platform = platform

        # Initialize components
        self.workflow = AnnotationWorkflow(
            WorkflowConfig(**task_config.get("workflow", {}))
        )
        self.quality_tracker = AnnotationQualityTracker(
            task_config.get("task_id", "default")
        )
        self.golden_manager = None
        self.bias_detector = BiasDetector()

    def setup_golden_set(self, golden_items: List[Dict]) -> None:
        """Set up golden set for quality monitoring."""
        items = [
            GoldenSetItem(
                item_id=item["id"],
                content=item["content"],
                ground_truth=item["label"],
                explanation=item.get("explanation", ""),
                difficulty=item.get("difficulty", "medium")
            )
            for item in golden_items
        ]
        self.golden_manager = GoldenSetManager(items)

    def create_batch(
        self,
        items: List[Dict],
        batch_size: int = 50
    ) -> List[Dict]:
        """
        Create annotation batch.

        Args:
            items: Items to annotate
            batch_size: Batch size

        Returns:
            Batch ready for annotation
        """
        if self.golden_manager:
            return self.golden_manager.create_batch_with_golden(
                items, batch_size
            )
        return items[:batch_size]

    def submit_annotations(
        self,
        annotations: List[Dict],
        annotator_id: str
    ) -> Dict:
        """
        Submit batch of annotations.

        Returns:
            Processing results and quality metrics
        """
        results = {
            "submitted": len(annotations),
            "accepted": 0,
            "quality_check": None
        }

        for ann in annotations:
            # Record in workflow
            self.workflow.submit_annotation(
                item_id=ann["item_id"],
                annotator_id=annotator_id,
                label=ann["label"],
                metadata=ann.get("metadata")
            )

            # Record for quality tracking
            self.quality_tracker.add_annotation(
                annotator_id=annotator_id,
                item_id=ann["item_id"],
                label=ann["label"],
                metadata=ann.get("metadata")
            )

            # Record for bias detection
            self.bias_detector.add_annotation(
                annotator_id=annotator_id,
                item_id=ann["item_id"],
                label=ann["label"],
                metadata=ann.get("metadata")
            )

            results["accepted"] += 1

        # Run golden set evaluation if available
        if self.golden_manager:
            results["quality_check"] = self.golden_manager.evaluate_annotator(
                annotator_id, annotations
            )

        return results

    def export_results(self, format: str = "jsonl") -> str:
        """
        Export final annotated dataset.

        Args:
            format: Output format ("jsonl", "parquet", "hf")

        Returns:
            Path to exported file
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = self.output_dir / f"annotations_{timestamp}.{format}"

        final_data = []
        for item_id, label in self.workflow.final_labels.items():
            annotations = self.workflow.annotations.get(item_id, [])
            final_data.append({
                "item_id": item_id,
                "label": label,
                "num_annotators": len(annotations),
                "agreement": self._calculate_agreement(annotations)
            })

        if format == "jsonl":
            with open(output_file, "w") as f:
                for item in final_data:
                    f.write(json.dumps(item) + "\n")

        elif format == "parquet":
            import pandas as pd
            df = pd.DataFrame(final_data)
            df.to_parquet(output_file)

        logger.info(f"Exported {len(final_data)} items to {output_file}")
        return str(output_file)

    def _calculate_agreement(self, annotations: List[Dict]) -> float:
        """Calculate agreement for a set of annotations."""
        if len(annotations) < 2:
            return 1.0

        labels = [a["label"] for a in annotations]
        from collections import Counter
        counts = Counter(labels)
        max_count = max(counts.values())
        return max_count / len(labels)

    def generate_report(self) -> Dict:
        """Generate comprehensive annotation report."""
        return {
            "workflow_status": self.workflow.get_status(),
            "quality_report": self.quality_tracker.generate_quality_report(),
            "bias_report": self.bias_detector.generate_bias_report(),
            "timestamp": datetime.now().isoformat()
        }


# Usage example
def run_annotation_project():
    """Example of running a complete annotation project."""

    # Configuration
    config = {
        "task_id": "sentiment_classification",
        "task_type": "classification",
        "workflow": {
            "workflow_type": "consensus",
            "annotators_per_item": 3,
            "agreement_threshold": 0.67
        }
    }

    # Initialize pipeline
    pipeline = AnnotationPipeline(
        task_config=config,
        output_dir="./annotation_output"
    )

    # Set up golden set
    golden_items = [
        {
            "id": "gold_1",
            "content": {"text": "This product is amazing!"},
            "label": "positive",
            "explanation": "Clear positive sentiment with 'amazing'",
            "difficulty": "easy"
        },
        # ... more golden items
    ]
    pipeline.setup_golden_set(golden_items)

    # Process batches
    # ... annotation collection logic ...

    # Export final dataset
    output_path = pipeline.export_results(format="jsonl")

    # Generate report
    report = pipeline.generate_report()
    print(json.dumps(report, indent=2))

    return output_path
```

---

## Configuration Templates

### Task Configuration Templates

```yaml
# config/classification_task.yaml
task:
  id: "sentiment_classification"
  type: "classification"
  name: "Sentiment Classification"
  description: "Classify text sentiment as positive, negative, neutral, or mixed"

labels:
  - name: "positive"
    description: "Text expresses positive sentiment"
    examples:
      - "I love this product!"
      - "Best experience ever"

  - name: "negative"
    description: "Text expresses negative sentiment"
    examples:
      - "Terrible service"
      - "Would not recommend"

  - name: "neutral"
    description: "Text is neutral or factual"
    examples:
      - "The store opens at 9am"
      - "Product weighs 5 pounds"

  - name: "mixed"
    description: "Text contains both positive and negative"
    examples:
      - "Good quality but expensive"
      - "Fast shipping, wrong item"

workflow:
  type: "consensus"
  annotators_per_item: 3
  agreement_threshold: 0.67
  adjudication_enabled: true

quality:
  golden_set_rate: 0.1
  min_accuracy: 0.85
  min_agreement: 0.80

annotators:
  qualification_test_required: true
  training_required: true
  min_annotations_per_session: 20
  max_annotations_per_session: 200

---
# config/rlhf_preference_task.yaml
task:
  id: "response_preference"
  type: "preference"
  name: "Response Quality Preference"
  description: "Compare two AI responses and select the better one"

comparison:
  show_prompt: true
  randomize_order: true
  allow_ties: true

dimensions:
  - name: "helpfulness"
    description: "How well does the response address the user's need?"
    scale: [1, 2, 3, 4, 5]

  - name: "accuracy"
    description: "Is the information factually correct?"
    scale: [1, 2, 3, 4, 5]

  - name: "safety"
    description: "Does it avoid harmful content?"
    scale: [1, 2, 3, 4, 5]

confidence_levels:
  - "low"
  - "medium"
  - "high"

workflow:
  type: "dual"
  annotators_per_item: 2
  agreement_required: true
  expert_adjudication: true

quality:
  golden_set_rate: 0.15
  position_bias_check: true
  length_bias_check: true
```

---

## Troubleshooting

### Common Issues

#### Low Inter-Annotator Agreement

**Symptoms:**
- Cohen's Kappa < 0.6
- High disagreement rates

**Solutions:**

1. **Improve guidelines**
   - Add more examples for edge cases
   - Clarify label definitions
   - Create decision flowcharts

2. **Increase training**
   - Additional calibration sessions
   - Review disagreements as a group
   - Pair annotators with different styles

3. **Simplify task**
   - Reduce number of labels
   - Split complex task into subtasks
   - Use hierarchical labeling

```python
# Script to identify disagreement patterns
def analyze_disagreements(annotations: List[Dict]) -> Dict:
    """Identify patterns in annotator disagreements."""
    from collections import defaultdict

    disagreements = defaultdict(list)

    # Group by item
    by_item = defaultdict(list)
    for ann in annotations:
        by_item[ann["item_id"]].append(ann)

    # Find items with disagreement
    for item_id, anns in by_item.items():
        labels = [a["label"] for a in anns]
        if len(set(labels)) > 1:
            disagreements["items"].append({
                "item_id": item_id,
                "labels": labels,
                "annotators": [a["annotator_id"] for a in anns]
            })

    # Analyze confusion patterns
    from itertools import combinations
    label_confusions = defaultdict(int)

    for item in disagreements["items"]:
        for l1, l2 in combinations(item["labels"], 2):
            if l1 != l2:
                pair = tuple(sorted([l1, l2]))
                label_confusions[pair] += 1

    disagreements["common_confusions"] = dict(label_confusions)

    return dict(disagreements)
```

#### Annotator Fatigue/Drift

**Symptoms:**
- Quality decreases over session
- Speed increases but accuracy drops

**Solutions:**

```python
def detect_fatigue_patterns(
    annotations: List[Dict],
    session_break_minutes: int = 30
) -> Dict:
    """Detect patterns indicating annotator fatigue."""
    from datetime import datetime, timedelta

    # Sort by timestamp
    sorted_anns = sorted(annotations, key=lambda x: x.get("timestamp", ""))

    sessions = []
    current_session = []

    for ann in sorted_anns:
        if ann.get("timestamp"):
            ts = datetime.fromisoformat(ann["timestamp"])

            if current_session:
                last_ts = datetime.fromisoformat(
                    current_session[-1]["timestamp"]
                )
                if ts - last_ts > timedelta(minutes=session_break_minutes):
                    sessions.append(current_session)
                    current_session = []

            current_session.append(ann)

    if current_session:
        sessions.append(current_session)

    # Analyze quality by position in session
    position_quality = defaultdict(list)
    for session in sessions:
        for i, ann in enumerate(session):
            # Assume metadata has "is_correct" from golden set
            if ann.get("metadata", {}).get("is_correct") is not None:
                quartile = i // (len(session) // 4 + 1)
                position_quality[quartile].append(
                    ann["metadata"]["is_correct"]
                )

    return {
        "num_sessions": len(sessions),
        "avg_session_length": sum(len(s) for s in sessions) / len(sessions),
        "quality_by_quartile": {
            q: sum(v) / len(v) if v else None
            for q, v in position_quality.items()
        }
    }
```

---

## Appendices

### Appendix A: Annotation Guideline Templates

Complete templates provided in the Annotation Task Design section.

### Appendix B: Inter-Annotator Agreement Calculation Scripts

Complete implementations provided in the Annotator Management section.

### Appendix C: Annotator Training Curriculum

See RLHF_TRAINING_CURRICULUM in the Annotator Management section.

### Appendix D: Cost Estimation Calculator

```python
def estimate_annotation_cost(
    num_items: int,
    task_type: str,
    annotators_per_item: int = 1,
    rate_per_item: float = None,
    avg_time_per_item_minutes: float = None,
    hourly_rate: float = 20.0
) -> Dict:
    """
    Estimate annotation project cost.

    Args:
        num_items: Number of items to annotate
        task_type: Type of task
        annotators_per_item: Number of annotators needed per item
        rate_per_item: Fixed rate per item (if using per-item pricing)
        avg_time_per_item_minutes: Average time per item
        hourly_rate: Hourly rate for annotators

    Returns:
        Cost breakdown
    """
    # Default rates by task type
    default_rates = {
        "classification": 0.05,
        "extraction": 0.15,
        "preference": 0.30,
        "generation": 1.50,
        "safety": 0.25
    }

    default_times = {
        "classification": 0.5,  # 30 seconds
        "extraction": 2.0,  # 2 minutes
        "preference": 3.0,  # 3 minutes
        "generation": 10.0,  # 10 minutes
        "safety": 2.0  # 2 minutes
    }

    rate = rate_per_item or default_rates.get(task_type, 0.10)
    time_per_item = avg_time_per_item_minutes or default_times.get(task_type, 2.0)

    total_annotations = num_items * annotators_per_item
    total_hours = (total_annotations * time_per_item) / 60

    # Cost calculations
    per_item_cost = total_annotations * rate
    hourly_cost = total_hours * hourly_rate

    return {
        "num_items": num_items,
        "annotators_per_item": annotators_per_item,
        "total_annotations": total_annotations,
        "estimated_hours": round(total_hours, 1),
        "cost_per_item_pricing": round(per_item_cost, 2),
        "cost_hourly_pricing": round(hourly_cost, 2),
        "recommended_budget": round(max(per_item_cost, hourly_cost) * 1.2, 2),
        "notes": "Budget includes 20% buffer for rework and quality issues"
    }
```

---

## Glossary

| Term | Definition |
|------|------------|
| **Adjudication** | Process of resolving disagreements between annotators |
| **Cohen's Kappa** | Statistical measure of inter-annotator agreement for two raters |
| **DPO** | Direct Preference Optimization - training method using preferences |
| **Fleiss' Kappa** | Agreement measure for multiple raters |
| **Golden Set** | Items with verified correct labels used to monitor quality |
| **IAA** | Inter-Annotator Agreement |
| **Krippendorff's Alpha** | Flexible agreement metric for any number of raters |
| **Preference Data** | Annotations comparing outputs (A vs B choices) |
| **RLHF** | Reinforcement Learning from Human Feedback |
| **Span Annotation** | Marking specific text ranges with labels |

---

## References

### Research Papers
1. Landis, J.R. & Koch, G.G. (1977). "The Measurement of Observer Agreement for Categorical Data"
2. Ouyang, L., et al. (2022). "Training language models to follow instructions with human feedback"
3. Casper, S., et al. (2023). "Open Problems and Fundamental Limitations of RLHF"
4. Stiennon, N., et al. (2020). "Learning to summarize from human feedback"

### Platforms
- [Label Studio](https://labelstud.io/) - Open source annotation platform
- [Argilla](https://argilla.io/) - NLP-focused annotation
- [Scale AI](https://scale.com/) - Enterprise annotation services
- [Labelbox](https://labelbox.com/) - Multi-purpose annotation platform
- [Surge AI](https://www.surgehq.ai/) - Quality-focused annotation

### External Resources
- [HuggingFace Annotation Guides](https://huggingface.co/docs/datasets/dataset_card)
- [Anthropic Constitutional AI](https://www.anthropic.com/research/constitutional-ai)
- [OpenAI RLHF Research](https://openai.com/research/learning-from-human-preferences)

---

*Document Version: 1.0 | Last Updated: December 2025 | Next Review: March 2026*

---

> **Navigation**
> [← 1.2 Data Cleaning](1.2_data_cleaning_preprocessing.md) | **[Index](../README.md#15-repository-structure)** | [1.4 Data Versioning →](1.4_data_versioning_lineage.md)
