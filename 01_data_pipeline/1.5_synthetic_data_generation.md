# Document 1.5: Synthetic Data Generation Guide

## Document Information
- **Version:** 1.0
- **Last Updated:** December 2025
- **Owner:** ML Engineering / Research Team
- **Category:** Data Pipeline & Preparation

> **Navigation** | [← 1.4 Data Versioning](1.4_data_versioning_lineage.md) | [1.6 Data Quality →](1.6_data_quality_assurance.md)
>
> | | |
> |---|---|
> | **Prerequisites** | [1.1 Data Collection](1.1_data_collection_sourcing.md), [2.2 Model Architecture](../02_model_training/2.2_model_architecture_selection.md) |
> | **Related** | [1.3 Data Labeling](1.3_data_labeling_annotation.md) &#124; [3.1 Supervised Fine-Tuning](../03_fine_tuning/3.1_supervised_fine_tuning.md) |
> | **Next** | [1.6 Data Quality Assurance](1.6_data_quality_assurance.md) |

---

## Executive Summary

Synthetic data generation has become a cornerstone of modern LLM development, with leading models like Nemotron-4, Phi, and Llama relying heavily on synthetically generated training data. This guide covers techniques for generating high-quality synthetic data for pre-training, instruction tuning, and preference alignment. When done correctly, synthetic data can reduce reliance on expensive human annotation, fill gaps in real data distributions, and accelerate model development.

**Key Outcomes:**
- Generate diverse, high-quality instruction-following data
- Implement knowledge distillation from teacher models
- Avoid common pitfalls like model collapse and contamination
- Scale synthetic generation to billions of tokens

---

## Prerequisites

### Required Knowledge
- Transformer architecture fundamentals
- Prompt engineering techniques
- Distributed computing basics
- Quality metrics for NLG

### Infrastructure Requirements
- GPU compute (H100/A100 recommended for generation at scale)
- API access to teacher models (optional)
- Object storage for generated data
- Compute orchestration (Kubernetes, Ray, or similar)

### Tool Installation
```bash
# Core generation libraries
pip install transformers accelerate vllm
pip install openai anthropic  # For API-based generation

# Quality filtering
pip install sentence-transformers
pip install nltk rouge-score

# Distributed generation
pip install ray[default]

# NVIDIA NeMo for production pipelines
pip install nemo-toolkit[nlp]
```

---

## 1. Synthetic Data Use Cases

### 1.1 Use Case Taxonomy

```
┌─────────────────────────────────────────────────────────────┐
│              Synthetic Data Use Cases                        │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  PRE-TRAINING                                                │
│  ┌───────────────────────────────────────────────────────┐  │
│  │ • Source Rephrasing (Nemotron-CC, WRAP)               │  │
│  │ • Knowledge Synthesis (Cosmopedia, textbooks)         │  │
│  │ • Code Generation (from specs/docs)                   │  │
│  │ • Multilingual Expansion                               │  │
│  └───────────────────────────────────────────────────────┘  │
│                                                              │
│  INSTRUCTION TUNING                                          │
│  ┌───────────────────────────────────────────────────────┐  │
│  │ • Self-Instruct (instruction generation)              │  │
│  │ • Evol-Instruct (instruction evolution)               │  │
│  │ • Distillation (teacher → student)                    │  │
│  │ • Persona-based generation                            │  │
│  └───────────────────────────────────────────────────────┘  │
│                                                              │
│  ALIGNMENT                                                   │
│  ┌───────────────────────────────────────────────────────┐  │
│  │ • Preference pairs (chosen/rejected)                  │  │
│  │ • Constitutional AI (principle-based)                 │  │
│  │ • Red-teaming scenarios                               │  │
│  │ • Safety refusals                                      │  │
│  └───────────────────────────────────────────────────────┘  │
│                                                              │
│  EVALUATION                                                  │
│  ┌───────────────────────────────────────────────────────┐  │
│  │ • Benchmark generation                                 │  │
│  │ • Edge case coverage                                   │  │
│  │ • Adversarial examples                                 │  │
│  │ • Domain-specific test sets                           │  │
│  └───────────────────────────────────────────────────────┘  │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### 1.2 When to Use Synthetic Data

| Scenario | Synthetic Data Benefit |
|----------|----------------------|
| **Limited labeled data** | Augment small datasets 10-100x |
| **Privacy constraints** | Train without real PII |
| **Rare edge cases** | Generate specific failure modes |
| **Domain adaptation** | Fill gaps in domain coverage |
| **Rapid iteration** | Generate data faster than annotation |
| **Cost optimization** | Reduce human annotation costs |

### 1.3 Real-World Examples

**Nemotron-4 340B (NVIDIA):**
- 98% of alignment data is synthetic
- Only ~20K human-annotated examples
- Teacher model generates SFT and preference data

**Cosmopedia (HuggingFace):**
- 25 billion tokens of synthetic textbooks/blogs
- Generated using Mixtral-8x7B
- 10K+ GPU hours of generation

**Phi-1.5 (Microsoft):**
- "Textbooks are all you need"
- High-quality synthetic educational content
- Outperformed larger models on benchmarks

---

## 2. Generation Methods

### 2.1 Self-Instruct

Self-Instruct generates instruction-following data by prompting an LLM to create tasks.

```python
from typing import List, Dict
import json
import random
from openai import OpenAI

class SelfInstructGenerator:
    """Generate instruction data using Self-Instruct method"""

    def __init__(self, model: str = "gpt-4"):
        self.client = OpenAI()
        self.model = model
        self.seed_tasks = self._load_seed_tasks()

    def _load_seed_tasks(self) -> List[Dict]:
        """Load human-written seed tasks"""
        return [
            {
                "instruction": "Write a poem about autumn leaves.",
                "input": "",
                "output": "Golden leaves dance down,\nCrisp air whispers change is here,\nNature's last hurrah."
            },
            {
                "instruction": "Explain the concept of recursion in programming.",
                "input": "",
                "output": "Recursion is when a function calls itself to solve a problem..."
            },
            {
                "instruction": "Translate the following English text to French.",
                "input": "Hello, how are you today?",
                "output": "Bonjour, comment allez-vous aujourd'hui?"
            },
            # Add 100+ diverse seed tasks for best results
        ]

    def generate_instruction(self, num_examples: int = 3) -> Dict:
        """Generate a new instruction based on seed examples"""

        # Sample diverse examples
        examples = random.sample(self.seed_tasks, min(num_examples, len(self.seed_tasks)))

        prompt = """Generate a new, unique instruction for an AI assistant.
The instruction should be:
1. Clear and specific
2. Diverse from the examples shown
3. Something a real user might ask

Here are some examples of instructions:
"""
        for i, ex in enumerate(examples, 1):
            prompt += f"\nExample {i}:\nInstruction: {ex['instruction']}\n"
            if ex.get('input'):
                prompt += f"Input: {ex['input']}\n"

        prompt += """
Now generate a new, different instruction. Output in JSON format:
{
    "instruction": "<the instruction>",
    "input": "<optional input context, or empty string>",
    "category": "<one of: writing, math, coding, reasoning, creative, factual>"
}
"""
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.9,
            max_tokens=500
        )

        return json.loads(response.choices[0].message.content)

    def generate_response(self, instruction: str, input_text: str = "") -> str:
        """Generate response for an instruction"""

        if input_text:
            prompt = f"Instruction: {instruction}\n\nInput: {input_text}\n\nResponse:"
        else:
            prompt = f"Instruction: {instruction}\n\nResponse:"

        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=2048
        )

        return response.choices[0].message.content

    def generate_batch(self, count: int = 100) -> List[Dict]:
        """Generate a batch of instruction-response pairs"""

        generated = []
        seen_instructions = set()

        while len(generated) < count:
            try:
                # Generate instruction
                new_task = self.generate_instruction()

                # Check for duplicates (fuzzy)
                instruction = new_task['instruction'].lower().strip()
                if any(self._similar(instruction, seen) for seen in seen_instructions):
                    continue

                seen_instructions.add(instruction)

                # Generate response
                response = self.generate_response(
                    new_task['instruction'],
                    new_task.get('input', '')
                )

                new_task['output'] = response
                generated.append(new_task)

                if len(generated) % 10 == 0:
                    print(f"Generated {len(generated)}/{count} examples")

            except Exception as e:
                print(f"Error: {e}")
                continue

        return generated

    def _similar(self, a: str, b: str, threshold: float = 0.8) -> bool:
        """Check if two instructions are too similar"""
        # Simple word overlap similarity
        words_a = set(a.split())
        words_b = set(b.split())

        if not words_a or not words_b:
            return False

        overlap = len(words_a & words_b)
        similarity = overlap / max(len(words_a), len(words_b))

        return similarity > threshold

# Usage
generator = SelfInstructGenerator(model="gpt-4")
data = generator.generate_batch(count=1000)

# Save as JSONL
with open("self_instruct_data.jsonl", "w") as f:
    for item in data:
        f.write(json.dumps(item) + "\n")
```

### 2.2 Evol-Instruct

Evol-Instruct evolves simple instructions into more complex ones.

```python
from enum import Enum
from typing import List, Dict, Optional
import random

class EvolutionType(Enum):
    ADD_CONSTRAINTS = "add_constraints"
    DEEPEN = "deepen"
    CONCRETIZE = "concretize"
    INCREASE_REASONING = "increase_reasoning"
    COMPLICATE_INPUT = "complicate_input"

class EvolInstructGenerator:
    """Generate evolved instructions using WizardLM method"""

    EVOLUTION_PROMPTS = {
        EvolutionType.ADD_CONSTRAINTS: """
Rewrite the following instruction to add more constraints or requirements.
The new instruction should be more specific and challenging.

Original instruction: {instruction}

Add 1-2 specific constraints (e.g., word limits, format requirements,
specific elements to include). Output ONLY the new instruction:
""",

        EvolutionType.DEEPEN: """
Rewrite the following instruction to require deeper understanding or knowledge.
Make it require more expertise to answer correctly.

Original instruction: {instruction}

Increase the depth by asking for more detailed explanations, edge cases,
or advanced concepts. Output ONLY the new instruction:
""",

        EvolutionType.CONCRETIZE: """
Rewrite the following instruction with more concrete, specific details.
Replace general concepts with specific examples or scenarios.

Original instruction: {instruction}

Add specific names, numbers, scenarios, or contexts. Output ONLY the new instruction:
""",

        EvolutionType.INCREASE_REASONING: """
Rewrite the following instruction to require more complex reasoning steps.
The answer should require multi-step thinking or analysis.

Original instruction: {instruction}

Add requirements for comparison, analysis, or step-by-step reasoning.
Output ONLY the new instruction:
""",

        EvolutionType.COMPLICATE_INPUT: """
Rewrite the following instruction to have a more complex input.
Add context, data, or background information that must be processed.

Original instruction: {instruction}

Create a more complex scenario with additional input data.
Output ONLY the new instruction:
"""
    }

    def __init__(self, model: str = "gpt-4"):
        self.client = OpenAI()
        self.model = model

    def evolve_instruction(
        self,
        instruction: str,
        evolution_type: Optional[EvolutionType] = None
    ) -> str:
        """Evolve a single instruction"""

        if evolution_type is None:
            evolution_type = random.choice(list(EvolutionType))

        prompt = self.EVOLUTION_PROMPTS[evolution_type].format(instruction=instruction)

        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=500
        )

        return response.choices[0].message.content.strip()

    def evolve_multiple_rounds(
        self,
        instruction: str,
        rounds: int = 3
    ) -> List[Dict]:
        """Evolve instruction through multiple rounds"""

        history = [{
            "round": 0,
            "instruction": instruction,
            "evolution_type": None
        }]

        current = instruction

        for i in range(1, rounds + 1):
            evolution_type = random.choice(list(EvolutionType))

            try:
                evolved = self.evolve_instruction(current, evolution_type)

                # Validate evolution (not too similar, not too long)
                if self._is_valid_evolution(current, evolved):
                    history.append({
                        "round": i,
                        "instruction": evolved,
                        "evolution_type": evolution_type.value
                    })
                    current = evolved
                else:
                    # Try different evolution type
                    continue

            except Exception as e:
                print(f"Evolution failed: {e}")
                continue

        return history

    def _is_valid_evolution(
        self,
        original: str,
        evolved: str,
        min_change_ratio: float = 0.2,
        max_length_ratio: float = 3.0
    ) -> bool:
        """Validate that evolution is meaningful"""

        # Check length constraints
        if len(evolved) > len(original) * max_length_ratio:
            return False

        if len(evolved) < 20:  # Too short
            return False

        # Check for meaningful change
        original_words = set(original.lower().split())
        evolved_words = set(evolved.lower().split())

        new_words = evolved_words - original_words
        change_ratio = len(new_words) / max(len(original_words), 1)

        return change_ratio >= min_change_ratio

    def generate_evolved_dataset(
        self,
        seed_instructions: List[str],
        evolutions_per_seed: int = 3,
        rounds_per_evolution: int = 2
    ) -> List[Dict]:
        """Generate evolved dataset from seed instructions"""

        all_evolved = []

        for seed in seed_instructions:
            for _ in range(evolutions_per_seed):
                history = self.evolve_multiple_rounds(seed, rounds=rounds_per_evolution)

                # Take the final evolved instruction
                if len(history) > 1:
                    final = history[-1]
                    final["seed_instruction"] = seed
                    final["evolution_history"] = history
                    all_evolved.append(final)

        return all_evolved

# Usage example
seed_instructions = [
    "Write a function to sort a list.",
    "Explain machine learning.",
    "Write a poem about nature.",
    "Solve a math problem.",
]

evol_generator = EvolInstructGenerator()
evolved_data = evol_generator.generate_evolved_dataset(
    seed_instructions,
    evolutions_per_seed=5,
    rounds_per_evolution=3
)
```

### 2.3 Knowledge Distillation

Distillation generates training data by having a teacher model create examples for a student.

```python
from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple
import json

@dataclass
class DistillationConfig:
    teacher_model: str = "gpt-4"
    student_model: str = "llama-7b"  # Target model
    temperature: float = 0.7
    num_samples_per_prompt: int = 1
    max_output_tokens: int = 2048

class KnowledgeDistillationPipeline:
    """Generate training data via knowledge distillation"""

    def __init__(self, config: DistillationConfig):
        self.config = config
        self.client = OpenAI()

    def generate_reasoning_traces(
        self,
        problems: List[str],
        domain: str = "general"
    ) -> List[Dict]:
        """Generate step-by-step reasoning traces from teacher"""

        results = []

        for problem in problems:
            prompt = f"""Solve the following problem step by step.
Show your complete reasoning process.

Problem: {problem}

Let's work through this carefully:
"""
            response = self.client.chat.completions.create(
                model=self.config.teacher_model,
                messages=[{"role": "user", "content": prompt}],
                temperature=self.config.temperature,
                max_tokens=self.config.max_output_tokens
            )

            result = {
                "problem": problem,
                "domain": domain,
                "reasoning_trace": response.choices[0].message.content,
                "teacher_model": self.config.teacher_model
            }
            results.append(result)

        return results

    def generate_qa_pairs(
        self,
        contexts: List[str],
        questions_per_context: int = 5
    ) -> List[Dict]:
        """Generate question-answer pairs from contexts"""

        results = []

        for context in contexts:
            # Generate questions
            question_prompt = f"""Given the following text, generate {questions_per_context}
diverse questions that can be answered using the text.
Include factual, inferential, and analytical questions.

Text: {context}

Generate questions in JSON format:
[{{"question": "...", "type": "factual|inferential|analytical"}}]
"""
            q_response = self.client.chat.completions.create(
                model=self.config.teacher_model,
                messages=[{"role": "user", "content": question_prompt}],
                temperature=0.8,
                max_tokens=1000
            )

            try:
                questions = json.loads(q_response.choices[0].message.content)
            except json.JSONDecodeError:
                continue

            # Generate answers
            for q in questions:
                answer_prompt = f"""Based on the following context, answer the question.
Provide a comprehensive answer.

Context: {context}

Question: {q['question']}

Answer:"""

                a_response = self.client.chat.completions.create(
                    model=self.config.teacher_model,
                    messages=[{"role": "user", "content": answer_prompt}],
                    temperature=0.7,
                    max_tokens=500
                )

                results.append({
                    "context": context,
                    "question": q['question'],
                    "question_type": q['type'],
                    "answer": a_response.choices[0].message.content,
                    "teacher_model": self.config.teacher_model
                })

        return results

    def generate_instruction_following(
        self,
        task_descriptions: List[str],
        examples_per_task: int = 10
    ) -> List[Dict]:
        """Generate instruction-following examples for each task type"""

        results = []

        for task in task_descriptions:
            prompt = f"""Generate {examples_per_task} diverse examples of the following task type.
Each example should include an instruction and a high-quality response.

Task type: {task}

Generate in JSON format:
[
    {{
        "instruction": "specific instruction for this task",
        "input": "optional input/context (can be empty)",
        "output": "high-quality response"
    }}
]

Make examples diverse in difficulty and specific content.
"""
            response = self.client.chat.completions.create(
                model=self.config.teacher_model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.9,
                max_tokens=4000
            )

            try:
                examples = json.loads(response.choices[0].message.content)
                for ex in examples:
                    ex['task_type'] = task
                    ex['teacher_model'] = self.config.teacher_model
                results.extend(examples)
            except json.JSONDecodeError:
                continue

        return results

    def generate_preference_pairs(
        self,
        prompts: List[str]
    ) -> List[Dict]:
        """Generate preference pairs (chosen/rejected) for RLHF"""

        results = []

        for prompt in prompts:
            # Generate good response
            good_response = self.client.chat.completions.create(
                model=self.config.teacher_model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,  # Lower temp for quality
                max_tokens=self.config.max_output_tokens
            )

            # Generate intentionally weaker response
            weak_prompt = f"""Respond to the following, but make your response:
- Less detailed than ideal
- Missing some nuance
- Slightly generic

User request: {prompt}

Response (make it adequate but not excellent):"""

            weak_response = self.client.chat.completions.create(
                model=self.config.teacher_model,
                messages=[{"role": "user", "content": weak_prompt}],
                temperature=0.8,
                max_tokens=self.config.max_output_tokens
            )

            results.append({
                "prompt": prompt,
                "chosen": good_response.choices[0].message.content,
                "rejected": weak_response.choices[0].message.content,
                "teacher_model": self.config.teacher_model
            })

        return results

# Usage
config = DistillationConfig(
    teacher_model="gpt-4",
    temperature=0.7
)

pipeline = KnowledgeDistillationPipeline(config)

# Generate different types of training data
reasoning_data = pipeline.generate_reasoning_traces([
    "Calculate the compound interest on $10,000 at 5% for 3 years.",
    "Explain why the sky appears blue.",
])

qa_data = pipeline.generate_qa_pairs([
    "The Python programming language was created by Guido van Rossum...",
])

preference_data = pipeline.generate_preference_pairs([
    "Explain quantum computing to a high school student.",
])
```

### 2.4 Source Rephrasing (Nemotron-CC / WRAP)

This method transforms web data into higher-quality formats.

```python
from typing import List, Dict
import re

class SourceRephraser:
    """Rephrase web content into higher-quality training data"""

    REPHRASE_STYLES = {
        "wikipedia": """Rewrite the following text in the style of a Wikipedia article.
Make it encyclopedic, neutral, well-structured, and informative.

Original text: {text}

Wikipedia-style version:""",

        "textbook": """Rewrite the following text as if it were from an educational textbook.
Include clear explanations, examples, and learning objectives.

Original text: {text}

Textbook-style version:""",

        "qa": """Convert the following text into a question-and-answer format.
Generate 3-5 Q&A pairs that cover the key information.

Original text: {text}

Q&A format:""",

        "summary": """Summarize the following text, capturing the key points concisely.
Then expand each point with additional context and explanation.

Original text: {text}

Expanded summary:""",

        "tutorial": """Rewrite the following as a step-by-step tutorial or how-to guide.
Include clear steps, tips, and potential pitfalls.

Original text: {text}

Tutorial version:"""
    }

    def __init__(self, model: str = "mixtral-8x7b"):
        self.model = model
        # Initialize local model or API client
        self._init_model()

    def _init_model(self):
        """Initialize the rephrasing model"""
        from vllm import LLM, SamplingParams

        self.llm = LLM(model=self.model, tensor_parallel_size=1)
        self.sampling_params = SamplingParams(
            temperature=0.7,
            max_tokens=2048,
            top_p=0.9
        )

    def rephrase(
        self,
        text: str,
        style: str = "textbook"
    ) -> str:
        """Rephrase text in specified style"""

        if style not in self.REPHRASE_STYLES:
            raise ValueError(f"Unknown style: {style}")

        prompt = self.REPHRASE_STYLES[style].format(text=text[:4000])  # Limit input

        outputs = self.llm.generate([prompt], self.sampling_params)
        return outputs[0].outputs[0].text

    def rephrase_batch(
        self,
        texts: List[str],
        styles: List[str] = None
    ) -> List[Dict]:
        """Rephrase batch of texts"""

        if styles is None:
            styles = list(self.REPHRASE_STYLES.keys())

        results = []

        # Generate all prompts
        prompts = []
        metadata = []

        for text in texts:
            # Select style (round-robin or random)
            style = styles[len(prompts) % len(styles)]
            prompt = self.REPHRASE_STYLES[style].format(text=text[:4000])
            prompts.append(prompt)
            metadata.append({"original": text, "style": style})

        # Batch generation
        outputs = self.llm.generate(prompts, self.sampling_params)

        for i, output in enumerate(outputs):
            results.append({
                "original": metadata[i]["original"],
                "style": metadata[i]["style"],
                "rephrased": output.outputs[0].text,
                "original_length": len(metadata[i]["original"]),
                "rephrased_length": len(output.outputs[0].text)
            })

        return results

    def process_web_crawl(
        self,
        documents: List[Dict],
        min_length: int = 500,
        max_length: int = 10000
    ) -> List[Dict]:
        """Process web crawl documents with quality filtering and rephrasing"""

        # Filter by length
        filtered = [
            doc for doc in documents
            if min_length <= len(doc.get('text', '')) <= max_length
        ]

        # Clean text
        cleaned = []
        for doc in filtered:
            text = self._clean_text(doc['text'])
            if len(text) >= min_length:
                cleaned.append({**doc, 'text': text})

        # Rephrase
        texts = [doc['text'] for doc in cleaned]
        rephrased = self.rephrase_batch(texts)

        # Combine
        results = []
        for doc, reph in zip(cleaned, rephrased):
            results.append({
                "url": doc.get('url'),
                "original_text": doc['text'],
                "rephrased_text": reph['rephrased'],
                "style": reph['style'],
                "metadata": doc.get('metadata', {})
            })

        return results

    def _clean_text(self, text: str) -> str:
        """Basic text cleaning"""
        # Remove excessive whitespace
        text = re.sub(r'\s+', ' ', text)
        # Remove common boilerplate
        text = re.sub(r'Cookie Policy|Privacy Policy|Terms of Service', '', text)
        # Remove navigation elements
        text = re.sub(r'(Home|About|Contact|Menu)\s*\|', '', text)
        return text.strip()

# Usage with Ray for distributed processing
import ray
from ray.data import from_items

@ray.remote
class DistributedRephraser:
    def __init__(self, model: str):
        self.rephraser = SourceRephraser(model)

    def process_batch(self, texts: List[str], style: str) -> List[Dict]:
        return self.rephraser.rephrase_batch(texts, [style])

def process_at_scale(
    documents: List[Dict],
    num_workers: int = 8,
    model: str = "mixtral-8x7b"
):
    ray.init()

    # Create workers
    workers = [DistributedRephraser.remote(model) for _ in range(num_workers)]

    # Distribute work
    batch_size = len(documents) // num_workers
    futures = []

    for i, worker in enumerate(workers):
        start = i * batch_size
        end = start + batch_size if i < num_workers - 1 else len(documents)
        batch_docs = documents[start:end]
        texts = [doc['text'] for doc in batch_docs]
        futures.append(worker.process_batch.remote(texts, "textbook"))

    # Collect results
    results = ray.get(futures)
    return [item for batch in results for item in batch]
```

### 2.5 Cosmopedia-Style Synthetic Textbooks

```python
from typing import List, Dict, Optional
import json
import random

class SyntheticTextbookGenerator:
    """Generate synthetic educational content like Cosmopedia"""

    TOPICS = {
        "science": [
            "physics", "chemistry", "biology", "astronomy", "geology",
            "environmental science", "materials science"
        ],
        "mathematics": [
            "algebra", "calculus", "statistics", "geometry", "number theory",
            "discrete mathematics", "linear algebra"
        ],
        "technology": [
            "computer science", "artificial intelligence", "cybersecurity",
            "networking", "databases", "software engineering"
        ],
        "humanities": [
            "history", "philosophy", "literature", "linguistics",
            "psychology", "sociology", "economics"
        ]
    }

    AUDIENCE_LEVELS = [
        "elementary school student",
        "middle school student",
        "high school student",
        "undergraduate student",
        "graduate student",
        "professional"
    ]

    CONTENT_TYPES = [
        "textbook_chapter",
        "lecture_notes",
        "tutorial",
        "explainer_article",
        "study_guide",
        "concept_review"
    ]

    def __init__(self, model: str = "gpt-4"):
        self.client = OpenAI()
        self.model = model

    def generate_topic_seeds(
        self,
        domain: str,
        count: int = 100
    ) -> List[str]:
        """Generate diverse topic seeds for a domain"""

        prompt = f"""Generate {count} specific topics within the domain of {domain}.
Each topic should be:
1. Specific enough to write a focused educational piece about
2. Not too broad or too narrow
3. Diverse in difficulty and subfield

Output as a JSON list of strings:
["topic 1", "topic 2", ...]
"""
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.9,
            max_tokens=4000
        )

        return json.loads(response.choices[0].message.content)

    def generate_textbook_chapter(
        self,
        topic: str,
        audience: str,
        word_count: int = 2000
    ) -> Dict:
        """Generate a synthetic textbook chapter"""

        prompt = f"""Write a textbook chapter about "{topic}" for a {audience}.

The chapter should include:
1. Introduction with learning objectives
2. Main content with clear explanations
3. Examples and illustrations (describe them)
4. Key concepts and definitions
5. Summary and review questions

Target length: approximately {word_count} words.
Write in an educational, clear style appropriate for the audience level.

Chapter:"""

        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=4000
        )

        content = response.choices[0].message.content

        return {
            "type": "textbook_chapter",
            "topic": topic,
            "audience": audience,
            "content": content,
            "word_count": len(content.split()),
            "model": self.model
        }

    def generate_blog_post(
        self,
        topic: str,
        style: str = "educational"
    ) -> Dict:
        """Generate an educational blog post"""

        styles = {
            "educational": "informative and educational, suitable for curious learners",
            "conversational": "friendly and conversational, as if explaining to a friend",
            "technical": "precise and technical, with accurate terminology",
            "storytelling": "narrative-driven, using stories and analogies"
        }

        prompt = f"""Write a blog post about "{topic}".

Style: {styles.get(style, styles['educational'])}

The blog post should:
1. Have an engaging introduction
2. Explain the topic clearly with examples
3. Include practical insights or applications
4. End with a thoughtful conclusion

Write naturally as a knowledgeable blogger would.

Blog Post:"""

        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.8,
            max_tokens=2000
        )

        content = response.choices[0].message.content

        return {
            "type": "blog_post",
            "topic": topic,
            "style": style,
            "content": content,
            "word_count": len(content.split()),
            "model": self.model
        }

    def generate_story_with_concepts(
        self,
        concepts: List[str],
        setting: str = "modern"
    ) -> Dict:
        """Generate a story that teaches concepts"""

        prompt = f"""Write a short story that naturally incorporates and teaches the following concepts:
{', '.join(concepts)}

Setting: {setting}

Requirements:
1. Create engaging characters and plot
2. Weave the concepts naturally into the narrative
3. Make the learning feel organic, not forced
4. End with a resolution that reinforces understanding

Short Story:"""

        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.9,
            max_tokens=3000
        )

        content = response.choices[0].message.content

        return {
            "type": "educational_story",
            "concepts": concepts,
            "setting": setting,
            "content": content,
            "word_count": len(content.split()),
            "model": self.model
        }

    def generate_dataset(
        self,
        target_tokens: int = 1_000_000,
        distribution: Dict[str, float] = None
    ) -> List[Dict]:
        """Generate a diverse synthetic dataset"""

        if distribution is None:
            distribution = {
                "textbook_chapter": 0.4,
                "blog_post": 0.3,
                "educational_story": 0.15,
                "lecture_notes": 0.15
            }

        results = []
        current_tokens = 0
        avg_tokens_per_doc = 800  # Rough estimate

        while current_tokens < target_tokens:
            # Select content type
            content_type = random.choices(
                list(distribution.keys()),
                weights=list(distribution.values())
            )[0]

            # Select random topic
            domain = random.choice(list(self.TOPICS.keys()))
            topic = random.choice(self.TOPICS[domain])

            try:
                if content_type == "textbook_chapter":
                    audience = random.choice(self.AUDIENCE_LEVELS)
                    doc = self.generate_textbook_chapter(topic, audience)
                elif content_type == "blog_post":
                    doc = self.generate_blog_post(topic)
                elif content_type == "educational_story":
                    concepts = random.sample(self.TOPICS[domain], min(3, len(self.TOPICS[domain])))
                    doc = self.generate_story_with_concepts(concepts)
                else:
                    continue

                results.append(doc)
                current_tokens += doc['word_count'] * 1.3  # Rough token estimate

                print(f"Generated {len(results)} docs, ~{current_tokens:,.0f} tokens")

            except Exception as e:
                print(f"Error: {e}")
                continue

        return results

# Usage
generator = SyntheticTextbookGenerator(model="gpt-4")
dataset = generator.generate_dataset(target_tokens=1_000_000)
```

---

## 3. Quality Control

### 3.1 Diversity Metrics

```python
from typing import List, Dict
import numpy as np
from collections import Counter
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

class DiversityMetrics:
    """Measure diversity of generated data"""

    def __init__(self):
        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')

    def ngram_diversity(
        self,
        texts: List[str],
        n: int = 3
    ) -> Dict[str, float]:
        """Calculate n-gram diversity metrics"""

        all_ngrams = []
        ngrams_per_doc = []

        for text in texts:
            words = text.lower().split()
            doc_ngrams = set()

            for i in range(len(words) - n + 1):
                ngram = tuple(words[i:i+n])
                all_ngrams.append(ngram)
                doc_ngrams.add(ngram)

            ngrams_per_doc.append(doc_ngrams)

        # Overall diversity
        unique_ngrams = len(set(all_ngrams))
        total_ngrams = len(all_ngrams)

        # Type-token ratio
        ttr = unique_ngrams / total_ngrams if total_ngrams > 0 else 0

        # Self-BLEU (lower is more diverse)
        self_bleu = self._calculate_self_bleu(texts, n)

        # Distinct-n
        distinct_n = unique_ngrams / total_ngrams if total_ngrams > 0 else 0

        return {
            f"distinct_{n}": distinct_n,
            "type_token_ratio": ttr,
            "self_bleu": self_bleu,
            "unique_ngrams": unique_ngrams,
            "total_ngrams": total_ngrams
        }

    def embedding_diversity(
        self,
        texts: List[str]
    ) -> Dict[str, float]:
        """Calculate embedding-based diversity"""

        embeddings = self.encoder.encode(texts)

        # Pairwise cosine similarities
        similarities = cosine_similarity(embeddings)

        # Average pairwise distance
        n = len(texts)
        distances = []
        for i in range(n):
            for j in range(i + 1, n):
                distances.append(1 - similarities[i, j])

        # Compute metrics
        avg_distance = np.mean(distances)
        min_distance = np.min(distances)
        std_distance = np.std(distances)

        # Cluster analysis (simple k-means)
        from sklearn.cluster import KMeans
        n_clusters = min(10, len(texts) // 10)
        if n_clusters > 1:
            kmeans = KMeans(n_clusters=n_clusters, random_state=42)
            labels = kmeans.fit_predict(embeddings)
            cluster_sizes = Counter(labels)
            entropy = -sum(
                (c / n) * np.log(c / n)
                for c in cluster_sizes.values()
            )
        else:
            entropy = 0

        return {
            "avg_pairwise_distance": float(avg_distance),
            "min_pairwise_distance": float(min_distance),
            "distance_std": float(std_distance),
            "cluster_entropy": float(entropy)
        }

    def _calculate_self_bleu(
        self,
        texts: List[str],
        n: int = 3
    ) -> float:
        """Calculate self-BLEU score"""
        from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

        smooth = SmoothingFunction().method1
        scores = []

        for i, text in enumerate(texts):
            # Compare against all other texts
            references = [t.split() for j, t in enumerate(texts) if j != i]
            hypothesis = text.split()

            if references:
                score = sentence_bleu(
                    references,
                    hypothesis,
                    weights=(1/n,) * n,
                    smoothing_function=smooth
                )
                scores.append(score)

        return np.mean(scores) if scores else 0.0

    def topic_diversity(
        self,
        texts: List[str],
        n_topics: int = 10
    ) -> Dict[str, float]:
        """Measure topic diversity using LDA"""
        from sklearn.feature_extraction.text import CountVectorizer
        from sklearn.decomposition import LatentDirichletAllocation

        vectorizer = CountVectorizer(max_features=5000, stop_words='english')
        X = vectorizer.fit_transform(texts)

        lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)
        topic_distributions = lda.fit_transform(X)

        # Topic coverage (how many topics are represented)
        dominant_topics = topic_distributions.argmax(axis=1)
        topics_used = len(set(dominant_topics))

        # Topic distribution entropy
        avg_distribution = topic_distributions.mean(axis=0)
        entropy = -sum(p * np.log(p + 1e-10) for p in avg_distribution)

        return {
            "topics_covered": topics_used,
            "topic_coverage_ratio": topics_used / n_topics,
            "topic_entropy": float(entropy),
            "max_topic_entropy": float(np.log(n_topics))
        }

# Usage
metrics = DiversityMetrics()

texts = ["Generated text 1...", "Generated text 2...", ...]

ngram_scores = metrics.ngram_diversity(texts, n=3)
embedding_scores = metrics.embedding_diversity(texts)
topic_scores = metrics.topic_diversity(texts)

print("Diversity Report:")
print(f"  Distinct-3: {ngram_scores['distinct_3']:.4f}")
print(f"  Self-BLEU: {ngram_scores['self_bleu']:.4f}")
print(f"  Avg Embedding Distance: {embedding_scores['avg_pairwise_distance']:.4f}")
print(f"  Topic Coverage: {topic_scores['topic_coverage_ratio']:.2%}")
```

### 3.2 Factuality Verification

```python
from typing import List, Dict, Tuple
import json

class FactualityVerifier:
    """Verify factuality of generated content"""

    def __init__(self, model: str = "gpt-4"):
        self.client = OpenAI()
        self.model = model

    def extract_claims(self, text: str) -> List[str]:
        """Extract factual claims from text"""

        prompt = f"""Extract all factual claims from the following text.
A factual claim is a statement that can be verified as true or false.
Do not include opinions, subjective statements, or hypotheticals.

Text: {text}

Output claims as a JSON list:
["claim 1", "claim 2", ...]
"""
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )

        return json.loads(response.choices[0].message.content)

    def verify_claim(
        self,
        claim: str,
        context: str = None
    ) -> Dict:
        """Verify a single claim"""

        prompt = f"""Evaluate the factual accuracy of the following claim.

Claim: {claim}
{f'Context: {context}' if context else ''}

Analyze and respond with:
1. "verdict": one of "supported", "refuted", "unverifiable"
2. "confidence": 0.0 to 1.0
3. "explanation": brief explanation of your verdict
4. "source_needed": whether this needs external verification

Output as JSON.
"""
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )

        return json.loads(response.choices[0].message.content)

    def batch_verify(
        self,
        texts: List[str],
        strict: bool = True
    ) -> List[Dict]:
        """Verify factuality of multiple texts"""

        results = []

        for text in texts:
            claims = self.extract_claims(text)

            verified_claims = []
            for claim in claims:
                verification = self.verify_claim(claim)
                verified_claims.append({
                    "claim": claim,
                    **verification
                })

            # Calculate overall score
            if verified_claims:
                supported = sum(1 for c in verified_claims if c['verdict'] == 'supported')
                refuted = sum(1 for c in verified_claims if c['verdict'] == 'refuted')
                unverifiable = sum(1 for c in verified_claims if c['verdict'] == 'unverifiable')

                if strict:
                    # Penalize refuted and unverifiable
                    score = supported / len(verified_claims)
                else:
                    # Only penalize refuted
                    score = 1 - (refuted / len(verified_claims))
            else:
                score = 1.0

            results.append({
                "text": text[:200] + "...",
                "num_claims": len(claims),
                "claims": verified_claims,
                "factuality_score": score,
                "pass": score >= 0.8  # Threshold
            })

        return results

    def cross_reference(
        self,
        generated_text: str,
        source_text: str
    ) -> Dict:
        """Cross-reference generated content against source"""

        prompt = f"""Compare the generated text against the source and identify:
1. Claims that are accurately reflected from the source
2. Claims that contradict the source
3. Claims that are not present in the source (hallucinations)

Source: {source_text}

Generated: {generated_text}

Output as JSON:
{{
    "accurate": ["claim 1", ...],
    "contradictions": ["claim and explanation", ...],
    "hallucinations": ["claim that's not in source", ...],
    "fidelity_score": 0.0 to 1.0
}}
"""
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )

        return json.loads(response.choices[0].message.content)
```

### 3.3 Contamination Prevention

```python
from typing import List, Set, Tuple
import hashlib
from collections import defaultdict
import numpy as np

class ContaminationChecker:
    """Check for train/test contamination"""

    def __init__(self):
        self.benchmark_hashes: Dict[str, Set[str]] = defaultdict(set)
        self.benchmark_ngrams: Dict[str, Set[Tuple]] = defaultdict(set)

    def register_benchmark(
        self,
        benchmark_name: str,
        texts: List[str],
        n: int = 13
    ):
        """Register benchmark texts for contamination checking"""

        for text in texts:
            # Store exact hash
            text_hash = hashlib.sha256(text.encode()).hexdigest()
            self.benchmark_hashes[benchmark_name].add(text_hash)

            # Store n-grams
            words = text.lower().split()
            for i in range(len(words) - n + 1):
                ngram = tuple(words[i:i+n])
                self.benchmark_ngrams[benchmark_name].add(ngram)

    def check_exact_match(
        self,
        text: str,
        benchmark_name: str = None
    ) -> Dict[str, bool]:
        """Check for exact matches against benchmarks"""

        text_hash = hashlib.sha256(text.encode()).hexdigest()

        results = {}

        if benchmark_name:
            benchmarks = {benchmark_name: self.benchmark_hashes[benchmark_name]}
        else:
            benchmarks = self.benchmark_hashes

        for name, hashes in benchmarks.items():
            results[name] = text_hash in hashes

        return results

    def check_ngram_overlap(
        self,
        text: str,
        n: int = 13,
        threshold: float = 0.8,
        benchmark_name: str = None
    ) -> Dict[str, Dict]:
        """Check for n-gram overlap with benchmarks"""

        words = text.lower().split()
        text_ngrams = set()

        for i in range(len(words) - n + 1):
            ngram = tuple(words[i:i+n])
            text_ngrams.add(ngram)

        results = {}

        if benchmark_name:
            benchmarks = {benchmark_name: self.benchmark_ngrams[benchmark_name]}
        else:
            benchmarks = self.benchmark_ngrams

        for name, benchmark_ngrams in benchmarks.items():
            overlap = text_ngrams & benchmark_ngrams
            overlap_ratio = len(overlap) / len(text_ngrams) if text_ngrams else 0

            results[name] = {
                "overlap_count": len(overlap),
                "overlap_ratio": overlap_ratio,
                "contaminated": overlap_ratio >= threshold
            }

        return results

    def filter_contaminated(
        self,
        generated_data: List[Dict],
        text_key: str = "text",
        n: int = 13,
        threshold: float = 0.2
    ) -> Tuple[List[Dict], List[Dict]]:
        """Filter out contaminated examples"""

        clean = []
        contaminated = []

        for item in generated_data:
            text = item[text_key]
            ngram_results = self.check_ngram_overlap(text, n=n, threshold=threshold)

            is_contaminated = any(r['contaminated'] for r in ngram_results.values())

            if is_contaminated:
                item['contamination_results'] = ngram_results
                contaminated.append(item)
            else:
                clean.append(item)

        return clean, contaminated

    def calculate_dataset_contamination(
        self,
        dataset: List[str]
    ) -> Dict:
        """Calculate overall contamination statistics"""

        total = len(dataset)
        contaminated_counts = defaultdict(int)

        for text in dataset:
            ngram_results = self.check_ngram_overlap(text)

            for name, result in ngram_results.items():
                if result['contaminated']:
                    contaminated_counts[name] += 1

        stats = {
            "total_examples": total,
            "contamination_by_benchmark": {},
            "overall_clean_ratio": 1.0
        }

        max_contaminated = 0
        for name, count in contaminated_counts.items():
            ratio = count / total
            stats["contamination_by_benchmark"][name] = {
                "count": count,
                "ratio": ratio
            }
            max_contaminated = max(max_contaminated, count)

        stats["overall_clean_ratio"] = 1 - (max_contaminated / total)

        return stats

# Usage
checker = ContaminationChecker()

# Register common benchmarks
checker.register_benchmark("mmlu", mmlu_questions)
checker.register_benchmark("hellaswag", hellaswag_texts)
checker.register_benchmark("truthfulqa", truthfulqa_questions)

# Check generated data
clean_data, contaminated_data = checker.filter_contaminated(
    generated_data,
    text_key="content",
    threshold=0.2
)

print(f"Clean: {len(clean_data)}, Contaminated: {len(contaminated_data)}")
```

### 3.4 Human Validation Sampling

```python
from typing import List, Dict
import random
import json
from dataclasses import dataclass

@dataclass
class ValidationSample:
    id: str
    text: str
    source_type: str
    generation_method: str
    metadata: Dict

class HumanValidationPipeline:
    """Create samples for human quality validation"""

    def __init__(self):
        self.validation_criteria = [
            "fluency",
            "coherence",
            "factuality",
            "helpfulness",
            "harmlessness",
            "relevance"
        ]

    def stratified_sample(
        self,
        dataset: List[Dict],
        sample_size: int,
        stratify_by: str
    ) -> List[Dict]:
        """Create stratified sample for validation"""

        # Group by stratification key
        groups = defaultdict(list)
        for item in dataset:
            key = item.get(stratify_by, "unknown")
            groups[key].append(item)

        # Sample proportionally from each group
        total = len(dataset)
        samples = []

        for key, items in groups.items():
            group_ratio = len(items) / total
            group_sample_size = max(1, int(sample_size * group_ratio))
            group_samples = random.sample(items, min(group_sample_size, len(items)))
            samples.extend(group_samples)

        # Shuffle final sample
        random.shuffle(samples)

        return samples[:sample_size]

    def create_validation_task(
        self,
        samples: List[Dict],
        task_type: str = "rating"
    ) -> List[Dict]:
        """Create validation tasks for annotators"""

        tasks = []

        for i, sample in enumerate(samples):
            task = {
                "task_id": f"val_{i:05d}",
                "sample_id": sample.get("id", i),
                "text": sample.get("text", sample.get("content")),
                "task_type": task_type,
                "metadata": {
                    "source_type": sample.get("type"),
                    "generation_method": sample.get("method"),
                }
            }

            if task_type == "rating":
                task["criteria"] = {
                    criterion: {
                        "description": self._get_criterion_description(criterion),
                        "scale": "1-5",
                        "rating": None  # To be filled by annotator
                    }
                    for criterion in self.validation_criteria
                }
            elif task_type == "comparison":
                task["comparison"] = {
                    "options": ["A is better", "B is better", "Tie"],
                    "choice": None
                }
            elif task_type == "binary":
                task["questions"] = {
                    "is_fluent": None,
                    "is_factual": None,
                    "is_helpful": None,
                    "is_safe": None
                }

            tasks.append(task)

        return tasks

    def _get_criterion_description(self, criterion: str) -> str:
        descriptions = {
            "fluency": "Is the text grammatically correct and reads naturally?",
            "coherence": "Does the text maintain logical flow and consistency?",
            "factuality": "Are the factual claims in the text accurate?",
            "helpfulness": "Would this text be helpful for the intended purpose?",
            "harmlessness": "Is the text free from harmful or toxic content?",
            "relevance": "Is the text relevant to the topic/prompt?"
        }
        return descriptions.get(criterion, criterion)

    def aggregate_ratings(
        self,
        completed_tasks: List[Dict]
    ) -> Dict:
        """Aggregate human validation results"""

        criterion_ratings = defaultdict(list)
        per_sample_scores = {}

        for task in completed_tasks:
            sample_id = task["sample_id"]
            sample_scores = []

            for criterion, data in task.get("criteria", {}).items():
                if data.get("rating") is not None:
                    rating = float(data["rating"])
                    criterion_ratings[criterion].append(rating)
                    sample_scores.append(rating)

            if sample_scores:
                per_sample_scores[sample_id] = {
                    "mean": np.mean(sample_scores),
                    "min": min(sample_scores)
                }

        # Calculate aggregate statistics
        stats = {
            "overall_mean": np.mean([s["mean"] for s in per_sample_scores.values()]),
            "by_criterion": {},
            "pass_rate": sum(1 for s in per_sample_scores.values() if s["min"] >= 3) / len(per_sample_scores)
        }

        for criterion, ratings in criterion_ratings.items():
            stats["by_criterion"][criterion] = {
                "mean": np.mean(ratings),
                "std": np.std(ratings),
                "min": min(ratings),
                "max": max(ratings)
            }

        return stats

    def identify_issues(
        self,
        completed_tasks: List[Dict],
        threshold: float = 2.5
    ) -> List[Dict]:
        """Identify samples with quality issues"""

        issues = []

        for task in completed_tasks:
            low_scoring_criteria = []

            for criterion, data in task.get("criteria", {}).items():
                if data.get("rating") is not None and float(data["rating"]) < threshold:
                    low_scoring_criteria.append({
                        "criterion": criterion,
                        "rating": data["rating"]
                    })

            if low_scoring_criteria:
                issues.append({
                    "sample_id": task["sample_id"],
                    "text": task["text"][:200] + "...",
                    "issues": low_scoring_criteria
                })

        return issues
```

---

## 4. Synthetic Data Pipelines

### 4.1 Scalable Generation Pipeline

```python
import ray
from ray import serve
from typing import List, Dict, Generator
import json
from dataclasses import dataclass
from enum import Enum

class GenerationType(Enum):
    SELF_INSTRUCT = "self_instruct"
    EVOL_INSTRUCT = "evol_instruct"
    DISTILLATION = "distillation"
    REPHRASING = "rephrasing"

@dataclass
class GenerationConfig:
    generation_type: GenerationType
    model: str
    temperature: float = 0.7
    max_tokens: int = 2048
    batch_size: int = 100
    quality_threshold: float = 0.8

@ray.remote(num_gpus=1)
class GenerationWorker:
    """GPU worker for synthetic data generation"""

    def __init__(self, config: GenerationConfig):
        self.config = config
        self._init_model()
        self._init_quality_checker()

    def _init_model(self):
        if "gpt" in self.config.model or "claude" in self.config.model:
            # API-based model
            self.client = OpenAI()
            self.use_api = True
        else:
            # Local model with vLLM
            from vllm import LLM, SamplingParams
            self.llm = LLM(model=self.config.model)
            self.sampling_params = SamplingParams(
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens
            )
            self.use_api = False

    def _init_quality_checker(self):
        self.diversity_checker = DiversityMetrics()
        self.factuality_checker = FactualityVerifier()

    def generate_batch(self, prompts: List[str]) -> List[Dict]:
        """Generate a batch of synthetic examples"""

        if self.use_api:
            results = []
            for prompt in prompts:
                response = self.client.chat.completions.create(
                    model=self.config.model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=self.config.temperature,
                    max_tokens=self.config.max_tokens
                )
                results.append({
                    "prompt": prompt,
                    "response": response.choices[0].message.content
                })
        else:
            outputs = self.llm.generate(prompts, self.sampling_params)
            results = [
                {"prompt": p, "response": o.outputs[0].text}
                for p, o in zip(prompts, outputs)
            ]

        return results

    def generate_and_filter(
        self,
        prompts: List[str],
        filter_fn = None
    ) -> List[Dict]:
        """Generate with quality filtering"""

        raw_results = self.generate_batch(prompts)

        if filter_fn is None:
            return raw_results

        filtered = []
        for result in raw_results:
            if filter_fn(result):
                filtered.append(result)

        return filtered

class SyntheticDataPipeline:
    """Orchestrate large-scale synthetic data generation"""

    def __init__(
        self,
        config: GenerationConfig,
        num_workers: int = 4
    ):
        self.config = config
        self.num_workers = num_workers
        ray.init(ignore_reinit_error=True)

    def generate(
        self,
        seed_data: List[str],
        target_count: int
    ) -> Generator[Dict, None, None]:
        """Generate synthetic data from seed data"""

        # Create workers
        workers = [
            GenerationWorker.remote(self.config)
            for _ in range(self.num_workers)
        ]

        # Prepare prompts based on generation type
        prompt_generator = self._get_prompt_generator()

        generated_count = 0
        batch_idx = 0

        while generated_count < target_count:
            # Generate prompts for this batch
            batch_prompts = []
            for _ in range(self.config.batch_size):
                prompt = prompt_generator(seed_data)
                batch_prompts.append(prompt)

            # Distribute to workers
            prompts_per_worker = len(batch_prompts) // self.num_workers
            futures = []

            for i, worker in enumerate(workers):
                start = i * prompts_per_worker
                end = start + prompts_per_worker if i < self.num_workers - 1 else len(batch_prompts)
                worker_prompts = batch_prompts[start:end]
                futures.append(worker.generate_batch.remote(worker_prompts))

            # Collect results
            all_results = ray.get(futures)

            for worker_results in all_results:
                for result in worker_results:
                    yield result
                    generated_count += 1

                    if generated_count >= target_count:
                        return

            batch_idx += 1
            print(f"Batch {batch_idx}: Generated {generated_count}/{target_count}")

    def _get_prompt_generator(self):
        """Get prompt generator based on generation type"""

        if self.config.generation_type == GenerationType.SELF_INSTRUCT:
            return self._self_instruct_prompt
        elif self.config.generation_type == GenerationType.EVOL_INSTRUCT:
            return self._evol_instruct_prompt
        elif self.config.generation_type == GenerationType.DISTILLATION:
            return self._distillation_prompt
        else:
            return self._rephrasing_prompt

    def _self_instruct_prompt(self, seed_data: List[str]) -> str:
        examples = random.sample(seed_data, min(3, len(seed_data)))
        return f"""Generate a new instruction-following example.

Examples:
{chr(10).join(examples)}

Generate a new, different instruction and response:"""

    def _evol_instruct_prompt(self, seed_data: List[str]) -> str:
        seed = random.choice(seed_data)
        evolution = random.choice(["add constraints", "increase complexity", "add reasoning"])
        return f"""Evolve this instruction to {evolution}:

Original: {seed}

Evolved instruction:"""

    def _distillation_prompt(self, seed_data: List[str]) -> str:
        topic = random.choice(seed_data)
        return f"""Create a detailed educational explanation about: {topic}

Include examples, step-by-step reasoning, and practical applications.

Explanation:"""

    def _rephrasing_prompt(self, seed_data: List[str]) -> str:
        text = random.choice(seed_data)
        style = random.choice(["textbook", "tutorial", "blog post"])
        return f"""Rewrite this text as a {style}:

Original: {text[:2000]}

Rewritten:"""

    def run_full_pipeline(
        self,
        seed_data: List[str],
        target_count: int,
        output_path: str
    ) -> Dict:
        """Run full generation pipeline with quality checks"""

        results = []
        quality_scores = []

        # Generate data
        for item in self.generate(seed_data, target_count * 1.5):  # Over-generate
            results.append(item)

        # Quality filtering
        print("Running quality checks...")
        checker = DiversityMetrics()

        texts = [r['response'] for r in results]
        diversity_scores = checker.embedding_diversity(texts)

        # Filter low-quality
        filtered = []
        for result in results:
            # Add your quality filters here
            if len(result['response']) > 100:  # Basic length filter
                filtered.append(result)

        # Sample to target count
        final = random.sample(filtered, min(target_count, len(filtered)))

        # Save
        with open(output_path, 'w') as f:
            for item in final:
                f.write(json.dumps(item) + '\n')

        return {
            "generated": len(results),
            "filtered": len(filtered),
            "final": len(final),
            "diversity_scores": diversity_scores,
            "output_path": output_path
        }

# Usage
config = GenerationConfig(
    generation_type=GenerationType.SELF_INSTRUCT,
    model="gpt-4",
    temperature=0.8,
    batch_size=100
)

pipeline = SyntheticDataPipeline(config, num_workers=4)

stats = pipeline.run_full_pipeline(
    seed_data=seed_instructions,
    target_count=10000,
    output_path="synthetic_data.jsonl"
)
```

### 4.2 Cost Optimization

```python
from typing import Dict, List
from dataclasses import dataclass
import json

@dataclass
class ModelCosts:
    """Cost per 1M tokens"""
    input_cost: float
    output_cost: float
    speed_tokens_per_sec: float

MODEL_COSTS = {
    "gpt-4": ModelCosts(30.0, 60.0, 20),
    "gpt-4-turbo": ModelCosts(10.0, 30.0, 40),
    "gpt-3.5-turbo": ModelCosts(0.5, 1.5, 100),
    "claude-3-opus": ModelCosts(15.0, 75.0, 15),
    "claude-3-sonnet": ModelCosts(3.0, 15.0, 50),
    "claude-3-haiku": ModelCosts(0.25, 1.25, 100),
    "mixtral-8x7b": ModelCosts(0.0, 0.0, 80),  # Self-hosted
    "llama-70b": ModelCosts(0.0, 0.0, 40),  # Self-hosted
}

class CostOptimizer:
    """Optimize synthetic data generation costs"""

    def __init__(self, budget: float):
        self.budget = budget

    def estimate_cost(
        self,
        model: str,
        num_examples: int,
        avg_input_tokens: int = 500,
        avg_output_tokens: int = 1000
    ) -> Dict:
        """Estimate generation cost"""

        costs = MODEL_COSTS.get(model)
        if not costs:
            raise ValueError(f"Unknown model: {model}")

        total_input_tokens = num_examples * avg_input_tokens
        total_output_tokens = num_examples * avg_output_tokens

        input_cost = (total_input_tokens / 1_000_000) * costs.input_cost
        output_cost = (total_output_tokens / 1_000_000) * costs.output_cost
        total_cost = input_cost + output_cost

        total_tokens = total_input_tokens + total_output_tokens
        time_hours = (total_tokens / costs.speed_tokens_per_sec) / 3600

        return {
            "model": model,
            "num_examples": num_examples,
            "total_tokens": total_tokens,
            "input_cost": input_cost,
            "output_cost": output_cost,
            "total_cost": total_cost,
            "estimated_hours": time_hours
        }

    def recommend_strategy(
        self,
        target_examples: int,
        quality_requirement: str = "high"
    ) -> Dict:
        """Recommend cost-effective generation strategy"""

        strategies = []

        if quality_requirement == "high":
            # High quality: Use best model for initial, cheaper for expansion
            strategies.append({
                "name": "Hybrid High-Quality",
                "stages": [
                    {"model": "gpt-4", "examples": target_examples // 10, "purpose": "seed generation"},
                    {"model": "gpt-3.5-turbo", "examples": target_examples * 9 // 10, "purpose": "expansion"},
                ],
                "total_cost": self.estimate_cost("gpt-4", target_examples // 10)["total_cost"] +
                              self.estimate_cost("gpt-3.5-turbo", target_examples * 9 // 10)["total_cost"]
            })

        elif quality_requirement == "medium":
            # Medium quality: Use mid-tier models
            strategies.append({
                "name": "Mid-tier Generation",
                "stages": [
                    {"model": "claude-3-sonnet", "examples": target_examples, "purpose": "full generation"},
                ],
                "total_cost": self.estimate_cost("claude-3-sonnet", target_examples)["total_cost"]
            })

        else:
            # Budget: Use cheapest options with quality filtering
            strategies.append({
                "name": "Budget with Filtering",
                "stages": [
                    {"model": "gpt-3.5-turbo", "examples": target_examples * 2, "purpose": "over-generate"},
                    {"model": "gpt-4", "examples": target_examples // 20, "purpose": "quality filtering"},
                ],
                "total_cost": self.estimate_cost("gpt-3.5-turbo", target_examples * 2)["total_cost"] +
                              self.estimate_cost("gpt-4", target_examples // 20)["total_cost"]
            })

        # Self-hosted option
        strategies.append({
            "name": "Self-Hosted (if available)",
            "stages": [
                {"model": "mixtral-8x7b", "examples": target_examples, "purpose": "full generation"},
            ],
            "total_cost": 0,
            "gpu_hours": self.estimate_cost("mixtral-8x7b", target_examples)["estimated_hours"] * 8  # 8 GPUs
        })

        # Filter by budget
        affordable = [s for s in strategies if s["total_cost"] <= self.budget]

        return {
            "target_examples": target_examples,
            "budget": self.budget,
            "quality_requirement": quality_requirement,
            "recommended": affordable[0] if affordable else None,
            "all_options": strategies
        }

    def optimize_batch_sizes(
        self,
        model: str,
        target_examples: int
    ) -> Dict:
        """Optimize batch sizes for throughput"""

        costs = MODEL_COSTS.get(model)

        # Larger batches = better throughput but higher latency per example
        batch_sizes = [1, 10, 50, 100, 500]

        recommendations = []
        for batch_size in batch_sizes:
            num_batches = (target_examples + batch_size - 1) // batch_size

            # Estimate overhead per batch (API calls, etc.)
            overhead_per_batch = 0.5  # seconds

            tokens_per_batch = batch_size * 1500  # avg tokens per example
            generation_time = tokens_per_batch / costs.speed_tokens_per_sec
            total_time = num_batches * (generation_time + overhead_per_batch)

            recommendations.append({
                "batch_size": batch_size,
                "num_batches": num_batches,
                "estimated_time_hours": total_time / 3600,
                "throughput_examples_per_hour": target_examples / (total_time / 3600)
            })

        best = max(recommendations, key=lambda x: x["throughput_examples_per_hour"])

        return {
            "model": model,
            "target_examples": target_examples,
            "recommended_batch_size": best["batch_size"],
            "estimated_throughput": best["throughput_examples_per_hour"],
            "all_options": recommendations
        }

# Usage
optimizer = CostOptimizer(budget=1000)  # $1000 budget

strategy = optimizer.recommend_strategy(
    target_examples=100000,
    quality_requirement="high"
)
print(json.dumps(strategy, indent=2))
```

---

## 5. Ethical Considerations

### 5.1 Disclosure Requirements

```python
@dataclass
class SyntheticDataDisclosure:
    """Metadata for synthetic data disclosure"""

    generation_date: str
    generation_method: str
    teacher_model: str
    target_use: str
    quality_metrics: Dict[str, float]
    contamination_checked: bool
    human_validation_sample_size: int
    known_limitations: List[str]

    def to_datasheet(self) -> str:
        return f"""
# Synthetic Data Disclosure

## Generation Information
- **Date:** {self.generation_date}
- **Method:** {self.generation_method}
- **Teacher Model:** {self.teacher_model}
- **Intended Use:** {self.target_use}

## Quality Metrics
{chr(10).join(f'- {k}: {v:.4f}' for k, v in self.quality_metrics.items())}

## Validation
- Contamination Checked: {'Yes' if self.contamination_checked else 'No'}
- Human Validation Sample Size: {self.human_validation_sample_size}

## Known Limitations
{chr(10).join(f'- {l}' for l in self.known_limitations)}

## Usage Guidelines
This dataset contains synthetically generated data. Users should:
1. Not treat this data as ground truth for factual claims
2. Be aware of potential biases from the teacher model
3. Validate critical use cases with human review
4. Cite the generation method and teacher model when publishing
"""
```

### 5.2 Bias Detection

```python
class BiasDetector:
    """Detect biases in synthetic data"""

    def __init__(self):
        self.sensitive_attributes = [
            "gender", "race", "religion", "nationality",
            "age", "disability", "sexual_orientation"
        ]

    def detect_representation_bias(
        self,
        texts: List[str],
        groups: Dict[str, List[str]]
    ) -> Dict:
        """Detect representation imbalances"""

        counts = {attr: {g: 0 for g in group_list}
                  for attr, group_list in groups.items()}

        for text in texts:
            text_lower = text.lower()
            for attr, group_list in groups.items():
                for group in group_list:
                    if group.lower() in text_lower:
                        counts[attr][group] += 1

        # Calculate imbalance ratios
        imbalances = {}
        for attr, group_counts in counts.items():
            if sum(group_counts.values()) > 0:
                max_count = max(group_counts.values())
                min_count = min(group_counts.values())
                imbalance_ratio = max_count / (min_count + 1)
                imbalances[attr] = {
                    "counts": group_counts,
                    "imbalance_ratio": imbalance_ratio,
                    "is_balanced": imbalance_ratio < 2.0
                }

        return imbalances

    def detect_stereotype_patterns(
        self,
        texts: List[str]
    ) -> List[Dict]:
        """Detect potentially stereotypical patterns"""

        # Simplified stereotype detection
        patterns = [
            {"pattern": r"women are (naturally |inherently )", "category": "gender"},
            {"pattern": r"men are (naturally |inherently )", "category": "gender"},
            {"pattern": r"(all|most) \w+ people (are|have)", "category": "race"},
            {"pattern": r"typical \w+ behavior", "category": "general"},
        ]

        findings = []
        for text in texts:
            for pattern_info in patterns:
                matches = re.findall(pattern_info["pattern"], text, re.IGNORECASE)
                if matches:
                    findings.append({
                        "text_excerpt": text[:100],
                        "pattern": pattern_info["pattern"],
                        "category": pattern_info["category"],
                        "matches": matches
                    })

        return findings
```

### 5.3 Model Collapse Prevention

```python
class CollapseMonitor:
    """Monitor for signs of model collapse in synthetic data"""

    def __init__(self):
        self.generation_history = []

    def check_generation_diversity(
        self,
        current_generation: List[str],
        previous_generations: List[List[str]] = None
    ) -> Dict:
        """Check if diversity is declining across generations"""

        metrics = DiversityMetrics()

        current_diversity = metrics.embedding_diversity(current_generation)

        if previous_generations:
            diversity_trend = []
            for gen in previous_generations:
                div = metrics.embedding_diversity(gen)
                diversity_trend.append(div['avg_pairwise_distance'])

            diversity_trend.append(current_diversity['avg_pairwise_distance'])

            # Check for declining trend
            if len(diversity_trend) >= 3:
                trend_slope = np.polyfit(range(len(diversity_trend)), diversity_trend, 1)[0]
                is_declining = trend_slope < -0.01
            else:
                is_declining = False

            return {
                "current_diversity": current_diversity,
                "diversity_trend": diversity_trend,
                "trend_slope": trend_slope if len(diversity_trend) >= 3 else None,
                "is_declining": is_declining,
                "warning": "Model collapse risk detected!" if is_declining else None
            }

        return {"current_diversity": current_diversity}

    def recommend_mitigation(
        self,
        collapse_indicators: Dict
    ) -> List[str]:
        """Recommend mitigations for model collapse risk"""

        recommendations = []

        if collapse_indicators.get("is_declining"):
            recommendations.extend([
                "Increase temperature during generation",
                "Add more diverse seed examples",
                "Mix in real data samples",
                "Use different teacher models for variety",
                "Apply diversity-promoting sampling (nucleus, top-k)",
                "Reduce number of consecutive synthetic generations"
            ])

        return recommendations
```

---

## 6. Troubleshooting

### Common Issues

| Issue | Symptoms | Solution |
|-------|----------|----------|
| Low diversity | Generated texts are repetitive | Increase temperature, add diverse seeds |
| Factual errors | Hallucinated information | Add fact-checking, use retrieval augmentation |
| Format inconsistency | Outputs don't follow format | Add format examples, use structured generation |
| API rate limits | 429 errors | Implement backoff, use async batching |
| High costs | Budget exceeded | Switch to cheaper models for expansion |
| Model collapse | Declining quality over generations | Mix real data, limit synthetic chains |

### Debug Checklist

```bash
# 1. Verify model access
python -c "from openai import OpenAI; OpenAI().models.list()"

# 2. Check generation quality
python scripts/sample_generations.py --count 10 --output samples.json

# 3. Measure diversity
python scripts/diversity_check.py --input synthetic_data.jsonl

# 4. Check contamination
python scripts/contamination_check.py --input synthetic_data.jsonl --benchmarks mmlu,hellaswag

# 5. Validate factuality (sample)
python scripts/factuality_check.py --input synthetic_data.jsonl --sample 100
```

---

## Appendices

### Appendix A: Self-Instruct Seed Tasks Template

```json
[
    {
        "instruction": "Summarize the following article in 3 sentences.",
        "input": "[Article text here]",
        "output": "[Summary here]",
        "category": "summarization"
    },
    {
        "instruction": "Write a Python function that calculates the factorial of a number.",
        "input": "",
        "output": "def factorial(n):\n    if n <= 1:\n        return 1\n    return n * factorial(n - 1)",
        "category": "coding"
    },
    {
        "instruction": "Explain why the sky is blue to a 10-year-old.",
        "input": "",
        "output": "The sky looks blue because of how sunlight interacts with our atmosphere...",
        "category": "explanation"
    }
]
```

### Appendix B: Quality Thresholds

| Metric | Minimum | Target | Excellent |
|--------|---------|--------|-----------|
| Distinct-3 | 0.70 | 0.85 | 0.95 |
| Self-BLEU (lower better) | < 0.4 | < 0.25 | < 0.15 |
| Embedding Distance | 0.3 | 0.5 | 0.7 |
| Factuality Score | 0.7 | 0.85 | 0.95 |
| Human Rating (1-5) | 3.0 | 4.0 | 4.5 |

---

## Glossary

| Term | Definition |
|------|------------|
| **Distillation** | Transferring knowledge from a large teacher model to training data |
| **Evol-Instruct** | Method for evolving simple instructions into complex ones |
| **Model Collapse** | Quality degradation when training on too much synthetic data |
| **Self-Instruct** | Generating instruction-following data from seed examples |
| **Synthetic Data** | Artificially generated data, often using LLMs |
| **Teacher Model** | Larger model used to generate training data |
| **Student Model** | Smaller model being trained on synthetic data |

---

## References

- [NVIDIA Nemotron-4 340B](https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/)
- [Cosmopedia: Large-Scale Synthetic Data](https://huggingface.co/blog/cosmopedia)
- [Self-Instruct Paper](https://arxiv.org/abs/2212.10560)
- [Evol-Instruct (WizardLM)](https://arxiv.org/abs/2304.12244)
- [Synthetic Data for LLMs (Red Hat)](https://www.redhat.com/en/blog/synthetic-data-secret-ingredient-better-language-models)
- [BeyondWeb: Scaling Synthetic Data](https://www.datologyai.com/blog/beyondweb)
- [NVIDIA NeMo Synthetic Data Docs](https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/syntheticdata.html)
