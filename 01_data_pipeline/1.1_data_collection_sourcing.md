# Document 1.1: Data Collection & Sourcing Guide

**Document ID:** 1.1
**Version:** 1.0
**Last Updated:** December 2025
**Author:** LLM Platform Team
**Status:** Complete
**Priority:** P0

> **Navigation** | [← README](../README.md) | [1.2 Data Cleaning →](1.2_data_cleaning_preprocessing.md)
>
> | | |
> |---|---|
> | **Prerequisites** | None (start here for data fundamentals) |
> | **Related** | [1.2 Data Cleaning](1.2_data_cleaning_preprocessing.md) &#124; [1.3 Data Labeling](1.3_data_labeling_annotation.md) &#124; [7.2 Embeddings](../07_rag_pipeline/7.2_embedding_model_guide.md) |
> | **Next** | [1.2 Data Cleaning & Preprocessing](1.2_data_cleaning_preprocessing.md) |

---

## Executive Summary

Data collection and sourcing form the foundation of any successful LLM or RAG system. This guide provides comprehensive coverage of identifying, acquiring, and ingesting data sources for pre-training, fine-tuning, and retrieval-augmented generation knowledge bases.

Key takeaways:
- Modern LLM training requires diverse, high-quality data from multiple sources
- Quality filtering at ingestion prevents downstream issues and reduces processing costs
- Legal and ethical considerations must be addressed before data acquisition
- Robust ingestion pipelines ensure reliability and auditability
- The trend in 2025 favors fewer, diverse tokens over sheer volume

---

## Table of Contents

1. [Prerequisites](#prerequisites)
2. [Data Source Taxonomy](#data-source-taxonomy)
3. [Data Acquisition Methods](#data-acquisition-methods)
4. [Data Ingestion Architecture](#data-ingestion-architecture)
5. [Legal & Ethical Considerations](#legal--ethical-considerations)
6. [Quality Gates at Ingestion](#quality-gates-at-ingestion)
7. [Implementation Guide](#implementation-guide)
8. [Code Examples](#code-examples)
9. [Configuration Templates](#configuration-templates)
10. [Troubleshooting](#troubleshooting)
11. [Appendices](#appendices)
12. [Glossary](#glossary)
13. [References](#references)

---

## Prerequisites

### Required Knowledge
- Understanding of data engineering fundamentals
- Basic Python programming
- Familiarity with distributed systems concepts
- Understanding of HTTP protocols and APIs

### Required Tools & Access
- Python 3.10+
- Apache Kafka or AWS Kinesis access
- Cloud storage (S3, GCS, or Azure Blob)
- Orchestration platform (Airflow, Dagster, or Prefect)
- Database access for metadata storage

### Environment Setup
```bash
# Create virtual environment
python -m venv .venv
source .venv/bin/activate

# Install core dependencies
pip install scrapy playwright trafilatura requests-html
pip install apache-airflow dagster prefect
pip install boto3 google-cloud-storage
pip install kafka-python confluent-kafka
pip install pyspark ray[data]
```

---

## Data Source Taxonomy

### 1. Public Datasets

#### Web Crawl Data
| Dataset | Size | Description | Use Case |
|---------|------|-------------|----------|
| Common Crawl | ~400TB/month | Monthly web archive snapshots | Pre-training base corpus |
| C4 | 750GB | Cleaned Common Crawl subset | Pre-training, fine-tuning |
| OSCAR | Multi-lingual | Cleaned Common Crawl by language | Multilingual models |
| FineWeb | 15T tokens | High-quality web data | Pre-training |
| RefinedWeb | 5T tokens | Deduplicated, filtered | High-quality pre-training |

#### Curated Text Sources
| Source | Description | Quality Level |
|--------|-------------|---------------|
| Wikipedia | Encyclopedia articles, 60M+ articles | Very High |
| arXiv | Scientific papers, 2M+ papers | Very High |
| GitHub | Open source code repositories | Variable |
| Stack Overflow | Q&A programming content | High |
| Project Gutenberg | Public domain books | High |
| PubMed | Biomedical literature | Very High |

#### Code Repositories
```python
# Example: GitHub data categories
CODE_SOURCES = {
    "github_code": {
        "source": "GitHub Archive",
        "languages": ["Python", "JavaScript", "Java", "C++", "Go", "Rust"],
        "quality_signals": ["stars", "forks", "license"],
        "estimated_size": "~500GB cleaned"
    },
    "stack_overflow": {
        "source": "Stack Exchange Data Dump",
        "content_types": ["questions", "answers", "comments"],
        "quality_signals": ["score", "accepted_answer"],
        "estimated_size": "~100GB"
    }
}
```

### 2. Licensed Datasets

| Provider | Content Type | Licensing Model |
|----------|--------------|-----------------|
| LexisNexis | Legal documents, news | Enterprise license |
| Elsevier | Scientific papers | Academic/commercial |
| Reuters | News archives | Per-article or bulk |
| Getty Images | Image captions | Rights-managed |
| Shutterstock | Image-text pairs | Subscription |

### 3. Proprietary Data

#### Internal Document Sources
- Corporate wikis and documentation
- Customer support transcripts
- Email archives (with consent)
- Internal chat logs
- Meeting transcripts
- Product documentation

#### Customer Interaction Data
- Support tickets
- Chat conversations
- Feedback surveys
- User-generated content

### 4. Synthetic Data

| Method | Description | Best For |
|--------|-------------|----------|
| Self-Instruct | LLM generates instruction-response pairs | Instruction tuning |
| Evol-Instruct | Iteratively complexify instructions | Complex reasoning |
| Backtranslation | Paraphrase via translation | Data augmentation |
| Distillation | Teacher model generates training data | Capability transfer |

### 5. Real-Time Streams

```python
# Example: Real-time data source configuration
STREAMING_SOURCES = {
    "news_feeds": {
        "providers": ["Reuters", "AP", "Bloomberg"],
        "protocol": "REST API / WebSocket",
        "update_frequency": "real-time",
        "use_case": "RAG knowledge base"
    },
    "social_media": {
        "providers": ["Twitter/X API", "Reddit API"],
        "protocol": "Streaming API",
        "update_frequency": "real-time",
        "use_case": "Trend analysis, sentiment"
    },
    "internal_systems": {
        "sources": ["CRM", "ERP", "Support tickets"],
        "protocol": "CDC (Debezium)",
        "update_frequency": "near real-time",
        "use_case": "Enterprise RAG"
    }
}
```

---

## Data Acquisition Methods

### 1. Web Scraping

#### Ethical Web Scraping Framework

```python
"""
Web scraping framework for LLM data collection.
Implements respectful crawling with rate limiting and robots.txt compliance.
"""

import time
import hashlib
from urllib.robotparser import RobotFileParser
from urllib.parse import urlparse
import requests
from dataclasses import dataclass
from typing import Optional, List
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class CrawlConfig:
    """Configuration for respectful web crawling."""
    user_agent: str = "LLMDataCollector/1.0 (+https://example.com/bot)"
    requests_per_second: float = 1.0
    respect_robots_txt: bool = True
    max_retries: int = 3
    timeout_seconds: int = 30


class RespectfulCrawler:
    """Web crawler that respects robots.txt and implements rate limiting."""

    def __init__(self, config: CrawlConfig):
        self.config = config
        self.robots_cache: dict[str, RobotFileParser] = {}
        self.last_request_time: dict[str, float] = {}
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": config.user_agent,
            "Accept": "text/html,application/xhtml+xml",
            "Accept-Language": "en-US,en;q=0.9"
        })

    def _get_robots_parser(self, url: str) -> RobotFileParser:
        """Get or create robots.txt parser for domain."""
        parsed = urlparse(url)
        domain = f"{parsed.scheme}://{parsed.netloc}"

        if domain not in self.robots_cache:
            rp = RobotFileParser()
            robots_url = f"{domain}/robots.txt"
            try:
                rp.set_url(robots_url)
                rp.read()
            except Exception as e:
                logger.warning(f"Could not fetch robots.txt for {domain}: {e}")
            self.robots_cache[domain] = rp

        return self.robots_cache[domain]

    def _can_fetch(self, url: str) -> bool:
        """Check if URL can be fetched according to robots.txt."""
        if not self.config.respect_robots_txt:
            return True
        parser = self._get_robots_parser(url)
        return parser.can_fetch(self.config.user_agent, url)

    def _rate_limit(self, url: str) -> None:
        """Implement rate limiting per domain."""
        parsed = urlparse(url)
        domain = parsed.netloc

        if domain in self.last_request_time:
            elapsed = time.time() - self.last_request_time[domain]
            min_interval = 1.0 / self.config.requests_per_second
            if elapsed < min_interval:
                time.sleep(min_interval - elapsed)

        self.last_request_time[domain] = time.time()

    def fetch(self, url: str) -> Optional[str]:
        """Fetch URL content with rate limiting and robots.txt compliance."""
        if not self._can_fetch(url):
            logger.info(f"Blocked by robots.txt: {url}")
            return None

        self._rate_limit(url)

        for attempt in range(self.config.max_retries):
            try:
                response = self.session.get(
                    url,
                    timeout=self.config.timeout_seconds
                )
                response.raise_for_status()
                return response.text
            except requests.RequestException as e:
                logger.warning(f"Attempt {attempt + 1} failed for {url}: {e}")
                if attempt < self.config.max_retries - 1:
                    time.sleep(2 ** attempt)  # Exponential backoff

        return None


# Example usage
if __name__ == "__main__":
    config = CrawlConfig(
        requests_per_second=0.5,  # Conservative rate
        respect_robots_txt=True
    )
    crawler = RespectfulCrawler(config)

    urls = [
        "https://example.com/page1",
        "https://example.com/page2",
    ]

    for url in urls:
        content = crawler.fetch(url)
        if content:
            logger.info(f"Successfully fetched {len(content)} bytes from {url}")
```

#### Using Crawl4AI for LLM-Ready Data

```python
"""
Crawl4AI integration for extracting LLM-ready markdown content.
"""

from crawl4ai import WebCrawler
from crawl4ai.extraction_strategy import LLMExtractionStrategy
import json

def crawl_for_llm(urls: list[str], output_dir: str) -> None:
    """
    Crawl websites and convert to clean markdown for LLM training.

    Args:
        urls: List of URLs to crawl
        output_dir: Directory to save markdown files
    """
    crawler = WebCrawler()
    crawler.warmup()

    for url in urls:
        result = crawler.run(
            url=url,
            word_count_threshold=100,  # Minimum content length
            bypass_cache=False,
            extract_media=False,  # Skip images for text-only corpus
        )

        if result.success:
            # Get clean markdown content
            markdown_content = result.markdown

            # Generate filename from URL
            filename = url.replace("https://", "").replace("/", "_")[:100]

            # Save with metadata header
            with open(f"{output_dir}/{filename}.md", "w") as f:
                f.write(f"---\n")
                f.write(f"source_url: {url}\n")
                f.write(f"crawl_date: {result.timestamp}\n")
                f.write(f"word_count: {len(markdown_content.split())}\n")
                f.write(f"---\n\n")
                f.write(markdown_content)

            print(f"Saved: {filename}.md ({len(markdown_content)} chars)")
        else:
            print(f"Failed to crawl: {url}")


# Batch crawling with Scrapy
SCRAPY_SPIDER_TEMPLATE = '''
import scrapy
from scrapy.spiders import CrawlSpider, Rule
from scrapy.linkextractors import LinkExtractor
import trafilatura

class LLMDataSpider(CrawlSpider):
    """Spider for collecting clean text content for LLM training."""

    name = "llm_data_spider"

    # Configure these per target site
    allowed_domains = ["example.com"]
    start_urls = ["https://example.com/"]

    rules = (
        Rule(
            LinkExtractor(
                allow=r"/articles/",
                deny=r"(login|signup|cart)"
            ),
            callback="parse_article",
            follow=True
        ),
    )

    custom_settings = {
        "CONCURRENT_REQUESTS": 8,
        "DOWNLOAD_DELAY": 1.0,
        "ROBOTSTXT_OBEY": True,
        "USER_AGENT": "LLMDataBot/1.0",
        "FEEDS": {
            "output/articles.jsonl": {
                "format": "jsonlines",
                "encoding": "utf8"
            }
        }
    }

    def parse_article(self, response):
        """Extract clean text using trafilatura."""
        # Extract main content, removing boilerplate
        text = trafilatura.extract(
            response.text,
            include_comments=False,
            include_tables=True,
            favor_precision=True
        )

        if text and len(text) > 200:  # Minimum content threshold
            yield {
                "url": response.url,
                "title": response.css("title::text").get(),
                "text": text,
                "word_count": len(text.split()),
                "crawl_timestamp": response.headers.get("Date", b"").decode()
            }
'''
```

### 2. API Integration Patterns

```python
"""
API integration patterns for data collection from various sources.
"""

import asyncio
import aiohttp
from typing import AsyncGenerator, Any
from dataclasses import dataclass
import backoff
import logging

logger = logging.getLogger(__name__)


@dataclass
class APIConfig:
    """Configuration for API data collection."""
    base_url: str
    api_key: str
    rate_limit: int = 100  # requests per minute
    timeout: int = 30
    max_retries: int = 3


class APICollector:
    """Generic API data collector with rate limiting and retry logic."""

    def __init__(self, config: APIConfig):
        self.config = config
        self.semaphore = asyncio.Semaphore(config.rate_limit // 60)

    @backoff.on_exception(
        backoff.expo,
        (aiohttp.ClientError, asyncio.TimeoutError),
        max_tries=3
    )
    async def _fetch(
        self,
        session: aiohttp.ClientSession,
        endpoint: str,
        params: dict = None
    ) -> dict:
        """Fetch single endpoint with retry logic."""
        async with self.semaphore:
            url = f"{self.config.base_url}/{endpoint}"
            headers = {"Authorization": f"Bearer {self.config.api_key}"}

            async with session.get(
                url,
                params=params,
                headers=headers,
                timeout=aiohttp.ClientTimeout(total=self.config.timeout)
            ) as response:
                response.raise_for_status()
                return await response.json()

    async def collect_paginated(
        self,
        endpoint: str,
        page_param: str = "page",
        limit_param: str = "limit",
        page_size: int = 100
    ) -> AsyncGenerator[dict, None]:
        """Collect all pages from a paginated API endpoint."""
        async with aiohttp.ClientSession() as session:
            page = 1
            while True:
                params = {page_param: page, limit_param: page_size}

                try:
                    data = await self._fetch(session, endpoint, params)
                except Exception as e:
                    logger.error(f"Failed to fetch page {page}: {e}")
                    break

                items = data.get("items", data.get("results", []))
                if not items:
                    break

                for item in items:
                    yield item

                page += 1

                # Check for explicit pagination end
                if len(items) < page_size:
                    break


# Example: GitHub API integration
class GitHubCollector:
    """Collect code and documentation from GitHub repositories."""

    def __init__(self, token: str):
        self.config = APIConfig(
            base_url="https://api.github.com",
            api_key=token,
            rate_limit=5000  # GitHub's authenticated rate limit
        )
        self.collector = APICollector(self.config)

    async def collect_repo_contents(
        self,
        owner: str,
        repo: str,
        path: str = "",
        extensions: list[str] = None
    ) -> AsyncGenerator[dict, None]:
        """
        Recursively collect file contents from a repository.

        Args:
            owner: Repository owner
            repo: Repository name
            path: Starting path (empty for root)
            extensions: File extensions to include (e.g., [".py", ".md"])
        """
        extensions = extensions or [".py", ".js", ".ts", ".md", ".txt"]

        async with aiohttp.ClientSession() as session:
            endpoint = f"repos/{owner}/{repo}/contents/{path}"
            headers = {"Authorization": f"Bearer {self.config.api_key}"}

            async with session.get(
                f"{self.config.base_url}/{endpoint}",
                headers=headers
            ) as response:
                items = await response.json()

            for item in items:
                if item["type"] == "dir":
                    # Recursively process directories
                    async for file_data in self.collect_repo_contents(
                        owner, repo, item["path"], extensions
                    ):
                        yield file_data

                elif item["type"] == "file":
                    # Check extension filter
                    if any(item["name"].endswith(ext) for ext in extensions):
                        # Fetch file content
                        async with session.get(
                            item["download_url"],
                            headers=headers
                        ) as file_response:
                            content = await file_response.text()

                        yield {
                            "path": item["path"],
                            "name": item["name"],
                            "size": item["size"],
                            "content": content,
                            "url": item["html_url"]
                        }


# Example: arXiv API integration
class ArXivCollector:
    """Collect papers from arXiv API."""

    BASE_URL = "http://export.arxiv.org/api/query"

    async def search_papers(
        self,
        query: str,
        max_results: int = 1000,
        categories: list[str] = None
    ) -> AsyncGenerator[dict, None]:
        """
        Search and collect papers from arXiv.

        Args:
            query: Search query
            max_results: Maximum number of papers to retrieve
            categories: arXiv categories to filter (e.g., ["cs.CL", "cs.LG"])
        """
        import feedparser

        # Build category filter
        if categories:
            cat_filter = " OR ".join(f"cat:{cat}" for cat in categories)
            query = f"({query}) AND ({cat_filter})"

        start = 0
        batch_size = 100

        while start < max_results:
            params = {
                "search_query": query,
                "start": start,
                "max_results": min(batch_size, max_results - start),
                "sortBy": "submittedDate",
                "sortOrder": "descending"
            }

            async with aiohttp.ClientSession() as session:
                async with session.get(self.BASE_URL, params=params) as response:
                    content = await response.text()

            feed = feedparser.parse(content)

            if not feed.entries:
                break

            for entry in feed.entries:
                yield {
                    "arxiv_id": entry.id.split("/abs/")[-1],
                    "title": entry.title,
                    "summary": entry.summary,
                    "authors": [a.name for a in entry.authors],
                    "categories": [t.term for t in entry.tags],
                    "published": entry.published,
                    "pdf_url": entry.id.replace("/abs/", "/pdf/") + ".pdf"
                }

            start += batch_size
            await asyncio.sleep(3)  # Respect rate limits
```

### 3. Database Extraction

```python
"""
Database extraction patterns for LLM training data.
Supports batch export and Change Data Capture (CDC).
"""

from typing import Generator, Any
import psycopg2
from psycopg2.extras import RealDictCursor
import json
from dataclasses import dataclass


@dataclass
class DBConfig:
    """Database connection configuration."""
    host: str
    port: int
    database: str
    user: str
    password: str


class DatabaseExtractor:
    """Extract training data from relational databases."""

    def __init__(self, config: DBConfig):
        self.config = config

    def _get_connection(self):
        """Create database connection."""
        return psycopg2.connect(
            host=self.config.host,
            port=self.config.port,
            database=self.config.database,
            user=self.config.user,
            password=self.config.password
        )

    def extract_batch(
        self,
        query: str,
        batch_size: int = 10000
    ) -> Generator[dict, None, None]:
        """
        Extract data in batches using server-side cursor.

        Args:
            query: SQL query to execute
            batch_size: Number of rows per batch
        """
        with self._get_connection() as conn:
            with conn.cursor(
                name="batch_cursor",
                cursor_factory=RealDictCursor
            ) as cursor:
                cursor.itersize = batch_size
                cursor.execute(query)

                for row in cursor:
                    yield dict(row)

    def extract_incremental(
        self,
        table: str,
        timestamp_column: str,
        last_timestamp: str,
        columns: list[str] = None
    ) -> Generator[dict, None, None]:
        """
        Extract incrementally based on timestamp.

        Args:
            table: Table name
            timestamp_column: Column to use for incremental extraction
            last_timestamp: Extract records after this timestamp
            columns: Columns to extract (None for all)
        """
        cols = ", ".join(columns) if columns else "*"
        query = f"""
            SELECT {cols}
            FROM {table}
            WHERE {timestamp_column} > %s
            ORDER BY {timestamp_column}
        """

        with self._get_connection() as conn:
            with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                cursor.execute(query, (last_timestamp,))
                for row in cursor:
                    yield dict(row)


# Debezium CDC configuration for real-time extraction
DEBEZIUM_CONFIG = {
    "name": "llm-training-connector",
    "config": {
        "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
        "database.hostname": "postgres.example.com",
        "database.port": "5432",
        "database.user": "debezium",
        "database.password": "${secrets:postgres-password}",
        "database.dbname": "training_data",
        "database.server.name": "training_server",
        "table.include.list": "public.documents,public.conversations",
        "plugin.name": "pgoutput",
        "publication.autocreate.mode": "filtered",
        "slot.name": "llm_training_slot",
        "transforms": "unwrap",
        "transforms.unwrap.type": "io.debezium.transforms.ExtractNewRecordState",
        "key.converter": "org.apache.kafka.connect.json.JsonConverter",
        "value.converter": "org.apache.kafka.connect.json.JsonConverter"
    }
}
```

### 4. File Ingestion

```python
"""
File ingestion patterns for various storage systems.
"""

import boto3
from google.cloud import storage
import hashlib
from pathlib import Path
from typing import Generator, BinaryIO
import mimetypes


class S3Ingester:
    """Ingest files from Amazon S3."""

    def __init__(self, bucket: str, prefix: str = ""):
        self.s3 = boto3.client("s3")
        self.bucket = bucket
        self.prefix = prefix

    def list_objects(
        self,
        extensions: list[str] = None
    ) -> Generator[dict, None, None]:
        """
        List objects in bucket with optional extension filter.

        Args:
            extensions: File extensions to include (e.g., [".txt", ".pdf"])
        """
        paginator = self.s3.get_paginator("list_objects_v2")

        for page in paginator.paginate(Bucket=self.bucket, Prefix=self.prefix):
            for obj in page.get("Contents", []):
                key = obj["Key"]

                # Filter by extension
                if extensions:
                    if not any(key.endswith(ext) for ext in extensions):
                        continue

                yield {
                    "key": key,
                    "size": obj["Size"],
                    "last_modified": obj["LastModified"].isoformat(),
                    "etag": obj["ETag"].strip('"')
                }

    def download_stream(self, key: str) -> BinaryIO:
        """Get streaming body for large files."""
        response = self.s3.get_object(Bucket=self.bucket, Key=key)
        return response["Body"]

    def download_content(self, key: str) -> bytes:
        """Download entire file content."""
        response = self.s3.get_object(Bucket=self.bucket, Key=key)
        return response["Body"].read()


class GCSIngester:
    """Ingest files from Google Cloud Storage."""

    def __init__(self, bucket: str, prefix: str = ""):
        self.client = storage.Client()
        self.bucket = self.client.bucket(bucket)
        self.prefix = prefix

    def list_blobs(
        self,
        extensions: list[str] = None
    ) -> Generator[dict, None, None]:
        """List blobs with optional extension filter."""
        blobs = self.client.list_blobs(self.bucket, prefix=self.prefix)

        for blob in blobs:
            if extensions:
                if not any(blob.name.endswith(ext) for ext in extensions):
                    continue

            yield {
                "name": blob.name,
                "size": blob.size,
                "updated": blob.updated.isoformat() if blob.updated else None,
                "md5_hash": blob.md5_hash
            }

    def download_content(self, blob_name: str) -> bytes:
        """Download blob content."""
        blob = self.bucket.blob(blob_name)
        return blob.download_as_bytes()


# Example: Multi-format file processor
class FileProcessor:
    """Process various file formats for LLM training."""

    PROCESSORS = {
        ".txt": "_process_text",
        ".md": "_process_text",
        ".pdf": "_process_pdf",
        ".html": "_process_html",
        ".json": "_process_json",
        ".jsonl": "_process_jsonl",
    }

    def process(self, content: bytes, filename: str) -> str:
        """
        Process file content based on extension.

        Args:
            content: Raw file bytes
            filename: Original filename for extension detection

        Returns:
            Extracted text content
        """
        ext = Path(filename).suffix.lower()
        processor_name = self.PROCESSORS.get(ext)

        if not processor_name:
            raise ValueError(f"Unsupported file type: {ext}")

        processor = getattr(self, processor_name)
        return processor(content)

    def _process_text(self, content: bytes) -> str:
        """Process plain text files."""
        # Try common encodings
        for encoding in ["utf-8", "latin-1", "cp1252"]:
            try:
                return content.decode(encoding)
            except UnicodeDecodeError:
                continue
        raise ValueError("Could not decode text file")

    def _process_pdf(self, content: bytes) -> str:
        """Process PDF files using PyMuPDF."""
        import fitz  # PyMuPDF

        text_parts = []
        with fitz.open(stream=content, filetype="pdf") as doc:
            for page in doc:
                text_parts.append(page.get_text())

        return "\n\n".join(text_parts)

    def _process_html(self, content: bytes) -> str:
        """Process HTML files using trafilatura."""
        import trafilatura

        html_text = content.decode("utf-8", errors="ignore")
        extracted = trafilatura.extract(
            html_text,
            include_comments=False,
            include_tables=True
        )
        return extracted or ""

    def _process_json(self, content: bytes) -> str:
        """Process JSON files - extract text fields."""
        import json

        data = json.loads(content.decode("utf-8"))
        return self._extract_text_from_json(data)

    def _process_jsonl(self, content: bytes) -> str:
        """Process JSON Lines files."""
        import json

        lines = content.decode("utf-8").strip().split("\n")
        texts = []
        for line in lines:
            data = json.loads(line)
            texts.append(self._extract_text_from_json(data))
        return "\n\n".join(texts)

    def _extract_text_from_json(self, data: Any, depth: int = 0) -> str:
        """Recursively extract text from JSON structure."""
        if depth > 10:  # Prevent infinite recursion
            return ""

        if isinstance(data, str):
            return data
        elif isinstance(data, list):
            return "\n".join(
                self._extract_text_from_json(item, depth + 1)
                for item in data
            )
        elif isinstance(data, dict):
            # Prioritize common text field names
            text_fields = ["text", "content", "body", "message", "description"]
            for field in text_fields:
                if field in data:
                    return str(data[field])

            # Fall back to concatenating all string values
            return "\n".join(
                self._extract_text_from_json(v, depth + 1)
                for v in data.values()
            )
        return ""
```

---

## Data Ingestion Architecture

### Batch vs Streaming Ingestion

| Aspect | Batch Ingestion | Streaming Ingestion |
|--------|-----------------|---------------------|
| **Latency** | Hours to days | Seconds to minutes |
| **Throughput** | Very high | Moderate to high |
| **Use Case** | Pre-training data | RAG knowledge base updates |
| **Complexity** | Lower | Higher |
| **Cost** | Lower | Higher |
| **Tools** | Spark, Airflow | Kafka, Kinesis, Flink |

### Batch Ingestion Pipeline

```python
"""
Airflow DAG for batch data ingestion pipeline.
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor
from airflow.providers.amazon.aws.operators.s3 import S3DeleteObjectsOperator

default_args = {
    "owner": "data-engineering",
    "depends_on_past": False,
    "email_on_failure": True,
    "email_on_retry": False,
    "retries": 3,
    "retry_delay": timedelta(minutes=5),
}

dag = DAG(
    "llm_data_ingestion",
    default_args=default_args,
    description="Ingest raw data for LLM training",
    schedule_interval="@daily",
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=["llm", "data-pipeline"],
)


def extract_from_sources(**context):
    """Extract data from configured sources."""
    execution_date = context["execution_date"]

    # Extract from various sources
    sources = [
        {"type": "api", "name": "news_api"},
        {"type": "s3", "name": "document_uploads"},
        {"type": "database", "name": "support_tickets"},
    ]

    extracted_files = []
    for source in sources:
        # Source-specific extraction logic
        output_path = f"s3://raw-data/{source['name']}/{execution_date.strftime('%Y/%m/%d')}/"
        extracted_files.append(output_path)

    return extracted_files


def validate_data(**context):
    """Run quality checks on extracted data."""
    ti = context["task_instance"]
    extracted_files = ti.xcom_pull(task_ids="extract")

    validation_results = []
    for file_path in extracted_files:
        # Run validation checks
        result = {
            "path": file_path,
            "row_count": 0,  # Actual count
            "schema_valid": True,
            "null_rate": 0.0,
            "duplicate_rate": 0.0,
        }
        validation_results.append(result)

    # Fail if quality thresholds not met
    for result in validation_results:
        if result["null_rate"] > 0.1:
            raise ValueError(f"High null rate in {result['path']}")

    return validation_results


def deduplicate_data(**context):
    """Remove duplicates using MinHash."""
    ti = context["task_instance"]
    validated_files = ti.xcom_pull(task_ids="validate")

    # Deduplication logic
    dedupe_output = "s3://processed-data/deduped/"
    return dedupe_output


def load_to_warehouse(**context):
    """Load processed data to data warehouse."""
    ti = context["task_instance"]
    dedupe_output = ti.xcom_pull(task_ids="deduplicate")

    # Load to warehouse
    return {"status": "success", "records_loaded": 0}


# Define tasks
extract_task = PythonOperator(
    task_id="extract",
    python_callable=extract_from_sources,
    dag=dag,
)

validate_task = PythonOperator(
    task_id="validate",
    python_callable=validate_data,
    dag=dag,
)

dedupe_task = PythonOperator(
    task_id="deduplicate",
    python_callable=deduplicate_data,
    dag=dag,
)

load_task = PythonOperator(
    task_id="load",
    python_callable=load_to_warehouse,
    dag=dag,
)

# Set dependencies
extract_task >> validate_task >> dedupe_task >> load_task
```

### Streaming Ingestion Pipeline

```python
"""
Kafka-based streaming ingestion pipeline.
"""

from confluent_kafka import Consumer, Producer, KafkaError
from confluent_kafka.admin import AdminClient, NewTopic
import json
from dataclasses import dataclass
from typing import Callable, Optional
import threading
import logging

logger = logging.getLogger(__name__)


@dataclass
class KafkaConfig:
    """Kafka connection configuration."""
    bootstrap_servers: str
    group_id: str
    auto_offset_reset: str = "earliest"
    enable_auto_commit: bool = False


class StreamingIngestionPipeline:
    """Real-time data ingestion using Kafka."""

    def __init__(self, config: KafkaConfig):
        self.config = config
        self.consumer = None
        self.producer = None
        self.running = False

    def _create_consumer(self, topics: list[str]) -> Consumer:
        """Create Kafka consumer."""
        consumer_config = {
            "bootstrap.servers": self.config.bootstrap_servers,
            "group.id": self.config.group_id,
            "auto.offset.reset": self.config.auto_offset_reset,
            "enable.auto.commit": self.config.enable_auto_commit,
        }
        consumer = Consumer(consumer_config)
        consumer.subscribe(topics)
        return consumer

    def _create_producer(self) -> Producer:
        """Create Kafka producer."""
        producer_config = {
            "bootstrap.servers": self.config.bootstrap_servers,
            "acks": "all",
            "retries": 3,
        }
        return Producer(producer_config)

    def process_stream(
        self,
        input_topics: list[str],
        output_topic: str,
        transform_fn: Callable[[dict], Optional[dict]],
        batch_size: int = 100
    ) -> None:
        """
        Process streaming data with transformation.

        Args:
            input_topics: Topics to consume from
            output_topic: Topic to produce to
            transform_fn: Transformation function
            batch_size: Commit batch size
        """
        self.consumer = self._create_consumer(input_topics)
        self.producer = self._create_producer()
        self.running = True

        processed_count = 0

        try:
            while self.running:
                msg = self.consumer.poll(timeout=1.0)

                if msg is None:
                    continue

                if msg.error():
                    if msg.error().code() == KafkaError._PARTITION_EOF:
                        continue
                    logger.error(f"Consumer error: {msg.error()}")
                    continue

                # Parse and transform message
                try:
                    data = json.loads(msg.value().decode("utf-8"))
                    transformed = transform_fn(data)

                    if transformed:
                        # Produce transformed message
                        self.producer.produce(
                            output_topic,
                            key=msg.key(),
                            value=json.dumps(transformed).encode("utf-8")
                        )
                        processed_count += 1

                    # Batch commit
                    if processed_count % batch_size == 0:
                        self.consumer.commit()
                        self.producer.flush()
                        logger.info(f"Committed batch, total processed: {processed_count}")

                except json.JSONDecodeError as e:
                    logger.warning(f"Failed to parse message: {e}")
                except Exception as e:
                    logger.error(f"Transform error: {e}")

        finally:
            self.consumer.close()
            self.producer.flush()

    def stop(self):
        """Stop the pipeline gracefully."""
        self.running = False


# Example transformation function
def enrich_document(doc: dict) -> Optional[dict]:
    """
    Enrich incoming document with metadata.

    Args:
        doc: Raw document from source

    Returns:
        Enriched document or None to filter out
    """
    # Filter out short documents
    text = doc.get("text", "")
    if len(text) < 100:
        return None

    # Add metadata
    doc["word_count"] = len(text.split())
    doc["char_count"] = len(text)
    doc["ingestion_timestamp"] = datetime.utcnow().isoformat()

    # Detect language (simplified)
    doc["detected_language"] = "en"  # Use actual detection

    return doc


# AWS Kinesis alternative
class KinesisIngestionPipeline:
    """Real-time ingestion using AWS Kinesis."""

    def __init__(self, stream_name: str, region: str = "us-east-1"):
        import boto3
        self.kinesis = boto3.client("kinesis", region_name=region)
        self.stream_name = stream_name

    def put_record(self, data: dict, partition_key: str) -> dict:
        """Put single record to Kinesis stream."""
        response = self.kinesis.put_record(
            StreamName=self.stream_name,
            Data=json.dumps(data).encode("utf-8"),
            PartitionKey=partition_key
        )
        return response

    def put_records_batch(
        self,
        records: list[dict],
        partition_key_fn: Callable[[dict], str]
    ) -> dict:
        """Put batch of records to Kinesis stream."""
        kinesis_records = [
            {
                "Data": json.dumps(record).encode("utf-8"),
                "PartitionKey": partition_key_fn(record)
            }
            for record in records
        ]

        response = self.kinesis.put_records(
            StreamName=self.stream_name,
            Records=kinesis_records
        )
        return response
```

### Idempotency and Exactly-Once Semantics

```python
"""
Patterns for ensuring exactly-once processing in data pipelines.
"""

import hashlib
import json
from typing import Optional
import redis
from dataclasses import dataclass


@dataclass
class IdempotencyConfig:
    """Configuration for idempotency handling."""
    redis_url: str
    key_prefix: str = "idempotency"
    ttl_seconds: int = 86400 * 7  # 7 days


class IdempotentProcessor:
    """
    Ensure exactly-once processing using Redis-based deduplication.
    """

    def __init__(self, config: IdempotencyConfig):
        self.config = config
        self.redis = redis.from_url(config.redis_url)

    def _compute_key(self, record: dict) -> str:
        """Compute idempotency key from record."""
        # Create deterministic hash of record
        canonical = json.dumps(record, sort_keys=True)
        hash_value = hashlib.sha256(canonical.encode()).hexdigest()[:16]
        return f"{self.config.key_prefix}:{hash_value}"

    def process_if_new(
        self,
        record: dict,
        processor_fn: callable
    ) -> tuple[bool, Optional[any]]:
        """
        Process record only if not previously processed.

        Args:
            record: Record to process
            processor_fn: Function to apply if record is new

        Returns:
            Tuple of (was_processed, result)
        """
        key = self._compute_key(record)

        # Try to acquire lock (atomic set-if-not-exists)
        acquired = self.redis.set(
            key,
            "processing",
            nx=True,
            ex=self.config.ttl_seconds
        )

        if not acquired:
            # Record already processed
            return False, None

        try:
            # Process the record
            result = processor_fn(record)

            # Mark as completed
            self.redis.set(key, "completed", ex=self.config.ttl_seconds)

            return True, result

        except Exception as e:
            # Remove key on failure to allow retry
            self.redis.delete(key)
            raise


# Transactional outbox pattern for reliable publishing
class TransactionalOutbox:
    """
    Implement transactional outbox pattern for reliable event publishing.
    Ensures database writes and event publishing are atomic.
    """

    def __init__(self, db_connection, event_publisher):
        self.db = db_connection
        self.publisher = event_publisher

    def save_with_event(
        self,
        record: dict,
        event_type: str,
        table: str = "documents"
    ) -> str:
        """
        Save record and create outbox event in single transaction.

        Args:
            record: Record to save
            event_type: Type of event to publish
            table: Target table name

        Returns:
            ID of saved record
        """
        import uuid

        record_id = str(uuid.uuid4())
        event_id = str(uuid.uuid4())

        with self.db.begin() as tx:
            # Insert the record
            tx.execute(
                f"INSERT INTO {table} (id, data) VALUES (%s, %s)",
                (record_id, json.dumps(record))
            )

            # Insert outbox event
            tx.execute(
                """
                INSERT INTO outbox_events
                (id, event_type, payload, status, created_at)
                VALUES (%s, %s, %s, 'pending', NOW())
                """,
                (event_id, event_type, json.dumps({
                    "record_id": record_id,
                    "data": record
                }))
            )

        return record_id

    def process_outbox(self, batch_size: int = 100) -> int:
        """
        Process pending outbox events.
        Run this as a background job.

        Returns:
            Number of events processed
        """
        # Fetch pending events
        events = self.db.execute(
            """
            SELECT id, event_type, payload
            FROM outbox_events
            WHERE status = 'pending'
            ORDER BY created_at
            LIMIT %s
            FOR UPDATE SKIP LOCKED
            """,
            (batch_size,)
        ).fetchall()

        processed = 0
        for event in events:
            try:
                # Publish event
                self.publisher.publish(
                    event_type=event["event_type"],
                    payload=json.loads(event["payload"])
                )

                # Mark as published
                self.db.execute(
                    "UPDATE outbox_events SET status = 'published' WHERE id = %s",
                    (event["id"],)
                )
                processed += 1

            except Exception as e:
                # Mark as failed
                self.db.execute(
                    """
                    UPDATE outbox_events
                    SET status = 'failed', error = %s
                    WHERE id = %s
                    """,
                    (str(e), event["id"])
                )

        return processed
```

---

## Legal & Ethical Considerations

### Copyright and Fair Use

| Data Type | Copyright Status | Fair Use Considerations |
|-----------|------------------|------------------------|
| Web content | Copyrighted by default | Training may qualify as transformative use |
| Open-source code | Licensed (varies) | Must comply with license terms |
| Academic papers | Publisher copyright | Check open access status |
| Social media | User-generated, platform ToS | Platform restrictions apply |
| Government docs | Often public domain | Verify jurisdiction |

### Terms of Service Compliance Checklist

```markdown
## ToS Compliance Checklist

### Before Scraping
- [ ] Read and understand the website's Terms of Service
- [ ] Check robots.txt for crawling restrictions
- [ ] Verify there's no explicit prohibition on data collection
- [ ] Confirm data won't be used in ways that violate ToS

### During Collection
- [ ] Implement respectful rate limiting
- [ ] Use identifiable User-Agent string
- [ ] Honor Crawl-Delay directives
- [ ] Don't circumvent access controls

### Data Usage
- [ ] Document the source and collection method
- [ ] Track licensing requirements
- [ ] Implement data retention limits
- [ ] Enable data deletion upon request

### Red Flags (Do Not Proceed)
- Explicit prohibition of scraping in ToS
- Login-wall or paywall circumvention required
- Personal data without consent mechanism
- Competitive intelligence restrictions
```

### PII Handling at Ingestion

```python
"""
PII detection and handling at ingestion time.
"""

from presidio_analyzer import AnalyzerEngine
from presidio_anonymizer import AnonymizerEngine
from presidio_anonymizer.entities import OperatorConfig
from typing import List, Dict
import re


class PIIHandler:
    """Detect and handle PII in incoming data."""

    def __init__(self, language: str = "en"):
        self.analyzer = AnalyzerEngine()
        self.anonymizer = AnonymizerEngine()
        self.language = language

        # Additional regex patterns for common PII
        self.custom_patterns = {
            "SSN": r"\b\d{3}-\d{2}-\d{4}\b",
            "CREDIT_CARD": r"\b\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}\b",
            "IP_ADDRESS": r"\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b",
        }

    def detect_pii(self, text: str) -> List[Dict]:
        """
        Detect PII entities in text.

        Args:
            text: Input text to analyze

        Returns:
            List of detected PII entities
        """
        # Use Presidio analyzer
        results = self.analyzer.analyze(
            text=text,
            language=self.language,
            entities=[
                "PERSON", "EMAIL_ADDRESS", "PHONE_NUMBER",
                "LOCATION", "DATE_TIME", "NRP", "MEDICAL_LICENSE",
                "US_SSN", "US_DRIVER_LICENSE", "CREDIT_CARD"
            ]
        )

        detected = []
        for result in results:
            detected.append({
                "entity_type": result.entity_type,
                "start": result.start,
                "end": result.end,
                "score": result.score,
                "text": text[result.start:result.end]
            })

        # Check custom patterns
        for pattern_name, pattern in self.custom_patterns.items():
            for match in re.finditer(pattern, text):
                detected.append({
                    "entity_type": pattern_name,
                    "start": match.start(),
                    "end": match.end(),
                    "score": 1.0,
                    "text": match.group()
                })

        return detected

    def anonymize(
        self,
        text: str,
        strategy: str = "replace"
    ) -> str:
        """
        Anonymize PII in text.

        Args:
            text: Input text
            strategy: One of "replace", "redact", "hash", "mask"

        Returns:
            Anonymized text
        """
        results = self.analyzer.analyze(
            text=text,
            language=self.language
        )

        operator_map = {
            "replace": {"type": "replace", "new_value": "[REDACTED]"},
            "redact": {"type": "redact"},
            "hash": {"type": "hash", "hash_type": "sha256"},
            "mask": {"type": "mask", "masking_char": "*", "chars_to_mask": 4}
        }

        operators = {
            "DEFAULT": OperatorConfig(**operator_map.get(strategy, operator_map["replace"]))
        }

        anonymized = self.anonymizer.anonymize(
            text=text,
            analyzer_results=results,
            operators=operators
        )

        return anonymized.text

    def should_filter_document(
        self,
        text: str,
        pii_threshold: float = 0.1
    ) -> tuple[bool, str]:
        """
        Determine if document should be filtered due to PII density.

        Args:
            text: Document text
            pii_threshold: Maximum PII ratio allowed

        Returns:
            Tuple of (should_filter, reason)
        """
        detected = self.detect_pii(text)

        if not detected:
            return False, ""

        # Calculate PII character ratio
        pii_chars = sum(d["end"] - d["start"] for d in detected)
        pii_ratio = pii_chars / len(text) if text else 0

        if pii_ratio > pii_threshold:
            return True, f"PII density {pii_ratio:.2%} exceeds threshold {pii_threshold:.2%}"

        # Check for high-sensitivity PII
        high_sensitivity = ["US_SSN", "CREDIT_CARD", "MEDICAL_LICENSE"]
        sensitive_found = [d for d in detected if d["entity_type"] in high_sensitivity]

        if sensitive_found:
            return True, f"High-sensitivity PII detected: {[d['entity_type'] for d in sensitive_found]}"

        return False, ""


# Example usage in ingestion pipeline
def process_document_with_pii_handling(
    document: dict,
    pii_handler: PIIHandler,
    anonymize: bool = True
) -> dict:
    """
    Process document with PII handling.

    Args:
        document: Input document with 'text' field
        pii_handler: PIIHandler instance
        anonymize: Whether to anonymize or filter PII-heavy docs

    Returns:
        Processed document
    """
    text = document.get("text", "")

    # Check if should filter
    should_filter, reason = pii_handler.should_filter_document(text)

    if should_filter:
        if anonymize:
            # Anonymize and keep
            document["text"] = pii_handler.anonymize(text)
            document["pii_anonymized"] = True
            document["pii_filter_reason"] = reason
        else:
            # Mark for exclusion
            document["excluded"] = True
            document["exclusion_reason"] = reason
    else:
        # Light anonymization for any detected PII
        pii_detected = pii_handler.detect_pii(text)
        if pii_detected:
            document["text"] = pii_handler.anonymize(text)
            document["pii_anonymized"] = True

    return document
```

### Data Provenance Tracking

```python
"""
Data provenance tracking for audit and compliance.
"""

from dataclasses import dataclass, field
from datetime import datetime
from typing import List, Dict, Optional
import hashlib
import json
import uuid


@dataclass
class ProvenanceRecord:
    """Record of data origin and transformations."""
    record_id: str
    source_type: str  # "web", "api", "database", "file"
    source_url: Optional[str] = None
    source_name: str = ""
    collection_timestamp: datetime = field(default_factory=datetime.utcnow)
    content_hash: str = ""
    transformations: List[Dict] = field(default_factory=list)
    metadata: Dict = field(default_factory=dict)

    def add_transformation(
        self,
        operation: str,
        parameters: dict = None,
        output_hash: str = None
    ) -> None:
        """Record a transformation applied to the data."""
        self.transformations.append({
            "operation": operation,
            "parameters": parameters or {},
            "timestamp": datetime.utcnow().isoformat(),
            "output_hash": output_hash
        })

    def to_dict(self) -> dict:
        """Convert to dictionary for storage."""
        return {
            "record_id": self.record_id,
            "source_type": self.source_type,
            "source_url": self.source_url,
            "source_name": self.source_name,
            "collection_timestamp": self.collection_timestamp.isoformat(),
            "content_hash": self.content_hash,
            "transformations": self.transformations,
            "metadata": self.metadata
        }


class ProvenanceTracker:
    """Track data provenance throughout the pipeline."""

    def __init__(self, storage_backend):
        self.storage = storage_backend

    def create_record(
        self,
        content: str,
        source_type: str,
        source_url: str = None,
        source_name: str = "",
        metadata: dict = None
    ) -> ProvenanceRecord:
        """
        Create a new provenance record.

        Args:
            content: The data content
            source_type: Type of source
            source_url: URL if applicable
            source_name: Human-readable source name
            metadata: Additional metadata

        Returns:
            ProvenanceRecord instance
        """
        record = ProvenanceRecord(
            record_id=str(uuid.uuid4()),
            source_type=source_type,
            source_url=source_url,
            source_name=source_name,
            content_hash=self._hash_content(content),
            metadata=metadata or {}
        )

        return record

    def _hash_content(self, content: str) -> str:
        """Create deterministic hash of content."""
        return hashlib.sha256(content.encode("utf-8")).hexdigest()

    def save_record(self, record: ProvenanceRecord) -> None:
        """Persist provenance record."""
        self.storage.save(record.record_id, record.to_dict())

    def get_lineage(self, record_id: str) -> List[Dict]:
        """Get full transformation lineage for a record."""
        record = self.storage.get(record_id)
        if not record:
            return []
        return record.get("transformations", [])

    def query_by_source(
        self,
        source_type: str = None,
        source_name: str = None,
        date_from: datetime = None,
        date_to: datetime = None
    ) -> List[ProvenanceRecord]:
        """Query records by source criteria."""
        return self.storage.query(
            source_type=source_type,
            source_name=source_name,
            date_from=date_from,
            date_to=date_to
        )
```

---

## Quality Gates at Ingestion

### Schema Validation

```python
"""
Schema validation for incoming data.
"""

from pydantic import BaseModel, Field, validator
from typing import Optional, List
from datetime import datetime
import json


class DocumentSchema(BaseModel):
    """Schema for ingested documents."""

    id: str = Field(..., min_length=1, max_length=128)
    text: str = Field(..., min_length=1)
    source_url: Optional[str] = None
    source_type: str = Field(..., pattern="^(web|api|database|file|synthetic)$")
    language: str = Field(default="en", pattern="^[a-z]{2}$")
    timestamp: datetime
    metadata: dict = Field(default_factory=dict)

    @validator("text")
    def text_not_empty(cls, v):
        if not v.strip():
            raise ValueError("Text cannot be empty or whitespace only")
        return v

    @validator("source_url")
    def validate_url(cls, v):
        if v and not v.startswith(("http://", "https://")):
            raise ValueError("Invalid URL format")
        return v

    class Config:
        extra = "forbid"  # Reject unknown fields


class DataValidator:
    """Validate incoming data against schema."""

    def __init__(self, schema_class: type = DocumentSchema):
        self.schema_class = schema_class
        self.validation_errors = []

    def validate(self, data: dict) -> tuple[bool, Optional[dict], List[str]]:
        """
        Validate data against schema.

        Returns:
            Tuple of (is_valid, validated_data, errors)
        """
        try:
            validated = self.schema_class(**data)
            return True, validated.dict(), []
        except Exception as e:
            errors = self._parse_validation_errors(e)
            return False, None, errors

    def _parse_validation_errors(self, exception) -> List[str]:
        """Parse validation errors into readable messages."""
        errors = []
        if hasattr(exception, "errors"):
            for error in exception.errors():
                loc = ".".join(str(l) for l in error["loc"])
                errors.append(f"{loc}: {error['msg']}")
        else:
            errors.append(str(exception))
        return errors

    def validate_batch(
        self,
        records: List[dict],
        fail_fast: bool = False
    ) -> tuple[List[dict], List[tuple[int, List[str]]]]:
        """
        Validate batch of records.

        Args:
            records: List of records to validate
            fail_fast: Stop on first error

        Returns:
            Tuple of (valid_records, errors_with_indices)
        """
        valid_records = []
        errors = []

        for i, record in enumerate(records):
            is_valid, validated, errs = self.validate(record)

            if is_valid:
                valid_records.append(validated)
            else:
                errors.append((i, errs))
                if fail_fast:
                    break

        return valid_records, errors
```

### Duplicate Detection

```python
"""
Duplicate detection using MinHash for fuzzy matching.
"""

from datasketch import MinHash, MinHashLSH
from typing import List, Tuple, Set
import hashlib
import re


class DuplicateDetector:
    """
    Detect exact and near-duplicate documents.
    Uses MinHash LSH for efficient fuzzy duplicate detection.
    """

    def __init__(
        self,
        threshold: float = 0.8,
        num_perm: int = 128
    ):
        """
        Initialize duplicate detector.

        Args:
            threshold: Jaccard similarity threshold for duplicates
            num_perm: Number of permutations for MinHash
        """
        self.threshold = threshold
        self.num_perm = num_perm
        self.lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)
        self.exact_hashes: Set[str] = set()
        self.document_count = 0

    def _tokenize(self, text: str) -> List[str]:
        """Tokenize text into shingles."""
        # Normalize
        text = text.lower()
        text = re.sub(r"\s+", " ", text)

        # Create 5-grams (shingles)
        words = text.split()
        if len(words) < 5:
            return words

        shingles = []
        for i in range(len(words) - 4):
            shingle = " ".join(words[i:i+5])
            shingles.append(shingle)

        return shingles

    def _compute_exact_hash(self, text: str) -> str:
        """Compute exact hash for identical detection."""
        normalized = re.sub(r"\s+", " ", text.lower().strip())
        return hashlib.sha256(normalized.encode()).hexdigest()

    def _compute_minhash(self, text: str) -> MinHash:
        """Compute MinHash signature for text."""
        minhash = MinHash(num_perm=self.num_perm)

        for shingle in self._tokenize(text):
            minhash.update(shingle.encode("utf-8"))

        return minhash

    def is_duplicate(self, text: str, doc_id: str = None) -> Tuple[bool, str, float]:
        """
        Check if text is duplicate of existing document.

        Args:
            text: Document text
            doc_id: Document ID (auto-generated if not provided)

        Returns:
            Tuple of (is_duplicate, matching_doc_id, similarity_score)
        """
        if doc_id is None:
            doc_id = f"doc_{self.document_count}"

        # Check exact duplicate first
        exact_hash = self._compute_exact_hash(text)
        if exact_hash in self.exact_hashes:
            return True, "exact_match", 1.0

        # Check fuzzy duplicate
        minhash = self._compute_minhash(text)
        similar_docs = self.lsh.query(minhash)

        if similar_docs:
            # Return first match (could compute actual similarity if needed)
            return True, similar_docs[0], self.threshold

        return False, "", 0.0

    def add_document(self, text: str, doc_id: str = None) -> str:
        """
        Add document to the index.

        Args:
            text: Document text
            doc_id: Document ID

        Returns:
            Document ID
        """
        if doc_id is None:
            doc_id = f"doc_{self.document_count}"

        # Add exact hash
        exact_hash = self._compute_exact_hash(text)
        self.exact_hashes.add(exact_hash)

        # Add to LSH index
        minhash = self._compute_minhash(text)
        self.lsh.insert(doc_id, minhash)

        self.document_count += 1
        return doc_id

    def process_batch(
        self,
        documents: List[Tuple[str, str]]
    ) -> Tuple[List[str], List[Tuple[str, str, float]]]:
        """
        Process batch of documents, filtering duplicates.

        Args:
            documents: List of (doc_id, text) tuples

        Returns:
            Tuple of (unique_doc_ids, duplicate_info)
        """
        unique = []
        duplicates = []

        for doc_id, text in documents:
            is_dup, matching_id, score = self.is_duplicate(text, doc_id)

            if is_dup:
                duplicates.append((doc_id, matching_id, score))
            else:
                self.add_document(text, doc_id)
                unique.append(doc_id)

        return unique, duplicates


# Example: Batch deduplication in Spark
SPARK_DEDUP_CODE = '''
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sha2, concat_ws, lower, regexp_replace

spark = SparkSession.builder.appName("Deduplication").getOrCreate()

def deduplicate_exact(df, text_column="text"):
    """Remove exact duplicates based on text hash."""
    # Normalize and hash
    df_hashed = df.withColumn(
        "text_hash",
        sha2(
            lower(regexp_replace(col(text_column), r"\s+", " ")),
            256
        )
    )

    # Keep first occurrence
    df_deduped = df_hashed.dropDuplicates(["text_hash"]).drop("text_hash")

    return df_deduped

# For MinHash LSH at scale, use spark-nlp or implement with RDDs
'''
```

### Language Identification

```python
"""
Language identification for multilingual data filtering.
"""

import fasttext
from typing import Tuple, List, Optional
import logging

logger = logging.getLogger(__name__)


class LanguageIdentifier:
    """
    Identify language of text using fastText.
    """

    def __init__(self, model_path: str = None):
        """
        Initialize language identifier.

        Args:
            model_path: Path to fastText model. If None, downloads default.
        """
        if model_path:
            self.model = fasttext.load_model(model_path)
        else:
            # Download pretrained model
            import urllib.request
            import tempfile
            import os

            model_url = "https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin"
            model_path = os.path.join(tempfile.gettempdir(), "lid.176.bin")

            if not os.path.exists(model_path):
                logger.info("Downloading fastText language model...")
                urllib.request.urlretrieve(model_url, model_path)

            self.model = fasttext.load_model(model_path)

    def identify(
        self,
        text: str,
        k: int = 1
    ) -> List[Tuple[str, float]]:
        """
        Identify language of text.

        Args:
            text: Input text
            k: Number of top predictions to return

        Returns:
            List of (language_code, confidence) tuples
        """
        # Clean text for prediction
        text = text.replace("\n", " ").strip()

        if not text:
            return [("unknown", 0.0)]

        predictions = self.model.predict(text, k=k)

        results = []
        for label, score in zip(predictions[0], predictions[1]):
            # Extract language code from label (format: __label__en)
            lang_code = label.replace("__label__", "")
            results.append((lang_code, float(score)))

        return results

    def filter_by_language(
        self,
        texts: List[str],
        target_languages: List[str],
        confidence_threshold: float = 0.8
    ) -> Tuple[List[str], List[str]]:
        """
        Filter texts by target languages.

        Args:
            texts: List of texts to filter
            target_languages: Language codes to keep
            confidence_threshold: Minimum confidence to accept

        Returns:
            Tuple of (kept_texts, filtered_texts)
        """
        kept = []
        filtered = []

        for text in texts:
            predictions = self.identify(text, k=1)
            lang, confidence = predictions[0]

            if lang in target_languages and confidence >= confidence_threshold:
                kept.append(text)
            else:
                filtered.append(text)

        return kept, filtered


# Example usage
def create_multilingual_corpus(
    documents: List[dict],
    language_proportions: dict
) -> List[dict]:
    """
    Create a balanced multilingual corpus.

    Args:
        documents: List of documents
        language_proportions: Target proportions {lang: proportion}

    Returns:
        Balanced document list
    """
    identifier = LanguageIdentifier()

    # Group by language
    by_language = {}
    for doc in documents:
        lang, conf = identifier.identify(doc["text"])[0]
        if lang not in by_language:
            by_language[lang] = []
        by_language[lang].append(doc)

    # Sample according to proportions
    total_target = len(documents)
    result = []

    for lang, proportion in language_proportions.items():
        if lang in by_language:
            target_count = int(total_target * proportion)
            available = by_language[lang]
            sampled = available[:target_count]  # Or use random sampling
            result.extend(sampled)

    return result
```

### Quality Filtering

```python
"""
Quality filtering for LLM training data.
Implements heuristic and model-based filtering.
"""

from dataclasses import dataclass
from typing import List, Tuple, Optional
import re
import math


@dataclass
class QualityMetrics:
    """Quality metrics for a document."""
    word_count: int
    avg_word_length: float
    uppercase_ratio: float
    digit_ratio: float
    special_char_ratio: float
    line_count: int
    avg_line_length: float
    duplicate_line_ratio: float
    bullet_list_ratio: float
    url_density: float
    stopword_ratio: float
    quality_score: float = 0.0


class HeuristicQualityFilter:
    """
    Filter documents based on heuristic quality rules.
    Based on C4, RefinedWeb, and FineWeb filtering approaches.
    """

    # Common English stopwords for ratio calculation
    STOPWORDS = {
        "the", "a", "an", "and", "or", "but", "in", "on", "at", "to",
        "for", "of", "with", "by", "from", "as", "is", "was", "are",
        "were", "been", "be", "have", "has", "had", "do", "does", "did",
        "will", "would", "could", "should", "may", "might", "must",
        "that", "which", "who", "whom", "this", "these", "those", "it"
    }

    def __init__(
        self,
        min_words: int = 50,
        max_words: int = 100000,
        min_avg_word_length: float = 3.0,
        max_avg_word_length: float = 10.0,
        max_uppercase_ratio: float = 0.3,
        max_digit_ratio: float = 0.3,
        max_special_char_ratio: float = 0.3,
        max_duplicate_line_ratio: float = 0.3,
        max_url_density: float = 0.1,
        min_stopword_ratio: float = 0.05,
        max_stopword_ratio: float = 0.5
    ):
        self.min_words = min_words
        self.max_words = max_words
        self.min_avg_word_length = min_avg_word_length
        self.max_avg_word_length = max_avg_word_length
        self.max_uppercase_ratio = max_uppercase_ratio
        self.max_digit_ratio = max_digit_ratio
        self.max_special_char_ratio = max_special_char_ratio
        self.max_duplicate_line_ratio = max_duplicate_line_ratio
        self.max_url_density = max_url_density
        self.min_stopword_ratio = min_stopword_ratio
        self.max_stopword_ratio = max_stopword_ratio

    def compute_metrics(self, text: str) -> QualityMetrics:
        """Compute quality metrics for text."""
        words = text.split()
        word_count = len(words)

        # Word length
        avg_word_length = sum(len(w) for w in words) / max(word_count, 1)

        # Character ratios
        total_chars = len(text)
        uppercase_count = sum(1 for c in text if c.isupper())
        digit_count = sum(1 for c in text if c.isdigit())
        special_count = sum(1 for c in text if not c.isalnum() and not c.isspace())

        uppercase_ratio = uppercase_count / max(total_chars, 1)
        digit_ratio = digit_count / max(total_chars, 1)
        special_char_ratio = special_count / max(total_chars, 1)

        # Line metrics
        lines = text.split("\n")
        line_count = len(lines)
        avg_line_length = sum(len(l) for l in lines) / max(line_count, 1)

        # Duplicate lines
        unique_lines = set(l.strip() for l in lines if l.strip())
        duplicate_line_ratio = 1 - len(unique_lines) / max(len([l for l in lines if l.strip()]), 1)

        # Bullet list ratio
        bullet_lines = sum(1 for l in lines if l.strip().startswith(("•", "-", "*", "►")))
        bullet_list_ratio = bullet_lines / max(line_count, 1)

        # URL density
        url_pattern = r"https?://[^\s]+"
        urls = re.findall(url_pattern, text)
        url_density = len(urls) / max(word_count, 1)

        # Stopword ratio
        lowercase_words = [w.lower() for w in words]
        stopword_count = sum(1 for w in lowercase_words if w in self.STOPWORDS)
        stopword_ratio = stopword_count / max(word_count, 1)

        return QualityMetrics(
            word_count=word_count,
            avg_word_length=avg_word_length,
            uppercase_ratio=uppercase_ratio,
            digit_ratio=digit_ratio,
            special_char_ratio=special_char_ratio,
            line_count=line_count,
            avg_line_length=avg_line_length,
            duplicate_line_ratio=duplicate_line_ratio,
            bullet_list_ratio=bullet_list_ratio,
            url_density=url_density,
            stopword_ratio=stopword_ratio
        )

    def filter(self, text: str) -> Tuple[bool, QualityMetrics, List[str]]:
        """
        Filter document based on quality heuristics.

        Returns:
            Tuple of (passes_filter, metrics, failure_reasons)
        """
        metrics = self.compute_metrics(text)
        failures = []

        # Check each criterion
        if metrics.word_count < self.min_words:
            failures.append(f"Too few words: {metrics.word_count} < {self.min_words}")

        if metrics.word_count > self.max_words:
            failures.append(f"Too many words: {metrics.word_count} > {self.max_words}")

        if metrics.avg_word_length < self.min_avg_word_length:
            failures.append(f"Avg word length too short: {metrics.avg_word_length:.2f}")

        if metrics.avg_word_length > self.max_avg_word_length:
            failures.append(f"Avg word length too long: {metrics.avg_word_length:.2f}")

        if metrics.uppercase_ratio > self.max_uppercase_ratio:
            failures.append(f"Too many uppercase: {metrics.uppercase_ratio:.2%}")

        if metrics.digit_ratio > self.max_digit_ratio:
            failures.append(f"Too many digits: {metrics.digit_ratio:.2%}")

        if metrics.special_char_ratio > self.max_special_char_ratio:
            failures.append(f"Too many special chars: {metrics.special_char_ratio:.2%}")

        if metrics.duplicate_line_ratio > self.max_duplicate_line_ratio:
            failures.append(f"Too many duplicate lines: {metrics.duplicate_line_ratio:.2%}")

        if metrics.url_density > self.max_url_density:
            failures.append(f"Too many URLs: {metrics.url_density:.2%}")

        if metrics.stopword_ratio < self.min_stopword_ratio:
            failures.append(f"Too few stopwords (not natural text): {metrics.stopword_ratio:.2%}")

        if metrics.stopword_ratio > self.max_stopword_ratio:
            failures.append(f"Too many stopwords: {metrics.stopword_ratio:.2%}")

        passes = len(failures) == 0

        # Compute overall quality score
        if passes:
            metrics.quality_score = self._compute_quality_score(metrics)

        return passes, metrics, failures

    def _compute_quality_score(self, metrics: QualityMetrics) -> float:
        """Compute overall quality score (0-1)."""
        # Higher is better
        scores = []

        # Word count score (prefer medium-length documents)
        optimal_words = 1000
        word_score = 1 - min(abs(metrics.word_count - optimal_words) / optimal_words, 1)
        scores.append(word_score)

        # Low duplicate ratio is good
        scores.append(1 - metrics.duplicate_line_ratio)

        # Low special char ratio is good
        scores.append(1 - metrics.special_char_ratio)

        # Moderate stopword ratio is good
        optimal_stopword = 0.2
        stopword_score = 1 - min(abs(metrics.stopword_ratio - optimal_stopword) / 0.3, 1)
        scores.append(stopword_score)

        return sum(scores) / len(scores)


# Model-based quality filtering using perplexity
class PerplexityFilter:
    """
    Filter documents based on language model perplexity.
    Low perplexity = more "natural" text.
    """

    def __init__(self, model_name: str = "gpt2"):
        from transformers import GPT2LMHeadModel, GPT2Tokenizer
        import torch

        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)
        self.model = GPT2LMHeadModel.from_pretrained(model_name).to(self.device)
        self.model.eval()

    def compute_perplexity(self, text: str, max_length: int = 512) -> float:
        """
        Compute perplexity of text.

        Args:
            text: Input text
            max_length: Maximum token length to consider

        Returns:
            Perplexity score
        """
        import torch

        encodings = self.tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            max_length=max_length
        )

        input_ids = encodings.input_ids.to(self.device)

        with torch.no_grad():
            outputs = self.model(input_ids, labels=input_ids)
            loss = outputs.loss

        perplexity = math.exp(loss.item())
        return perplexity

    def filter(
        self,
        text: str,
        max_perplexity: float = 1000,
        min_perplexity: float = 1
    ) -> Tuple[bool, float]:
        """
        Filter based on perplexity threshold.

        Args:
            text: Input text
            max_perplexity: Maximum allowed perplexity
            min_perplexity: Minimum perplexity (filter gibberish)

        Returns:
            Tuple of (passes_filter, perplexity)
        """
        perplexity = self.compute_perplexity(text)
        passes = min_perplexity <= perplexity <= max_perplexity
        return passes, perplexity
```

---

## Implementation Guide

### Step-by-Step Pipeline Setup

#### Step 1: Infrastructure Setup

```bash
#!/bin/bash
# Infrastructure setup script

# 1. Create project directory structure
mkdir -p data_pipeline/{config,scripts,tests,logs}
cd data_pipeline

# 2. Set up Python environment
python -m venv .venv
source .venv/bin/activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Set up configuration
cp config/example.yaml config/production.yaml
# Edit production.yaml with your settings

# 5. Initialize databases
python scripts/init_db.py

# 6. Start services
docker-compose up -d kafka redis postgres

# 7. Verify setup
python scripts/health_check.py
```

#### Step 2: Configure Data Sources

```yaml
# config/sources.yaml
sources:
  web_crawl:
    type: web
    config:
      user_agent: "LLMDataCollector/1.0"
      rate_limit: 1.0  # requests per second
      respect_robots_txt: true
      seed_urls:
        - "https://example.com/docs"
      allowed_domains:
        - "example.com"
      url_patterns:
        include:
          - "/docs/*"
          - "/blog/*"
        exclude:
          - "/login"
          - "/signup"
    filters:
      min_content_length: 100
      languages: ["en"]

  api_source:
    type: api
    config:
      base_url: "https://api.example.com"
      auth_type: bearer
      auth_secret: "${API_SECRET}"
      rate_limit: 100  # requests per minute
      endpoints:
        - path: "/articles"
          pagination: true
          page_size: 100

  database_source:
    type: database
    config:
      connection_string: "${DB_CONNECTION_STRING}"
      query: |
        SELECT id, content, created_at
        FROM documents
        WHERE status = 'published'
      incremental_column: created_at
      batch_size: 10000

  s3_source:
    type: s3
    config:
      bucket: "training-data-raw"
      prefix: "uploads/"
      extensions: [".txt", ".pdf", ".html"]
      aws_region: "us-east-1"
```

#### Step 3: Implement Ingestion Pipeline

```python
"""
Main ingestion pipeline orchestration.
"""

import yaml
from pathlib import Path
from typing import Generator
import logging
from datetime import datetime

from sources import WebSource, APISource, DatabaseSource, S3Source
from processors import (
    SchemaValidator,
    DuplicateDetector,
    LanguageFilter,
    QualityFilter,
    PIIHandler
)
from storage import DataLakeWriter
from monitoring import MetricsCollector

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class IngestionPipeline:
    """Main data ingestion pipeline."""

    def __init__(self, config_path: str):
        with open(config_path) as f:
            self.config = yaml.safe_load(f)

        # Initialize components
        self.sources = self._init_sources()
        self.validator = SchemaValidator()
        self.deduplicator = DuplicateDetector(threshold=0.8)
        self.language_filter = LanguageFilter(languages=["en"])
        self.quality_filter = QualityFilter()
        self.pii_handler = PIIHandler()
        self.writer = DataLakeWriter(self.config["output"])
        self.metrics = MetricsCollector()

    def _init_sources(self) -> dict:
        """Initialize data sources from config."""
        sources = {}
        source_classes = {
            "web": WebSource,
            "api": APISource,
            "database": DatabaseSource,
            "s3": S3Source
        }

        for name, source_config in self.config["sources"].items():
            source_type = source_config["type"]
            source_class = source_classes.get(source_type)
            if source_class:
                sources[name] = source_class(source_config["config"])

        return sources

    def run(self, source_name: str = None) -> dict:
        """
        Run ingestion pipeline.

        Args:
            source_name: Specific source to run, or None for all

        Returns:
            Summary statistics
        """
        stats = {
            "total_extracted": 0,
            "validation_passed": 0,
            "duplicates_removed": 0,
            "language_filtered": 0,
            "quality_filtered": 0,
            "pii_handled": 0,
            "final_output": 0,
            "errors": 0
        }

        sources_to_run = (
            {source_name: self.sources[source_name]}
            if source_name
            else self.sources
        )

        for name, source in sources_to_run.items():
            logger.info(f"Processing source: {name}")

            try:
                for doc in source.extract():
                    stats["total_extracted"] += 1

                    # Step 1: Validate schema
                    is_valid, validated_doc, errors = self.validator.validate(doc)
                    if not is_valid:
                        logger.debug(f"Validation failed: {errors}")
                        continue
                    stats["validation_passed"] += 1

                    # Step 2: Check duplicates
                    is_dup, _, _ = self.deduplicator.is_duplicate(
                        validated_doc["text"],
                        validated_doc["id"]
                    )
                    if is_dup:
                        stats["duplicates_removed"] += 1
                        continue
                    self.deduplicator.add_document(
                        validated_doc["text"],
                        validated_doc["id"]
                    )

                    # Step 3: Language filter
                    if not self.language_filter.passes(validated_doc["text"]):
                        stats["language_filtered"] += 1
                        continue

                    # Step 4: Quality filter
                    passes, metrics, failures = self.quality_filter.filter(
                        validated_doc["text"]
                    )
                    if not passes:
                        stats["quality_filtered"] += 1
                        continue

                    # Step 5: PII handling
                    validated_doc = self.pii_handler.process(validated_doc)
                    if validated_doc.get("pii_anonymized"):
                        stats["pii_handled"] += 1

                    # Step 6: Write to output
                    self.writer.write(validated_doc)
                    stats["final_output"] += 1

                    # Log progress
                    if stats["final_output"] % 1000 == 0:
                        logger.info(f"Processed {stats['final_output']} documents")

            except Exception as e:
                logger.error(f"Error processing source {name}: {e}")
                stats["errors"] += 1

        # Finalize
        self.writer.flush()
        self.metrics.record(stats)

        logger.info(f"Pipeline complete: {stats}")
        return stats


# Entry point
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Run data ingestion pipeline")
    parser.add_argument("--config", default="config/production.yaml")
    parser.add_argument("--source", default=None, help="Specific source to run")
    args = parser.parse_args()

    pipeline = IngestionPipeline(args.config)
    stats = pipeline.run(args.source)

    print(f"\nPipeline Summary:")
    for key, value in stats.items():
        print(f"  {key}: {value}")
```

#### Step 4: Set Up Monitoring

```python
"""
Pipeline monitoring and alerting.
"""

from prometheus_client import Counter, Histogram, Gauge, start_http_server
import logging
from datetime import datetime

# Metrics
documents_processed = Counter(
    "ingestion_documents_processed_total",
    "Total documents processed",
    ["source", "status"]
)

processing_duration = Histogram(
    "ingestion_processing_duration_seconds",
    "Document processing duration",
    ["source"]
)

pipeline_status = Gauge(
    "ingestion_pipeline_status",
    "Pipeline status (1=running, 0=stopped)",
    ["source"]
)

quality_scores = Histogram(
    "ingestion_quality_score",
    "Document quality scores",
    ["source"],
    buckets=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
)


class MetricsCollector:
    """Collect and expose pipeline metrics."""

    def __init__(self, port: int = 9090):
        start_http_server(port)
        logging.info(f"Metrics server started on port {port}")

    def record_document(
        self,
        source: str,
        status: str,
        duration: float,
        quality_score: float = None
    ):
        """Record document processing metrics."""
        documents_processed.labels(source=source, status=status).inc()
        processing_duration.labels(source=source).observe(duration)

        if quality_score is not None:
            quality_scores.labels(source=source).observe(quality_score)

    def set_pipeline_status(self, source: str, running: bool):
        """Set pipeline running status."""
        pipeline_status.labels(source=source).set(1 if running else 0)


# Alerting rules (for Prometheus AlertManager)
ALERTING_RULES = """
groups:
  - name: ingestion_alerts
    rules:
      - alert: IngestionPipelineStopped
        expr: ingestion_pipeline_status == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Ingestion pipeline stopped"
          description: "Pipeline {{ $labels.source }} has been stopped for 5+ minutes"

      - alert: HighErrorRate
        expr: rate(ingestion_documents_processed_total{status="error"}[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High error rate in ingestion"
          description: "Error rate is {{ $value }} for source {{ $labels.source }}"

      - alert: LowThroughput
        expr: rate(ingestion_documents_processed_total{status="success"}[5m]) < 10
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "Low ingestion throughput"
          description: "Throughput is {{ $value }} docs/sec for source {{ $labels.source }}"
"""
```

---

## Configuration Templates

### Docker Compose for Local Development

```yaml
# docker-compose.yaml
version: "3.8"

services:
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    ports:
      - "9092:9092"
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      CLUSTER_ID: MkU3OEVBNTcwNTJENDM2Qk
    volumes:
      - kafka_data:/var/lib/kafka/data

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

  postgres:
    image: postgres:15-alpine
    ports:
      - "5432:5432"
    environment:
      POSTGRES_DB: ingestion
      POSTGRES_USER: ingestion
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-localdev}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql

  minio:
    image: minio/minio:latest
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data

  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: admin
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./grafana/datasources:/etc/grafana/provisioning/datasources

volumes:
  kafka_data:
  redis_data:
  postgres_data:
  minio_data:
  prometheus_data:
  grafana_data:
```

### Terraform Module for Cloud Deployment

```hcl
# terraform/main.tf

terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

variable "environment" {
  description = "Environment name"
  type        = string
  default     = "production"
}

variable "vpc_id" {
  description = "VPC ID"
  type        = string
}

# S3 Buckets for data lake
resource "aws_s3_bucket" "raw_data" {
  bucket = "llm-training-raw-${var.environment}"

  tags = {
    Environment = var.environment
    Purpose     = "LLM training data - raw"
  }
}

resource "aws_s3_bucket" "processed_data" {
  bucket = "llm-training-processed-${var.environment}"

  tags = {
    Environment = var.environment
    Purpose     = "LLM training data - processed"
  }
}

resource "aws_s3_bucket_versioning" "raw_data" {
  bucket = aws_s3_bucket.raw_data.id
  versioning_configuration {
    status = "Enabled"
  }
}

resource "aws_s3_bucket_lifecycle_configuration" "raw_data" {
  bucket = aws_s3_bucket.raw_data.id

  rule {
    id     = "archive-old-data"
    status = "Enabled"

    transition {
      days          = 90
      storage_class = "GLACIER"
    }

    expiration {
      days = 365
    }
  }
}

# MSK (Managed Kafka)
resource "aws_msk_cluster" "ingestion" {
  cluster_name           = "llm-ingestion-${var.environment}"
  kafka_version          = "3.5.1"
  number_of_broker_nodes = 3

  broker_node_group_info {
    instance_type   = "kafka.m5.large"
    client_subnets  = data.aws_subnets.private.ids
    security_groups = [aws_security_group.msk.id]

    storage_info {
      ebs_storage_info {
        volume_size = 1000
      }
    }
  }

  encryption_info {
    encryption_in_transit {
      client_broker = "TLS"
      in_cluster    = true
    }
  }

  configuration_info {
    arn      = aws_msk_configuration.ingestion.arn
    revision = aws_msk_configuration.ingestion.latest_revision
  }
}

resource "aws_msk_configuration" "ingestion" {
  name = "llm-ingestion-config-${var.environment}"

  server_properties = <<PROPERTIES
auto.create.topics.enable=true
default.replication.factor=3
min.insync.replicas=2
num.partitions=12
log.retention.hours=168
PROPERTIES
}

# ElastiCache (Redis) for deduplication
resource "aws_elasticache_cluster" "dedup" {
  cluster_id           = "llm-dedup-${var.environment}"
  engine               = "redis"
  node_type            = "cache.r6g.large"
  num_cache_nodes      = 1
  parameter_group_name = "default.redis7"
  port                 = 6379
  security_group_ids   = [aws_security_group.redis.id]
  subnet_group_name    = aws_elasticache_subnet_group.main.name
}

# RDS for metadata
resource "aws_db_instance" "metadata" {
  identifier           = "llm-metadata-${var.environment}"
  engine               = "postgres"
  engine_version       = "15.4"
  instance_class       = "db.r6g.large"
  allocated_storage    = 100
  storage_type         = "gp3"
  db_name              = "ingestion_metadata"
  username             = "admin"
  password             = var.db_password
  skip_final_snapshot  = var.environment != "production"
  vpc_security_group_ids = [aws_security_group.rds.id]
  db_subnet_group_name = aws_db_subnet_group.main.name

  backup_retention_period = var.environment == "production" ? 30 : 7
  backup_window           = "03:00-04:00"
  maintenance_window      = "Mon:04:00-Mon:05:00"

  tags = {
    Environment = var.environment
    Purpose     = "Ingestion metadata"
  }
}

# ECS for pipeline workers
resource "aws_ecs_cluster" "ingestion" {
  name = "llm-ingestion-${var.environment}"

  setting {
    name  = "containerInsights"
    value = "enabled"
  }
}

resource "aws_ecs_task_definition" "worker" {
  family                   = "ingestion-worker"
  network_mode             = "awsvpc"
  requires_compatibilities = ["FARGATE"]
  cpu                      = 2048
  memory                   = 4096
  execution_role_arn       = aws_iam_role.ecs_execution.arn
  task_role_arn            = aws_iam_role.ecs_task.arn

  container_definitions = jsonencode([
    {
      name  = "worker"
      image = "${aws_ecr_repository.ingestion.repository_url}:latest"

      environment = [
        {
          name  = "KAFKA_BROKERS"
          value = aws_msk_cluster.ingestion.bootstrap_brokers_tls
        },
        {
          name  = "REDIS_HOST"
          value = aws_elasticache_cluster.dedup.cache_nodes[0].address
        },
        {
          name  = "S3_OUTPUT_BUCKET"
          value = aws_s3_bucket.processed_data.id
        }
      ]

      logConfiguration = {
        logDriver = "awslogs"
        options = {
          "awslogs-group"         = "/ecs/ingestion-worker"
          "awslogs-region"        = data.aws_region.current.name
          "awslogs-stream-prefix" = "worker"
        }
      }
    }
  ])
}

# Auto Scaling
resource "aws_appautoscaling_target" "ecs" {
  max_capacity       = 20
  min_capacity       = 2
  resource_id        = "service/${aws_ecs_cluster.ingestion.name}/${aws_ecs_service.worker.name}"
  scalable_dimension = "ecs:service:DesiredCount"
  service_namespace  = "ecs"
}

resource "aws_appautoscaling_policy" "cpu" {
  name               = "cpu-scaling"
  policy_type        = "TargetTrackingScaling"
  resource_id        = aws_appautoscaling_target.ecs.resource_id
  scalable_dimension = aws_appautoscaling_target.ecs.scalable_dimension
  service_namespace  = aws_appautoscaling_target.ecs.service_namespace

  target_tracking_scaling_policy_configuration {
    target_value = 70

    predefined_metric_specification {
      predefined_metric_type = "ECSServiceAverageCPUUtilization"
    }
  }
}

# Outputs
output "kafka_brokers" {
  value = aws_msk_cluster.ingestion.bootstrap_brokers_tls
}

output "raw_bucket" {
  value = aws_s3_bucket.raw_data.id
}

output "processed_bucket" {
  value = aws_s3_bucket.processed_data.id
}
```

---

## Troubleshooting

### Common Issues and Solutions

#### Issue 1: Web Scraping Returns Empty Content

**Symptoms:**
- Crawler runs but returns empty or minimal text
- HTML is retrieved but content extraction fails

**Solutions:**

```python
# 1. Check if JavaScript rendering is required
from playwright.sync_api import sync_playwright

def scrape_js_rendered(url: str) -> str:
    """Scrape JavaScript-rendered pages."""
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        page.goto(url, wait_until="networkidle")
        content = page.content()
        browser.close()
        return content

# 2. Try different extraction methods
import trafilatura
from readability import Document
from bs4 import BeautifulSoup

def extract_with_fallback(html: str) -> str:
    """Try multiple extraction methods."""
    # Method 1: trafilatura (best for articles)
    text = trafilatura.extract(html)
    if text and len(text) > 100:
        return text

    # Method 2: readability
    doc = Document(html)
    text = BeautifulSoup(doc.summary(), "html.parser").get_text()
    if text and len(text) > 100:
        return text

    # Method 3: BeautifulSoup with content heuristics
    soup = BeautifulSoup(html, "html.parser")

    # Remove script/style
    for tag in soup(["script", "style", "nav", "footer", "header"]):
        tag.decompose()

    # Find main content area
    main = soup.find("main") or soup.find("article") or soup.find("body")
    if main:
        return main.get_text(separator=" ", strip=True)

    return ""
```

#### Issue 2: High Duplicate Rates

**Symptoms:**
- More than 30% of documents flagged as duplicates
- Near-duplicate detection catching dissimilar documents

**Solutions:**

```python
# 1. Tune MinHash parameters
from datasketch import MinHash, MinHashLSH

# For stricter matching (fewer false positives)
lsh_strict = MinHashLSH(
    threshold=0.9,  # Higher threshold
    num_perm=256    # More permutations for accuracy
)

# For looser matching (fewer false negatives)
lsh_loose = MinHashLSH(
    threshold=0.7,
    num_perm=128
)

# 2. Adjust shingle size
def create_shingles(text: str, k: int = 5) -> set:
    """Create k-shingles from text."""
    words = text.lower().split()

    # For short documents, use smaller shingles
    if len(words) < 50:
        k = 3

    shingles = set()
    for i in range(len(words) - k + 1):
        shingle = " ".join(words[i:i+k])
        shingles.add(shingle)

    return shingles

# 3. Pre-filter obvious non-duplicates
def quick_duplicate_check(text1: str, text2: str) -> bool:
    """Quick check before expensive MinHash comparison."""
    # Length difference check
    len_ratio = len(text1) / len(text2) if text2 else 0
    if len_ratio < 0.5 or len_ratio > 2.0:
        return False

    # Word count difference
    words1 = set(text1.lower().split())
    words2 = set(text2.lower().split())
    overlap = len(words1 & words2) / len(words1 | words2)

    if overlap < 0.3:
        return False

    return True  # Proceed with MinHash check
```

#### Issue 3: Pipeline Backlog Growing

**Symptoms:**
- Kafka consumer lag increasing
- Processing throughput decreasing

**Solutions:**

```python
# 1. Scale horizontally
# Increase consumer group size in Kubernetes
"""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ingestion-worker
spec:
  replicas: 10  # Increase from default
  ...
"""

# 2. Optimize batch processing
class BatchProcessor:
    """Process documents in efficient batches."""

    def __init__(self, batch_size: int = 100):
        self.batch_size = batch_size
        self.batch = []

    def add(self, doc: dict) -> list:
        """Add document, return batch if full."""
        self.batch.append(doc)

        if len(self.batch) >= self.batch_size:
            batch = self.batch
            self.batch = []
            return batch

        return []

    def process_batch(self, batch: list) -> list:
        """Process batch with vectorized operations."""
        # Vectorize text processing
        texts = [d["text"] for d in batch]

        # Batch language detection
        languages = self.batch_detect_language(texts)

        # Batch quality scoring
        scores = self.batch_quality_score(texts)

        # Filter and return
        results = []
        for doc, lang, score in zip(batch, languages, scores):
            if lang == "en" and score > 0.5:
                doc["language"] = lang
                doc["quality_score"] = score
                results.append(doc)

        return results

# 3. Use async processing
import asyncio
from concurrent.futures import ProcessPoolExecutor

async def process_documents_parallel(docs: list, workers: int = 4):
    """Process documents in parallel."""
    loop = asyncio.get_event_loop()

    with ProcessPoolExecutor(max_workers=workers) as executor:
        tasks = [
            loop.run_in_executor(executor, process_single, doc)
            for doc in docs
        ]
        results = await asyncio.gather(*tasks)

    return [r for r in results if r is not None]
```

#### Issue 4: Memory Issues with Large Documents

**Symptoms:**
- Out of memory errors
- Process killed by OOM killer

**Solutions:**

```python
# 1. Stream large files
def process_large_file(file_path: str, chunk_size: int = 10000):
    """Process large file in chunks."""
    with open(file_path, "r") as f:
        chunk = []
        for line in f:
            chunk.append(line)
            if len(chunk) >= chunk_size:
                yield chunk
                chunk = []

        if chunk:
            yield chunk

# 2. Use memory-efficient data structures
import mmap

def count_lines_mmap(file_path: str) -> int:
    """Count lines using memory-mapped file."""
    with open(file_path, "r") as f:
        mm = mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)
        return mm.read().count(b"\n")

# 3. Limit document size at ingestion
MAX_DOC_SIZE = 1_000_000  # 1MB

def validate_size(content: bytes) -> bool:
    """Check document size."""
    if len(content) > MAX_DOC_SIZE:
        logger.warning(f"Document too large: {len(content)} bytes")
        return False
    return True
```

---

## Appendices

### Appendix A: Data Source Evaluation Rubric

| Criterion | Weight | Score 1 (Poor) | Score 3 (Good) | Score 5 (Excellent) |
|-----------|--------|----------------|----------------|---------------------|
| **Data Quality** | 25% | Noisy, many errors | Some quality issues | Clean, well-structured |
| **Relevance** | 25% | Low domain match | Partial domain match | High domain relevance |
| **Volume** | 15% | < 1GB | 1-100GB | > 100GB |
| **Freshness** | 15% | > 2 years old | 6 months - 2 years | < 6 months old |
| **Legal Clarity** | 10% | Unclear licensing | Standard license | Open license/owned |
| **Acquisition Cost** | 10% | High cost | Moderate cost | Free/low cost |

### Appendix B: Sample Data Partnership Agreement Template

```markdown
# Data Partnership Agreement

## Parties
- Data Provider: [Company Name]
- Data Consumer: [Your Company]

## Data Description
- Dataset Name: [Name]
- Data Type: [Text/Image/Structured]
- Estimated Size: [Volume]
- Update Frequency: [Frequency]

## Usage Rights
- [ ] Training machine learning models
- [ ] Internal research
- [ ] Commercial deployment
- [ ] Redistribution allowed
- [ ] Derivative works allowed

## Restrictions
- [ ] No reverse engineering of source identity
- [ ] No PII extraction
- [ ] Attribution required
- [ ] Geographic restrictions: [Regions]

## Data Handling
- Retention Period: [Duration]
- Deletion Requirements: [Requirements]
- Security Requirements: [Standards]

## Compensation
- Type: [One-time/Recurring/Revenue Share]
- Amount: [Amount]
- Payment Terms: [Terms]

## Term and Termination
- Start Date: [Date]
- End Date: [Date]
- Termination Notice: [Days]

Signatures:
___________________ Date: ______
[Data Provider Representative]

___________________ Date: ______
[Data Consumer Representative]
```

### Appendix C: Web Scraping Code Templates

See the code examples section for complete implementations:
- `RespectfulCrawler` class for rate-limited, robots.txt-compliant scraping
- `crawl_for_llm()` function using Crawl4AI
- Scrapy spider template for large-scale crawling

### Appendix D: Data Ingestion Pipeline Terraform Modules

See Configuration Templates section for complete Terraform configurations including:
- S3 buckets with lifecycle policies
- MSK (Managed Kafka) cluster
- ElastiCache Redis cluster
- RDS PostgreSQL for metadata
- ECS Fargate for workers
- Auto-scaling configuration

---

## Glossary

| Term | Definition |
|------|------------|
| **CDC** | Change Data Capture - technique for tracking database changes in real-time |
| **Common Crawl** | Nonprofit organization providing free web crawl data |
| **Deduplication** | Process of identifying and removing duplicate content |
| **Exactly-once semantics** | Processing guarantee where each message is processed exactly one time |
| **Idempotency** | Property where an operation produces same result regardless of repetition |
| **MinHash** | Algorithm for estimating Jaccard similarity between sets |
| **LSH** | Locality-Sensitive Hashing - technique for finding similar items efficiently |
| **PII** | Personally Identifiable Information |
| **Provenance** | Record of data origin and transformation history |
| **robots.txt** | File that tells web crawlers which pages to access |
| **Schema validation** | Process of verifying data conforms to expected structure |
| **Shingle** | Contiguous subsequence of tokens (n-gram) |
| **Streaming ingestion** | Real-time data ingestion as events occur |
| **Web scraping** | Automated extraction of data from websites |

---

## References

### Research Papers
1. Penedo, G., et al. (2023). "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"
2. Soldaini, L., et al. (2024). "Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research"
3. Longpre, S., et al. (2023). "A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity"

### Tools and Frameworks
- [Crawl4AI](https://github.com/unclecode/crawl4ai) - LLM-friendly web crawler
- [trafilatura](https://github.com/adbar/trafilatura) - Web content extraction
- [datasketch](https://github.com/ekzhu/datasketch) - MinHash implementation
- [Presidio](https://github.com/microsoft/presidio) - PII detection and anonymization
- [Apache Kafka](https://kafka.apache.org/) - Distributed streaming platform
- [NVIDIA NeMo Curator](https://github.com/NVIDIA/NeMo-Curator) - GPU-accelerated data curation

### External Resources
- [Common Crawl](https://commoncrawl.org/) - Web crawl archive
- [HuggingFace Datasets](https://huggingface.co/datasets) - Dataset repository
- [Data Provenance Initiative](https://www.dataprovenance.org/) - Training data documentation

---

> **Navigation**
> [← README](../README.md) | **[Index](../README.md#15-repository-structure)** | [1.2 Data Cleaning →](1.2_data_cleaning_preprocessing.md)

---

*Document Version: 1.0 | Last Updated: December 2025 | Next Review: March 2026*
