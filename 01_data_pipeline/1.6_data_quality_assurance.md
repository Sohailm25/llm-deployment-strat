# Document 1.6: Data Quality Assurance Framework

## Document Information
- **Version:** 1.0
- **Last Updated:** December 2025
- **Owner:** Data Engineering / MLOps Team
- **Category:** Data Pipeline & Preparation

---

## Executive Summary

Data quality is the foundation of successful ML systems. Poor data quality leads to degraded model performance, unreliable predictions, and expensive debugging cycles. This framework provides a systematic approach to defining, measuring, monitoring, and remediating data quality issues throughout the ML lifecycle. It covers automated validation, statistical monitoring, anomaly detection, and remediation workflows.

**Key Outcomes:**
- Establish measurable data quality dimensions and KPIs
- Implement automated quality gates in data pipelines
- Enable proactive anomaly detection and alerting
- Reduce time-to-resolution for data quality incidents

---

## Prerequisites

### Required Knowledge
- Data pipeline architecture (ETL/ELT concepts)
- SQL and Python proficiency
- Statistical concepts (distributions, hypothesis testing)
- Monitoring and observability basics

### Infrastructure Requirements
- Data warehouse or data lake
- Pipeline orchestrator (Airflow, Prefect, Dagster)
- Monitoring platform (Prometheus, Datadog, or similar)
- Dashboard tool (Grafana, Looker, or similar)

### Tool Installation
```bash
# Great Expectations (primary DQ framework)
pip install great-expectations

# Statistical monitoring
pip install whylogs evidently scipy

# Additional utilities
pip install pandera pydantic

# Alerting
pip install slack-sdk pagerduty

# Database connectivity
pip install sqlalchemy psycopg2-binary
```

---

## 1. Data Quality Dimensions

### 1.1 Core Quality Dimensions

```
┌─────────────────────────────────────────────────────────────┐
│              Data Quality Dimensions                         │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  INTRINSIC DIMENSIONS (Data itself)                         │
│  ┌───────────────────────────────────────────────────────┐  │
│  │ Accuracy     - Data reflects reality correctly        │  │
│  │ Completeness - Required data is present               │  │
│  │ Consistency  - Data agrees across sources/time        │  │
│  │ Uniqueness   - No unwanted duplicates                 │  │
│  │ Validity     - Data conforms to format/rules          │  │
│  └───────────────────────────────────────────────────────┘  │
│                                                              │
│  CONTEXTUAL DIMENSIONS (Data for use case)                  │
│  ┌───────────────────────────────────────────────────────┐  │
│  │ Timeliness   - Data is fresh enough for use           │  │
│  │ Relevance    - Data is applicable to the task         │  │
│  │ Reliability  - Data source is trustworthy             │  │
│  │ Usability    - Data is easy to work with              │  │
│  └───────────────────────────────────────────────────────┘  │
│                                                              │
│  ML-SPECIFIC DIMENSIONS                                      │
│  ┌───────────────────────────────────────────────────────┐  │
│  │ Distribution - Data distribution matches expectations │  │
│  │ Coverage     - Edge cases and minorities represented  │  │
│  │ Balance      - Class/category proportions appropriate │  │
│  │ Leakage      - No target leakage in features          │  │
│  └───────────────────────────────────────────────────────┘  │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### 1.2 Dimension Definitions and Metrics

```python
from dataclasses import dataclass
from typing import List, Dict, Optional, Callable
from enum import Enum

class QualityDimension(Enum):
    ACCURACY = "accuracy"
    COMPLETENESS = "completeness"
    CONSISTENCY = "consistency"
    UNIQUENESS = "uniqueness"
    VALIDITY = "validity"
    TIMELINESS = "timeliness"
    DISTRIBUTION = "distribution"
    COVERAGE = "coverage"

@dataclass
class DimensionSpec:
    """Specification for a data quality dimension"""
    dimension: QualityDimension
    description: str
    metrics: List[str]
    target_threshold: float
    critical_threshold: float
    measurement_sql: Optional[str] = None
    measurement_func: Optional[Callable] = None

# Define dimension specifications
DIMENSION_SPECS = {
    QualityDimension.ACCURACY: DimensionSpec(
        dimension=QualityDimension.ACCURACY,
        description="Data values correctly represent the real-world entity or event",
        metrics=["error_rate", "precision_vs_source", "validation_pass_rate"],
        target_threshold=0.99,
        critical_threshold=0.95
    ),
    QualityDimension.COMPLETENESS: DimensionSpec(
        dimension=QualityDimension.COMPLETENESS,
        description="All required data fields are populated with non-null values",
        metrics=["null_rate", "missing_rate", "population_rate"],
        target_threshold=0.98,
        critical_threshold=0.90,
        measurement_sql="""
            SELECT
                column_name,
                COUNT(*) - COUNT({column}) as null_count,
                1.0 - (COUNT(*) - COUNT({column})) / COUNT(*) as completeness
            FROM {table}
            GROUP BY 1
        """
    ),
    QualityDimension.CONSISTENCY: DimensionSpec(
        dimension=QualityDimension.CONSISTENCY,
        description="Data is consistent across sources, time periods, and related records",
        metrics=["cross_source_match_rate", "temporal_consistency", "referential_integrity"],
        target_threshold=0.99,
        critical_threshold=0.95
    ),
    QualityDimension.UNIQUENESS: DimensionSpec(
        dimension=QualityDimension.UNIQUENESS,
        description="Records are unique based on defined key columns",
        metrics=["duplicate_rate", "unique_key_violations"],
        target_threshold=1.0,
        critical_threshold=0.999,
        measurement_sql="""
            SELECT
                COUNT(*) as total_rows,
                COUNT(DISTINCT {key_columns}) as unique_rows,
                1.0 - COUNT(DISTINCT {key_columns}) / COUNT(*) as duplicate_rate
            FROM {table}
        """
    ),
    QualityDimension.VALIDITY: DimensionSpec(
        dimension=QualityDimension.VALIDITY,
        description="Data conforms to defined formats, types, and business rules",
        metrics=["format_compliance", "range_compliance", "pattern_compliance"],
        target_threshold=1.0,
        critical_threshold=0.98
    ),
    QualityDimension.TIMELINESS: DimensionSpec(
        dimension=QualityDimension.TIMELINESS,
        description="Data is available when needed and reflects current state",
        metrics=["freshness_hours", "lag_from_source", "update_frequency"],
        target_threshold=1.0,  # 1 hour or less
        critical_threshold=24.0  # 24 hours max
    ),
    QualityDimension.DISTRIBUTION: DimensionSpec(
        dimension=QualityDimension.DISTRIBUTION,
        description="Data distribution is stable and matches expected patterns",
        metrics=["distribution_drift", "statistical_distance", "outlier_rate"],
        target_threshold=0.95,  # 95% similarity to baseline
        critical_threshold=0.80
    ),
    QualityDimension.COVERAGE: DimensionSpec(
        dimension=QualityDimension.COVERAGE,
        description="Data adequately represents all relevant categories and edge cases",
        metrics=["category_coverage", "minority_representation", "edge_case_count"],
        target_threshold=0.90,
        critical_threshold=0.70
    )
}
```

---

## 2. Quality Metrics & KPIs

### 2.1 Metric Catalog

```python
from typing import Any, Dict, List, Union
import pandas as pd
import numpy as np
from scipy import stats

class QualityMetricCalculator:
    """Calculate data quality metrics"""

    def __init__(self, df: pd.DataFrame):
        self.df = df

    # =====================
    # COMPLETENESS METRICS
    # =====================

    def null_rate(self, column: str) -> float:
        """Percentage of null values in a column"""
        return self.df[column].isna().mean()

    def completeness_score(self, columns: List[str] = None) -> float:
        """Overall completeness across specified columns"""
        if columns is None:
            columns = self.df.columns.tolist()
        return 1 - self.df[columns].isna().mean().mean()

    def row_completeness(self, required_columns: List[str]) -> float:
        """Percentage of rows with all required fields populated"""
        complete_rows = self.df[required_columns].dropna().shape[0]
        return complete_rows / len(self.df)

    # =====================
    # UNIQUENESS METRICS
    # =====================

    def duplicate_rate(self, key_columns: List[str]) -> float:
        """Percentage of duplicate rows based on key columns"""
        total = len(self.df)
        unique = self.df[key_columns].drop_duplicates().shape[0]
        return 1 - (unique / total) if total > 0 else 0

    def exact_duplicate_rate(self) -> float:
        """Percentage of exact duplicate rows (all columns)"""
        total = len(self.df)
        unique = self.df.drop_duplicates().shape[0]
        return 1 - (unique / total) if total > 0 else 0

    # =====================
    # VALIDITY METRICS
    # =====================

    def pattern_compliance(self, column: str, pattern: str) -> float:
        """Percentage of values matching regex pattern"""
        return self.df[column].str.match(pattern, na=False).mean()

    def range_compliance(
        self,
        column: str,
        min_val: float = None,
        max_val: float = None
    ) -> float:
        """Percentage of values within specified range"""
        series = self.df[column].dropna()
        if len(series) == 0:
            return 1.0

        in_range = pd.Series([True] * len(series))
        if min_val is not None:
            in_range &= series >= min_val
        if max_val is not None:
            in_range &= series <= max_val

        return in_range.mean()

    def type_compliance(self, column: str, expected_type: type) -> float:
        """Percentage of values that are of expected type"""
        def check_type(x):
            if pd.isna(x):
                return True  # Nulls handled separately
            try:
                expected_type(x)
                return True
            except (ValueError, TypeError):
                return False

        return self.df[column].apply(check_type).mean()

    def categorical_compliance(self, column: str, valid_values: List[Any]) -> float:
        """Percentage of values in allowed category list"""
        non_null = self.df[column].dropna()
        if len(non_null) == 0:
            return 1.0
        return non_null.isin(valid_values).mean()

    # =====================
    # DISTRIBUTION METRICS
    # =====================

    def distribution_drift(
        self,
        column: str,
        baseline: pd.Series,
        method: str = "ks"
    ) -> float:
        """Measure distribution drift from baseline"""
        current = self.df[column].dropna()

        if method == "ks":
            # Kolmogorov-Smirnov test
            statistic, p_value = stats.ks_2samp(baseline, current)
            return statistic
        elif method == "psi":
            # Population Stability Index
            return self._calculate_psi(baseline, current)
        elif method == "js":
            # Jensen-Shannon divergence
            return self._calculate_js_divergence(baseline, current)
        else:
            raise ValueError(f"Unknown method: {method}")

    def _calculate_psi(
        self,
        baseline: pd.Series,
        current: pd.Series,
        buckets: int = 10
    ) -> float:
        """Calculate Population Stability Index"""
        # Create buckets from baseline
        _, bins = pd.qcut(baseline, buckets, retbins=True, duplicates='drop')

        # Assign current values to buckets
        baseline_counts = pd.cut(baseline, bins=bins, include_lowest=True).value_counts(normalize=True)
        current_counts = pd.cut(current, bins=bins, include_lowest=True).value_counts(normalize=True)

        # Align indices
        baseline_counts = baseline_counts.reindex(baseline_counts.index.union(current_counts.index), fill_value=0.0001)
        current_counts = current_counts.reindex(baseline_counts.index, fill_value=0.0001)

        # Calculate PSI
        psi = np.sum((current_counts - baseline_counts) * np.log(current_counts / baseline_counts))
        return psi

    def _calculate_js_divergence(
        self,
        baseline: pd.Series,
        current: pd.Series,
        bins: int = 50
    ) -> float:
        """Calculate Jensen-Shannon divergence"""
        from scipy.spatial.distance import jensenshannon

        # Create histograms
        all_data = pd.concat([baseline, current])
        bin_edges = np.histogram_bin_edges(all_data, bins=bins)

        baseline_hist, _ = np.histogram(baseline, bins=bin_edges, density=True)
        current_hist, _ = np.histogram(current, bins=bin_edges, density=True)

        # Add small value to avoid log(0)
        baseline_hist = baseline_hist + 1e-10
        current_hist = current_hist + 1e-10

        return jensenshannon(baseline_hist, current_hist)

    def outlier_rate(
        self,
        column: str,
        method: str = "iqr",
        threshold: float = 1.5
    ) -> float:
        """Percentage of outliers in a numeric column"""
        series = self.df[column].dropna()

        if method == "iqr":
            q1 = series.quantile(0.25)
            q3 = series.quantile(0.75)
            iqr = q3 - q1
            lower = q1 - threshold * iqr
            upper = q3 + threshold * iqr
            outliers = (series < lower) | (series > upper)
        elif method == "zscore":
            z_scores = np.abs(stats.zscore(series))
            outliers = z_scores > threshold
        else:
            raise ValueError(f"Unknown method: {method}")

        return outliers.mean()

    # =====================
    # CONSISTENCY METRICS
    # =====================

    def referential_integrity(
        self,
        foreign_key: str,
        reference_df: pd.DataFrame,
        primary_key: str
    ) -> float:
        """Percentage of foreign keys that exist in reference table"""
        fk_values = self.df[foreign_key].dropna()
        pk_values = set(reference_df[primary_key].dropna())

        valid_refs = fk_values.isin(pk_values).mean()
        return valid_refs

    def cross_field_consistency(
        self,
        rule: str  # e.g., "start_date <= end_date"
    ) -> float:
        """Percentage of rows satisfying cross-field rule"""
        return self.df.eval(rule).mean()

    # =====================
    # COVERAGE METRICS
    # =====================

    def category_coverage(
        self,
        column: str,
        expected_categories: List[Any]
    ) -> float:
        """Percentage of expected categories present in data"""
        present = set(self.df[column].dropna().unique())
        expected = set(expected_categories)
        return len(present & expected) / len(expected) if expected else 1.0

    def minority_representation(
        self,
        column: str,
        min_threshold: float = 0.01
    ) -> Dict[str, float]:
        """Check if minority categories meet minimum threshold"""
        value_counts = self.df[column].value_counts(normalize=True)

        representation = {}
        for category, proportion in value_counts.items():
            representation[str(category)] = {
                "proportion": proportion,
                "meets_threshold": proportion >= min_threshold
            }

        return representation

    # =====================
    # AGGREGATE SCORE
    # =====================

    def calculate_overall_score(
        self,
        metrics: Dict[str, float],
        weights: Dict[str, float] = None
    ) -> float:
        """Calculate weighted overall quality score"""
        if weights is None:
            weights = {k: 1.0 for k in metrics}

        total_weight = sum(weights.get(k, 1.0) for k in metrics)
        weighted_sum = sum(v * weights.get(k, 1.0) for k, v in metrics.items())

        return weighted_sum / total_weight

# Usage example
df = pd.read_parquet("training_data.parquet")
calculator = QualityMetricCalculator(df)

metrics = {
    "completeness": calculator.completeness_score(["text", "label"]),
    "uniqueness": 1 - calculator.duplicate_rate(["id"]),
    "validity_text": calculator.pattern_compliance("text", r".{10,}"),  # Min 10 chars
    "validity_label": calculator.categorical_compliance("label", ["positive", "negative", "neutral"]),
    "outlier_rate": 1 - calculator.outlier_rate("quality_score"),
}

overall = calculator.calculate_overall_score(metrics)
print(f"Overall Quality Score: {overall:.2%}")
```

### 2.2 KPI Dashboard Specifications

```python
from dataclasses import dataclass
from datetime import datetime
from typing import List, Dict

@dataclass
class QualityKPI:
    """Data quality KPI specification"""
    name: str
    dimension: QualityDimension
    metric: str
    target: float
    warning_threshold: float
    critical_threshold: float
    unit: str
    refresh_frequency: str

# Define KPIs for an LLM training data pipeline
LLM_DATA_KPIS = [
    QualityKPI(
        name="Text Completeness",
        dimension=QualityDimension.COMPLETENESS,
        metric="1 - null_rate('text')",
        target=1.0,
        warning_threshold=0.99,
        critical_threshold=0.95,
        unit="percentage",
        refresh_frequency="hourly"
    ),
    QualityKPI(
        name="Duplicate Rate",
        dimension=QualityDimension.UNIQUENESS,
        metric="duplicate_rate(['text'])",
        target=0.0,
        warning_threshold=0.01,
        critical_threshold=0.05,
        unit="percentage",
        refresh_frequency="daily"
    ),
    QualityKPI(
        name="Label Validity",
        dimension=QualityDimension.VALIDITY,
        metric="categorical_compliance('label', valid_labels)",
        target=1.0,
        warning_threshold=0.99,
        critical_threshold=0.95,
        unit="percentage",
        refresh_frequency="hourly"
    ),
    QualityKPI(
        name="Data Freshness",
        dimension=QualityDimension.TIMELINESS,
        metric="max_age_hours",
        target=1.0,
        warning_threshold=6.0,
        critical_threshold=24.0,
        unit="hours",
        refresh_frequency="hourly"
    ),
    QualityKPI(
        name="Distribution Stability",
        dimension=QualityDimension.DISTRIBUTION,
        metric="1 - distribution_drift('text_length', baseline)",
        target=0.95,
        warning_threshold=0.90,
        critical_threshold=0.80,
        unit="percentage",
        refresh_frequency="daily"
    ),
    QualityKPI(
        name="Category Coverage",
        dimension=QualityDimension.COVERAGE,
        metric="category_coverage('domain', expected_domains)",
        target=1.0,
        warning_threshold=0.90,
        critical_threshold=0.70,
        unit="percentage",
        refresh_frequency="daily"
    ),
    QualityKPI(
        name="Minority Representation",
        dimension=QualityDimension.COVERAGE,
        metric="min(category_proportions)",
        target=0.05,
        warning_threshold=0.02,
        critical_threshold=0.01,
        unit="percentage",
        refresh_frequency="daily"
    ),
    QualityKPI(
        name="Schema Compliance",
        dimension=QualityDimension.VALIDITY,
        metric="schema_validation_pass_rate",
        target=1.0,
        warning_threshold=0.999,
        critical_threshold=0.99,
        unit="percentage",
        refresh_frequency="per_batch"
    )
]

class KPITracker:
    """Track and report on data quality KPIs"""

    def __init__(self, kpis: List[QualityKPI]):
        self.kpis = {kpi.name: kpi for kpi in kpis}
        self.history: Dict[str, List[Dict]] = {kpi.name: [] for kpi in kpis}

    def record(self, kpi_name: str, value: float):
        """Record a KPI measurement"""
        kpi = self.kpis[kpi_name]
        status = self._get_status(kpi, value)

        self.history[kpi_name].append({
            "timestamp": datetime.utcnow().isoformat(),
            "value": value,
            "status": status,
            "target": kpi.target
        })

    def _get_status(self, kpi: QualityKPI, value: float) -> str:
        """Determine KPI status based on thresholds"""
        # For metrics where higher is better
        if kpi.target >= kpi.critical_threshold:
            if value >= kpi.target:
                return "healthy"
            elif value >= kpi.warning_threshold:
                return "warning"
            elif value >= kpi.critical_threshold:
                return "critical"
            else:
                return "failure"
        # For metrics where lower is better (e.g., duplicate rate)
        else:
            if value <= kpi.target:
                return "healthy"
            elif value <= kpi.warning_threshold:
                return "warning"
            elif value <= kpi.critical_threshold:
                return "critical"
            else:
                return "failure"

    def get_dashboard_data(self) -> Dict:
        """Get data for dashboard display"""
        dashboard = {
            "timestamp": datetime.utcnow().isoformat(),
            "overall_status": "healthy",
            "kpis": []
        }

        statuses = []

        for name, kpi in self.kpis.items():
            if self.history[name]:
                latest = self.history[name][-1]
                statuses.append(latest["status"])

                dashboard["kpis"].append({
                    "name": name,
                    "dimension": kpi.dimension.value,
                    "current_value": latest["value"],
                    "target": kpi.target,
                    "status": latest["status"],
                    "unit": kpi.unit,
                    "trend": self._calculate_trend(name)
                })

        # Overall status is worst status
        if "failure" in statuses:
            dashboard["overall_status"] = "failure"
        elif "critical" in statuses:
            dashboard["overall_status"] = "critical"
        elif "warning" in statuses:
            dashboard["overall_status"] = "warning"

        return dashboard

    def _calculate_trend(self, kpi_name: str, lookback: int = 7) -> str:
        """Calculate trend over recent measurements"""
        history = self.history[kpi_name][-lookback:]

        if len(history) < 2:
            return "stable"

        values = [h["value"] for h in history]
        trend = (values[-1] - values[0]) / len(values)

        if trend > 0.01:
            return "improving"
        elif trend < -0.01:
            return "degrading"
        else:
            return "stable"
```

---

## 3. Automated Quality Checks

### 3.1 Great Expectations Implementation

```python
import great_expectations as gx
from great_expectations.core.expectation_configuration import ExpectationConfiguration
from great_expectations.checkpoint import Checkpoint

class DataQualityValidator:
    """Implement data quality checks using Great Expectations"""

    def __init__(self, context_root: str = "gx"):
        self.context = gx.get_context(context_root_dir=context_root)

    def create_expectation_suite(
        self,
        suite_name: str,
        expectations: List[Dict]
    ):
        """Create an expectation suite from specifications"""

        suite = self.context.add_expectation_suite(
            expectation_suite_name=suite_name
        )

        for exp_config in expectations:
            expectation = ExpectationConfiguration(
                expectation_type=exp_config["type"],
                kwargs=exp_config.get("kwargs", {}),
                meta=exp_config.get("meta", {})
            )
            suite.add_expectation(expectation)

        self.context.save_expectation_suite(suite)

        return suite

    def validate_batch(
        self,
        data_path: str,
        suite_name: str
    ) -> Dict:
        """Validate a data batch against expectation suite"""

        # Create batch request
        batch_request = {
            "datasource_name": "my_datasource",
            "data_connector_name": "default_inferred_data_connector_name",
            "data_asset_name": data_path,
        }

        # Get validator
        validator = self.context.get_validator(
            batch_request=batch_request,
            expectation_suite_name=suite_name
        )

        # Run validation
        results = validator.validate()

        return {
            "success": results.success,
            "statistics": results.statistics,
            "results": [
                {
                    "expectation": r.expectation_config.expectation_type,
                    "success": r.success,
                    "observed_value": r.result.get("observed_value"),
                    "details": r.result
                }
                for r in results.results
            ]
        }

# Define expectations for LLM training data
LLM_TRAINING_DATA_EXPECTATIONS = [
    # Completeness
    {
        "type": "expect_column_values_to_not_be_null",
        "kwargs": {"column": "text"},
        "meta": {"dimension": "completeness", "severity": "critical"}
    },
    {
        "type": "expect_column_values_to_not_be_null",
        "kwargs": {"column": "id"},
        "meta": {"dimension": "completeness", "severity": "critical"}
    },

    # Uniqueness
    {
        "type": "expect_column_values_to_be_unique",
        "kwargs": {"column": "id"},
        "meta": {"dimension": "uniqueness", "severity": "critical"}
    },

    # Validity - Text length
    {
        "type": "expect_column_value_lengths_to_be_between",
        "kwargs": {
            "column": "text",
            "min_value": 10,
            "max_value": 100000
        },
        "meta": {"dimension": "validity", "severity": "warning"}
    },

    # Validity - Label values
    {
        "type": "expect_column_values_to_be_in_set",
        "kwargs": {
            "column": "label",
            "value_set": ["positive", "negative", "neutral"]
        },
        "meta": {"dimension": "validity", "severity": "critical"}
    },

    # Validity - Quality score range
    {
        "type": "expect_column_values_to_be_between",
        "kwargs": {
            "column": "quality_score",
            "min_value": 0.0,
            "max_value": 1.0
        },
        "meta": {"dimension": "validity", "severity": "warning"}
    },

    # Distribution - Quality score mean
    {
        "type": "expect_column_mean_to_be_between",
        "kwargs": {
            "column": "quality_score",
            "min_value": 0.6,
            "max_value": 0.95
        },
        "meta": {"dimension": "distribution", "severity": "warning"}
    },

    # Completeness - Table level
    {
        "type": "expect_table_row_count_to_be_between",
        "kwargs": {
            "min_value": 1000,
            "max_value": 100000000
        },
        "meta": {"dimension": "completeness", "severity": "critical"}
    },

    # Consistency - No empty strings
    {
        "type": "expect_column_values_to_match_regex",
        "kwargs": {
            "column": "text",
            "regex": r"^(?!\s*$).+",  # Not empty or whitespace-only
            "mostly": 0.99
        },
        "meta": {"dimension": "validity", "severity": "warning"}
    }
]

# Usage
validator = DataQualityValidator()
validator.create_expectation_suite("llm_training_data", LLM_TRAINING_DATA_EXPECTATIONS)

results = validator.validate_batch("data/training_batch_001.parquet", "llm_training_data")

if not results["success"]:
    failed = [r for r in results["results"] if not r["success"]]
    print(f"Validation failed! {len(failed)} expectations not met:")
    for f in failed:
        print(f"  - {f['expectation']}: {f['observed_value']}")
```

### 3.2 Schema Validation with Pandera

```python
import pandera as pa
from pandera import Column, Check, DataFrameSchema
from pandera.typing import Series
import pandas as pd
from typing import Optional

# Define schema for LLM training data
LLMTrainingDataSchema = DataFrameSchema(
    columns={
        "id": Column(
            str,
            checks=[
                Check.str_matches(r"^[a-zA-Z0-9_-]+$"),
                Check.str_length(min_value=1, max_value=128)
            ],
            unique=True,
            nullable=False
        ),
        "text": Column(
            str,
            checks=[
                Check.str_length(min_value=10, max_value=100000),
                Check(lambda s: s.str.strip().str.len() > 0, error="Text cannot be whitespace only")
            ],
            nullable=False
        ),
        "label": Column(
            str,
            checks=[
                Check.isin(["positive", "negative", "neutral"])
            ],
            nullable=True
        ),
        "quality_score": Column(
            float,
            checks=[
                Check.between(0.0, 1.0)
            ],
            nullable=True
        ),
        "source": Column(
            str,
            checks=[
                Check.isin(["web", "books", "wiki", "code", "other"])
            ],
            nullable=False
        ),
        "created_at": Column(
            "datetime64[ns]",
            nullable=False
        ),
        "token_count": Column(
            int,
            checks=[
                Check.greater_than(0),
                Check.less_than(200000)  # Max context length
            ],
            nullable=False
        )
    },
    checks=[
        # Cross-column validation
        Check(
            lambda df: df["token_count"] <= df["text"].str.len() * 0.5,
            error="Token count seems too high relative to text length"
        )
    ],
    index=None,
    strict="filter",  # Drop columns not in schema
    coerce=True,  # Coerce to specified types
)

class SchemaValidator:
    """Validate data against schema definitions"""

    def __init__(self, schema: DataFrameSchema):
        self.schema = schema

    def validate(self, df: pd.DataFrame) -> Dict:
        """Validate DataFrame and return results"""

        try:
            validated_df = self.schema.validate(df, lazy=True)

            return {
                "success": True,
                "valid_rows": len(validated_df),
                "total_rows": len(df),
                "errors": []
            }

        except pa.errors.SchemaErrors as e:
            errors = []
            for error in e.schema_errors:
                errors.append({
                    "check": str(error.check),
                    "column": error.schema.name if error.schema else None,
                    "failure_count": error.failure_cases.shape[0] if hasattr(error, 'failure_cases') else 1,
                    "samples": error.failure_cases.head().to_dict() if hasattr(error, 'failure_cases') else None
                })

            return {
                "success": False,
                "valid_rows": 0,
                "total_rows": len(df),
                "errors": errors
            }

    def validate_streaming(
        self,
        df_iterator,
        batch_size: int = 10000
    ):
        """Validate data in streaming fashion"""

        batch_results = []

        for batch_num, batch_df in enumerate(df_iterator):
            result = self.validate(batch_df)
            result["batch_num"] = batch_num
            batch_results.append(result)

            if not result["success"]:
                yield {
                    "batch": batch_num,
                    "status": "failed",
                    "errors": result["errors"]
                }
            else:
                yield {
                    "batch": batch_num,
                    "status": "passed",
                    "rows_validated": result["valid_rows"]
                }

# Usage
validator = SchemaValidator(LLMTrainingDataSchema)

# Validate a batch
df = pd.read_parquet("training_data.parquet")
result = validator.validate(df)

if not result["success"]:
    print(f"Schema validation failed with {len(result['errors'])} error types")
    for error in result["errors"]:
        print(f"  - {error['column']}: {error['check']} ({error['failure_count']} failures)")
```

### 3.3 Custom Validation Framework

```python
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import List, Dict, Any, Callable, Optional
from enum import Enum

class Severity(Enum):
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"

@dataclass
class ValidationResult:
    rule_name: str
    passed: bool
    severity: Severity
    message: str
    details: Dict[str, Any]

class ValidationRule(ABC):
    """Base class for validation rules"""

    def __init__(
        self,
        name: str,
        severity: Severity = Severity.ERROR,
        description: str = ""
    ):
        self.name = name
        self.severity = severity
        self.description = description

    @abstractmethod
    def validate(self, data: pd.DataFrame) -> ValidationResult:
        pass

class ColumnNotNullRule(ValidationRule):
    """Validate column has no null values"""

    def __init__(
        self,
        column: str,
        threshold: float = 1.0,
        **kwargs
    ):
        super().__init__(f"not_null_{column}", **kwargs)
        self.column = column
        self.threshold = threshold

    def validate(self, data: pd.DataFrame) -> ValidationResult:
        if self.column not in data.columns:
            return ValidationResult(
                rule_name=self.name,
                passed=False,
                severity=self.severity,
                message=f"Column {self.column} not found",
                details={"column": self.column}
            )

        null_rate = data[self.column].isna().mean()
        non_null_rate = 1 - null_rate
        passed = non_null_rate >= self.threshold

        return ValidationResult(
            rule_name=self.name,
            passed=passed,
            severity=self.severity,
            message=f"Non-null rate: {non_null_rate:.2%} (threshold: {self.threshold:.2%})",
            details={
                "column": self.column,
                "null_rate": null_rate,
                "non_null_rate": non_null_rate,
                "threshold": self.threshold
            }
        )

class DistributionStabilityRule(ValidationRule):
    """Validate distribution stability against baseline"""

    def __init__(
        self,
        column: str,
        baseline: pd.Series,
        max_drift: float = 0.1,
        **kwargs
    ):
        super().__init__(f"distribution_stability_{column}", **kwargs)
        self.column = column
        self.baseline = baseline
        self.max_drift = max_drift

    def validate(self, data: pd.DataFrame) -> ValidationResult:
        from scipy import stats

        current = data[self.column].dropna()

        # KS test
        statistic, p_value = stats.ks_2samp(self.baseline, current)

        passed = statistic <= self.max_drift

        return ValidationResult(
            rule_name=self.name,
            passed=passed,
            severity=self.severity,
            message=f"Distribution drift: {statistic:.4f} (max: {self.max_drift})",
            details={
                "column": self.column,
                "ks_statistic": statistic,
                "p_value": p_value,
                "max_drift": self.max_drift
            }
        )

class CustomRule(ValidationRule):
    """Custom validation rule with lambda function"""

    def __init__(
        self,
        name: str,
        check_fn: Callable[[pd.DataFrame], bool],
        message_fn: Callable[[pd.DataFrame], str] = None,
        **kwargs
    ):
        super().__init__(name, **kwargs)
        self.check_fn = check_fn
        self.message_fn = message_fn or (lambda df: "Check passed" if self.check_fn(df) else "Check failed")

    def validate(self, data: pd.DataFrame) -> ValidationResult:
        try:
            passed = self.check_fn(data)
            message = self.message_fn(data)
        except Exception as e:
            passed = False
            message = f"Validation error: {str(e)}"

        return ValidationResult(
            rule_name=self.name,
            passed=passed,
            severity=self.severity,
            message=message,
            details={}
        )

class ValidationPipeline:
    """Run multiple validation rules"""

    def __init__(self, rules: List[ValidationRule]):
        self.rules = rules

    def validate(
        self,
        data: pd.DataFrame,
        fail_fast: bool = False
    ) -> Dict:
        """Run all validation rules"""

        results = []
        passed_count = 0
        failed_by_severity = {s: 0 for s in Severity}

        for rule in self.rules:
            result = rule.validate(data)
            results.append(result)

            if result.passed:
                passed_count += 1
            else:
                failed_by_severity[result.severity] += 1

                if fail_fast and result.severity in [Severity.ERROR, Severity.CRITICAL]:
                    break

        overall_passed = failed_by_severity[Severity.ERROR] == 0 and \
                         failed_by_severity[Severity.CRITICAL] == 0

        return {
            "overall_passed": overall_passed,
            "total_rules": len(results),
            "passed_count": passed_count,
            "failed_by_severity": {s.value: c for s, c in failed_by_severity.items()},
            "results": [
                {
                    "rule": r.rule_name,
                    "passed": r.passed,
                    "severity": r.severity.value,
                    "message": r.message,
                    "details": r.details
                }
                for r in results
            ]
        }

# Build validation pipeline
pipeline = ValidationPipeline([
    ColumnNotNullRule("text", threshold=1.0, severity=Severity.CRITICAL),
    ColumnNotNullRule("id", threshold=1.0, severity=Severity.CRITICAL),
    ColumnNotNullRule("quality_score", threshold=0.95, severity=Severity.WARNING),

    DistributionStabilityRule(
        "quality_score",
        baseline=baseline_quality_scores,
        max_drift=0.1,
        severity=Severity.WARNING
    ),

    CustomRule(
        name="min_text_length",
        check_fn=lambda df: (df["text"].str.len() >= 10).all(),
        message_fn=lambda df: f"Min text length: {df['text'].str.len().min()}",
        severity=Severity.ERROR
    ),

    CustomRule(
        name="no_duplicates",
        check_fn=lambda df: df["id"].is_unique,
        message_fn=lambda df: f"Duplicate count: {df['id'].duplicated().sum()}",
        severity=Severity.CRITICAL
    )
])

# Run validation
results = pipeline.validate(df)
print(f"Overall passed: {results['overall_passed']}")
print(f"Passed: {results['passed_count']}/{results['total_rules']}")
```

### 3.4 CI/CD Integration

```python
# tests/test_data_quality.py
import pytest
import pandas as pd
from data_quality import ValidationPipeline, SchemaValidator

class TestDataQuality:
    """Data quality tests for CI/CD pipeline"""

    @pytest.fixture
    def sample_data(self):
        return pd.read_parquet("tests/fixtures/sample_training_data.parquet")

    @pytest.fixture
    def validation_pipeline(self):
        return ValidationPipeline([...])  # Define rules

    def test_schema_compliance(self, sample_data):
        """Test that data matches expected schema"""
        validator = SchemaValidator(LLMTrainingDataSchema)
        result = validator.validate(sample_data)

        assert result["success"], f"Schema validation failed: {result['errors']}"

    def test_no_null_required_fields(self, sample_data):
        """Test that required fields have no nulls"""
        required_columns = ["id", "text", "source"]

        for col in required_columns:
            null_count = sample_data[col].isna().sum()
            assert null_count == 0, f"Column {col} has {null_count} null values"

    def test_no_duplicates(self, sample_data):
        """Test that IDs are unique"""
        duplicate_count = sample_data["id"].duplicated().sum()
        assert duplicate_count == 0, f"Found {duplicate_count} duplicate IDs"

    def test_value_ranges(self, sample_data):
        """Test that numeric values are within expected ranges"""
        assert sample_data["quality_score"].between(0, 1).all(), \
            "Quality scores must be between 0 and 1"

        assert sample_data["token_count"].gt(0).all(), \
            "Token count must be positive"

    def test_text_not_empty(self, sample_data):
        """Test that text field is not empty or whitespace"""
        empty_text = sample_data["text"].str.strip().str.len() == 0
        assert not empty_text.any(), \
            f"Found {empty_text.sum()} empty text entries"

    def test_label_values(self, sample_data):
        """Test that labels are valid"""
        valid_labels = {"positive", "negative", "neutral", None}
        invalid = sample_data[~sample_data["label"].isin(valid_labels)]

        assert len(invalid) == 0, \
            f"Found {len(invalid)} invalid labels: {invalid['label'].unique()}"

    def test_distribution_stability(self, sample_data, baseline_data):
        """Test that distributions haven't drifted significantly"""
        from scipy import stats

        for column in ["quality_score", "token_count"]:
            current = sample_data[column].dropna()
            baseline = baseline_data[column].dropna()

            statistic, p_value = stats.ks_2samp(baseline, current)

            assert statistic < 0.2, \
                f"Distribution drift detected for {column}: KS={statistic:.4f}"

    def test_overall_validation_pipeline(self, sample_data, validation_pipeline):
        """Test complete validation pipeline"""
        results = validation_pipeline.validate(sample_data)

        assert results["overall_passed"], \
            f"Validation pipeline failed: {[r for r in results['results'] if not r['passed']]}"

# GitHub Actions workflow
"""
# .github/workflows/data-quality.yml
name: Data Quality Checks

on:
  push:
    paths:
      - 'data/**'
  schedule:
    - cron: '0 */6 * * *'  # Every 6 hours

jobs:
  validate-data:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install -r requirements-test.txt

      - name: Run data quality tests
        run: pytest tests/test_data_quality.py -v --tb=short

      - name: Generate quality report
        if: always()
        run: python scripts/generate_quality_report.py

      - name: Upload report
        uses: actions/upload-artifact@v4
        with:
          name: quality-report
          path: reports/quality_report.html
"""
```

---

## 4. Quality Monitoring

### 4.1 Real-Time Monitoring with whylogs

```python
import whylogs as why
from whylogs.core.constraints import Constraints
from whylogs.core.constraints.factories import (
    greater_than_number,
    smaller_than_number,
    is_in_range,
    null_percentage_below,
    no_missing_values,
)
from datetime import datetime
import pandas as pd

class RealTimeQualityMonitor:
    """Real-time data quality monitoring with whylogs"""

    def __init__(self, dataset_name: str):
        self.dataset_name = dataset_name
        self.profiles = []
        self.constraints = self._build_constraints()

    def _build_constraints(self) -> Constraints:
        """Build data quality constraints"""

        constraints = {
            # Completeness constraints
            "text": [
                no_missing_values(),
            ],
            "id": [
                no_missing_values(),
            ],
            # Range constraints
            "quality_score": [
                is_in_range(0.0, 1.0),
                null_percentage_below(0.05),  # Allow 5% nulls
            ],
            "token_count": [
                greater_than_number(0),
                smaller_than_number(200000),
            ],
        }

        return Constraints(constraints)

    def log_batch(self, df: pd.DataFrame) -> Dict:
        """Log a batch of data and check constraints"""

        # Create profile
        result = why.log(df)
        profile = result.profile()

        # Store profile
        self.profiles.append({
            "timestamp": datetime.utcnow().isoformat(),
            "profile": profile,
            "row_count": len(df)
        })

        # Check constraints
        report = self.constraints.validate(profile)

        # Format report
        violations = []
        for column, results in report.items():
            for constraint_result in results:
                if not constraint_result.passed:
                    violations.append({
                        "column": column,
                        "constraint": constraint_result.name,
                        "expected": str(constraint_result.constraint),
                        "actual": str(constraint_result.value)
                    })

        return {
            "timestamp": datetime.utcnow().isoformat(),
            "row_count": len(df),
            "passed": len(violations) == 0,
            "violation_count": len(violations),
            "violations": violations
        }

    def get_profile_summary(self) -> Dict:
        """Get summary statistics from profiles"""

        if not self.profiles:
            return {}

        latest = self.profiles[-1]["profile"]
        view = latest.view()

        summary = {}
        for column_name in view.get_columns().keys():
            col_profile = view.get_column(column_name)
            summary[column_name] = {
                "count": col_profile.get_metric("counts").n.value,
                "null_count": col_profile.get_metric("counts").null.value,
                "dtype": str(col_profile.get_metric("types").inferred_type)
            }

            # Numeric columns
            if col_profile.get_metric("distribution") is not None:
                dist = col_profile.get_metric("distribution")
                summary[column_name].update({
                    "mean": dist.mean.value,
                    "stddev": dist.stddev.value,
                    "min": dist.min.value,
                    "max": dist.max.value
                })

        return summary

    def calculate_drift(self, baseline_profile) -> Dict:
        """Calculate drift between current and baseline"""

        if not self.profiles:
            return {}

        current = self.profiles[-1]["profile"]

        from whylogs.viz.drift import DriftCalculator

        drift_calc = DriftCalculator(baseline_profile, current)
        drift_report = drift_calc.calculate()

        return {
            "overall_drift_score": drift_report.overall_score,
            "column_scores": drift_report.column_scores,
            "significant_drift": [
                col for col, score in drift_report.column_scores.items()
                if score > 0.1  # Threshold
            ]
        }

    def export_profile(self, output_path: str):
        """Export latest profile for storage"""
        if self.profiles:
            self.profiles[-1]["profile"].writer("local").write(output_path)

# Usage
monitor = RealTimeQualityMonitor("training_data")

# Monitor incoming batches
for batch in data_stream:
    result = monitor.log_batch(batch)

    if not result["passed"]:
        print(f"Quality violation detected!")
        for v in result["violations"]:
            print(f"  {v['column']}: {v['constraint']}")
        # Trigger alert
```

### 4.2 Evidently for ML-Focused Monitoring

```python
import evidently
from evidently.report import Report
from evidently.metric_preset import DataDriftPreset, DataQualityPreset
from evidently.metrics import (
    DatasetSummaryMetric,
    DatasetMissingValuesMetric,
    DatasetCorrelationsMetric,
    ColumnDriftMetric,
    ColumnDistributionMetric,
)
from evidently.test_suite import TestSuite
from evidently.tests import (
    TestNumberOfColumnsWithMissingValues,
    TestNumberOfRowsWithMissingValues,
    TestNumberOfDuplicatedRows,
    TestColumnsType,
    TestNumberOfDriftedColumns,
)

class MLDataQualityMonitor:
    """ML-focused data quality monitoring with Evidently"""

    def __init__(self, reference_data: pd.DataFrame):
        self.reference_data = reference_data

    def generate_data_quality_report(
        self,
        current_data: pd.DataFrame,
        output_path: str = None
    ) -> Dict:
        """Generate comprehensive data quality report"""

        report = Report(metrics=[
            DatasetSummaryMetric(),
            DatasetMissingValuesMetric(),
            DatasetCorrelationsMetric(),
            DataQualityPreset(),
        ])

        report.run(
            reference_data=self.reference_data,
            current_data=current_data
        )

        if output_path:
            report.save_html(output_path)

        return report.as_dict()

    def generate_drift_report(
        self,
        current_data: pd.DataFrame,
        columns: List[str] = None
    ) -> Dict:
        """Generate data drift report"""

        metrics = [DataDriftPreset()]

        if columns:
            for col in columns:
                metrics.extend([
                    ColumnDriftMetric(column_name=col),
                    ColumnDistributionMetric(column_name=col)
                ])

        report = Report(metrics=metrics)

        report.run(
            reference_data=self.reference_data,
            current_data=current_data
        )

        return report.as_dict()

    def run_test_suite(
        self,
        current_data: pd.DataFrame
    ) -> Dict:
        """Run automated test suite"""

        test_suite = TestSuite(tests=[
            # Missing values tests
            TestNumberOfColumnsWithMissingValues(eq=0),
            TestNumberOfRowsWithMissingValues(lte=0.01),  # Max 1% rows with missing

            # Duplicate tests
            TestNumberOfDuplicatedRows(eq=0),

            # Schema tests
            TestColumnsType(),

            # Drift tests
            TestNumberOfDriftedColumns(lt=3),  # Max 3 drifted columns
        ])

        test_suite.run(
            reference_data=self.reference_data,
            current_data=current_data
        )

        results = test_suite.as_dict()

        return {
            "passed": results["summary"]["all_passed"],
            "total_tests": results["summary"]["total_tests"],
            "passed_tests": results["summary"]["success_tests"],
            "failed_tests": [
                {
                    "name": t["name"],
                    "status": t["status"],
                    "description": t.get("description", "")
                }
                for t in results["tests"]
                if t["status"] != "SUCCESS"
            ]
        }

    def create_monitoring_dashboard(
        self,
        historical_data: List[pd.DataFrame],
        output_path: str
    ):
        """Create monitoring dashboard from historical data"""

        from evidently.ui.dashboards import DashboardConfig, ReportConfig

        dashboard = DashboardConfig(
            name="Data Quality Dashboard",
            panels=[
                ReportConfig(
                    title="Data Quality Overview",
                    metrics=[DataQualityPreset()]
                ),
                ReportConfig(
                    title="Data Drift",
                    metrics=[DataDriftPreset()]
                )
            ]
        )

        # Generate reports for each historical snapshot
        for i, data in enumerate(historical_data):
            report = Report(metrics=[DataQualityPreset(), DataDriftPreset()])
            report.run(
                reference_data=self.reference_data,
                current_data=data
            )
            report.save_html(f"{output_path}/report_{i}.html")

# Usage
monitor = MLDataQualityMonitor(reference_data=baseline_df)

# Generate reports
quality_report = monitor.generate_data_quality_report(
    current_df,
    output_path="reports/quality_report.html"
)

drift_report = monitor.generate_drift_report(
    current_df,
    columns=["quality_score", "token_count", "text_length"]
)

# Run tests
test_results = monitor.run_test_suite(current_df)
if not test_results["passed"]:
    print("Quality tests failed!")
    for test in test_results["failed_tests"]:
        print(f"  - {test['name']}: {test['description']}")
```

### 4.3 Anomaly Detection

```python
import numpy as np
from scipy import stats
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from typing import List, Dict, Tuple
import pandas as pd

class DataAnomalyDetector:
    """Detect anomalies in data distributions"""

    def __init__(self, sensitivity: float = 0.95):
        self.sensitivity = sensitivity
        self.baselines = {}
        self.models = {}

    def fit_baseline(
        self,
        data: pd.DataFrame,
        columns: List[str]
    ):
        """Fit baseline statistics for anomaly detection"""

        for col in columns:
            series = data[col].dropna()

            if pd.api.types.is_numeric_dtype(series):
                # Numeric column - store distribution stats
                self.baselines[col] = {
                    "type": "numeric",
                    "mean": series.mean(),
                    "std": series.std(),
                    "median": series.median(),
                    "q1": series.quantile(0.25),
                    "q3": series.quantile(0.75),
                    "min": series.min(),
                    "max": series.max(),
                    "hist": np.histogram(series, bins=50, density=True)
                }

                # Fit isolation forest for multivariate
                if len(series) > 100:
                    model = IsolationForest(
                        contamination=1 - self.sensitivity,
                        random_state=42
                    )
                    self.models[col] = model.fit(series.values.reshape(-1, 1))

            else:
                # Categorical column - store value counts
                self.baselines[col] = {
                    "type": "categorical",
                    "value_counts": series.value_counts(normalize=True).to_dict(),
                    "unique_count": series.nunique(),
                    "top_values": series.value_counts().head(100).index.tolist()
                }

    def detect_statistical_anomalies(
        self,
        data: pd.DataFrame,
        column: str
    ) -> Dict:
        """Detect statistical anomalies in a column"""

        if column not in self.baselines:
            raise ValueError(f"No baseline for column {column}")

        baseline = self.baselines[column]
        series = data[column].dropna()

        if baseline["type"] == "numeric":
            return self._detect_numeric_anomalies(series, baseline)
        else:
            return self._detect_categorical_anomalies(series, baseline)

    def _detect_numeric_anomalies(
        self,
        series: pd.Series,
        baseline: Dict
    ) -> Dict:
        """Detect anomalies in numeric column"""

        anomalies = {
            "column": series.name,
            "type": "numeric",
            "checks": []
        }

        # Mean shift detection
        current_mean = series.mean()
        z_score = (current_mean - baseline["mean"]) / (baseline["std"] + 1e-10)

        anomalies["checks"].append({
            "check": "mean_shift",
            "baseline": baseline["mean"],
            "current": current_mean,
            "z_score": z_score,
            "anomaly": abs(z_score) > 2
        })

        # Variance change detection (F-test)
        current_var = series.var()
        baseline_var = baseline["std"] ** 2

        f_stat = current_var / (baseline_var + 1e-10)

        anomalies["checks"].append({
            "check": "variance_change",
            "baseline_var": baseline_var,
            "current_var": current_var,
            "f_statistic": f_stat,
            "anomaly": f_stat > 2 or f_stat < 0.5
        })

        # Distribution shift (KS test)
        # Reconstruct baseline distribution from histogram
        baseline_hist, baseline_bins = baseline["hist"]
        baseline_samples = np.repeat(
            (baseline_bins[:-1] + baseline_bins[1:]) / 2,
            (baseline_hist * 1000).astype(int)
        )

        if len(series) > 0 and len(baseline_samples) > 0:
            ks_stat, p_value = stats.ks_2samp(baseline_samples, series)

            anomalies["checks"].append({
                "check": "distribution_shift",
                "ks_statistic": ks_stat,
                "p_value": p_value,
                "anomaly": p_value < 0.01  # Significant shift
            })

        # Outlier percentage
        q1, q3 = baseline["q1"], baseline["q3"]
        iqr = q3 - q1
        lower = q1 - 1.5 * iqr
        upper = q3 + 1.5 * iqr

        outlier_pct = ((series < lower) | (series > upper)).mean()

        anomalies["checks"].append({
            "check": "outlier_percentage",
            "outlier_rate": outlier_pct,
            "threshold": 0.05,
            "anomaly": outlier_pct > 0.05
        })

        # Overall anomaly score
        anomaly_count = sum(1 for c in anomalies["checks"] if c["anomaly"])
        anomalies["is_anomalous"] = anomaly_count >= 2
        anomalies["anomaly_score"] = anomaly_count / len(anomalies["checks"])

        return anomalies

    def _detect_categorical_anomalies(
        self,
        series: pd.Series,
        baseline: Dict
    ) -> Dict:
        """Detect anomalies in categorical column"""

        anomalies = {
            "column": series.name,
            "type": "categorical",
            "checks": []
        }

        current_counts = series.value_counts(normalize=True)

        # New categories
        new_categories = set(current_counts.index) - set(baseline["value_counts"].keys())

        anomalies["checks"].append({
            "check": "new_categories",
            "new_categories": list(new_categories),
            "count": len(new_categories),
            "anomaly": len(new_categories) > 0
        })

        # Missing expected categories
        missing_categories = set(baseline["top_values"][:10]) - set(current_counts.index)

        anomalies["checks"].append({
            "check": "missing_categories",
            "missing_categories": list(missing_categories),
            "count": len(missing_categories),
            "anomaly": len(missing_categories) > 2
        })

        # Proportion shift
        max_shift = 0
        for category, baseline_prop in baseline["value_counts"].items():
            current_prop = current_counts.get(category, 0)
            shift = abs(current_prop - baseline_prop)
            max_shift = max(max_shift, shift)

        anomalies["checks"].append({
            "check": "proportion_shift",
            "max_shift": max_shift,
            "threshold": 0.1,
            "anomaly": max_shift > 0.1
        })

        # Cardinality change
        current_cardinality = series.nunique()
        baseline_cardinality = baseline["unique_count"]
        cardinality_ratio = current_cardinality / baseline_cardinality

        anomalies["checks"].append({
            "check": "cardinality_change",
            "baseline": baseline_cardinality,
            "current": current_cardinality,
            "ratio": cardinality_ratio,
            "anomaly": cardinality_ratio > 1.5 or cardinality_ratio < 0.7
        })

        anomaly_count = sum(1 for c in anomalies["checks"] if c["anomaly"])
        anomalies["is_anomalous"] = anomaly_count >= 2
        anomalies["anomaly_score"] = anomaly_count / len(anomalies["checks"])

        return anomalies

    def detect_all_anomalies(
        self,
        data: pd.DataFrame
    ) -> Dict:
        """Detect anomalies across all baselined columns"""

        results = {
            "timestamp": datetime.utcnow().isoformat(),
            "columns_checked": len(self.baselines),
            "anomalous_columns": [],
            "details": {}
        }

        for column in self.baselines:
            if column in data.columns:
                anomaly_result = self.detect_statistical_anomalies(data, column)
                results["details"][column] = anomaly_result

                if anomaly_result["is_anomalous"]:
                    results["anomalous_columns"].append(column)

        results["anomaly_rate"] = len(results["anomalous_columns"]) / len(self.baselines)
        results["is_healthy"] = results["anomaly_rate"] < 0.2

        return results

# Usage
detector = DataAnomalyDetector(sensitivity=0.95)

# Fit on baseline data
detector.fit_baseline(
    baseline_df,
    columns=["quality_score", "token_count", "source", "label"]
)

# Detect anomalies in new data
results = detector.detect_all_anomalies(current_df)

if not results["is_healthy"]:
    print(f"Anomalies detected in {len(results['anomalous_columns'])} columns:")
    for col in results["anomalous_columns"]:
        print(f"  - {col}")
```

### 4.4 Alerting Configuration

```python
from dataclasses import dataclass
from enum import Enum
from typing import List, Dict, Callable, Optional
import json
import requests
from datetime import datetime

class AlertSeverity(Enum):
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"

class AlertChannel(Enum):
    SLACK = "slack"
    PAGERDUTY = "pagerduty"
    EMAIL = "email"
    WEBHOOK = "webhook"

@dataclass
class AlertRule:
    name: str
    condition: Callable[[Dict], bool]
    severity: AlertSeverity
    channels: List[AlertChannel]
    message_template: str
    cooldown_minutes: int = 30

@dataclass
class Alert:
    rule_name: str
    severity: AlertSeverity
    message: str
    details: Dict
    timestamp: str

class AlertManager:
    """Manage data quality alerts"""

    def __init__(self, config: Dict):
        self.config = config
        self.rules: List[AlertRule] = []
        self.alert_history: List[Alert] = []
        self._last_alert_times: Dict[str, datetime] = {}

    def add_rule(self, rule: AlertRule):
        """Add an alert rule"""
        self.rules.append(rule)

    def evaluate(self, quality_results: Dict) -> List[Alert]:
        """Evaluate all rules against quality results"""

        triggered_alerts = []

        for rule in self.rules:
            # Check cooldown
            if rule.name in self._last_alert_times:
                last_alert = self._last_alert_times[rule.name]
                minutes_since = (datetime.utcnow() - last_alert).seconds / 60
                if minutes_since < rule.cooldown_minutes:
                    continue

            # Evaluate condition
            try:
                if rule.condition(quality_results):
                    alert = self._create_alert(rule, quality_results)
                    triggered_alerts.append(alert)
                    self._last_alert_times[rule.name] = datetime.utcnow()
            except Exception as e:
                print(f"Error evaluating rule {rule.name}: {e}")

        return triggered_alerts

    def _create_alert(self, rule: AlertRule, results: Dict) -> Alert:
        """Create alert from rule and results"""

        message = rule.message_template.format(**results)

        return Alert(
            rule_name=rule.name,
            severity=rule.severity,
            message=message,
            details=results,
            timestamp=datetime.utcnow().isoformat()
        )

    def send_alerts(self, alerts: List[Alert]):
        """Send alerts to configured channels"""

        for alert in alerts:
            rule = next(r for r in self.rules if r.name == alert.rule_name)

            for channel in rule.channels:
                try:
                    if channel == AlertChannel.SLACK:
                        self._send_slack(alert)
                    elif channel == AlertChannel.PAGERDUTY:
                        self._send_pagerduty(alert)
                    elif channel == AlertChannel.WEBHOOK:
                        self._send_webhook(alert)

                    self.alert_history.append(alert)
                except Exception as e:
                    print(f"Failed to send alert to {channel}: {e}")

    def _send_slack(self, alert: Alert):
        """Send alert to Slack"""
        webhook_url = self.config.get("slack_webhook_url")

        color_map = {
            AlertSeverity.INFO: "#36a64f",
            AlertSeverity.WARNING: "#ff9800",
            AlertSeverity.ERROR: "#f44336",
            AlertSeverity.CRITICAL: "#b71c1c"
        }

        payload = {
            "attachments": [{
                "color": color_map[alert.severity],
                "title": f"[{alert.severity.value.upper()}] {alert.rule_name}",
                "text": alert.message,
                "fields": [
                    {"title": "Timestamp", "value": alert.timestamp, "short": True},
                    {"title": "Severity", "value": alert.severity.value, "short": True}
                ],
                "footer": "Data Quality Monitor"
            }]
        }

        requests.post(webhook_url, json=payload)

    def _send_pagerduty(self, alert: Alert):
        """Send alert to PagerDuty"""
        routing_key = self.config.get("pagerduty_routing_key")

        severity_map = {
            AlertSeverity.INFO: "info",
            AlertSeverity.WARNING: "warning",
            AlertSeverity.ERROR: "error",
            AlertSeverity.CRITICAL: "critical"
        }

        payload = {
            "routing_key": routing_key,
            "event_action": "trigger",
            "payload": {
                "summary": f"{alert.rule_name}: {alert.message}",
                "severity": severity_map[alert.severity],
                "source": "data-quality-monitor",
                "custom_details": alert.details
            }
        }

        requests.post(
            "https://events.pagerduty.com/v2/enqueue",
            json=payload
        )

    def _send_webhook(self, alert: Alert):
        """Send alert to generic webhook"""
        webhook_url = self.config.get("webhook_url")

        payload = {
            "rule_name": alert.rule_name,
            "severity": alert.severity.value,
            "message": alert.message,
            "details": alert.details,
            "timestamp": alert.timestamp
        }

        requests.post(webhook_url, json=payload)

# Configure alert rules
alert_manager = AlertManager({
    "slack_webhook_url": "https://hooks.slack.com/services/...",
    "pagerduty_routing_key": "...",
})

alert_manager.add_rule(AlertRule(
    name="High Null Rate",
    condition=lambda r: r.get("null_rate", 0) > 0.05,
    severity=AlertSeverity.ERROR,
    channels=[AlertChannel.SLACK, AlertChannel.PAGERDUTY],
    message_template="Null rate is {null_rate:.2%} (threshold: 5%)",
    cooldown_minutes=60
))

alert_manager.add_rule(AlertRule(
    name="Data Freshness",
    condition=lambda r: r.get("data_age_hours", 0) > 6,
    severity=AlertSeverity.WARNING,
    channels=[AlertChannel.SLACK],
    message_template="Data is {data_age_hours:.1f} hours old (threshold: 6 hours)",
    cooldown_minutes=30
))

alert_manager.add_rule(AlertRule(
    name="Distribution Drift",
    condition=lambda r: r.get("drift_score", 0) > 0.2,
    severity=AlertSeverity.WARNING,
    channels=[AlertChannel.SLACK],
    message_template="Distribution drift detected: {drift_score:.2f}",
    cooldown_minutes=120
))

alert_manager.add_rule(AlertRule(
    name="Duplicate Spike",
    condition=lambda r: r.get("duplicate_rate", 0) > 0.01,
    severity=AlertSeverity.CRITICAL,
    channels=[AlertChannel.SLACK, AlertChannel.PAGERDUTY],
    message_template="Duplicate rate spiked to {duplicate_rate:.2%}",
    cooldown_minutes=15
))

# Evaluate and send alerts
quality_results = {
    "null_rate": 0.08,
    "data_age_hours": 2.5,
    "drift_score": 0.15,
    "duplicate_rate": 0.005
}

alerts = alert_manager.evaluate(quality_results)
alert_manager.send_alerts(alerts)
```

---

## 5. Remediation Workflows

### 5.1 Quarantine and Review

```python
from dataclasses import dataclass
from typing import Optional, Callable
from datetime import datetime
import uuid

@dataclass
class QuarantineRecord:
    id: str
    original_data: Dict
    issue_type: str
    issue_details: str
    quarantine_time: str
    source_batch: str
    review_status: str  # pending, approved, rejected, auto_fixed
    reviewer: Optional[str] = None
    resolution: Optional[str] = None
    resolution_time: Optional[str] = None

class QuarantineManager:
    """Manage quarantined data records"""

    def __init__(self, storage_path: str):
        self.storage_path = storage_path
        self.quarantine_store: List[QuarantineRecord] = []

    def quarantine(
        self,
        data: Dict,
        issue_type: str,
        issue_details: str,
        source_batch: str
    ) -> QuarantineRecord:
        """Add data to quarantine"""

        record = QuarantineRecord(
            id=str(uuid.uuid4()),
            original_data=data,
            issue_type=issue_type,
            issue_details=issue_details,
            quarantine_time=datetime.utcnow().isoformat(),
            source_batch=source_batch,
            review_status="pending"
        )

        self.quarantine_store.append(record)
        self._persist()

        return record

    def quarantine_batch(
        self,
        df: pd.DataFrame,
        issues: List[Dict],
        source_batch: str
    ) -> List[QuarantineRecord]:
        """Quarantine multiple records from a DataFrame"""

        records = []

        for issue in issues:
            row_idx = issue.get("row_index")
            if row_idx is not None:
                row_data = df.iloc[row_idx].to_dict()
            else:
                row_data = issue.get("data", {})

            record = self.quarantine(
                data=row_data,
                issue_type=issue["issue_type"],
                issue_details=issue.get("details", ""),
                source_batch=source_batch
            )
            records.append(record)

        return records

    def review(
        self,
        record_id: str,
        approved: bool,
        reviewer: str,
        resolution: str = None,
        fixed_data: Dict = None
    ) -> QuarantineRecord:
        """Review a quarantined record"""

        record = next((r for r in self.quarantine_store if r.id == record_id), None)

        if not record:
            raise ValueError(f"Record {record_id} not found")

        record.review_status = "approved" if approved else "rejected"
        record.reviewer = reviewer
        record.resolution = resolution
        record.resolution_time = datetime.utcnow().isoformat()

        if fixed_data:
            record.original_data = fixed_data
            record.review_status = "auto_fixed"

        self._persist()

        return record

    def get_pending_reviews(
        self,
        issue_type: str = None,
        limit: int = 100
    ) -> List[QuarantineRecord]:
        """Get records pending review"""

        pending = [r for r in self.quarantine_store if r.review_status == "pending"]

        if issue_type:
            pending = [r for r in pending if r.issue_type == issue_type]

        return pending[:limit]

    def get_statistics(self) -> Dict:
        """Get quarantine statistics"""

        status_counts = {}
        issue_counts = {}

        for record in self.quarantine_store:
            status = record.review_status
            issue = record.issue_type

            status_counts[status] = status_counts.get(status, 0) + 1
            issue_counts[issue] = issue_counts.get(issue, 0) + 1

        return {
            "total_quarantined": len(self.quarantine_store),
            "by_status": status_counts,
            "by_issue_type": issue_counts,
            "pending_review_count": status_counts.get("pending", 0)
        }

    def _persist(self):
        """Persist quarantine store to disk"""
        import json

        with open(f"{self.storage_path}/quarantine.jsonl", "a") as f:
            for record in self.quarantine_store:
                f.write(json.dumps({
                    "id": record.id,
                    "issue_type": record.issue_type,
                    "issue_details": record.issue_details,
                    "quarantine_time": record.quarantine_time,
                    "source_batch": record.source_batch,
                    "review_status": record.review_status,
                    "reviewer": record.reviewer,
                    "resolution": record.resolution
                }) + "\n")

# Usage with data pipeline
def process_with_quarantine(
    df: pd.DataFrame,
    batch_id: str,
    quarantine_manager: QuarantineManager
) -> pd.DataFrame:
    """Process data with quarantine for bad records"""

    issues = []
    clean_indices = []

    for idx, row in df.iterrows():
        row_issues = []

        # Check for issues
        if pd.isna(row["text"]) or len(str(row["text"]).strip()) == 0:
            row_issues.append("empty_text")

        if row.get("quality_score", 1) < 0.3:
            row_issues.append("low_quality")

        if row_issues:
            issues.append({
                "row_index": idx,
                "issue_type": ",".join(row_issues),
                "details": f"Issues: {row_issues}"
            })
        else:
            clean_indices.append(idx)

    # Quarantine bad records
    if issues:
        quarantine_manager.quarantine_batch(df, issues, batch_id)
        print(f"Quarantined {len(issues)} records from batch {batch_id}")

    # Return clean data
    return df.loc[clean_indices]
```

### 5.2 Automated Correction Rules

```python
from abc import ABC, abstractmethod
from typing import Any, Optional
import re

class CorrectionRule(ABC):
    """Base class for automated correction rules"""

    def __init__(self, name: str, column: str):
        self.name = name
        self.column = column

    @abstractmethod
    def can_correct(self, value: Any) -> bool:
        """Check if value can be corrected"""
        pass

    @abstractmethod
    def correct(self, value: Any) -> Any:
        """Apply correction to value"""
        pass

class TrimWhitespaceRule(CorrectionRule):
    """Trim leading/trailing whitespace"""

    def can_correct(self, value: Any) -> bool:
        if not isinstance(value, str):
            return False
        return value != value.strip()

    def correct(self, value: Any) -> Any:
        return value.strip()

class NormalizeUnicodeRule(CorrectionRule):
    """Normalize Unicode to NFC form"""

    def can_correct(self, value: Any) -> bool:
        if not isinstance(value, str):
            return False
        import unicodedata
        return value != unicodedata.normalize('NFC', value)

    def correct(self, value: Any) -> Any:
        import unicodedata
        return unicodedata.normalize('NFC', value)

class FillDefaultRule(CorrectionRule):
    """Fill null values with default"""

    def __init__(self, name: str, column: str, default_value: Any):
        super().__init__(name, column)
        self.default_value = default_value

    def can_correct(self, value: Any) -> bool:
        return pd.isna(value)

    def correct(self, value: Any) -> Any:
        return self.default_value

class ClampRangeRule(CorrectionRule):
    """Clamp numeric values to valid range"""

    def __init__(
        self,
        name: str,
        column: str,
        min_value: float = None,
        max_value: float = None
    ):
        super().__init__(name, column)
        self.min_value = min_value
        self.max_value = max_value

    def can_correct(self, value: Any) -> bool:
        if pd.isna(value):
            return False
        try:
            v = float(value)
            if self.min_value is not None and v < self.min_value:
                return True
            if self.max_value is not None and v > self.max_value:
                return True
            return False
        except (ValueError, TypeError):
            return False

    def correct(self, value: Any) -> Any:
        v = float(value)
        if self.min_value is not None:
            v = max(v, self.min_value)
        if self.max_value is not None:
            v = min(v, self.max_value)
        return v

class RegexReplaceRule(CorrectionRule):
    """Apply regex-based corrections"""

    def __init__(
        self,
        name: str,
        column: str,
        pattern: str,
        replacement: str
    ):
        super().__init__(name, column)
        self.pattern = re.compile(pattern)
        self.replacement = replacement

    def can_correct(self, value: Any) -> bool:
        if not isinstance(value, str):
            return False
        return bool(self.pattern.search(value))

    def correct(self, value: Any) -> Any:
        return self.pattern.sub(self.replacement, value)

class AutoCorrector:
    """Apply automatic corrections to data"""

    def __init__(self, rules: List[CorrectionRule]):
        self.rules = rules

    def correct(
        self,
        df: pd.DataFrame,
        log_corrections: bool = True
    ) -> Tuple[pd.DataFrame, Dict]:
        """Apply corrections to DataFrame"""

        df = df.copy()
        correction_log = []

        for rule in self.rules:
            if rule.column not in df.columns:
                continue

            mask = df[rule.column].apply(rule.can_correct)
            correctable_count = mask.sum()

            if correctable_count > 0:
                original_values = df.loc[mask, rule.column].copy()
                df.loc[mask, rule.column] = df.loc[mask, rule.column].apply(rule.correct)

                if log_corrections:
                    correction_log.append({
                        "rule": rule.name,
                        "column": rule.column,
                        "corrected_count": correctable_count,
                        "sample_before": original_values.head(3).tolist(),
                        "sample_after": df.loc[mask, rule.column].head(3).tolist()
                    })

        return df, {
            "total_corrections": sum(c["corrected_count"] for c in correction_log),
            "corrections_by_rule": correction_log
        }

# Configure auto-correction rules
corrector = AutoCorrector([
    TrimWhitespaceRule("trim_text", "text"),
    NormalizeUnicodeRule("normalize_text", "text"),
    ClampRangeRule("clamp_quality", "quality_score", min_value=0.0, max_value=1.0),
    FillDefaultRule("default_source", "source", "unknown"),
    RegexReplaceRule("fix_double_spaces", "text", r"\s{2,}", " "),
])

# Apply corrections
df_corrected, correction_report = corrector.correct(df)
print(f"Applied {correction_report['total_corrections']} corrections")
```

### 5.3 Root Cause Analysis

```python
from typing import List, Dict, Tuple
from datetime import datetime, timedelta
import pandas as pd

class RootCauseAnalyzer:
    """Analyze root causes of data quality issues"""

    def __init__(self, historical_metrics: pd.DataFrame):
        """
        historical_metrics should have columns:
        - timestamp
        - metric_name
        - metric_value
        - source (optional)
        - pipeline_stage (optional)
        """
        self.historical_metrics = historical_metrics

    def analyze_quality_drop(
        self,
        metric_name: str,
        current_value: float,
        lookback_days: int = 7
    ) -> Dict:
        """Analyze cause of quality metric drop"""

        # Get historical data for metric
        metric_data = self.historical_metrics[
            self.historical_metrics["metric_name"] == metric_name
        ].copy()

        metric_data = metric_data.sort_values("timestamp")

        # Detect when drop started
        recent = metric_data.tail(lookback_days * 24)  # Hourly data
        baseline_mean = recent["metric_value"].iloc[:-24].mean()
        baseline_std = recent["metric_value"].iloc[:-24].std()

        # Find when values dropped below threshold
        threshold = baseline_mean - 2 * baseline_std
        drops = recent[recent["metric_value"] < threshold]

        if len(drops) == 0:
            return {"conclusion": "No significant drop detected"}

        drop_start = drops.iloc[0]["timestamp"]

        analysis = {
            "metric": metric_name,
            "current_value": current_value,
            "baseline_mean": baseline_mean,
            "drop_started": drop_start,
            "drop_magnitude": baseline_mean - current_value,
            "potential_causes": []
        }

        # Analyze potential causes
        analysis["potential_causes"].extend(
            self._analyze_source_changes(drop_start)
        )
        analysis["potential_causes"].extend(
            self._analyze_volume_changes(drop_start)
        )
        analysis["potential_causes"].extend(
            self._analyze_correlated_metrics(metric_name, drop_start)
        )

        return analysis

    def _analyze_source_changes(
        self,
        since: datetime
    ) -> List[Dict]:
        """Analyze if data source distribution changed"""

        causes = []

        if "source" not in self.historical_metrics.columns:
            return causes

        # Compare source distribution before and after
        before = self.historical_metrics[
            self.historical_metrics["timestamp"] < since
        ]["source"].value_counts(normalize=True)

        after = self.historical_metrics[
            self.historical_metrics["timestamp"] >= since
        ]["source"].value_counts(normalize=True)

        # Find significant changes
        for source in set(before.index) | set(after.index):
            before_pct = before.get(source, 0)
            after_pct = after.get(source, 0)
            change = after_pct - before_pct

            if abs(change) > 0.05:  # 5% change threshold
                causes.append({
                    "type": "source_distribution_change",
                    "source": source,
                    "before": before_pct,
                    "after": after_pct,
                    "change": change,
                    "confidence": "high" if abs(change) > 0.1 else "medium"
                })

        return causes

    def _analyze_volume_changes(
        self,
        since: datetime
    ) -> List[Dict]:
        """Analyze if data volume changed significantly"""

        causes = []

        daily_volumes = self.historical_metrics.groupby(
            self.historical_metrics["timestamp"].dt.date
        ).size()

        before_avg = daily_volumes[daily_volumes.index < since.date()].mean()
        after_avg = daily_volumes[daily_volumes.index >= since.date()].mean()

        volume_change = (after_avg - before_avg) / before_avg if before_avg > 0 else 0

        if abs(volume_change) > 0.2:  # 20% change
            causes.append({
                "type": "volume_change",
                "before_avg": before_avg,
                "after_avg": after_avg,
                "change_pct": volume_change,
                "direction": "increase" if volume_change > 0 else "decrease",
                "confidence": "high" if abs(volume_change) > 0.5 else "medium"
            })

        return causes

    def _analyze_correlated_metrics(
        self,
        metric_name: str,
        since: datetime
    ) -> List[Dict]:
        """Find other metrics that changed at same time"""

        causes = []

        pivot = self.historical_metrics.pivot_table(
            index="timestamp",
            columns="metric_name",
            values="metric_value"
        )

        if metric_name not in pivot.columns:
            return causes

        target = pivot[metric_name]

        for other_metric in pivot.columns:
            if other_metric == metric_name:
                continue

            # Calculate correlation
            corr = target.corr(pivot[other_metric])

            if abs(corr) > 0.7:  # Strong correlation
                # Check if other metric also dropped
                before_mean = pivot[other_metric][pivot.index < since].mean()
                after_mean = pivot[other_metric][pivot.index >= since].mean()

                if (before_mean - after_mean) / before_mean > 0.1:
                    causes.append({
                        "type": "correlated_metric_drop",
                        "metric": other_metric,
                        "correlation": corr,
                        "change_pct": (before_mean - after_mean) / before_mean,
                        "confidence": "high" if corr > 0.9 else "medium"
                    })

        return causes

    def generate_report(self, issues: List[Dict]) -> str:
        """Generate root cause analysis report"""

        report = "# Root Cause Analysis Report\n\n"
        report += f"Generated: {datetime.utcnow().isoformat()}\n\n"

        for issue in issues:
            analysis = self.analyze_quality_drop(
                issue["metric"],
                issue["current_value"]
            )

            report += f"## {issue['metric']}\n\n"
            report += f"- Current Value: {analysis.get('current_value', 'N/A')}\n"
            report += f"- Baseline: {analysis.get('baseline_mean', 'N/A'):.4f}\n"
            report += f"- Drop Started: {analysis.get('drop_started', 'N/A')}\n\n"

            if analysis.get("potential_causes"):
                report += "### Potential Causes\n\n"
                for cause in analysis["potential_causes"]:
                    report += f"- **{cause['type']}**: {cause}\n"

            report += "\n---\n\n"

        return report
```

---

## 6. Configuration Templates

### 6.1 Great Expectations Configuration

```yaml
# great_expectations/great_expectations.yml
config_version: 3.0

datasources:
  training_data:
    class_name: Datasource
    module_name: great_expectations.datasource
    execution_engine:
      class_name: SparkDFExecutionEngine
      module_name: great_expectations.execution_engine
    data_connectors:
      default_inferred_data_connector:
        class_name: InferredAssetS3DataConnector
        bucket: my-data-bucket
        prefix: training/
        default_regex:
          pattern: (.*)\.parquet
          group_names:
            - data_asset_name

stores:
  expectations_store:
    class_name: ExpectationsStore
    store_backend:
      class_name: TupleS3StoreBackend
      bucket: my-gx-store
      prefix: expectations/

  validations_store:
    class_name: ValidationsStore
    store_backend:
      class_name: TupleS3StoreBackend
      bucket: my-gx-store
      prefix: validations/

  checkpoint_store:
    class_name: CheckpointStore
    store_backend:
      class_name: TupleS3StoreBackend
      bucket: my-gx-store
      prefix: checkpoints/

data_docs_sites:
  s3_site:
    class_name: SiteBuilder
    store_backend:
      class_name: TupleS3StoreBackend
      bucket: my-data-docs
      prefix: docs/
    site_index_builder:
      class_name: DefaultSiteIndexBuilder

checkpoint_config:
  name: training_data_checkpoint
  config_version: 1.0
  class_name: Checkpoint
  run_name_template: "%Y%m%d-%H%M%S"
  validations:
    - batch_request:
        datasource_name: training_data
        data_connector_name: default_inferred_data_connector
        data_asset_name: latest
      expectation_suite_name: training_data_suite
      action_list:
        - name: store_validation_result
          action:
            class_name: StoreValidationResultAction
        - name: update_data_docs
          action:
            class_name: UpdateDataDocsAction
        - name: send_slack_notification
          action:
            class_name: SlackNotificationAction
            slack_webhook: ${SLACK_WEBHOOK}
            notify_on: failure
```

### 6.2 Grafana Dashboard Template

```json
{
  "dashboard": {
    "title": "Data Quality Dashboard",
    "panels": [
      {
        "title": "Overall Quality Score",
        "type": "gauge",
        "gridPos": {"h": 8, "w": 6, "x": 0, "y": 0},
        "targets": [
          {
            "expr": "data_quality_overall_score",
            "legendFormat": "Quality Score"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "thresholds": {
              "steps": [
                {"color": "red", "value": 0},
                {"color": "yellow", "value": 0.8},
                {"color": "green", "value": 0.95}
              ]
            },
            "min": 0,
            "max": 1
          }
        }
      },
      {
        "title": "Quality Dimensions",
        "type": "timeseries",
        "gridPos": {"h": 8, "w": 12, "x": 6, "y": 0},
        "targets": [
          {"expr": "data_quality_completeness", "legendFormat": "Completeness"},
          {"expr": "data_quality_uniqueness", "legendFormat": "Uniqueness"},
          {"expr": "data_quality_validity", "legendFormat": "Validity"},
          {"expr": "data_quality_consistency", "legendFormat": "Consistency"}
        ]
      },
      {
        "title": "Data Freshness",
        "type": "stat",
        "gridPos": {"h": 4, "w": 6, "x": 0, "y": 8},
        "targets": [
          {"expr": "data_freshness_hours", "legendFormat": "Hours Since Update"}
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "h",
            "thresholds": {
              "steps": [
                {"color": "green", "value": 0},
                {"color": "yellow", "value": 6},
                {"color": "red", "value": 24}
              ]
            }
          }
        }
      },
      {
        "title": "Null Rate by Column",
        "type": "barchart",
        "gridPos": {"h": 8, "w": 8, "x": 0, "y": 12},
        "targets": [
          {"expr": "data_quality_null_rate{column=~\".+\"}", "legendFormat": "{{column}}"}
        ]
      },
      {
        "title": "Distribution Drift",
        "type": "heatmap",
        "gridPos": {"h": 8, "w": 8, "x": 8, "y": 12},
        "targets": [
          {"expr": "data_quality_drift_score{column=~\".+\"}", "legendFormat": "{{column}}"}
        ]
      },
      {
        "title": "Recent Alerts",
        "type": "table",
        "gridPos": {"h": 8, "w": 8, "x": 16, "y": 12},
        "targets": [
          {"expr": "ALERTS{alertname=~\"DataQuality.*\"}"}
        ]
      }
    ]
  }
}
```

### 6.3 Alert Playbooks

```yaml
# playbooks/data_quality_alerts.yml
playbooks:
  - name: High Null Rate
    trigger: data_quality_null_rate > 0.05
    severity: error
    runbook: |
      ## High Null Rate Detected

      ### Immediate Actions
      1. Check data source availability
      2. Verify ETL pipeline logs for errors
      3. Check if schema changed at source

      ### Investigation Steps
      1. Query: `SELECT source, COUNT(*) as null_count FROM data WHERE column IS NULL GROUP BY source`
      2. Compare null rates by time period
      3. Check for correlation with deployments

      ### Escalation
      - If null rate > 20%: Page on-call data engineer
      - If affecting production models: Page ML on-call

  - name: Distribution Drift
    trigger: data_quality_drift_score > 0.2
    severity: warning
    runbook: |
      ## Distribution Drift Detected

      ### Immediate Actions
      1. Generate distribution comparison report
      2. Check if drift is expected (seasonal, new data source)
      3. Verify no data pipeline bugs

      ### Investigation Steps
      1. Compare current vs baseline distributions
      2. Check for new categories or value ranges
      3. Analyze by data source

      ### Decision Tree
      - Expected drift (holiday, campaign): Document and update baseline
      - Unexpected drift: Investigate root cause before proceeding
      - Severe drift affecting models: Consider model retraining

  - name: Duplicate Spike
    trigger: data_quality_duplicate_rate > 0.01
    severity: critical
    runbook: |
      ## Duplicate Spike Detected

      ### Immediate Actions
      1. STOP data ingestion if rate > 5%
      2. Identify duplicate records
      3. Check for replay or double-processing

      ### Investigation Steps
      1. Query: `SELECT id, COUNT(*) FROM data GROUP BY id HAVING COUNT(*) > 1`
      2. Check ingestion timestamps
      3. Review pipeline retry logic

      ### Resolution
      1. Fix root cause (dedup logic, idempotency)
      2. Run deduplication on affected data
      3. Verify downstream consumers notified
```

---

## 7. Troubleshooting

### Common Issues

| Issue | Symptoms | Root Cause | Solution |
|-------|----------|------------|----------|
| False positive alerts | Alerts for expected patterns | Thresholds too tight | Adjust thresholds, add seasonality |
| Slow validation | Pipeline delays | Too many checks | Prioritize critical checks, sample |
| Missing metrics | Gaps in dashboards | Metric emission failures | Add fallback metrics, check scraping |
| Drift detection noise | Constant drift alerts | Unstable baseline | Use rolling baseline, smooth metrics |

### Debug Commands

```bash
# Check Great Expectations validation results
great_expectations suite list
great_expectations checkpoint run training_data_checkpoint

# Query whylogs profile
whylogs profile --input data.parquet --output profile.json
cat profile.json | jq '.columns["text"].counters'

# Test alert routing
curl -X POST https://hooks.slack.com/services/... \
  -H 'Content-Type: application/json' \
  -d '{"text": "Test alert from data quality system"}'

# Verify Prometheus metrics
curl localhost:9090/api/v1/query?query=data_quality_overall_score
```

---

## Glossary

| Term | Definition |
|------|------------|
| **Completeness** | Percentage of required data that is present |
| **Consistency** | Agreement of data across sources and time |
| **Data Drift** | Change in data distribution over time |
| **DQ** | Data Quality |
| **Expectation** | A testable assertion about data (Great Expectations) |
| **KPI** | Key Performance Indicator |
| **PSI** | Population Stability Index (drift metric) |
| **Quarantine** | Isolation of suspect data for review |
| **Schema** | Structure definition for data |
| **SLA** | Service Level Agreement |
| **Validity** | Conformance to defined rules and formats |

---

## References

- [Great Expectations Documentation](https://docs.greatexpectations.io/)
- [Evidently AI Documentation](https://docs.evidentlyai.com/)
- [whylogs Documentation](https://whylogs.readthedocs.io/)
- [Pandera Documentation](https://pandera.readthedocs.io/)
- [ISO 25012 Data Quality Standard](https://www.iso.org/standard/35736.html)
- [Data Quality Survey for ML Pipelines](https://dl.acm.org/doi/10.1145/3592616)
- [Monte Carlo Data Quality Framework](https://www.montecarlodata.com/blog-data-quality-framework/)
