# Document 1.4: Data Versioning & Lineage Guide

## Document Information
- **Version:** 1.0
- **Last Updated:** December 2025
- **Owner:** Data Engineering / MLOps Team
- **Category:** Data Pipeline & Preparation

---

## Executive Summary

Data versioning and lineage tracking are foundational capabilities for reproducible ML systems. This guide establishes practices for version-controlling datasets, tracking data transformations, and maintaining complete audit trails from raw data to trained models. Proper implementation enables teams to debug training issues, comply with regulatory requirements, reproduce experiments exactly, and roll back to known-good states when problems arise.

**Key Outcomes:**
- Bit-for-bit reproducible training runs
- Complete audit trails for compliance (GDPR, HIPAA, SOX)
- Efficient debugging through lineage tracing
- Reduced storage costs via deduplication

---

## Prerequisites

### Required Knowledge
- Git fundamentals (branching, commits, merges)
- Data lake concepts (object storage, partitioning)
- SQL and data warehouse basics
- Python/CLI proficiency

### Infrastructure Requirements
- Object storage (S3, GCS, Azure Blob)
- Git repository (GitHub, GitLab, Bitbucket)
- Compute for versioning operations
- Metadata store (PostgreSQL recommended)

### Tool Installation
```bash
# DVC (Data Version Control)
pip install dvc[s3,gs,azure]

# LakeFS CLI
curl https://raw.githubusercontent.com/treeverse/lakeFS/master/scripts/install.sh | bash

# Delta Lake (Python)
pip install delta-spark deltalake

# Apache Iceberg (Python)
pip install pyiceberg

# OpenLineage (Lineage tracking)
pip install openlineage-python
```

---

## 1. Data Versioning Fundamentals

### 1.1 Why Version Data

Data versioning addresses critical challenges in ML development:

| Challenge | How Versioning Helps |
|-----------|---------------------|
| **Reproducibility** | Re-train models with exact same data |
| **Debugging** | Identify when data issues were introduced |
| **Compliance** | Prove which data trained a model (GDPR Art. 22) |
| **Collaboration** | Teams work on different data versions safely |
| **Rollback** | Revert to known-good states after problems |
| **Experimentation** | A/B test different data compositions |

### 1.2 Version Granularity Levels

```
┌─────────────────────────────────────────────────────────────┐
│                    Granularity Spectrum                      │
├─────────────────────────────────────────────────────────────┤
│  Coarse ◄────────────────────────────────────────► Fine     │
│                                                              │
│  Dataset      File/Partition    Record      Field/Cell      │
│  Version      Version           Version      Version        │
│                                                              │
│  • Snapshot   • Individual      • Row-level  • Cell-level   │
│    entire       file tracking     changes      audit        │
│    dataset                                                   │
│                                                              │
│  Use: ML      Use: ETL         Use: OLTP   Use: Audit      │
│  Training     Pipelines        Systems      Compliance      │
└─────────────────────────────────────────────────────────────┘
```

**Recommended for ML:**
- **Dataset-level** for training snapshots
- **File-level** for incremental pipeline changes
- **Record-level** only when required for compliance

### 1.3 Immutability Principles

Data versioning works best with immutable data patterns:

```python
# WRONG: Mutable data pattern
def process_data(df):
    df.to_parquet('s3://bucket/data/training.parquet')  # Overwrites!

# CORRECT: Immutable pattern with versioning
def process_data(df, version: str):
    path = f's3://bucket/data/training/v={version}/data.parquet'
    df.to_parquet(path)
    return path
```

**Core Immutability Rules:**
1. **Never overwrite** - Create new versions instead
2. **Append-only logs** - Track all operations
3. **Soft deletes** - Mark as deleted, don't remove
4. **Time-stamped partitions** - Include ingestion time

### 1.4 Storage-Efficient Versioning

Naive versioning duplicates entire datasets. Efficient approaches:

```
┌─────────────────────────────────────────────────────────────┐
│              Storage Efficiency Techniques                   │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  Content-Addressable Storage (CAS):                         │
│  ┌─────────┐     hash("data")      ┌─────────────────────┐  │
│  │  Data   │ ────────────────────► │  .dvc/cache/ab/     │  │
│  │  File   │                        │  cd1234...          │  │
│  └─────────┘                        └─────────────────────┘  │
│                                                              │
│  Deduplication:                                             │
│  v1: [A, B, C, D]   Only store   ┌───┬───┬───┬───┬───┐     │
│  v2: [A, B, E, D]   ──────────►  │ A │ B │ C │ D │ E │     │
│  v3: [A, F, E, D]   changed      └───┴───┴───┴───┴───┘     │
│                     blocks        (5 blocks, not 12)        │
│                                                              │
│  Delta Encoding:                                            │
│  v1: Full snapshot (100 GB)                                 │
│  v2: Delta from v1 (2 GB changed)                           │
│  v3: Delta from v2 (1 GB changed)                           │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

---

## 2. Versioning Tools & Platforms

### 2.1 DVC (Data Version Control)

DVC extends Git to handle large files by storing pointers in Git and actual data in remote storage.

**Architecture:**
```
┌─────────────────────────────────────────────────────────────┐
│                      DVC Architecture                        │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  Git Repository                Remote Storage                │
│  ┌─────────────────┐          ┌─────────────────┐           │
│  │ data.csv.dvc    │ ◄──────► │ s3://bucket/    │           │
│  │ (pointer file)  │          │   .dvc/cache/   │           │
│  │                 │          │   ab/cd1234...  │           │
│  │ dvc.yaml        │          │                 │           │
│  │ (pipeline def)  │          │                 │           │
│  │                 │          │                 │           │
│  │ dvc.lock        │          │                 │           │
│  │ (locked state)  │          │                 │           │
│  └─────────────────┘          └─────────────────┘           │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

**Setup and Configuration:**

```bash
# Initialize DVC in a Git repository
cd my-ml-project
git init
dvc init

# Configure remote storage
dvc remote add -d s3remote s3://my-bucket/dvc-store
dvc remote modify s3remote region us-east-1

# For team access with credentials
dvc remote modify --local s3remote access_key_id 'AKIAIOSFODNN7EXAMPLE'
dvc remote modify --local s3remote secret_access_key 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'
```

**Tracking Data:**

```bash
# Add a dataset to DVC tracking
dvc add data/training_data.parquet

# This creates data/training_data.parquet.dvc (pointer file)
# and adds data/training_data.parquet to .gitignore

# Commit the pointer file to Git
git add data/training_data.parquet.dvc data/.gitignore
git commit -m "Add training dataset v1"

# Push data to remote storage
dvc push
```

**DVC Pointer File (.dvc):**

```yaml
# data/training_data.parquet.dvc
outs:
  - md5: ab12cd34ef56gh78ij90...
    size: 1073741824
    hash: md5
    path: training_data.parquet
```

**Version Switching:**

```bash
# Create a new version
# (modify data/training_data.parquet)
dvc add data/training_data.parquet
git add data/training_data.parquet.dvc
git commit -m "Add training dataset v2"
dvc push

# Switch between versions
git checkout v1.0  # Switch Git to v1.0 tag
dvc checkout       # Pull corresponding data version

# Compare versions
dvc diff HEAD~1    # Show data changes from previous commit
```

**DVC Pipelines (dvc.yaml):**

```yaml
# dvc.yaml - Define reproducible pipelines
stages:
  preprocess:
    cmd: python src/preprocess.py
    deps:
      - src/preprocess.py
      - data/raw/
    params:
      - preprocess.min_length
      - preprocess.max_length
    outs:
      - data/processed/

  train:
    cmd: python src/train.py
    deps:
      - src/train.py
      - data/processed/
    params:
      - train.learning_rate
      - train.batch_size
      - train.epochs
    outs:
      - models/model.pt
    metrics:
      - metrics/train_metrics.json:
          cache: false
    plots:
      - plots/loss_curve.csv:
          x: epoch
          y: loss

  evaluate:
    cmd: python src/evaluate.py
    deps:
      - src/evaluate.py
      - models/model.pt
      - data/test/
    metrics:
      - metrics/eval_metrics.json:
          cache: false
```

**Running Pipelines:**

```bash
# Run entire pipeline
dvc repro

# Run specific stage
dvc repro train

# Force re-run even if inputs unchanged
dvc repro --force

# Visualize pipeline DAG
dvc dag
```

### 2.2 LakeFS

LakeFS provides Git-like operations directly on data lakes with zero-copy branching.

**Architecture:**
```
┌─────────────────────────────────────────────────────────────┐
│                     LakeFS Architecture                      │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│   Application Layer                                          │
│   ┌─────────────┐  ┌─────────────┐  ┌─────────────┐        │
│   │   Spark     │  │    Presto   │  │    Trino    │        │
│   │   Flink     │  │    Hive     │  │   dbt       │        │
│   └──────┬──────┘  └──────┬──────┘  └──────┬──────┘        │
│          │                │                │                 │
│          └────────────────┼────────────────┘                 │
│                           ▼                                  │
│   ┌─────────────────────────────────────────────────────┐   │
│   │                   LakeFS Server                      │   │
│   │  ┌───────────┐  ┌────────────┐  ┌────────────────┐  │   │
│   │  │  S3 API   │  │  Branching │  │  Commit Log    │  │   │
│   │  │  Gateway  │  │  Engine    │  │  (PostgreSQL)  │  │   │
│   │  └───────────┘  └────────────┘  └────────────────┘  │   │
│   └─────────────────────────────────────────────────────┘   │
│                           │                                  │
│                           ▼                                  │
│   ┌─────────────────────────────────────────────────────┐   │
│   │              Object Storage (S3/GCS/Azure)           │   │
│   │  lakefs://repo/main/data/   (virtual namespace)      │   │
│   │  s3://bucket/_lakefs/...    (actual storage)         │   │
│   └─────────────────────────────────────────────────────┘   │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

**Installation and Setup:**

```bash
# Run LakeFS via Docker
docker run -d \
  --name lakefs \
  -p 8000:8000 \
  -e LAKEFS_DATABASE_TYPE=postgres \
  -e LAKEFS_DATABASE_POSTGRES_CONNECTION_STRING="postgres://user:pass@host/lakefs" \
  -e LAKEFS_BLOCKSTORE_TYPE=s3 \
  -e LAKEFS_BLOCKSTORE_S3_REGION=us-east-1 \
  treeverse/lakefs:latest

# Configure CLI
lakectl config
# Enter endpoint: http://localhost:8000
# Enter access key ID and secret

# Create a repository
lakectl repo create lakefs://ml-data s3://my-bucket/lakefs-storage
```

**Branch Operations:**

```bash
# Create a branch for experimentation
lakectl branch create lakefs://ml-data/experiment-v2 --source lakefs://ml-data/main

# List branches
lakectl branch list lakefs://ml-data

# Upload data to branch
aws s3 cp training_data.parquet s3://my-bucket/ml-data/experiment-v2/data/ \
  --endpoint-url http://localhost:8000

# Commit changes
lakectl commit lakefs://ml-data/experiment-v2 \
  -m "Add cleaned training data v2"

# Merge to main after validation
lakectl merge lakefs://ml-data/experiment-v2 lakefs://ml-data/main
```

**Zero-Copy Branching Benefits:**
```
Main Branch (100 TB)
├── data/training/ (80 TB)
├── data/validation/ (15 TB)
└── data/test/ (5 TB)

Create Branch: experiment-v1
├── (Zero copy - instant, no storage cost)
└── Only stores delta when you write changes

Result: Branch with 100 TB accessible, <1 GB stored
```

**Spark Integration:**

```python
# PySpark with LakeFS
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("LakeFS Example") \
    .config("spark.hadoop.fs.s3a.endpoint", "http://lakefs:8000") \
    .config("spark.hadoop.fs.s3a.access.key", "AKIAIOSFODNN7EXAMPLE") \
    .config("spark.hadoop.fs.s3a.secret.key", "wJalrXUtnFEMI/...") \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .getOrCreate()

# Read from a specific branch
df = spark.read.parquet("s3a://ml-data/experiment-v2/data/training/")

# Process and write back
processed_df = df.filter(df.quality_score > 0.8)
processed_df.write.parquet("s3a://ml-data/experiment-v2/data/training_filtered/")
```

### 2.3 Delta Lake

Delta Lake adds ACID transactions, time travel, and schema enforcement to Parquet files.

**Core Features:**
```
┌─────────────────────────────────────────────────────────────┐
│                    Delta Lake Features                       │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  1. ACID Transactions          2. Time Travel               │
│     ┌───────────────┐             SELECT * FROM table       │
│     │ Transaction   │             VERSION AS OF 5           │
│     │ Log (_delta_  │             TIMESTAMP AS OF '2024-...'│
│     │ log/)         │                                       │
│     └───────────────┘                                        │
│                                                              │
│  3. Schema Enforcement         4. Unified Batch/Stream      │
│     Reject invalid             Same table for batch         │
│     data types                 and streaming writes         │
│                                                              │
│  5. Audit History              6. Scalable Metadata         │
│     DESCRIBE HISTORY           Handles billions of files    │
│     table                                                    │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

**Python API (delta-rs/deltalake):**

```python
from deltalake import DeltaTable, write_deltalake
import pandas as pd

# Write a Delta table
df = pd.DataFrame({
    'text': ['example 1', 'example 2'],
    'label': ['positive', 'negative'],
    'quality_score': [0.95, 0.87]
})

write_deltalake(
    's3://bucket/delta/training_data',
    df,
    mode='overwrite',
    storage_options={
        'AWS_ACCESS_KEY_ID': '...',
        'AWS_SECRET_ACCESS_KEY': '...',
        'AWS_REGION': 'us-east-1'
    }
)

# Read Delta table
dt = DeltaTable('s3://bucket/delta/training_data')
df = dt.to_pandas()

# Time travel - read specific version
dt_v2 = DeltaTable('s3://bucket/delta/training_data', version=2)
df_v2 = dt_v2.to_pandas()

# Time travel - read at timestamp
import datetime
dt_ts = DeltaTable(
    's3://bucket/delta/training_data',
    storage_options={...}
)
df_ts = dt_ts.load_as_version(
    datetime.datetime(2024, 1, 15, 12, 0, 0)
).to_pandas()

# View history
history = dt.history()
for entry in history:
    print(f"Version {entry['version']}: {entry['timestamp']} - {entry['operation']}")
```

**Spark Integration:**

```python
from pyspark.sql import SparkSession
from delta import configure_spark_with_delta_pip

builder = SparkSession.builder \
    .appName("DeltaLake") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")

spark = configure_spark_with_delta_pip(builder).getOrCreate()

# Write with Delta
df.write.format("delta").mode("overwrite").save("s3://bucket/delta/table")

# Read with time travel
df_v5 = spark.read.format("delta").option("versionAsOf", 5).load("s3://bucket/delta/table")

# SQL time travel
spark.sql("""
    SELECT * FROM delta.`s3://bucket/delta/table`
    VERSION AS OF 5
""")

# Vacuum old versions (keep last 7 days)
from delta.tables import DeltaTable
dt = DeltaTable.forPath(spark, "s3://bucket/delta/table")
dt.vacuum(168)  # 168 hours = 7 days
```

### 2.4 Apache Iceberg

Iceberg is a high-performance table format designed for huge datasets.

```python
from pyiceberg.catalog import load_catalog
from pyiceberg.schema import Schema
from pyiceberg.types import NestedField, StringType, FloatType, LongType

# Connect to catalog
catalog = load_catalog(
    "default",
    **{
        "type": "glue",
        "warehouse": "s3://my-bucket/iceberg/warehouse",
        "region_name": "us-east-1"
    }
)

# Define schema
schema = Schema(
    NestedField(1, "id", LongType(), required=True),
    NestedField(2, "text", StringType(), required=True),
    NestedField(3, "embedding", StringType(), required=False),
    NestedField(4, "quality_score", FloatType(), required=False),
)

# Create table
catalog.create_table(
    identifier="ml_database.training_data",
    schema=schema,
)

# Time travel
table = catalog.load_table("ml_database.training_data")

# Read current snapshot
current_df = table.scan().to_pandas()

# Read specific snapshot
snapshot_id = table.history()[2].snapshot_id
historical_df = table.scan(snapshot_id=snapshot_id).to_pandas()

# Read at timestamp
from datetime import datetime
ts = datetime(2024, 6, 1, 12, 0, 0)
ts_df = table.scan(as_of=ts).to_pandas()
```

### 2.5 Tool Comparison Matrix

| Feature | DVC | LakeFS | Delta Lake | Iceberg |
|---------|-----|--------|------------|---------|
| **Primary Use Case** | ML pipelines | Data lake versioning | Data warehouse | Massive tables |
| **Git Integration** | Native | Separate | None | None |
| **Branching** | Via Git | Native zero-copy | Via cloning | Snapshot isolation |
| **Time Travel** | Via Git history | Native | Native | Native |
| **ACID Transactions** | No | Yes | Yes | Yes |
| **Storage Backend** | S3, GCS, Azure, etc. | S3, GCS, Azure, MinIO | S3, GCS, Azure, HDFS | S3, GCS, Azure, HDFS |
| **Query Engine Support** | CLI/Python | Spark, Presto, Trino | Spark, Presto, Trino | Spark, Presto, Trino |
| **Best For** | Small-medium ML teams | Data lake isolation | Data engineering | Petabyte-scale tables |
| **Learning Curve** | Low | Medium | Medium | High |
| **Community** | Large | Growing | Very Large | Large |

---

## 3. Data Lineage Tracking

### 3.1 Lineage Concepts

```
┌─────────────────────────────────────────────────────────────┐
│                    Lineage Levels                            │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  Dataset Lineage:                                           │
│  raw_data ──► cleaned_data ──► training_data ──► model      │
│                                                              │
│  Column Lineage:                                            │
│  source.name ──► cleaned.name ──► training.text_input       │
│  source.desc ──►─┘               (concatenated)             │
│                                                              │
│  Model Lineage:                                             │
│  training_v3.parquet ──► experiment_42 ──► model_v1.0       │
│  config.yaml ─────────►─┘                                   │
│  hyperparams.json ────►─┘                                   │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### 3.2 OpenLineage Integration

OpenLineage is an open standard for lineage metadata collection.

```python
from openlineage.client import OpenLineageClient
from openlineage.client.run import Run, RunEvent, RunState, Job
from openlineage.client.facet import (
    DataSourceDatasetFacet,
    SchemaDatasetFacet,
    SchemaField,
    SqlJobFacet,
)
import uuid
from datetime import datetime

# Initialize client
client = OpenLineageClient(
    url="http://marquez:5000",
    api_key="your-api-key"
)

# Define a job (transformation)
job = Job(
    namespace="ml-pipeline",
    name="preprocess_training_data"
)

# Start run
run_id = str(uuid.uuid4())
start_event = RunEvent(
    eventType=RunState.START,
    eventTime=datetime.utcnow().isoformat() + "Z",
    run=Run(runId=run_id),
    job=job,
    inputs=[
        {
            "namespace": "s3://my-bucket",
            "name": "raw/training_data",
            "facets": {
                "schema": SchemaDatasetFacet(
                    fields=[
                        SchemaField(name="text", type="STRING"),
                        SchemaField(name="label", type="STRING"),
                    ]
                )
            }
        }
    ],
    outputs=[
        {
            "namespace": "s3://my-bucket",
            "name": "processed/training_data_v2",
            "facets": {
                "schema": SchemaDatasetFacet(
                    fields=[
                        SchemaField(name="text_clean", type="STRING"),
                        SchemaField(name="label", type="STRING"),
                        SchemaField(name="quality_score", type="FLOAT"),
                    ]
                )
            }
        }
    ]
)
client.emit(start_event)

# ... perform actual processing ...

# Complete run
complete_event = RunEvent(
    eventType=RunState.COMPLETE,
    eventTime=datetime.utcnow().isoformat() + "Z",
    run=Run(runId=run_id),
    job=job,
    inputs=start_event.inputs,
    outputs=start_event.outputs
)
client.emit(complete_event)
```

### 3.3 Lineage with Apache Atlas

```python
from apache_atlas.client.base_client import AtlasClient
from apache_atlas.model.instance import AtlasEntity, AtlasEntityWithExtInfo

# Initialize Atlas client
atlas = AtlasClient("http://atlas:21000", ("admin", "admin"))

# Create dataset entity
dataset_entity = AtlasEntity()
dataset_entity.typeName = "DataSet"
dataset_entity.attributes = {
    "name": "training_data_v2",
    "qualifiedName": "s3://bucket/training_data_v2",
    "description": "Preprocessed training data",
    "owner": "ml-team"
}

# Create process entity (transformation)
process_entity = AtlasEntity()
process_entity.typeName = "Process"
process_entity.attributes = {
    "name": "preprocess_job",
    "qualifiedName": "airflow://preprocess_training_data_20240115",
    "inputs": [{"guid": input_dataset_guid}],
    "outputs": [{"guid": output_dataset_guid}],
    "startTime": 1705305600000,
    "endTime": 1705309200000
}

# Create entities
atlas.entity.create_entities(
    AtlasEntityWithExtInfo(entities=[dataset_entity, process_entity])
)
```

### 3.4 DataHub Integration

```python
import datahub.emitter.mce_builder as builder
from datahub.emitter.rest_emitter import DatahubRestEmitter
from datahub.metadata.schema_classes import (
    DatasetLineageTypeClass,
    UpstreamLineageClass,
    UpstreamClass,
)

# Connect to DataHub
emitter = DatahubRestEmitter(gms_server="http://datahub-gms:8080")

# Define upstream lineage
upstream_lineage = UpstreamLineageClass(
    upstreams=[
        UpstreamClass(
            dataset=builder.make_dataset_urn("s3", "bucket/raw/data"),
            type=DatasetLineageTypeClass.TRANSFORMED,
        ),
        UpstreamClass(
            dataset=builder.make_dataset_urn("s3", "bucket/reference/labels"),
            type=DatasetLineageTypeClass.TRANSFORMED,
        ),
    ]
)

# Emit lineage
emitter.emit_mce(
    builder.make_lineage_mce(
        upstream_urns=[
            builder.make_dataset_urn("s3", "bucket/raw/data"),
            builder.make_dataset_urn("s3", "bucket/reference/labels"),
        ],
        downstream_urn=builder.make_dataset_urn("s3", "bucket/processed/training"),
    )
)
```

### 3.5 ML-Specific Lineage with MLflow

```python
import mlflow
from mlflow.tracking import MlflowClient

mlflow.set_tracking_uri("http://mlflow:5000")
mlflow.set_experiment("llm-training")

client = MlflowClient()

with mlflow.start_run(run_name="training_run_v3") as run:
    # Log data lineage
    mlflow.log_param("training_data_version", "v2.3.1")
    mlflow.log_param("training_data_hash", "abc123def456")
    mlflow.log_param("training_data_path", "s3://bucket/training/v2.3.1/")
    mlflow.log_param("training_data_rows", 1_000_000)

    # Log preprocessing lineage
    mlflow.log_param("preprocessing_script", "scripts/preprocess_v2.py")
    mlflow.log_param("preprocessing_script_hash", "git:abc123")
    mlflow.log_param("preprocessing_config", "configs/preprocess.yaml")

    # Log data transformations
    mlflow.set_tag("data.source", "s3://bucket/raw/")
    mlflow.set_tag("data.transformations", "normalize,dedupe,filter")
    mlflow.set_tag("data.output", "s3://bucket/training/v2.3.1/")

    # ... training code ...

    # Log model artifact with lineage
    mlflow.pytorch.log_model(
        model,
        "model",
        registered_model_name="llm-base-v1",
        input_example=sample_input
    )

    # Log full lineage as artifact
    lineage = {
        "data": {
            "sources": ["s3://bucket/raw/data_v1/", "s3://bucket/raw/data_v2/"],
            "version": "v2.3.1",
            "transformations": [
                {"step": "normalize", "script": "normalize.py", "version": "1.2.0"},
                {"step": "dedupe", "script": "dedupe.py", "version": "2.0.1"},
                {"step": "filter", "script": "filter.py", "version": "1.5.0"},
            ]
        },
        "model": {
            "architecture": "llama-7b",
            "base_model": "meta-llama/Llama-2-7b",
            "training_config": "configs/train.yaml"
        }
    }

    import json
    with open("/tmp/lineage.json", "w") as f:
        json.dump(lineage, f, indent=2)
    mlflow.log_artifact("/tmp/lineage.json")
```

---

## 4. Implementation Patterns

### 4.1 Git-Based Workflow (DVC + Git)

```
┌─────────────────────────────────────────────────────────────┐
│               DVC + Git Workflow                             │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  1. Development Branch                                       │
│     git checkout -b feature/new-preprocessing               │
│     dvc checkout                                             │
│                                                              │
│  2. Modify Data                                              │
│     python preprocess.py --output data/processed/            │
│     dvc add data/processed/                                  │
│                                                              │
│  3. Push Changes                                             │
│     dvc push                                                 │
│     git add data/processed.dvc                               │
│     git commit -m "Update preprocessing"                     │
│     git push origin feature/new-preprocessing               │
│                                                              │
│  4. Code Review                                              │
│     - Review code changes in PR                              │
│     - dvc diff shows data changes                            │
│     - Reviewer can checkout and test                         │
│                                                              │
│  5. Merge to Main                                            │
│     git checkout main                                        │
│     git merge feature/new-preprocessing                      │
│     git push                                                 │
│                                                              │
│  6. CI/CD Pulls Data                                         │
│     dvc pull                                                 │
│     python train.py                                          │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

**Complete DVC Workflow Script:**

```bash
#!/bin/bash
# scripts/dvc_workflow.sh

set -e

# Setup
PROJECT_ROOT=$(pwd)
BRANCH_NAME="data-update-$(date +%Y%m%d)"

# 1. Create feature branch
git checkout main
git pull origin main
git checkout -b $BRANCH_NAME
dvc checkout

# 2. Run data pipeline
echo "Running data pipeline..."
dvc repro

# 3. Check what changed
echo "Data changes:"
dvc diff

# 4. Push data to remote
echo "Pushing data..."
dvc push

# 5. Commit changes
git add dvc.lock *.dvc
git commit -m "Update data pipeline outputs

Changes:
$(dvc diff --show-hash)

Data version: $(dvc version)
DVC cache: $(dvc remote list)"

# 6. Push to remote
git push origin $BRANCH_NAME

echo "Created branch $BRANCH_NAME with data updates"
echo "Create a PR at: https://github.com/org/repo/compare/$BRANCH_NAME"
```

### 4.2 Medallion Architecture (Bronze/Silver/Gold)

```
┌─────────────────────────────────────────────────────────────┐
│              Medallion Architecture                          │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  ┌──────────────────────────────────────────────────────┐   │
│  │                     BRONZE LAYER                      │   │
│  │  Raw ingested data (immutable, append-only)          │   │
│  │  s3://bucket/bronze/source=api/date=2024-01-15/      │   │
│  │  - No transformations                                 │   │
│  │  - Full history retained                              │   │
│  │  - Schema evolution supported                         │   │
│  └──────────────────────────────────────────────────────┘   │
│                           │                                  │
│                           ▼                                  │
│  ┌──────────────────────────────────────────────────────┐   │
│  │                     SILVER LAYER                      │   │
│  │  Cleaned, validated, deduplicated data               │   │
│  │  s3://bucket/silver/training_data/version=v2/        │   │
│  │  - PII removed                                        │   │
│  │  - Duplicates removed                                 │   │
│  │  - Schema enforced                                    │   │
│  │  - Versioned (Delta Lake / Iceberg)                  │   │
│  └──────────────────────────────────────────────────────┘   │
│                           │                                  │
│                           ▼                                  │
│  ┌──────────────────────────────────────────────────────┐   │
│  │                      GOLD LAYER                       │   │
│  │  ML-ready datasets, features, embeddings             │   │
│  │  s3://bucket/gold/llm_training/experiment=42/        │   │
│  │  - Tokenized and formatted                           │   │
│  │  - Feature engineered                                 │   │
│  │  - Ready for training                                 │   │
│  └──────────────────────────────────────────────────────┘   │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

**Implementation with Delta Lake:**

```python
from pyspark.sql import SparkSession
from delta.tables import DeltaTable

spark = SparkSession.builder \
    .appName("MedallionArchitecture") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .getOrCreate()

class MedallionPipeline:
    def __init__(self, base_path: str):
        self.bronze_path = f"{base_path}/bronze"
        self.silver_path = f"{base_path}/silver"
        self.gold_path = f"{base_path}/gold"

    def ingest_to_bronze(self, source_path: str, source_name: str):
        """Ingest raw data to bronze layer (append-only)"""
        from datetime import datetime

        df = spark.read.json(source_path)

        # Add metadata columns
        df = df.withColumn("_ingested_at", lit(datetime.utcnow().isoformat()))
        df = df.withColumn("_source", lit(source_name))

        # Append to bronze (never overwrite)
        df.write \
            .format("delta") \
            .mode("append") \
            .partitionBy("_source") \
            .save(f"{self.bronze_path}/{source_name}")

        return df

    def bronze_to_silver(self, source_name: str, version: str):
        """Transform bronze to silver (cleaned, deduplicated)"""
        # Read bronze
        bronze_df = spark.read.format("delta").load(f"{self.bronze_path}/{source_name}")

        # Clean and transform
        silver_df = bronze_df \
            .dropDuplicates(["id"]) \
            .filter(col("text").isNotNull()) \
            .withColumn("text_clean", self.clean_text(col("text"))) \
            .withColumn("_processed_at", lit(datetime.utcnow().isoformat())) \
            .withColumn("_version", lit(version))

        # Write to silver with merge (upsert)
        silver_path = f"{self.silver_path}/{source_name}"

        if DeltaTable.isDeltaTable(spark, silver_path):
            delta_table = DeltaTable.forPath(spark, silver_path)
            delta_table.alias("target").merge(
                silver_df.alias("source"),
                "target.id = source.id"
            ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()
        else:
            silver_df.write.format("delta").save(silver_path)

        return silver_df

    def silver_to_gold(self, source_name: str, experiment_id: str, config: dict):
        """Transform silver to gold (ML-ready)"""
        silver_df = spark.read.format("delta").load(f"{self.silver_path}/{source_name}")

        # Apply ML-specific transformations
        gold_df = silver_df \
            .filter(col("quality_score") >= config["min_quality"]) \
            .withColumn("tokens", self.tokenize(col("text_clean"))) \
            .withColumn("_experiment_id", lit(experiment_id)) \
            .withColumn("_config", to_json(lit(config)))

        # Write to gold (versioned by experiment)
        gold_df.write \
            .format("delta") \
            .mode("overwrite") \
            .save(f"{self.gold_path}/{source_name}/experiment={experiment_id}")

        return gold_df
```

### 4.3 Catalog Integration

```python
from pyiceberg.catalog import load_catalog

# AWS Glue Catalog integration
catalog = load_catalog(
    "glue",
    **{
        "type": "glue",
        "warehouse": "s3://bucket/iceberg/",
        "region_name": "us-east-1"
    }
)

# Create namespace
catalog.create_namespace("ml_data")

# Create table with full schema
from pyiceberg.schema import Schema
from pyiceberg.types import *
from pyiceberg.partitioning import PartitionSpec, PartitionField
from pyiceberg.transforms import DayTransform

schema = Schema(
    NestedField(1, "id", LongType(), required=True),
    NestedField(2, "text", StringType(), required=True),
    NestedField(3, "source", StringType(), required=True),
    NestedField(4, "quality_score", FloatType()),
    NestedField(5, "created_at", TimestampType()),
)

partition_spec = PartitionSpec(
    PartitionField(
        source_id=5,
        field_id=1000,
        transform=DayTransform(),
        name="created_day"
    )
)

table = catalog.create_table(
    identifier="ml_data.training_corpus",
    schema=schema,
    partition_spec=partition_spec,
    properties={
        "write.format.default": "parquet",
        "write.parquet.compression-codec": "zstd"
    }
)
```

---

## 5. Compliance & Audit

### 5.1 GDPR Right to Explanation

GDPR Article 22 requires ability to explain automated decisions. Data lineage enables this:

```python
class GDPRLineageTracker:
    """Track data lineage for GDPR compliance"""

    def __init__(self, lineage_store: str):
        self.lineage_store = lineage_store

    def record_data_usage(
        self,
        user_id: str,
        data_id: str,
        model_id: str,
        prediction_id: str
    ):
        """Record when user data contributes to a model/prediction"""
        record = {
            "timestamp": datetime.utcnow().isoformat(),
            "user_id": user_id,
            "data_id": data_id,
            "model_id": model_id,
            "prediction_id": prediction_id,
            "data_version": self.get_data_version(data_id),
            "model_version": self.get_model_version(model_id),
            "training_data_hash": self.get_training_data_hash(model_id)
        }
        self.store_record(record)

    def get_user_data_usage(self, user_id: str) -> List[dict]:
        """Get all ways a user's data was used (for data subject requests)"""
        return self.query_lineage(f"user_id = '{user_id}'")

    def explain_prediction(self, prediction_id: str) -> dict:
        """Explain how a prediction was made (Article 22 compliance)"""
        record = self.get_record(prediction_id)

        return {
            "prediction_id": prediction_id,
            "model_used": record["model_id"],
            "model_version": record["model_version"],
            "training_data_version": record["training_data_hash"],
            "data_sources": self.get_training_sources(record["model_id"]),
            "processing_steps": self.get_processing_steps(record["model_id"]),
            "user_data_contribution": self.get_user_contribution(
                record["user_id"], record["model_id"]
            )
        }

    def process_deletion_request(self, user_id: str) -> dict:
        """Process GDPR deletion request with lineage tracking"""
        # Find all data associated with user
        user_data = self.get_user_data_usage(user_id)

        # Identify affected models
        affected_models = set(r["model_id"] for r in user_data)

        # Delete user data
        deleted_records = self.delete_user_data(user_id)

        return {
            "user_id": user_id,
            "deleted_records": len(deleted_records),
            "affected_models": list(affected_models),
            "action_required": "Retrain models" if affected_models else "None",
            "deletion_certificate": self.generate_deletion_certificate(user_id)
        }
```

### 5.2 Model Cards and Datasheets

```python
from dataclasses import dataclass
from typing import List, Optional
import json

@dataclass
class DataSheet:
    """Datasheet for ML Datasets (Gebru et al. 2021)"""

    # Motivation
    purpose: str
    creators: List[str]
    funding: Optional[str]

    # Composition
    instance_count: int
    data_types: List[str]
    sampling_strategy: str
    is_sample: bool

    # Collection
    collection_mechanism: str
    collection_timeframe: str
    ethical_review: bool
    consent_obtained: bool

    # Preprocessing
    preprocessing_steps: List[str]
    raw_data_available: bool

    # Uses
    intended_uses: List[str]
    not_suitable_for: List[str]

    # Distribution
    license: str
    access_restrictions: str

    # Maintenance
    maintainer: str
    update_frequency: str
    versioning_scheme: str

    def to_markdown(self) -> str:
        return f"""
# Datasheet for {self.purpose}

## Motivation
- **Purpose:** {self.purpose}
- **Creators:** {', '.join(self.creators)}
- **Funding:** {self.funding or 'Not specified'}

## Composition
- **Instance count:** {self.instance_count:,}
- **Data types:** {', '.join(self.data_types)}
- **Sampling strategy:** {self.sampling_strategy}
- **Is sample of larger dataset:** {self.is_sample}

## Collection Process
- **Mechanism:** {self.collection_mechanism}
- **Timeframe:** {self.collection_timeframe}
- **Ethical review:** {'Yes' if self.ethical_review else 'No'}
- **Consent obtained:** {'Yes' if self.consent_obtained else 'No'}

## Preprocessing
- **Steps:** {'; '.join(self.preprocessing_steps)}
- **Raw data available:** {'Yes' if self.raw_data_available else 'No'}

## Uses
- **Intended uses:** {'; '.join(self.intended_uses)}
- **Not suitable for:** {'; '.join(self.not_suitable_for)}

## Distribution
- **License:** {self.license}
- **Access restrictions:** {self.access_restrictions}

## Maintenance
- **Maintainer:** {self.maintainer}
- **Update frequency:** {self.update_frequency}
- **Versioning:** {self.versioning_scheme}
"""

# Example usage
datasheet = DataSheet(
    purpose="LLM pre-training corpus for enterprise applications",
    creators=["ML Platform Team", "Data Engineering"],
    funding="Internal R&D",
    instance_count=10_000_000_000,
    data_types=["text", "code", "structured"],
    sampling_strategy="Stratified by domain and quality",
    is_sample=False,
    collection_mechanism="Web crawl + licensed content + internal docs",
    collection_timeframe="2020-2024",
    ethical_review=True,
    consent_obtained=True,
    preprocessing_steps=[
        "Unicode normalization",
        "Deduplication (MinHash)",
        "Quality filtering (perplexity < 100)",
        "PII removal (Presidio)",
        "Toxicity filtering"
    ],
    raw_data_available=False,
    intended_uses=["Pre-training", "Fine-tuning", "RAG retrieval"],
    not_suitable_for=["Direct user-facing without moderation", "Medical diagnosis"],
    license="Internal use only",
    access_restrictions="Requires ML Platform access approval",
    maintainer="ml-platform@company.com",
    update_frequency="Quarterly",
    versioning_scheme="Semantic versioning (MAJOR.MINOR.PATCH)"
)
```

### 5.3 Audit Trail Implementation

```python
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any, Dict, List
import hashlib
import json

@dataclass
class AuditEvent:
    """Immutable audit event for data operations"""
    timestamp: str
    event_type: str  # CREATE, READ, UPDATE, DELETE, TRANSFORM
    actor: str
    resource: str
    resource_version: str
    details: Dict[str, Any]
    parent_event_id: Optional[str] = None

    def __post_init__(self):
        # Compute immutable event ID
        content = json.dumps({
            "timestamp": self.timestamp,
            "event_type": self.event_type,
            "actor": self.actor,
            "resource": self.resource,
            "details": self.details
        }, sort_keys=True)
        self.event_id = hashlib.sha256(content.encode()).hexdigest()[:16]

class AuditLog:
    """Append-only audit log for compliance"""

    def __init__(self, storage_path: str):
        self.storage_path = storage_path
        self._events: List[AuditEvent] = []

    def log_event(
        self,
        event_type: str,
        actor: str,
        resource: str,
        resource_version: str,
        details: Dict[str, Any],
        parent_event_id: Optional[str] = None
    ) -> AuditEvent:
        """Log an immutable audit event"""
        event = AuditEvent(
            timestamp=datetime.utcnow().isoformat() + "Z",
            event_type=event_type,
            actor=actor,
            resource=resource,
            resource_version=resource_version,
            details=details,
            parent_event_id=parent_event_id
        )

        # Append to log (never modify existing)
        self._append_to_storage(event)
        self._events.append(event)

        return event

    def log_data_access(self, actor: str, dataset: str, version: str, purpose: str):
        """Log data access for compliance"""
        return self.log_event(
            event_type="READ",
            actor=actor,
            resource=dataset,
            resource_version=version,
            details={
                "purpose": purpose,
                "access_time": datetime.utcnow().isoformat()
            }
        )

    def log_transformation(
        self,
        actor: str,
        input_datasets: List[Dict[str, str]],
        output_dataset: str,
        output_version: str,
        transformation: str
    ):
        """Log data transformation for lineage"""
        return self.log_event(
            event_type="TRANSFORM",
            actor=actor,
            resource=output_dataset,
            resource_version=output_version,
            details={
                "inputs": input_datasets,
                "transformation": transformation,
                "script_hash": self._get_script_hash(transformation)
            }
        )

    def get_resource_history(self, resource: str) -> List[AuditEvent]:
        """Get complete history for a resource"""
        return [e for e in self._events if e.resource == resource]

    def get_lineage(self, resource: str, version: str) -> Dict:
        """Reconstruct full lineage for a resource version"""
        events = self.get_resource_history(resource)

        # Find the creation/transform event for this version
        version_event = next(
            (e for e in events
             if e.resource_version == version
             and e.event_type in ["CREATE", "TRANSFORM"]),
            None
        )

        if not version_event:
            return {"error": "Version not found"}

        lineage = {
            "resource": resource,
            "version": version,
            "created_at": version_event.timestamp,
            "created_by": version_event.actor,
            "inputs": []
        }

        # Recursively get input lineage
        if version_event.event_type == "TRANSFORM":
            inputs = version_event.details.get("inputs", [])
            for inp in inputs:
                input_lineage = self.get_lineage(inp["resource"], inp["version"])
                lineage["inputs"].append(input_lineage)

        return lineage

    def generate_compliance_report(
        self,
        resource: str,
        start_date: str,
        end_date: str
    ) -> Dict:
        """Generate compliance report for auditors"""
        events = [
            e for e in self._events
            if e.resource == resource
            and start_date <= e.timestamp <= end_date
        ]

        return {
            "resource": resource,
            "period": {"start": start_date, "end": end_date},
            "total_events": len(events),
            "events_by_type": self._count_by_type(events),
            "unique_actors": list(set(e.actor for e in events)),
            "version_history": self._get_version_timeline(events),
            "events": [self._event_to_dict(e) for e in events]
        }
```

---

## 6. Configuration Templates

### 6.1 DVC Configuration

```yaml
# .dvc/config
[core]
    remote = s3storage
    autostage = true

[remote "s3storage"]
    url = s3://my-bucket/dvc-store
    region = us-east-1

[remote "gcs_backup"]
    url = gs://backup-bucket/dvc-store

# Cache settings
[cache]
    type = reflink,copy
    local = /mnt/fast-ssd/.dvc/cache
```

```yaml
# dvc.yaml - Complete pipeline definition
stages:
  download:
    cmd: python src/download.py --output data/raw/
    deps:
      - src/download.py
    params:
      - download.sources
      - download.date_range
    outs:
      - data/raw/:
          persist: true

  preprocess:
    cmd: python src/preprocess.py
    deps:
      - src/preprocess.py
      - data/raw/
    params:
      - preprocess
    outs:
      - data/processed/:
          cache: true

  deduplicate:
    cmd: python src/dedupe.py
    deps:
      - src/dedupe.py
      - data/processed/
    params:
      - dedupe.threshold
      - dedupe.method
    outs:
      - data/deduped/

  train:
    cmd: python src/train.py
    deps:
      - src/train.py
      - data/deduped/
    params:
      - train
    outs:
      - models/checkpoint/:
          persist: true
    metrics:
      - metrics/train.json:
          cache: false
    plots:
      - plots/loss.csv:
          x: step
          y: loss
```

### 6.2 LakeFS Configuration

```yaml
# lakefs-config.yaml
database:
  type: postgres
  connection_string: postgres://lakefs:password@postgres:5432/lakefs

blockstore:
  type: s3
  s3:
    region: us-east-1
    credentials_file: /etc/lakefs/.aws/credentials

auth:
  encrypt:
    secret_key: "your-256-bit-secret-key"

logging:
  format: json
  level: INFO

garbageCollection:
  enabled: true
  default_retention_days: 7

actions:
  enabled: true
```

**Pre-merge Hook (Lua):**
```lua
-- hooks/pre_merge.lua
-- Validate data quality before merging

local json = require("json")

function validate_merge(action)
    local source_branch = action.source_ref
    local target_branch = action.branch_id

    -- Check for required metadata
    local metadata = lakefs.stat(
        source_branch,
        "_metadata/quality_report.json"
    )

    if not metadata then
        return false, "Quality report missing"
    end

    -- Parse quality report
    local content = lakefs.read(source_branch, "_metadata/quality_report.json")
    local report = json.decode(content)

    -- Validate quality thresholds
    if report.duplicate_rate > 0.01 then
        return false, "Duplicate rate too high: " .. report.duplicate_rate
    end

    if report.null_rate > 0.05 then
        return false, "Null rate too high: " .. report.null_rate
    end

    return true, "Quality checks passed"
end

return validate_merge
```

### 6.3 Delta Lake Configuration

```python
# delta_config.py
DELTA_TABLE_PROPERTIES = {
    # Optimization settings
    "delta.autoOptimize.optimizeWrite": "true",
    "delta.autoOptimize.autoCompact": "true",

    # Data retention
    "delta.logRetentionDuration": "interval 30 days",
    "delta.deletedFileRetentionDuration": "interval 7 days",

    # Enable Change Data Feed for lineage
    "delta.enableChangeDataFeed": "true",

    # Column statistics
    "delta.dataSkippingNumIndexedCols": "32",

    # Checkpointing
    "delta.checkpointInterval": "10"
}

def create_delta_table_with_config(
    spark,
    df,
    path: str,
    partition_cols: List[str] = None
):
    """Create Delta table with optimized configuration"""

    writer = df.write.format("delta")

    # Apply properties
    for key, value in DELTA_TABLE_PROPERTIES.items():
        writer = writer.option(key, value)

    # Partitioning
    if partition_cols:
        writer = writer.partitionBy(*partition_cols)

    writer.save(path)

    # Enable CDF for existing table
    spark.sql(f"""
        ALTER TABLE delta.`{path}`
        SET TBLPROPERTIES (delta.enableChangeDataFeed = true)
    """)
```

---

## 7. Troubleshooting

### Common Issues

| Issue | Symptoms | Solution |
|-------|----------|----------|
| DVC push fails | "Permission denied" or timeout | Check remote credentials, network access |
| LakeFS branch conflicts | Merge fails | Use three-way merge, resolve conflicts manually |
| Delta Lake version mismatch | Reader can't read table | Upgrade reader or downgrade writer version |
| Lineage gaps | Missing transformation records | Ensure all jobs emit lineage events |
| Storage costs high | Unexpectedly large bills | Enable deduplication, vacuum old versions |

### DVC Debugging

```bash
# Verbose output
dvc push -v

# Check remote access
dvc remote list --show-url
aws s3 ls s3://bucket/dvc-store/

# Verify cache integrity
dvc cache verify

# Recover from corrupted cache
dvc gc -w  # Garbage collect workspace
dvc cache move /path/to/new/cache

# Debug pipeline issues
dvc repro -v --dry  # Dry run with verbose output
dvc dag --full      # Full dependency graph
```

### LakeFS Debugging

```bash
# Check server status
lakectl doctor

# Verify repository
lakectl repo show lakefs://repo

# Debug branch issues
lakectl branch list lakefs://repo
lakectl log lakefs://repo/branch --amount 10

# Check for uncommitted changes
lakectl diff lakefs://repo/branch

# Recover deleted branch
lakectl branch create lakefs://repo/recovered --source lakefs://repo/main@<commit-id>
```

### Delta Lake Debugging

```python
# Check table history
spark.sql("DESCRIBE HISTORY delta.`s3://bucket/table`").show(truncate=False)

# Verify table integrity
from delta.tables import DeltaTable
dt = DeltaTable.forPath(spark, "s3://bucket/table")
dt.detail().show()

# Restore corrupted table
spark.sql("""
    RESTORE TABLE delta.`s3://bucket/table`
    TO VERSION AS OF 5
""")

# Force vacuum with low retention (dangerous!)
spark.conf.set("spark.databricks.delta.retentionDurationCheck.enabled", "false")
dt.vacuum(0)  # Remove all old files
```

---

## Appendices

### Appendix A: Quick Reference Commands

```bash
# DVC Quick Reference
dvc init                    # Initialize DVC
dvc add <file>              # Track file
dvc push                    # Push to remote
dvc pull                    # Pull from remote
dvc checkout                # Sync workspace with DVC files
dvc repro                   # Reproduce pipeline
dvc diff                    # Show changes
dvc gc                      # Garbage collect

# LakeFS Quick Reference
lakectl repo create <repo> <storage>  # Create repo
lakectl branch create <branch>         # Create branch
lakectl commit <branch>                 # Commit changes
lakectl merge <source> <target>         # Merge branches
lakectl diff <ref1> <ref2>              # Compare refs
lakectl log <ref>                       # View history

# Delta Lake Quick Reference (SQL)
DESCRIBE HISTORY table              # View history
SELECT * FROM table VERSION AS OF n # Time travel
RESTORE TABLE table TO VERSION AS OF n  # Restore
VACUUM table RETAIN n HOURS         # Clean old files
OPTIMIZE table                      # Compact files
```

### Appendix B: Data Versioning Policy Template

```markdown
# Data Versioning Policy

## Version Numbering
- MAJOR.MINOR.PATCH format
- MAJOR: Schema changes, breaking changes
- MINOR: New data sources, feature additions
- PATCH: Bug fixes, quality improvements

## Branching Strategy
- `main`: Production-ready data
- `staging`: Pre-production validation
- `feature/*`: Experimental changes
- `hotfix/*`: Urgent fixes

## Retention Policy
- Production versions: 1 year
- Staging versions: 90 days
- Feature branches: 30 days after merge/close

## Access Control
- Read access: All ML engineers
- Write access: Data team only
- Delete access: Data leads only (with approval)

## Quality Gates
- All data must pass automated quality checks
- Changes > 10% require manual review
- Schema changes require architecture review
```

---

## Glossary

| Term | Definition |
|------|------------|
| **ACID** | Atomicity, Consistency, Isolation, Durability - transaction guarantees |
| **CAS** | Content-Addressable Storage - storage addressed by content hash |
| **CDC** | Change Data Capture - tracking changes in databases |
| **CDF** | Change Data Feed - Delta Lake feature for tracking changes |
| **Data Lineage** | Record of data's origins and transformations |
| **Deduplication** | Storing only unique data blocks to save space |
| **Delta Encoding** | Storing only differences between versions |
| **GDPR** | General Data Protection Regulation - EU privacy law |
| **Medallion Architecture** | Bronze/Silver/Gold data lake pattern |
| **Time Travel** | Querying historical versions of data |
| **Zero-Copy Branching** | Creating branches without duplicating data |

---

## References

- [DVC Documentation](https://dvc.org/doc)
- [LakeFS Documentation](https://docs.lakefs.io/)
- [Delta Lake Documentation](https://docs.delta.io/)
- [Apache Iceberg Documentation](https://iceberg.apache.org/docs/latest/)
- [OpenLineage Specification](https://openlineage.io/)
- [DataHub Documentation](https://datahubproject.io/docs/)
- [MLflow Data Versioning](https://lakefs.io/blog/mlflow-data-versioning/)
- [Medallion Architecture](https://www.databricks.com/glossary/medallion-architecture)
