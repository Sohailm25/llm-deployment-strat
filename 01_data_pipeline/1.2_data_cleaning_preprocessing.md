# Document 1.2: Data Cleaning & Preprocessing Guide

**Document ID:** 1.2
**Version:** 1.0
**Last Updated:** December 2025
**Author:** LLM Platform Team
**Status:** Complete
**Priority:** P0

> **Navigation** | [← 1.1 Data Collection](1.1_data_collection_sourcing.md) | [1.3 Data Labeling →](1.3_data_labeling_annotation.md)
>
> | | |
> |---|---|
> | **Prerequisites** | [1.1 Data Collection](1.1_data_collection_sourcing.md) |
> | **Related** | [1.3 Data Labeling](1.3_data_labeling_annotation.md) &#124; [1.6 Data Quality](1.6_data_quality_assurance.md) &#124; [7.3 Chunking](../07_rag_pipeline/7.3_chunking_strategies_guide.md) |
> | **Next** | [1.3 Data Labeling & Annotation](1.3_data_labeling_annotation.md) |

---

## Executive Summary

Data cleaning and preprocessing transform raw data into high-quality training corpora. This guide covers the complete preprocessing pipeline from text normalization through quality filtering, leveraging both CPU-based and GPU-accelerated approaches for scale.

Key takeaways:
- Modern preprocessing pipelines process petabytes using distributed frameworks (NeMo Curator, datatrove)
- Quality filtering at scale requires multi-stage approaches combining heuristics and model-based scoring
- Deduplication (exact + fuzzy) typically removes 20-30% of raw web data
- GPU acceleration via RAPIDS can achieve 10-100x speedups over CPU processing
- The FineWeb/RefinedWeb approach represents current best practices

---

## Table of Contents

1. [Prerequisites](#prerequisites)
2. [Text Preprocessing Pipeline](#text-preprocessing-pipeline)
3. [Document Structure Extraction](#document-structure-extraction)
4. [Domain-Specific Preprocessing](#domain-specific-preprocessing)
5. [Preprocessing at Scale](#preprocessing-at-scale)
6. [Quality Metrics & Validation](#quality-metrics--validation)
7. [Implementation Guide](#implementation-guide)
8. [Code Examples](#code-examples)
9. [Configuration Templates](#configuration-templates)
10. [Troubleshooting](#troubleshooting)
11. [Appendices](#appendices)
12. [Glossary](#glossary)
13. [References](#references)

---

## Prerequisites

### Required Knowledge
- Python programming (intermediate to advanced)
- Understanding of text processing fundamentals
- Basic NLP concepts (tokenization, n-grams)
- Familiarity with distributed computing concepts

### Required Tools & Access
```bash
# Core dependencies
pip install nemo-curator  # NVIDIA's GPU-accelerated curation
pip install datatrove     # HuggingFace's processing library
pip install trafilatura   # Web content extraction
pip install pyspark       # Distributed processing
pip install ray[data]     # Ray Data for processing
pip install dask[distributed]  # Dask distributed
pip install fasttext      # Language detection
pip install presidio-analyzer presidio-anonymizer  # PII detection
pip install datasketch    # MinHash deduplication
pip install ftfy          # Text encoding fixes
pip install nltk spacy    # NLP utilities
```

### Hardware Requirements
| Scale | Recommended Setup |
|-------|-------------------|
| < 100GB | Single machine, 32GB+ RAM |
| 100GB - 1TB | Multi-core CPU, 128GB+ RAM |
| 1TB - 10TB | Distributed cluster (Spark/Dask) |
| > 10TB | GPU cluster (NeMo Curator, RAPIDS) |

---

## Text Preprocessing Pipeline

### Pipeline Overview

```
Raw Data → Extract → Normalize → Filter → Dedupe → Quality Score → Output
    │         │          │         │        │            │
    └── HTML/PDF         │         │        │            └── Perplexity
        Extraction       │         │        │                scoring
                         │         │        └── MinHash LSH
                         │         │            Exact hash
                         │         └── Language detection
                         │             Length filters
                         │             Heuristic rules
                         └── Unicode normalization
                             Encoding fixes
                             Whitespace standardization
```

### 1. Text Normalization

```python
"""
Text normalization utilities for LLM training data.
Handles Unicode, encoding, and whitespace issues.
"""

import unicodedata
import re
import ftfy
from typing import Optional


class TextNormalizer:
    """
    Normalize text for LLM training.
    Based on FineWeb/RefinedWeb preprocessing approaches.
    """

    def __init__(
        self,
        unicode_form: str = "NFKC",
        fix_encoding: bool = True,
        normalize_whitespace: bool = True,
        remove_control_chars: bool = True,
        normalize_quotes: bool = True
    ):
        self.unicode_form = unicode_form
        self.fix_encoding = fix_encoding
        self.normalize_whitespace = normalize_whitespace
        self.remove_control_chars = remove_control_chars
        self.normalize_quotes = normalize_quotes

        # Control characters to remove (except newlines and tabs)
        self.control_char_pattern = re.compile(
            r"[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]"
        )

        # Quote normalization mappings
        self.quote_map = {
            """: '"', """: '"', "'": "'", "'": "'",
            "«": '"', "»": '"', "‹": "'", "›": "'",
            "„": '"', "‚": "'", "『": '"', "』": '"',
            "「": "'", "」": "'", "【": "[", "】": "]"
        }

    def normalize(self, text: str) -> str:
        """
        Apply full normalization pipeline.

        Args:
            text: Raw input text

        Returns:
            Normalized text
        """
        if not text:
            return ""

        # Step 1: Fix encoding issues using ftfy
        if self.fix_encoding:
            text = ftfy.fix_text(
                text,
                normalization="NFKC" if self.unicode_form == "NFKC" else None,
                fix_latin_ligatures=True,
                fix_character_width=True,
                uncurl_quotes=False,  # We handle this separately
                fix_line_breaks=True
            )

        # Step 2: Unicode normalization
        text = unicodedata.normalize(self.unicode_form, text)

        # Step 3: Remove control characters
        if self.remove_control_chars:
            text = self.control_char_pattern.sub("", text)

        # Step 4: Normalize quotes
        if self.normalize_quotes:
            for old, new in self.quote_map.items():
                text = text.replace(old, new)

        # Step 5: Normalize whitespace
        if self.normalize_whitespace:
            # Replace multiple spaces with single space
            text = re.sub(r" +", " ", text)
            # Normalize line endings
            text = text.replace("\r\n", "\n").replace("\r", "\n")
            # Remove excessive newlines (more than 2 consecutive)
            text = re.sub(r"\n{3,}", "\n\n", text)
            # Strip leading/trailing whitespace from lines
            lines = [line.strip() for line in text.split("\n")]
            text = "\n".join(lines)
            # Strip document
            text = text.strip()

        return text

    def normalize_batch(self, texts: list[str]) -> list[str]:
        """Normalize batch of texts."""
        return [self.normalize(text) for text in texts]


# Example usage for common encoding issues
ENCODING_EXAMPLES = """
Common encoding issues fixed by normalization:

Before → After
──────────────
â€™ → '        (UTF-8 interpreted as Windows-1252)
â€œ → "        (Double quote mojibake)
Ã© → é        (Accented character mojibake)
&amp; → &     (HTML entities)
\xa0 → ' '    (Non-breaking space to regular space)
\u200b → ''   (Zero-width space removed)
ﬁ → fi        (Ligature decomposed)
"""


# Specialized normalizers for different content types
class CodeNormalizer:
    """Normalize code content while preserving structure."""

    def __init__(self):
        self.text_normalizer = TextNormalizer(
            normalize_whitespace=False,  # Preserve indentation
            normalize_quotes=False       # Preserve string quotes
        )

    def normalize(self, code: str, language: str = None) -> str:
        """
        Normalize code while preserving semantics.

        Args:
            code: Source code
            language: Programming language (optional)

        Returns:
            Normalized code
        """
        # Fix encoding issues
        code = ftfy.fix_text(code)

        # Normalize line endings
        code = code.replace("\r\n", "\n").replace("\r", "\n")

        # Remove trailing whitespace from lines
        lines = [line.rstrip() for line in code.split("\n")]
        code = "\n".join(lines)

        # Ensure file ends with newline
        if code and not code.endswith("\n"):
            code += "\n"

        return code


class HTMLTextExtractor:
    """Extract and normalize text from HTML."""

    def __init__(self):
        self.normalizer = TextNormalizer()

    def extract(self, html: str) -> str:
        """
        Extract clean text from HTML.

        Args:
            html: Raw HTML content

        Returns:
            Extracted and normalized text
        """
        import trafilatura

        # Extract using trafilatura (handles boilerplate removal)
        text = trafilatura.extract(
            html,
            include_comments=False,
            include_tables=True,
            include_links=False,
            include_images=False,
            favor_precision=True,
            deduplicate=True
        )

        if text:
            text = self.normalizer.normalize(text)

        return text or ""
```

### 2. Deduplication Methods

```python
"""
Deduplication methods for LLM training data.
Supports exact matching, MinHash LSH, and SimHash.
"""

import hashlib
import re
from typing import List, Tuple, Set, Optional, Generator
from dataclasses import dataclass
from collections import defaultdict
import struct


@dataclass
class DeduplicationResult:
    """Result of deduplication analysis."""
    is_duplicate: bool
    duplicate_of: Optional[str] = None
    similarity_score: float = 0.0
    method: str = "unknown"


class ExactDeduplicator:
    """
    Exact duplicate detection using content hashing.
    Fast and memory-efficient for exact matches.
    """

    def __init__(self, normalize: bool = True):
        self.normalize = normalize
        self.seen_hashes: Set[str] = set()
        self.hash_to_id: dict[str, str] = {}

    def _normalize_text(self, text: str) -> str:
        """Normalize text before hashing."""
        if not self.normalize:
            return text

        # Lowercase and normalize whitespace
        text = text.lower()
        text = re.sub(r"\s+", " ", text)
        text = text.strip()
        return text

    def _compute_hash(self, text: str) -> str:
        """Compute SHA-256 hash of text."""
        normalized = self._normalize_text(text)
        return hashlib.sha256(normalized.encode("utf-8")).hexdigest()

    def check(self, text: str, doc_id: str = None) -> DeduplicationResult:
        """
        Check if text is exact duplicate.

        Args:
            text: Document text
            doc_id: Optional document ID

        Returns:
            DeduplicationResult
        """
        content_hash = self._compute_hash(text)

        if content_hash in self.seen_hashes:
            return DeduplicationResult(
                is_duplicate=True,
                duplicate_of=self.hash_to_id.get(content_hash),
                similarity_score=1.0,
                method="exact_hash"
            )

        return DeduplicationResult(
            is_duplicate=False,
            method="exact_hash"
        )

    def add(self, text: str, doc_id: str) -> str:
        """Add document to index."""
        content_hash = self._compute_hash(text)
        self.seen_hashes.add(content_hash)
        self.hash_to_id[content_hash] = doc_id
        return content_hash


class MinHashDeduplicator:
    """
    Near-duplicate detection using MinHash LSH.
    Based on FineWeb/SlimPajama methodology.
    """

    def __init__(
        self,
        num_perm: int = 128,
        threshold: float = 0.8,
        ngram_size: int = 5,
        min_length: int = 0
    ):
        """
        Initialize MinHash deduplicator.

        Args:
            num_perm: Number of hash permutations (higher = more accurate)
            threshold: Jaccard similarity threshold for duplicates
            ngram_size: Size of n-grams for shingling
            min_length: Minimum document length to process
        """
        from datasketch import MinHash, MinHashLSH

        self.num_perm = num_perm
        self.threshold = threshold
        self.ngram_size = ngram_size
        self.min_length = min_length

        self.lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)
        self.minhashes: dict[str, MinHash] = {}

    def _get_ngrams(self, text: str) -> List[str]:
        """Extract n-grams from text."""
        # Normalize
        text = text.lower()
        text = re.sub(r"\s+", " ", text)

        # Split into words
        words = text.split()

        if len(words) < self.ngram_size:
            return [" ".join(words)]

        # Create n-grams
        ngrams = []
        for i in range(len(words) - self.ngram_size + 1):
            ngram = " ".join(words[i:i + self.ngram_size])
            ngrams.append(ngram)

        return ngrams

    def _compute_minhash(self, text: str) -> "MinHash":
        """Compute MinHash signature for text."""
        from datasketch import MinHash

        minhash = MinHash(num_perm=self.num_perm)

        for ngram in self._get_ngrams(text):
            minhash.update(ngram.encode("utf-8"))

        return minhash

    def check(self, text: str, doc_id: str = None) -> DeduplicationResult:
        """
        Check if text is near-duplicate of existing document.

        Args:
            text: Document text
            doc_id: Optional document ID

        Returns:
            DeduplicationResult with similarity info
        """
        if len(text) < self.min_length:
            return DeduplicationResult(
                is_duplicate=False,
                method="minhash_lsh"
            )

        minhash = self._compute_minhash(text)

        # Query LSH index
        candidates = self.lsh.query(minhash)

        if candidates:
            # Find best match
            best_match = None
            best_similarity = 0.0

            for candidate_id in candidates:
                if candidate_id in self.minhashes:
                    similarity = minhash.jaccard(self.minhashes[candidate_id])
                    if similarity > best_similarity:
                        best_similarity = similarity
                        best_match = candidate_id

            if best_similarity >= self.threshold:
                return DeduplicationResult(
                    is_duplicate=True,
                    duplicate_of=best_match,
                    similarity_score=best_similarity,
                    method="minhash_lsh"
                )

        return DeduplicationResult(
            is_duplicate=False,
            method="minhash_lsh"
        )

    def add(self, text: str, doc_id: str) -> None:
        """Add document to LSH index."""
        if len(text) < self.min_length:
            return

        minhash = self._compute_minhash(text)
        self.lsh.insert(doc_id, minhash)
        self.minhashes[doc_id] = minhash


class SimHashDeduplicator:
    """
    Near-duplicate detection using SimHash.
    Alternative to MinHash, works well for longer documents.
    """

    def __init__(self, hash_bits: int = 64, hamming_threshold: int = 3):
        """
        Initialize SimHash deduplicator.

        Args:
            hash_bits: Number of bits in SimHash (64 or 128)
            hamming_threshold: Max Hamming distance for duplicates
        """
        self.hash_bits = hash_bits
        self.hamming_threshold = hamming_threshold
        self.hashes: dict[str, int] = {}

    def _tokenize(self, text: str) -> List[str]:
        """Tokenize text into features."""
        text = text.lower()
        # Use word-level tokens
        words = re.findall(r"\w+", text)
        return words

    def _compute_simhash(self, text: str) -> int:
        """Compute SimHash for text."""
        tokens = self._tokenize(text)

        if not tokens:
            return 0

        # Initialize bit counters
        v = [0] * self.hash_bits

        for token in tokens:
            # Hash each token
            token_hash = int(
                hashlib.md5(token.encode()).hexdigest()[:16],
                16
            )

            # Update counters
            for i in range(self.hash_bits):
                if token_hash & (1 << i):
                    v[i] += 1
                else:
                    v[i] -= 1

        # Generate final hash
        simhash = 0
        for i in range(self.hash_bits):
            if v[i] > 0:
                simhash |= (1 << i)

        return simhash

    def _hamming_distance(self, hash1: int, hash2: int) -> int:
        """Compute Hamming distance between two hashes."""
        return bin(hash1 ^ hash2).count("1")

    def check(self, text: str, doc_id: str = None) -> DeduplicationResult:
        """Check if text is near-duplicate using SimHash."""
        simhash = self._compute_simhash(text)

        for existing_id, existing_hash in self.hashes.items():
            distance = self._hamming_distance(simhash, existing_hash)

            if distance <= self.hamming_threshold:
                similarity = 1.0 - (distance / self.hash_bits)
                return DeduplicationResult(
                    is_duplicate=True,
                    duplicate_of=existing_id,
                    similarity_score=similarity,
                    method="simhash"
                )

        return DeduplicationResult(
            is_duplicate=False,
            method="simhash"
        )

    def add(self, text: str, doc_id: str) -> int:
        """Add document to index."""
        simhash = self._compute_simhash(text)
        self.hashes[doc_id] = simhash
        return simhash


class CombinedDeduplicator:
    """
    Combined exact + fuzzy deduplication.
    Recommended approach for production pipelines.
    """

    def __init__(
        self,
        exact_normalize: bool = True,
        minhash_threshold: float = 0.8,
        minhash_num_perm: int = 128,
        ngram_size: int = 5
    ):
        self.exact_dedup = ExactDeduplicator(normalize=exact_normalize)
        self.minhash_dedup = MinHashDeduplicator(
            threshold=minhash_threshold,
            num_perm=minhash_num_perm,
            ngram_size=ngram_size
        )

    def check(self, text: str, doc_id: str = None) -> DeduplicationResult:
        """
        Check for duplicates using combined approach.
        First checks exact, then fuzzy if not exact duplicate.
        """
        # Fast exact check first
        exact_result = self.exact_dedup.check(text, doc_id)
        if exact_result.is_duplicate:
            return exact_result

        # Fuzzy check for near-duplicates
        return self.minhash_dedup.check(text, doc_id)

    def add(self, text: str, doc_id: str) -> None:
        """Add document to both indices."""
        self.exact_dedup.add(text, doc_id)
        self.minhash_dedup.add(text, doc_id)


# Spark-based deduplication for large scale
SPARK_DEDUP_EXAMPLE = """
# PySpark MinHash deduplication at scale

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, udf, sha2, lower, regexp_replace
from pyspark.sql.types import StringType, ArrayType
from pyspark.ml.feature import MinHashLSH, NGram, HashingTF

spark = SparkSession.builder \\
    .appName("LLM_Deduplication") \\
    .config("spark.sql.shuffle.partitions", 1000) \\
    .getOrCreate()

# Load data
df = spark.read.parquet("s3://bucket/raw_data/")

# Step 1: Exact deduplication
df_exact = df.withColumn(
    "content_hash",
    sha2(lower(regexp_replace(col("text"), r"\\s+", " ")), 256)
).dropDuplicates(["content_hash"])

# Step 2: Create n-grams for MinHash
tokenizer_udf = udf(
    lambda x: x.lower().split() if x else [],
    ArrayType(StringType())
)

df_tokenized = df_exact.withColumn("tokens", tokenizer_udf(col("text")))

ngram = NGram(n=5, inputCol="tokens", outputCol="ngrams")
df_ngrams = ngram.transform(df_tokenized)

# Step 3: Hash n-grams
hashingTF = HashingTF(inputCol="ngrams", outputCol="features", numFeatures=10000)
df_hashed = hashingTF.transform(df_ngrams)

# Step 4: Apply MinHash LSH
mh = MinHashLSH(inputCol="features", outputCol="hashes", numHashTables=5)
model = mh.fit(df_hashed)

# Find similar pairs
similar_pairs = model.approxSimilarityJoin(
    df_hashed, df_hashed,
    threshold=0.2,  # 1 - Jaccard similarity
    distCol="distance"
).filter(col("datasetA.id") < col("datasetB.id"))

# Remove duplicates (keep first occurrence)
duplicates_to_remove = similar_pairs.select(
    col("datasetB.id").alias("dup_id")
).distinct()

df_deduped = df_hashed.join(
    duplicates_to_remove,
    df_hashed.id == duplicates_to_remove.dup_id,
    "left_anti"
)

df_deduped.write.parquet("s3://bucket/deduped_data/")
"""
```

### 3. Quality Filtering

```python
"""
Quality filtering for LLM training data.
Implements heuristic, classifier-based, and perplexity filtering.
"""

from dataclasses import dataclass, field
from typing import List, Tuple, Dict, Optional, Callable
import re
import math
from abc import ABC, abstractmethod


@dataclass
class QualityScore:
    """Quality assessment result."""
    passes: bool
    score: float
    reasons: List[str] = field(default_factory=list)
    metrics: Dict[str, float] = field(default_factory=dict)


class QualityFilter(ABC):
    """Base class for quality filters."""

    @abstractmethod
    def score(self, text: str) -> QualityScore:
        """Score document quality."""
        pass


class GopherQualityFilter(QualityFilter):
    """
    Quality filter based on DeepMind's Gopher rules.
    Reference: Rae et al., "Scaling Language Models: Methods, Analysis
    & Insights from Training Gopher"
    """

    def __init__(
        self,
        min_words: int = 50,
        max_words: int = 100000,
        min_mean_word_length: float = 3.0,
        max_mean_word_length: float = 10.0,
        max_symbol_to_word_ratio: float = 0.1,
        max_bullet_ratio: float = 0.9,
        max_ellipsis_ratio: float = 0.3,
        min_alpha_ratio: float = 0.8,
        max_duplicate_line_char_ratio: float = 0.3,
        max_duplicate_paragraph_ratio: float = 0.3,
        top_ngram_thresholds: Dict[int, float] = None
    ):
        self.min_words = min_words
        self.max_words = max_words
        self.min_mean_word_length = min_mean_word_length
        self.max_mean_word_length = max_mean_word_length
        self.max_symbol_to_word_ratio = max_symbol_to_word_ratio
        self.max_bullet_ratio = max_bullet_ratio
        self.max_ellipsis_ratio = max_ellipsis_ratio
        self.min_alpha_ratio = min_alpha_ratio
        self.max_duplicate_line_char_ratio = max_duplicate_line_char_ratio
        self.max_duplicate_paragraph_ratio = max_duplicate_paragraph_ratio

        # N-gram repetition thresholds (more lenient than Gopher)
        self.top_ngram_thresholds = top_ngram_thresholds or {
            2: 0.20,  # 20% for bigrams
            3: 0.18,  # 18% for trigrams
            4: 0.16,  # 16% for 4-grams
        }

    def score(self, text: str) -> QualityScore:
        """Apply Gopher quality rules."""
        reasons = []
        metrics = {}

        # Word-level metrics
        words = text.split()
        word_count = len(words)
        metrics["word_count"] = word_count

        # Check word count bounds
        if word_count < self.min_words:
            reasons.append(f"Too few words: {word_count} < {self.min_words}")
        if word_count > self.max_words:
            reasons.append(f"Too many words: {word_count} > {self.max_words}")

        if word_count == 0:
            return QualityScore(
                passes=False,
                score=0.0,
                reasons=["Empty document"],
                metrics=metrics
            )

        # Mean word length
        mean_word_length = sum(len(w) for w in words) / word_count
        metrics["mean_word_length"] = mean_word_length

        if mean_word_length < self.min_mean_word_length:
            reasons.append(
                f"Mean word length too short: {mean_word_length:.2f}"
            )
        if mean_word_length > self.max_mean_word_length:
            reasons.append(
                f"Mean word length too long: {mean_word_length:.2f}"
            )

        # Symbol to word ratio
        symbols = re.findall(r"[#…]", text)
        symbol_ratio = len(symbols) / word_count
        metrics["symbol_ratio"] = symbol_ratio

        if symbol_ratio > self.max_symbol_to_word_ratio:
            reasons.append(f"Too many symbols: {symbol_ratio:.2%}")

        # Bullet point ratio
        lines = text.split("\n")
        bullet_lines = sum(
            1 for line in lines
            if line.strip().startswith(("•", "-", "*", "►", "●", "○"))
        )
        bullet_ratio = bullet_lines / max(len(lines), 1)
        metrics["bullet_ratio"] = bullet_ratio

        if bullet_ratio > self.max_bullet_ratio:
            reasons.append(f"Too many bullet points: {bullet_ratio:.2%}")

        # Ellipsis ratio
        ellipsis_count = text.count("...") + text.count("…")
        ellipsis_ratio = ellipsis_count / word_count
        metrics["ellipsis_ratio"] = ellipsis_ratio

        if ellipsis_ratio > self.max_ellipsis_ratio:
            reasons.append(f"Too many ellipses: {ellipsis_ratio:.2%}")

        # Alphabetic character ratio
        alpha_chars = sum(1 for c in text if c.isalpha())
        total_non_space = sum(1 for c in text if not c.isspace())
        alpha_ratio = alpha_chars / max(total_non_space, 1)
        metrics["alpha_ratio"] = alpha_ratio

        if alpha_ratio < self.min_alpha_ratio:
            reasons.append(
                f"Too few alphabetic characters: {alpha_ratio:.2%}"
            )

        # Duplicate line ratio
        line_texts = [l.strip() for l in lines if l.strip()]
        if line_texts:
            unique_lines = set(line_texts)
            dup_chars = sum(
                len(l) for l in line_texts
                if line_texts.count(l) > 1
            )
            total_chars = sum(len(l) for l in line_texts)
            dup_line_ratio = dup_chars / max(total_chars, 1)
            metrics["duplicate_line_char_ratio"] = dup_line_ratio

            if dup_line_ratio > self.max_duplicate_line_char_ratio:
                reasons.append(
                    f"Too many duplicate lines: {dup_line_ratio:.2%}"
                )

        # Duplicate paragraph ratio
        paragraphs = [p.strip() for p in text.split("\n\n") if p.strip()]
        if len(paragraphs) > 1:
            unique_paragraphs = set(paragraphs)
            dup_para_ratio = 1 - len(unique_paragraphs) / len(paragraphs)
            metrics["duplicate_paragraph_ratio"] = dup_para_ratio

            if dup_para_ratio > self.max_duplicate_paragraph_ratio:
                reasons.append(
                    f"Too many duplicate paragraphs: {dup_para_ratio:.2%}"
                )

        # N-gram repetition check
        for n, threshold in self.top_ngram_thresholds.items():
            ngram_ratio = self._top_ngram_ratio(words, n)
            metrics[f"top_{n}gram_ratio"] = ngram_ratio

            if ngram_ratio > threshold:
                reasons.append(
                    f"Too many repeated {n}-grams: {ngram_ratio:.2%} > {threshold:.2%}"
                )

        # Calculate overall score (0-1)
        if reasons:
            # Penalize based on number of failures
            score = max(0, 1 - len(reasons) * 0.2)
        else:
            score = 1.0

        return QualityScore(
            passes=len(reasons) == 0,
            score=score,
            reasons=reasons,
            metrics=metrics
        )

    def _top_ngram_ratio(self, words: List[str], n: int) -> float:
        """Calculate ratio of most common n-gram."""
        if len(words) < n:
            return 0.0

        ngrams = []
        for i in range(len(words) - n + 1):
            ngram = tuple(words[i:i + n])
            ngrams.append(ngram)

        if not ngrams:
            return 0.0

        # Count occurrences
        from collections import Counter
        counts = Counter(ngrams)
        most_common_count = counts.most_common(1)[0][1]

        return most_common_count / len(ngrams)


class C4QualityFilter(QualityFilter):
    """
    Quality filter based on C4 (Colossal Clean Crawled Corpus) rules.
    Reference: Raffel et al., "Exploring the Limits of Transfer Learning
    with a Unified Text-to-Text Transformer"
    """

    CURSED_SUBSTRINGS = [
        "javascript",
        "cookie",
        "privacy policy",
        "terms of service",
        "terms of use",
        "sign up",
        "sign in",
        "log in",
        "login",
        "register",
        "subscribe",
        "newsletter",
        "click here",
        "read more",
        "learn more",
        "{",  # JSON artifacts
        "<",  # HTML artifacts
    ]

    SENTENCE_ENDINGS = {".", "!", "?", '"', "'"}

    def __init__(
        self,
        min_words: int = 50,
        min_sentences: int = 3,
        require_sentence_ending: bool = True,
        filter_curly_braces: bool = True,
        filter_lorem_ipsum: bool = True,
        filter_javascript: bool = True
    ):
        self.min_words = min_words
        self.min_sentences = min_sentences
        self.require_sentence_ending = require_sentence_ending
        self.filter_curly_braces = filter_curly_braces
        self.filter_lorem_ipsum = filter_lorem_ipsum
        self.filter_javascript = filter_javascript

    def score(self, text: str) -> QualityScore:
        """Apply C4 quality rules."""
        reasons = []
        metrics = {}

        text_lower = text.lower()

        # Word count
        words = text.split()
        word_count = len(words)
        metrics["word_count"] = word_count

        if word_count < self.min_words:
            reasons.append(f"Too few words: {word_count}")

        # Sentence count
        sentences = re.split(r"[.!?]+", text)
        sentences = [s.strip() for s in sentences if s.strip()]
        sentence_count = len(sentences)
        metrics["sentence_count"] = sentence_count

        if sentence_count < self.min_sentences:
            reasons.append(f"Too few sentences: {sentence_count}")

        # Check sentence ending
        if self.require_sentence_ending:
            if text and text[-1] not in self.SENTENCE_ENDINGS:
                reasons.append("Does not end with sentence punctuation")

        # Check for curly braces (JSON/code artifacts)
        if self.filter_curly_braces:
            if "{" in text:
                reasons.append("Contains curly braces (possible JSON/code)")

        # Check for lorem ipsum
        if self.filter_lorem_ipsum:
            if "lorem ipsum" in text_lower:
                reasons.append("Contains lorem ipsum placeholder")

        # Check for JavaScript keywords
        if self.filter_javascript:
            js_keywords = ["javascript", "var ", "function(", "document."]
            for keyword in js_keywords:
                if keyword in text_lower:
                    reasons.append(f"Contains JavaScript keyword: {keyword}")
                    break

        # Check for boilerplate
        boilerplate_count = sum(
            1 for substr in self.CURSED_SUBSTRINGS
            if substr in text_lower
        )
        metrics["boilerplate_matches"] = boilerplate_count

        if boilerplate_count > 3:
            reasons.append(f"Too much boilerplate: {boilerplate_count} matches")

        score = 1.0 if not reasons else max(0, 1 - len(reasons) * 0.15)

        return QualityScore(
            passes=len(reasons) == 0,
            score=score,
            reasons=reasons,
            metrics=metrics
        )


class PerplexityFilter(QualityFilter):
    """
    Quality filter based on language model perplexity.
    Lower perplexity = more "natural" text.
    """

    def __init__(
        self,
        model_name: str = "gpt2",
        max_perplexity: float = 1000,
        min_perplexity: float = 5,
        device: str = "cuda"
    ):
        import torch
        from transformers import GPT2LMHeadModel, GPT2TokenizerFast

        self.max_perplexity = max_perplexity
        self.min_perplexity = min_perplexity

        self.device = torch.device(
            device if torch.cuda.is_available() and device == "cuda"
            else "cpu"
        )

        self.tokenizer = GPT2TokenizerFast.from_pretrained(model_name)
        self.model = GPT2LMHeadModel.from_pretrained(model_name).to(self.device)
        self.model.eval()

    def _compute_perplexity(self, text: str, stride: int = 512) -> float:
        """Compute perplexity using sliding window."""
        import torch

        encodings = self.tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            max_length=1024
        )

        max_length = self.model.config.n_positions
        seq_len = encodings.input_ids.size(1)

        nlls = []
        prev_end_loc = 0

        for begin_loc in range(0, seq_len, stride):
            end_loc = min(begin_loc + max_length, seq_len)
            trg_len = end_loc - prev_end_loc

            input_ids = encodings.input_ids[:, begin_loc:end_loc].to(self.device)
            target_ids = input_ids.clone()
            target_ids[:, :-trg_len] = -100

            with torch.no_grad():
                outputs = self.model(input_ids, labels=target_ids)
                neg_log_likelihood = outputs.loss * trg_len

            nlls.append(neg_log_likelihood)
            prev_end_loc = end_loc

            if end_loc == seq_len:
                break

        ppl = torch.exp(torch.stack(nlls).sum() / end_loc)
        return ppl.item()

    def score(self, text: str) -> QualityScore:
        """Score document based on perplexity."""
        reasons = []
        metrics = {}

        try:
            perplexity = self._compute_perplexity(text)
            metrics["perplexity"] = perplexity

            if perplexity > self.max_perplexity:
                reasons.append(
                    f"Perplexity too high: {perplexity:.2f} > {self.max_perplexity}"
                )

            if perplexity < self.min_perplexity:
                reasons.append(
                    f"Perplexity too low (possibly repetitive): "
                    f"{perplexity:.2f} < {self.min_perplexity}"
                )

            # Normalize score to 0-1 range
            # Optimal perplexity around 50-200
            optimal_ppl = 100
            score = max(0, 1 - abs(math.log(perplexity / optimal_ppl)) / 5)

        except Exception as e:
            reasons.append(f"Perplexity calculation failed: {e}")
            score = 0.0
            metrics["perplexity"] = -1

        return QualityScore(
            passes=len(reasons) == 0,
            score=score,
            reasons=reasons,
            metrics=metrics
        )


class ClassifierQualityFilter(QualityFilter):
    """
    Quality filter using trained classifier.
    Similar to FineWeb's quality classifier approach.
    """

    def __init__(
        self,
        model_path: str,
        threshold: float = 0.5,
        model_type: str = "fasttext"
    ):
        self.threshold = threshold
        self.model_type = model_type

        if model_type == "fasttext":
            import fasttext
            self.model = fasttext.load_model(model_path)
        else:
            raise ValueError(f"Unsupported model type: {model_type}")

    def score(self, text: str) -> QualityScore:
        """Score document using classifier."""
        reasons = []
        metrics = {}

        # Get prediction
        text_clean = text.replace("\n", " ")[:10000]  # Limit length

        if self.model_type == "fasttext":
            predictions = self.model.predict(text_clean, k=1)
            label = predictions[0][0].replace("__label__", "")
            confidence = predictions[1][0]

            metrics["classifier_label"] = label
            metrics["classifier_confidence"] = confidence

            # Assume labels are "high_quality" and "low_quality"
            if label == "low_quality" and confidence > self.threshold:
                reasons.append(
                    f"Classified as low quality with confidence {confidence:.2%}"
                )

            score = confidence if label == "high_quality" else 1 - confidence

        return QualityScore(
            passes=len(reasons) == 0,
            score=score,
            reasons=reasons,
            metrics=metrics
        )


class CompositeQualityFilter(QualityFilter):
    """
    Combine multiple quality filters with configurable weights.
    """

    def __init__(
        self,
        filters: List[Tuple[QualityFilter, float]],
        require_all_pass: bool = False,
        min_combined_score: float = 0.5
    ):
        """
        Args:
            filters: List of (filter, weight) tuples
            require_all_pass: If True, all filters must pass
            min_combined_score: Minimum weighted score to pass
        """
        self.filters = filters
        self.require_all_pass = require_all_pass
        self.min_combined_score = min_combined_score

        # Normalize weights
        total_weight = sum(w for _, w in filters)
        self.normalized_weights = [w / total_weight for _, w in filters]

    def score(self, text: str) -> QualityScore:
        """Apply all filters and combine scores."""
        all_reasons = []
        all_metrics = {}
        all_passed = True

        weighted_score = 0.0

        for i, (filter_obj, _) in enumerate(self.filters):
            result = filter_obj.score(text)

            filter_name = type(filter_obj).__name__
            all_metrics[f"{filter_name}_score"] = result.score
            all_metrics.update({
                f"{filter_name}_{k}": v
                for k, v in result.metrics.items()
            })

            if not result.passes:
                all_passed = False
                all_reasons.extend([
                    f"[{filter_name}] {r}" for r in result.reasons
                ])

            weighted_score += result.score * self.normalized_weights[i]

        all_metrics["combined_score"] = weighted_score

        if self.require_all_pass:
            passes = all_passed
        else:
            passes = weighted_score >= self.min_combined_score

        return QualityScore(
            passes=passes,
            score=weighted_score,
            reasons=all_reasons,
            metrics=all_metrics
        )
```

### 4. PII Removal

```python
"""
PII detection and removal for training data.
Uses Microsoft Presidio and custom patterns.
"""

from typing import List, Dict, Optional, Tuple
import re
from dataclasses import dataclass
from enum import Enum


class PIIType(Enum):
    """Types of PII that can be detected."""
    EMAIL = "email"
    PHONE = "phone"
    SSN = "ssn"
    CREDIT_CARD = "credit_card"
    IP_ADDRESS = "ip_address"
    NAME = "name"
    ADDRESS = "address"
    DATE_OF_BIRTH = "date_of_birth"
    MEDICAL_RECORD = "medical_record"
    FINANCIAL_ACCOUNT = "financial_account"
    CUSTOM = "custom"


@dataclass
class PIIMatch:
    """A detected PII instance."""
    pii_type: PIIType
    start: int
    end: int
    text: str
    confidence: float


class PIIDetector:
    """
    Detect PII using regex patterns and NER models.
    """

    # Regex patterns for common PII
    PATTERNS = {
        PIIType.EMAIL: r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b",
        PIIType.PHONE: r"\b(?:\+?1[-.\s]?)?\(?[0-9]{3}\)?[-.\s]?[0-9]{3}[-.\s]?[0-9]{4}\b",
        PIIType.SSN: r"\b\d{3}[-\s]?\d{2}[-\s]?\d{4}\b",
        PIIType.CREDIT_CARD: r"\b(?:\d{4}[-\s]?){3}\d{4}\b",
        PIIType.IP_ADDRESS: r"\b(?:\d{1,3}\.){3}\d{1,3}\b",
    }

    def __init__(
        self,
        use_presidio: bool = True,
        use_regex: bool = True,
        custom_patterns: Dict[str, str] = None
    ):
        self.use_presidio = use_presidio
        self.use_regex = use_regex
        self.custom_patterns = custom_patterns or {}

        if use_presidio:
            from presidio_analyzer import AnalyzerEngine
            self.analyzer = AnalyzerEngine()

    def detect(
        self,
        text: str,
        pii_types: List[PIIType] = None,
        min_confidence: float = 0.5
    ) -> List[PIIMatch]:
        """
        Detect PII in text.

        Args:
            text: Input text
            pii_types: Types to detect (None = all)
            min_confidence: Minimum confidence threshold

        Returns:
            List of PIIMatch objects
        """
        matches = []

        # Regex-based detection
        if self.use_regex:
            patterns_to_use = self.PATTERNS.copy()
            patterns_to_use.update({
                PIIType.CUSTOM: p for p in self.custom_patterns.values()
            })

            for pii_type, pattern in patterns_to_use.items():
                if pii_types and pii_type not in pii_types:
                    continue

                for match in re.finditer(pattern, text, re.IGNORECASE):
                    matches.append(PIIMatch(
                        pii_type=pii_type,
                        start=match.start(),
                        end=match.end(),
                        text=match.group(),
                        confidence=0.9  # High confidence for regex
                    ))

        # Presidio-based detection
        if self.use_presidio:
            presidio_types = [
                "PERSON", "EMAIL_ADDRESS", "PHONE_NUMBER",
                "LOCATION", "CREDIT_CARD", "US_SSN",
                "MEDICAL_LICENSE", "DATE_TIME"
            ]

            results = self.analyzer.analyze(
                text=text,
                entities=presidio_types,
                language="en"
            )

            for result in results:
                if result.score < min_confidence:
                    continue

                # Map Presidio types to our types
                type_map = {
                    "EMAIL_ADDRESS": PIIType.EMAIL,
                    "PHONE_NUMBER": PIIType.PHONE,
                    "US_SSN": PIIType.SSN,
                    "CREDIT_CARD": PIIType.CREDIT_CARD,
                    "PERSON": PIIType.NAME,
                    "LOCATION": PIIType.ADDRESS,
                }

                pii_type = type_map.get(result.entity_type, PIIType.CUSTOM)

                if pii_types and pii_type not in pii_types:
                    continue

                matches.append(PIIMatch(
                    pii_type=pii_type,
                    start=result.start,
                    end=result.end,
                    text=text[result.start:result.end],
                    confidence=result.score
                ))

        # Remove overlapping matches (keep highest confidence)
        matches = self._dedupe_matches(matches)

        return matches

    def _dedupe_matches(self, matches: List[PIIMatch]) -> List[PIIMatch]:
        """Remove overlapping matches, keeping highest confidence."""
        if not matches:
            return []

        # Sort by confidence (descending)
        matches = sorted(matches, key=lambda x: -x.confidence)

        kept = []
        for match in matches:
            # Check if overlaps with any kept match
            overlaps = False
            for kept_match in kept:
                if (match.start < kept_match.end and
                    match.end > kept_match.start):
                    overlaps = True
                    break

            if not overlaps:
                kept.append(match)

        # Sort by position
        return sorted(kept, key=lambda x: x.start)


class PIIAnonymizer:
    """
    Anonymize PII with various strategies.
    """

    def __init__(
        self,
        strategy: str = "replace",
        replacement_map: Dict[PIIType, str] = None
    ):
        """
        Args:
            strategy: "replace", "redact", "hash", "mask"
            replacement_map: Custom replacement strings per type
        """
        self.strategy = strategy
        self.replacement_map = replacement_map or {
            PIIType.EMAIL: "[EMAIL]",
            PIIType.PHONE: "[PHONE]",
            PIIType.SSN: "[SSN]",
            PIIType.CREDIT_CARD: "[CREDIT_CARD]",
            PIIType.IP_ADDRESS: "[IP]",
            PIIType.NAME: "[NAME]",
            PIIType.ADDRESS: "[ADDRESS]",
            PIIType.CUSTOM: "[REDACTED]",
        }

    def anonymize(
        self,
        text: str,
        matches: List[PIIMatch]
    ) -> str:
        """
        Anonymize PII in text.

        Args:
            text: Original text
            matches: List of PII matches to anonymize

        Returns:
            Anonymized text
        """
        if not matches:
            return text

        # Sort matches by position (reverse for safe replacement)
        sorted_matches = sorted(matches, key=lambda x: -x.start)

        result = text
        for match in sorted_matches:
            replacement = self._get_replacement(match)
            result = result[:match.start] + replacement + result[match.end:]

        return result

    def _get_replacement(self, match: PIIMatch) -> str:
        """Get replacement string for match based on strategy."""
        if self.strategy == "replace":
            return self.replacement_map.get(match.pii_type, "[REDACTED]")

        elif self.strategy == "redact":
            return ""

        elif self.strategy == "hash":
            import hashlib
            hash_val = hashlib.sha256(match.text.encode()).hexdigest()[:8]
            return f"[{match.pii_type.value.upper()}_{hash_val}]"

        elif self.strategy == "mask":
            # Keep first and last character, mask middle
            if len(match.text) <= 2:
                return "*" * len(match.text)
            return match.text[0] + "*" * (len(match.text) - 2) + match.text[-1]

        return "[REDACTED]"


class PIIProcessor:
    """
    Combined PII detection and anonymization pipeline.
    """

    def __init__(
        self,
        detector: PIIDetector = None,
        anonymizer: PIIAnonymizer = None,
        pii_threshold: float = 0.1,
        filter_high_pii: bool = True
    ):
        self.detector = detector or PIIDetector()
        self.anonymizer = anonymizer or PIIAnonymizer()
        self.pii_threshold = pii_threshold
        self.filter_high_pii = filter_high_pii

    def process(
        self,
        text: str,
        anonymize: bool = True
    ) -> Tuple[str, Dict]:
        """
        Process text for PII.

        Args:
            text: Input text
            anonymize: Whether to anonymize PII

        Returns:
            Tuple of (processed_text, metadata)
        """
        matches = self.detector.detect(text)

        metadata = {
            "pii_count": len(matches),
            "pii_types": list(set(m.pii_type.value for m in matches)),
            "pii_char_ratio": sum(
                m.end - m.start for m in matches
            ) / max(len(text), 1)
        }

        # Check if should filter due to high PII density
        if self.filter_high_pii:
            if metadata["pii_char_ratio"] > self.pii_threshold:
                metadata["filtered"] = True
                metadata["filter_reason"] = (
                    f"PII ratio {metadata['pii_char_ratio']:.2%} "
                    f"exceeds threshold {self.pii_threshold:.2%}"
                )
                return "", metadata

        # Anonymize if requested
        if anonymize and matches:
            text = self.anonymizer.anonymize(text, matches)
            metadata["anonymized"] = True

        return text, metadata
```

---

## Document Structure Extraction

### HTML Extraction

```python
"""
HTML content extraction with boilerplate removal.
"""

import trafilatura
from bs4 import BeautifulSoup
from readability import Document
from typing import Optional, Dict
import re


class HTMLExtractor:
    """
    Extract clean text from HTML with multiple fallback methods.
    """

    def __init__(
        self,
        include_tables: bool = True,
        include_links: bool = False,
        include_images: bool = False,
        favor_precision: bool = True,
        min_length: int = 100
    ):
        self.include_tables = include_tables
        self.include_links = include_links
        self.include_images = include_images
        self.favor_precision = favor_precision
        self.min_length = min_length

    def extract(self, html: str, url: str = None) -> Optional[Dict]:
        """
        Extract content from HTML using multiple methods.

        Args:
            html: Raw HTML content
            url: Optional URL for context

        Returns:
            Dict with extracted content and metadata
        """
        # Method 1: trafilatura (best for articles)
        text = self._extract_trafilatura(html, url)

        # Method 2: readability fallback
        if not text or len(text) < self.min_length:
            text = self._extract_readability(html)

        # Method 3: BeautifulSoup fallback
        if not text or len(text) < self.min_length:
            text = self._extract_beautifulsoup(html)

        if not text:
            return None

        # Extract metadata
        metadata = self._extract_metadata(html)

        return {
            "text": text,
            "title": metadata.get("title"),
            "description": metadata.get("description"),
            "language": metadata.get("language"),
            "word_count": len(text.split())
        }

    def _extract_trafilatura(
        self,
        html: str,
        url: str = None
    ) -> Optional[str]:
        """Extract using trafilatura."""
        try:
            return trafilatura.extract(
                html,
                url=url,
                include_comments=False,
                include_tables=self.include_tables,
                include_links=self.include_links,
                include_images=self.include_images,
                favor_precision=self.favor_precision,
                deduplicate=True
            )
        except Exception:
            return None

    def _extract_readability(self, html: str) -> Optional[str]:
        """Extract using readability-lxml."""
        try:
            doc = Document(html)
            summary = doc.summary()
            soup = BeautifulSoup(summary, "html.parser")
            return soup.get_text(separator="\n", strip=True)
        except Exception:
            return None

    def _extract_beautifulsoup(self, html: str) -> Optional[str]:
        """Extract using BeautifulSoup with heuristics."""
        try:
            soup = BeautifulSoup(html, "html.parser")

            # Remove unwanted elements
            for element in soup(["script", "style", "nav", "footer",
                                "header", "aside", "noscript", "iframe"]):
                element.decompose()

            # Try to find main content
            main = (
                soup.find("main") or
                soup.find("article") or
                soup.find("div", class_=re.compile(r"content|article|post")) or
                soup.find("body")
            )

            if main:
                text = main.get_text(separator="\n", strip=True)
                # Clean up whitespace
                text = re.sub(r"\n{3,}", "\n\n", text)
                return text

            return None
        except Exception:
            return None

    def _extract_metadata(self, html: str) -> Dict:
        """Extract metadata from HTML."""
        metadata = {}
        try:
            soup = BeautifulSoup(html, "html.parser")

            # Title
            title_tag = soup.find("title")
            if title_tag:
                metadata["title"] = title_tag.get_text(strip=True)

            # Meta description
            desc_meta = soup.find("meta", attrs={"name": "description"})
            if desc_meta:
                metadata["description"] = desc_meta.get("content", "")

            # Language
            html_tag = soup.find("html")
            if html_tag:
                metadata["language"] = html_tag.get("lang", "")

            # OpenGraph metadata
            og_title = soup.find("meta", property="og:title")
            if og_title and not metadata.get("title"):
                metadata["title"] = og_title.get("content", "")

        except Exception:
            pass

        return metadata
```

### PDF Extraction

```python
"""
PDF text extraction with OCR fallback.
"""

import fitz  # PyMuPDF
from typing import Optional, List, Dict
import io


class PDFExtractor:
    """
    Extract text from PDF documents.
    Supports native text extraction and OCR fallback.
    """

    def __init__(
        self,
        ocr_enabled: bool = True,
        ocr_language: str = "eng",
        min_text_per_page: int = 50
    ):
        self.ocr_enabled = ocr_enabled
        self.ocr_language = ocr_language
        self.min_text_per_page = min_text_per_page

        if ocr_enabled:
            try:
                import pytesseract
                from PIL import Image
                self.pytesseract = pytesseract
                self.Image = Image
            except ImportError:
                self.ocr_enabled = False

    def extract(
        self,
        pdf_content: bytes,
        extract_images: bool = False
    ) -> Optional[Dict]:
        """
        Extract text from PDF.

        Args:
            pdf_content: PDF file bytes
            extract_images: Whether to extract image descriptions

        Returns:
            Dict with extracted content and metadata
        """
        try:
            doc = fitz.open(stream=pdf_content, filetype="pdf")
        except Exception as e:
            return {"error": str(e)}

        pages = []
        needs_ocr_pages = []

        for page_num in range(len(doc)):
            page = doc[page_num]

            # Try native text extraction first
            text = page.get_text()

            if len(text.strip()) < self.min_text_per_page:
                needs_ocr_pages.append(page_num)
            else:
                pages.append({
                    "page": page_num + 1,
                    "text": text,
                    "method": "native"
                })

        # OCR fallback for pages with little text
        if self.ocr_enabled and needs_ocr_pages:
            for page_num in needs_ocr_pages:
                page = doc[page_num]
                text = self._ocr_page(page)
                pages.append({
                    "page": page_num + 1,
                    "text": text,
                    "method": "ocr"
                })

        # Sort by page number
        pages.sort(key=lambda x: x["page"])

        # Combine text
        full_text = "\n\n".join(p["text"] for p in pages if p["text"])

        # Extract metadata
        metadata = doc.metadata

        doc.close()

        return {
            "text": full_text,
            "page_count": len(doc),
            "title": metadata.get("title"),
            "author": metadata.get("author"),
            "pages": pages,
            "word_count": len(full_text.split())
        }

    def _ocr_page(self, page) -> str:
        """OCR a single page."""
        if not self.ocr_enabled:
            return ""

        try:
            # Render page to image
            pix = page.get_pixmap(dpi=300)
            img_data = pix.tobytes("png")

            # Convert to PIL Image
            image = self.Image.open(io.BytesIO(img_data))

            # Run OCR
            text = self.pytesseract.image_to_string(
                image,
                lang=self.ocr_language
            )

            return text

        except Exception:
            return ""


class TableExtractor:
    """
    Extract tables from documents and convert to text.
    """

    def __init__(self, linearization_format: str = "markdown"):
        """
        Args:
            linearization_format: "markdown", "csv", or "plain"
        """
        self.format = linearization_format

    def extract_from_html(self, html: str) -> List[str]:
        """Extract tables from HTML."""
        from bs4 import BeautifulSoup

        soup = BeautifulSoup(html, "html.parser")
        tables = soup.find_all("table")

        linearized = []
        for table in tables:
            text = self._linearize_html_table(table)
            if text:
                linearized.append(text)

        return linearized

    def extract_from_pdf(
        self,
        pdf_content: bytes,
        page_num: int = None
    ) -> List[str]:
        """Extract tables from PDF using pdfplumber."""
        import pdfplumber

        linearized = []

        with pdfplumber.open(io.BytesIO(pdf_content)) as pdf:
            pages = [pdf.pages[page_num]] if page_num else pdf.pages

            for page in pages:
                tables = page.extract_tables()
                for table in tables:
                    text = self._linearize_table(table)
                    if text:
                        linearized.append(text)

        return linearized

    def _linearize_html_table(self, table) -> str:
        """Convert HTML table to text."""
        rows = []
        for tr in table.find_all("tr"):
            cells = []
            for td in tr.find_all(["td", "th"]):
                cells.append(td.get_text(strip=True))
            if cells:
                rows.append(cells)

        return self._linearize_table(rows)

    def _linearize_table(self, rows: List[List[str]]) -> str:
        """Linearize table rows to text."""
        if not rows:
            return ""

        if self.format == "markdown":
            return self._to_markdown(rows)
        elif self.format == "csv":
            return self._to_csv(rows)
        else:
            return self._to_plain(rows)

    def _to_markdown(self, rows: List[List[str]]) -> str:
        """Convert to markdown table."""
        if not rows:
            return ""

        # Determine column widths
        num_cols = max(len(row) for row in rows)

        # Pad rows to same length
        padded = [row + [""] * (num_cols - len(row)) for row in rows]

        lines = []

        # Header row
        lines.append("| " + " | ".join(padded[0]) + " |")
        lines.append("|" + "|".join(["---"] * num_cols) + "|")

        # Data rows
        for row in padded[1:]:
            lines.append("| " + " | ".join(row) + " |")

        return "\n".join(lines)

    def _to_csv(self, rows: List[List[str]]) -> str:
        """Convert to CSV format."""
        import csv
        import io

        output = io.StringIO()
        writer = csv.writer(output)
        writer.writerows(rows)
        return output.getvalue()

    def _to_plain(self, rows: List[List[str]]) -> str:
        """Convert to plain text."""
        lines = []
        for i, row in enumerate(rows):
            if i == 0:
                # Header
                lines.append("Headers: " + ", ".join(row))
            else:
                # Data rows as key-value pairs
                if rows[0]:
                    pairs = []
                    for j, cell in enumerate(row):
                        header = rows[0][j] if j < len(rows[0]) else f"Col{j}"
                        pairs.append(f"{header}: {cell}")
                    lines.append("Row " + str(i) + ": " + "; ".join(pairs))

        return "\n".join(lines)
```

---

## Domain-Specific Preprocessing

### Legal Documents

```python
"""
Preprocessing for legal documents.
"""

import re
from typing import List, Dict, Optional


class LegalPreprocessor:
    """Preprocess legal documents for LLM training."""

    # Citation patterns
    CASE_CITATION = re.compile(
        r"\b\d+\s+[A-Z][a-z]+\.?\s*\d*d?\s+\d+\b"  # e.g., "123 F.3d 456"
    )
    STATUTE_CITATION = re.compile(
        r"\b\d+\s+U\.?S\.?C\.?\s+§?\s*\d+\b"  # e.g., "42 U.S.C. § 1983"
    )

    def __init__(
        self,
        normalize_citations: bool = True,
        remove_page_numbers: bool = True,
        extract_sections: bool = True
    ):
        self.normalize_citations = normalize_citations
        self.remove_page_numbers = remove_page_numbers
        self.extract_sections = extract_sections

    def preprocess(self, text: str) -> Dict:
        """
        Preprocess legal document.

        Returns:
            Dict with processed text and extracted metadata
        """
        metadata = {}

        # Remove page numbers
        if self.remove_page_numbers:
            text = re.sub(r"\n\s*-?\s*\d+\s*-?\s*\n", "\n", text)
            text = re.sub(r"Page \d+ of \d+", "", text)

        # Extract citations
        case_citations = self.CASE_CITATION.findall(text)
        statute_citations = self.STATUTE_CITATION.findall(text)
        metadata["citations"] = {
            "cases": list(set(case_citations)),
            "statutes": list(set(statute_citations))
        }

        # Normalize citations if requested
        if self.normalize_citations:
            text = self._normalize_citations(text)

        # Extract sections
        if self.extract_sections:
            sections = self._extract_sections(text)
            metadata["sections"] = sections

        return {
            "text": text,
            "metadata": metadata
        }

    def _normalize_citations(self, text: str) -> str:
        """Normalize citation formats."""
        # Normalize U.S. Code citations
        text = re.sub(
            r"(\d+)\s*U\.?\s*S\.?\s*C\.?\s*§?\s*(\d+)",
            r"\1 U.S.C. § \2",
            text
        )
        return text

    def _extract_sections(self, text: str) -> List[Dict]:
        """Extract document sections."""
        sections = []

        # Common section patterns
        patterns = [
            r"^(I{1,3}V?|V?I{0,3})\.\s+(.+)$",  # Roman numerals
            r"^(\d+)\.\s+(.+)$",  # Numbers
            r"^([A-Z])\.\s+(.+)$",  # Letters
        ]

        lines = text.split("\n")
        for i, line in enumerate(lines):
            for pattern in patterns:
                match = re.match(pattern, line.strip())
                if match:
                    sections.append({
                        "number": match.group(1),
                        "title": match.group(2),
                        "line": i
                    })
                    break

        return sections
```

### Medical Documents

```python
"""
Preprocessing for medical documents with HIPAA-compliant PII handling.
"""

import re
from typing import Dict, List


class MedicalPreprocessor:
    """Preprocess medical documents with de-identification."""

    # HIPAA Safe Harbor identifiers
    HIPAA_IDENTIFIERS = [
        "name", "address", "date", "phone", "fax", "email",
        "ssn", "mrn", "health_plan", "account", "license",
        "vehicle", "device", "url", "ip", "biometric", "photo"
    ]

    # Medical terminology patterns
    ICD_CODE = re.compile(r"\b[A-Z]\d{2}(?:\.\d{1,2})?\b")
    CPT_CODE = re.compile(r"\b\d{5}(?:[A-Z])?\b")
    NDC_CODE = re.compile(r"\b\d{4,5}-\d{3,4}-\d{1,2}\b")

    def __init__(
        self,
        deidentify: bool = True,
        normalize_codes: bool = True,
        extract_medications: bool = True
    ):
        self.deidentify = deidentify
        self.normalize_codes = normalize_codes
        self.extract_medications = extract_medications

    def preprocess(self, text: str) -> Dict:
        """
        Preprocess medical document.

        Args:
            text: Raw medical text

        Returns:
            Dict with processed text and metadata
        """
        metadata = {}

        # De-identify if requested
        if self.deidentify:
            text, phi_counts = self._deidentify(text)
            metadata["phi_removed"] = phi_counts

        # Extract medical codes
        metadata["codes"] = {
            "icd": list(set(self.ICD_CODE.findall(text))),
            "cpt": list(set(self.CPT_CODE.findall(text))),
            "ndc": list(set(self.NDC_CODE.findall(text)))
        }

        # Normalize codes if requested
        if self.normalize_codes:
            text = self._normalize_codes(text)

        # Extract medications
        if self.extract_medications:
            medications = self._extract_medications(text)
            metadata["medications"] = medications

        return {
            "text": text,
            "metadata": metadata
        }

    def _deidentify(self, text: str) -> tuple:
        """
        Remove Protected Health Information (PHI).
        Implements HIPAA Safe Harbor method.
        """
        phi_counts = {}

        # Date patterns (keep year for temporal context)
        date_pattern = r"\b\d{1,2}[/-]\d{1,2}[/-]\d{2,4}\b"
        dates_found = len(re.findall(date_pattern, text))
        text = re.sub(date_pattern, "[DATE]", text)
        phi_counts["dates"] = dates_found

        # MRN (Medical Record Number)
        mrn_pattern = r"\bMRN[:\s]*\d+\b"
        mrns_found = len(re.findall(mrn_pattern, text, re.IGNORECASE))
        text = re.sub(mrn_pattern, "[MRN]", text, flags=re.IGNORECASE)
        phi_counts["mrn"] = mrns_found

        # Ages over 89
        age_pattern = r"\b(9\d|[1-9]\d{2})\s*(?:year|yr|y/?o)\b"
        text = re.sub(age_pattern, "[AGE>89]", text, flags=re.IGNORECASE)

        # Use Presidio for comprehensive de-identification
        try:
            from presidio_analyzer import AnalyzerEngine
            from presidio_anonymizer import AnonymizerEngine

            analyzer = AnalyzerEngine()
            anonymizer = AnonymizerEngine()

            results = analyzer.analyze(
                text=text,
                entities=["PERSON", "LOCATION", "PHONE_NUMBER",
                         "EMAIL_ADDRESS", "DATE_TIME"],
                language="en"
            )

            text = anonymizer.anonymize(text=text, analyzer_results=results).text
            phi_counts["presidio_entities"] = len(results)

        except ImportError:
            pass

        return text, phi_counts

    def _normalize_codes(self, text: str) -> str:
        """Normalize medical code formats."""
        # Normalize ICD codes (add decimal if missing)
        def normalize_icd(match):
            code = match.group(0)
            if "." not in code and len(code) > 3:
                return code[:3] + "." + code[3:]
            return code

        text = self.ICD_CODE.sub(normalize_icd, text)
        return text

    def _extract_medications(self, text: str) -> List[Dict]:
        """Extract medication mentions."""
        medications = []

        # Common medication pattern
        med_pattern = re.compile(
            r"(\w+(?:\s+\w+)?)\s+"
            r"(\d+(?:\.\d+)?)\s*(mg|mcg|g|ml|units?)\s*"
            r"(?:(po|iv|im|sc|sl|pr|top|inh)\s*)?"
            r"(?:(qd|bid|tid|qid|prn|q\d+h?)\s*)?",
            re.IGNORECASE
        )

        for match in med_pattern.finditer(text):
            medications.append({
                "name": match.group(1),
                "dose": match.group(2),
                "unit": match.group(3),
                "route": match.group(4),
                "frequency": match.group(5)
            })

        return medications
```

### Code Preprocessing

```python
"""
Preprocessing for source code.
"""

import re
from typing import Dict, List, Optional
from dataclasses import dataclass


@dataclass
class CodeBlock:
    """Extracted code block."""
    language: str
    code: str
    start_line: int
    end_line: int


class CodePreprocessor:
    """Preprocess source code for LLM training."""

    # Language detection patterns
    LANGUAGE_PATTERNS = {
        "python": [
            r"^import\s+\w+",
            r"^from\s+\w+\s+import",
            r"^def\s+\w+\(",
            r"^class\s+\w+[:\(]",
        ],
        "javascript": [
            r"^const\s+\w+\s*=",
            r"^let\s+\w+\s*=",
            r"^function\s+\w+\(",
            r"^import\s+.*\s+from\s+['\"]",
        ],
        "java": [
            r"^public\s+class\s+\w+",
            r"^private\s+\w+\s+\w+",
            r"^import\s+java\.",
        ],
        "go": [
            r"^package\s+\w+",
            r"^func\s+\w+\(",
            r"^import\s+\(",
        ],
        "rust": [
            r"^fn\s+\w+\(",
            r"^use\s+\w+::",
            r"^impl\s+\w+",
        ],
    }

    def __init__(
        self,
        remove_comments: bool = False,
        normalize_whitespace: bool = True,
        extract_docstrings: bool = True,
        min_lines: int = 3,
        max_lines: int = 10000
    ):
        self.remove_comments = remove_comments
        self.normalize_whitespace = normalize_whitespace
        self.extract_docstrings = extract_docstrings
        self.min_lines = min_lines
        self.max_lines = max_lines

    def preprocess(self, code: str, language: str = None) -> Dict:
        """
        Preprocess source code.

        Args:
            code: Source code string
            language: Programming language (auto-detect if None)

        Returns:
            Dict with processed code and metadata
        """
        metadata = {}

        # Detect language if not provided
        if not language:
            language = self._detect_language(code)
        metadata["language"] = language

        lines = code.split("\n")
        metadata["original_lines"] = len(lines)

        # Filter by line count
        if len(lines) < self.min_lines:
            return {"filtered": True, "reason": "too_short", "metadata": metadata}
        if len(lines) > self.max_lines:
            return {"filtered": True, "reason": "too_long", "metadata": metadata}

        # Normalize line endings
        code = code.replace("\r\n", "\n").replace("\r", "\n")

        # Remove trailing whitespace
        lines = [line.rstrip() for line in code.split("\n")]
        code = "\n".join(lines)

        # Ensure file ends with newline
        if code and not code.endswith("\n"):
            code += "\n"

        # Remove comments if requested
        if self.remove_comments:
            code = self._remove_comments(code, language)

        # Extract docstrings
        if self.extract_docstrings:
            docstrings = self._extract_docstrings(code, language)
            metadata["docstrings"] = docstrings

        # Extract imports
        imports = self._extract_imports(code, language)
        metadata["imports"] = imports

        # Calculate metrics
        metadata["final_lines"] = len(code.split("\n"))
        metadata["has_tests"] = self._has_tests(code, language)

        return {
            "code": code,
            "metadata": metadata
        }

    def _detect_language(self, code: str) -> str:
        """Detect programming language from code patterns."""
        scores = {}

        for language, patterns in self.LANGUAGE_PATTERNS.items():
            score = 0
            for pattern in patterns:
                if re.search(pattern, code, re.MULTILINE):
                    score += 1
            scores[language] = score

        if scores:
            best = max(scores, key=scores.get)
            if scores[best] > 0:
                return best

        return "unknown"

    def _remove_comments(self, code: str, language: str) -> str:
        """Remove comments from code."""
        if language in ["python"]:
            # Remove # comments
            code = re.sub(r"#.*$", "", code, flags=re.MULTILINE)
        elif language in ["javascript", "java", "go", "rust"]:
            # Remove // comments
            code = re.sub(r"//.*$", "", code, flags=re.MULTILINE)
            # Remove /* */ comments
            code = re.sub(r"/\*.*?\*/", "", code, flags=re.DOTALL)

        return code

    def _extract_docstrings(self, code: str, language: str) -> List[str]:
        """Extract docstrings from code."""
        docstrings = []

        if language == "python":
            # Triple-quoted strings
            pattern = r'"""(.*?)"""|\'\'\'(.*?)\'\'\''
            for match in re.finditer(pattern, code, re.DOTALL):
                docstring = match.group(1) or match.group(2)
                if docstring:
                    docstrings.append(docstring.strip())

        elif language in ["javascript", "java"]:
            # JSDoc style
            pattern = r"/\*\*(.*?)\*/"
            for match in re.finditer(pattern, code, re.DOTALL):
                docstrings.append(match.group(1).strip())

        return docstrings

    def _extract_imports(self, code: str, language: str) -> List[str]:
        """Extract import statements."""
        imports = []

        if language == "python":
            pattern = r"^(?:from\s+(\S+)\s+)?import\s+(.+)$"
            for match in re.finditer(pattern, code, re.MULTILINE):
                if match.group(1):
                    imports.append(match.group(1))
                else:
                    imports.extend(match.group(2).split(","))

        elif language == "javascript":
            pattern = r"import\s+.*?\s+from\s+['\"]([^'\"]+)['\"]"
            for match in re.finditer(pattern, code):
                imports.append(match.group(1))

        return [i.strip() for i in imports]

    def _has_tests(self, code: str, language: str) -> bool:
        """Check if code contains tests."""
        test_patterns = {
            "python": [r"def\s+test_", r"class\s+Test", r"unittest", r"pytest"],
            "javascript": [r"describe\(", r"it\(", r"test\(", r"expect\("],
            "java": [r"@Test", r"@Before", r"@After", r"junit"],
            "go": [r"func\s+Test\w+\(", r"testing\.T"],
            "rust": [r"#\[test\]", r"#\[cfg\(test\)\]"],
        }

        patterns = test_patterns.get(language, [])
        return any(re.search(p, code) for p in patterns)
```

---

## Preprocessing at Scale

### NVIDIA NeMo Curator Pipeline

```python
"""
NeMo Curator pipeline for GPU-accelerated preprocessing.
Based on NVIDIA's approach for Nemotron-CC and other large-scale datasets.
"""

# NeMo Curator pipeline configuration
NEMO_CURATOR_CONFIG = """
# config.yaml for NeMo Curator pipeline

# Data source configuration
input:
  type: "jsonl"
  path: "s3://bucket/raw_data/"
  columns:
    text: "text"
    id: "id"

# Output configuration
output:
  type: "parquet"
  path: "s3://bucket/processed_data/"
  compression: "zstd"

# Processing stages
stages:
  # 1. Unicode normalization
  - name: "unicode_normalization"
    type: "UnicodeNormalization"
    params:
      form: "NFKC"

  # 2. Language filtering
  - name: "language_filter"
    type: "FastTextLanguageFilter"
    params:
      model_path: "lid.176.bin"
      target_languages: ["en"]
      min_confidence: 0.65

  # 3. Heuristic quality filters
  - name: "quality_filter"
    type: "HeuristicFilter"
    params:
      min_words: 50
      max_words: 100000
      min_mean_word_length: 3.0
      max_mean_word_length: 10.0
      max_symbol_word_ratio: 0.1
      max_bullet_line_ratio: 0.9

  # 4. Classifier-based quality scoring
  - name: "quality_classifier"
    type: "ClassifierFilter"
    params:
      model_path: "quality_classifier.bin"
      threshold: 0.5

  # 5. PII removal
  - name: "pii_redaction"
    type: "PIIRedaction"
    params:
      entities: ["EMAIL", "PHONE", "SSN", "CREDIT_CARD"]
      strategy: "replace"

  # 6. Exact deduplication
  - name: "exact_dedup"
    type: "ExactDuplicateFilter"
    params:
      hash_method: "sha256"
      normalize: true

  # 7. Fuzzy deduplication
  - name: "fuzzy_dedup"
    type: "MinHashLSH"
    params:
      num_perm: 128
      threshold: 0.8
      ngram_size: 5

# Cluster configuration
cluster:
  type: "dask-cuda"
  n_workers: 8
  device_memory_limit: "80GB"
"""


# Example NeMo Curator Python pipeline
def run_nemo_curator_pipeline():
    """
    Run NeMo Curator preprocessing pipeline.
    Requires: pip install nemo-curator
    """
    from nemo_curator import (
        Sequential,
        ScoreFilter,
        DocumentFilter
    )
    from nemo_curator.filters import (
        WordCountFilter,
        FastTextQualityFilter,
        ExactDuplicateFilter
    )
    from nemo_curator.modifiers import (
        UnicodeReformatter,
        PIIRedactor
    )
    from nemo_curator.utils.distributed_utils import get_client
    import dask.dataframe as dd

    # Initialize Dask client with GPU workers
    client = get_client(
        cluster_type="gpu",
        n_workers=8,
        device_memory_limit="80GB"
    )

    # Load data
    input_path = "s3://bucket/raw_data/*.jsonl"
    df = dd.read_json(input_path, lines=True)

    # Build pipeline
    pipeline = Sequential([
        # Text normalization
        UnicodeReformatter(unicode_form="NFKC"),

        # Quality filters
        WordCountFilter(min_words=50, max_words=100000),
        FastTextQualityFilter(
            model_path="lid.176.bin",
            min_score=0.65
        ),

        # PII handling
        PIIRedactor(
            entities=["EMAIL", "PHONE"],
            replace_value="[REDACTED]"
        ),

        # Deduplication
        ExactDuplicateFilter(
            id_field="id",
            text_field="text"
        ),
    ])

    # Run pipeline
    result_df = pipeline(df)

    # Save output
    result_df.to_parquet(
        "s3://bucket/processed_data/",
        compression="zstd"
    )

    print(f"Processed {len(result_df)} documents")

    client.close()
```

### Apache Spark Pipeline

```python
"""
Apache Spark pipeline for large-scale preprocessing.
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, udf, length, size, split, lower,
    regexp_replace, sha2, when, lit
)
from pyspark.sql.types import StringType, BooleanType, FloatType
import re


def create_spark_session(app_name: str = "LLM_Preprocessing") -> SparkSession:
    """Create Spark session with optimized settings."""
    return SparkSession.builder \
        .appName(app_name) \
        .config("spark.sql.shuffle.partitions", 1000) \
        .config("spark.sql.adaptive.enabled", True) \
        .config("spark.sql.adaptive.coalescePartitions.enabled", True) \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .config("spark.sql.execution.arrow.pyspark.enabled", True) \
        .getOrCreate()


# UDFs for text processing
@udf(returnType=StringType())
def normalize_unicode(text):
    """Normalize Unicode text."""
    import unicodedata
    if text is None:
        return None
    return unicodedata.normalize("NFKC", text)


@udf(returnType=BooleanType())
def is_english(text):
    """Detect if text is English using fastText."""
    import fasttext
    if text is None or len(text) < 50:
        return False

    model = fasttext.load_model("lid.176.bin")
    text_clean = text.replace("\n", " ")[:1000]
    prediction = model.predict(text_clean)

    return prediction[0][0] == "__label__en" and prediction[1][0] > 0.65


@udf(returnType=FloatType())
def quality_score(text):
    """Compute heuristic quality score."""
    if text is None:
        return 0.0

    words = text.split()
    word_count = len(words)

    if word_count < 50:
        return 0.0

    # Average word length
    avg_word_len = sum(len(w) for w in words) / word_count
    if avg_word_len < 3 or avg_word_len > 10:
        return 0.0

    # Alphabetic ratio
    alpha_chars = sum(1 for c in text if c.isalpha())
    total_chars = sum(1 for c in text if not c.isspace())
    alpha_ratio = alpha_chars / max(total_chars, 1)

    if alpha_ratio < 0.7:
        return 0.0

    return min(1.0, alpha_ratio)


def run_spark_pipeline(input_path: str, output_path: str):
    """
    Run Spark preprocessing pipeline.

    Args:
        input_path: Path to input data (parquet, json, etc.)
        output_path: Path for processed output
    """
    spark = create_spark_session()

    # Read data
    df = spark.read.parquet(input_path)

    # Pipeline stages
    df_processed = df \
        .filter(col("text").isNotNull()) \
        .filter(length(col("text")) > 100) \
        .withColumn("text", normalize_unicode(col("text"))) \
        .withColumn("word_count", size(split(col("text"), r"\s+"))) \
        .filter(col("word_count").between(50, 100000)) \
        .filter(is_english(col("text"))) \
        .withColumn("quality", quality_score(col("text"))) \
        .filter(col("quality") > 0.5) \
        .withColumn(
            "content_hash",
            sha2(lower(regexp_replace(col("text"), r"\s+", " ")), 256)
        ) \
        .dropDuplicates(["content_hash"]) \
        .select("id", "text", "quality", "word_count")

    # Write output
    df_processed.write \
        .mode("overwrite") \
        .parquet(output_path)

    # Log stats
    input_count = df.count()
    output_count = df_processed.count()

    print(f"Input: {input_count} documents")
    print(f"Output: {output_count} documents")
    print(f"Filtered: {input_count - output_count} ({(1 - output_count/input_count)*100:.1f}%)")

    spark.stop()
```

### Ray Data Pipeline

```python
"""
Ray Data pipeline for distributed preprocessing.
"""

import ray
from ray.data import Dataset
from typing import Dict, Any
import re


def create_ray_pipeline():
    """Create Ray-based preprocessing pipeline."""

    # Initialize Ray
    ray.init(
        num_cpus=32,
        num_gpus=4,
        object_store_memory=100 * 1024**3  # 100GB
    )

    def normalize_text(batch: Dict[str, Any]) -> Dict[str, Any]:
        """Normalize text in batch."""
        import unicodedata

        texts = batch["text"]
        normalized = []

        for text in texts:
            if text:
                text = unicodedata.normalize("NFKC", text)
                text = re.sub(r"\s+", " ", text)
                text = text.strip()
            normalized.append(text)

        batch["text"] = normalized
        return batch

    def filter_quality(batch: Dict[str, Any]) -> Dict[str, Any]:
        """Filter documents by quality."""
        texts = batch["text"]
        keep_indices = []

        for i, text in enumerate(texts):
            if text is None:
                continue

            words = text.split()
            word_count = len(words)

            # Basic quality checks
            if word_count < 50 or word_count > 100000:
                continue

            avg_word_len = sum(len(w) for w in words) / word_count
            if avg_word_len < 3 or avg_word_len > 10:
                continue

            keep_indices.append(i)

        # Filter all columns
        return {
            key: [values[i] for i in keep_indices]
            for key, values in batch.items()
        }

    def detect_language(batch: Dict[str, Any]) -> Dict[str, Any]:
        """Add language detection."""
        import fasttext

        model = fasttext.load_model("lid.176.bin")

        texts = batch["text"]
        languages = []
        confidences = []

        for text in texts:
            if text:
                text_clean = text.replace("\n", " ")[:1000]
                pred = model.predict(text_clean)
                lang = pred[0][0].replace("__label__", "")
                conf = float(pred[1][0])
            else:
                lang = "unknown"
                conf = 0.0

            languages.append(lang)
            confidences.append(conf)

        batch["language"] = languages
        batch["language_confidence"] = confidences
        return batch

    def compute_quality_score(batch: Dict[str, Any]) -> Dict[str, Any]:
        """Compute quality scores."""
        texts = batch["text"]
        scores = []

        for text in texts:
            if not text:
                scores.append(0.0)
                continue

            words = text.split()
            word_count = len(words)

            # Calculate metrics
            alpha_chars = sum(1 for c in text if c.isalpha())
            total_chars = sum(1 for c in text if not c.isspace())
            alpha_ratio = alpha_chars / max(total_chars, 1)

            # Score (simplified)
            score = min(1.0, alpha_ratio)
            scores.append(score)

        batch["quality_score"] = scores
        return batch

    # Build pipeline
    def process_dataset(input_path: str, output_path: str):
        """Run the full pipeline."""

        # Read data
        ds = ray.data.read_parquet(input_path)

        # Apply transformations
        ds = ds.map_batches(
            normalize_text,
            batch_format="numpy"
        ).map_batches(
            filter_quality,
            batch_format="numpy"
        ).map_batches(
            detect_language,
            batch_format="numpy",
            num_gpus=0.5  # GPU for fasttext
        ).map_batches(
            compute_quality_score,
            batch_format="numpy"
        )

        # Filter by language and quality
        ds = ds.filter(
            lambda row: (
                row["language"] == "en" and
                row["language_confidence"] > 0.65 and
                row["quality_score"] > 0.5
            )
        )

        # Write output
        ds.write_parquet(output_path)

        print(f"Processed dataset saved to {output_path}")
        print(ds.stats())

    return process_dataset


# Usage
if __name__ == "__main__":
    pipeline = create_ray_pipeline()
    pipeline(
        "s3://bucket/raw_data/",
        "s3://bucket/processed_data/"
    )
    ray.shutdown()
```

---

## Quality Metrics & Validation

### Token Distribution Analysis

```python
"""
Analyze token and vocabulary distributions in processed corpus.
"""

from collections import Counter
from typing import Dict, List, Tuple
import numpy as np
from dataclasses import dataclass


@dataclass
class CorpusStats:
    """Statistics for a text corpus."""
    total_documents: int
    total_tokens: int
    unique_tokens: int
    avg_doc_length: float
    median_doc_length: float
    vocabulary_size: int
    token_coverage: Dict[int, float]  # Top-N tokens cover X% of corpus


class CorpusAnalyzer:
    """Analyze corpus quality and token distributions."""

    def __init__(self, tokenizer=None):
        """
        Args:
            tokenizer: HuggingFace tokenizer or None for whitespace
        """
        self.tokenizer = tokenizer

    def _tokenize(self, text: str) -> List[str]:
        """Tokenize text."""
        if self.tokenizer:
            return self.tokenizer.tokenize(text)
        return text.split()

    def analyze(self, documents: List[str]) -> CorpusStats:
        """
        Analyze corpus statistics.

        Args:
            documents: List of document texts

        Returns:
            CorpusStats object
        """
        all_tokens = []
        doc_lengths = []

        for doc in documents:
            tokens = self._tokenize(doc)
            all_tokens.extend(tokens)
            doc_lengths.append(len(tokens))

        # Token frequency
        token_counts = Counter(all_tokens)

        # Calculate coverage
        total = len(all_tokens)
        sorted_counts = sorted(token_counts.values(), reverse=True)
        cumsum = np.cumsum(sorted_counts)

        coverage = {}
        for n in [100, 1000, 10000, 50000]:
            if n <= len(sorted_counts):
                coverage[n] = cumsum[n-1] / total
            else:
                coverage[n] = 1.0

        return CorpusStats(
            total_documents=len(documents),
            total_tokens=total,
            unique_tokens=len(token_counts),
            avg_doc_length=np.mean(doc_lengths),
            median_doc_length=np.median(doc_lengths),
            vocabulary_size=len(token_counts),
            token_coverage=coverage
        )

    def compare_distributions(
        self,
        corpus1: List[str],
        corpus2: List[str]
    ) -> Dict:
        """
        Compare token distributions between two corpora.
        Useful for checking train/test distribution match.
        """
        from scipy import stats

        # Tokenize both
        tokens1 = []
        for doc in corpus1:
            tokens1.extend(self._tokenize(doc))

        tokens2 = []
        for doc in corpus2:
            tokens2.extend(self._tokenize(doc))

        # Get frequency distributions
        freq1 = Counter(tokens1)
        freq2 = Counter(tokens2)

        # Common vocabulary
        common_vocab = set(freq1.keys()) & set(freq2.keys())

        # Calculate overlap
        vocab_overlap = len(common_vocab) / len(set(freq1.keys()) | set(freq2.keys()))

        # KL divergence on common vocab
        vocab = list(common_vocab)
        p = np.array([freq1[t] for t in vocab], dtype=float)
        q = np.array([freq2[t] for t in vocab], dtype=float)
        p = p / p.sum()
        q = q / q.sum()

        # Add smoothing to avoid division by zero
        epsilon = 1e-10
        p = p + epsilon
        q = q + epsilon
        p = p / p.sum()
        q = q / q.sum()

        kl_divergence = stats.entropy(p, q)

        return {
            "vocab_overlap": vocab_overlap,
            "kl_divergence": kl_divergence,
            "corpus1_vocab_size": len(freq1),
            "corpus2_vocab_size": len(freq2),
            "common_vocab_size": len(common_vocab)
        }


def validate_preprocessing_pipeline(
    input_path: str,
    output_path: str,
    sample_size: int = 10000
) -> Dict:
    """
    Validate preprocessing pipeline by comparing input/output.

    Returns metrics on filtering rates and quality improvement.
    """
    import random

    # Load samples
    # (Implementation depends on data format)
    input_docs = []  # Load from input_path
    output_docs = []  # Load from output_path

    # Sample if needed
    if len(input_docs) > sample_size:
        input_docs = random.sample(input_docs, sample_size)
    if len(output_docs) > sample_size:
        output_docs = random.sample(output_docs, sample_size)

    analyzer = CorpusAnalyzer()

    input_stats = analyzer.analyze(input_docs)
    output_stats = analyzer.analyze(output_docs)

    distribution_comparison = analyzer.compare_distributions(
        input_docs, output_docs
    )

    return {
        "input_stats": input_stats,
        "output_stats": output_stats,
        "filtering_rate": 1 - output_stats.total_documents / input_stats.total_documents,
        "token_reduction": 1 - output_stats.total_tokens / input_stats.total_tokens,
        "distribution_comparison": distribution_comparison
    }
```

---

## Implementation Guide

### Complete Pipeline Setup

```bash
#!/bin/bash
# Setup script for preprocessing pipeline

# 1. Create project structure
mkdir -p preprocessing/{src,config,scripts,tests,logs}
cd preprocessing

# 2. Create virtual environment
python -m venv .venv
source .venv/bin/activate

# 3. Install dependencies
cat > requirements.txt << 'EOF'
# Core processing
ftfy>=6.1.0
trafilatura>=1.6.0
beautifulsoup4>=4.12.0
lxml>=4.9.0

# NLP
fasttext>=0.9.2
spacy>=3.7.0
nltk>=3.8.0

# PII detection
presidio-analyzer>=2.2.0
presidio-anonymizer>=2.2.0

# Deduplication
datasketch>=1.6.0

# Distributed processing
pyspark>=3.5.0
dask[distributed]>=2024.1.0
ray[data]>=2.9.0

# NVIDIA NeMo Curator (optional, requires GPU)
# nemo-curator>=0.2.0

# Document processing
PyMuPDF>=1.23.0
pdfplumber>=0.10.0
pytesseract>=0.3.10

# Quality metrics
transformers>=4.36.0
torch>=2.1.0

# Utilities
pandas>=2.1.0
numpy>=1.26.0
pyarrow>=14.0.0
tqdm>=4.66.0
pyyaml>=6.0.0
EOF

pip install -r requirements.txt

# 4. Download language detection model
wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin

# 5. Download spaCy model
python -m spacy download en_core_web_sm

# 6. Create configuration
cat > config/pipeline.yaml << 'EOF'
# Preprocessing pipeline configuration
input:
  path: "data/raw/"
  format: "jsonl"

output:
  path: "data/processed/"
  format: "parquet"

stages:
  normalization:
    unicode_form: "NFKC"
    fix_encoding: true

  language_filter:
    target_languages: ["en"]
    min_confidence: 0.65

  quality_filter:
    min_words: 50
    max_words: 100000
    min_alpha_ratio: 0.7

  deduplication:
    exact: true
    fuzzy: true
    fuzzy_threshold: 0.8

  pii:
    enabled: true
    strategy: "replace"
EOF

echo "Setup complete!"
```

### Step-by-Step Execution

```python
"""
Main preprocessing pipeline execution.
"""

import yaml
import logging
from pathlib import Path
from datetime import datetime
from typing import Iterator, Dict
import json

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class PreprocessingPipeline:
    """Main preprocessing pipeline."""

    def __init__(self, config_path: str):
        with open(config_path) as f:
            self.config = yaml.safe_load(f)

        # Initialize components
        self.normalizer = TextNormalizer()
        self.lang_filter = LanguageIdentifier()
        self.quality_filter = GopherQualityFilter()
        self.deduplicator = CombinedDeduplicator()
        self.pii_processor = PIIProcessor()

        # Stats
        self.stats = {
            "total_input": 0,
            "after_normalization": 0,
            "after_language_filter": 0,
            "after_quality_filter": 0,
            "after_dedup": 0,
            "after_pii": 0,
            "final_output": 0
        }

    def read_input(self) -> Iterator[Dict]:
        """Read input documents."""
        input_path = Path(self.config["input"]["path"])
        input_format = self.config["input"]["format"]

        if input_format == "jsonl":
            for file_path in input_path.glob("*.jsonl"):
                with open(file_path) as f:
                    for line in f:
                        yield json.loads(line)
        elif input_format == "parquet":
            import pandas as pd
            for file_path in input_path.glob("*.parquet"):
                df = pd.read_parquet(file_path)
                for _, row in df.iterrows():
                    yield row.to_dict()

    def process_document(self, doc: Dict) -> Dict:
        """Process single document through pipeline."""
        text = doc.get("text", "")
        doc_id = doc.get("id", "")

        # Step 1: Normalization
        text = self.normalizer.normalize(text)
        if not text:
            return None

        # Step 2: Language filter
        lang, conf = self.lang_filter.identify(text)[0]
        if lang not in self.config["stages"]["language_filter"]["target_languages"]:
            return None
        if conf < self.config["stages"]["language_filter"]["min_confidence"]:
            return None

        # Step 3: Quality filter
        quality_result = self.quality_filter.score(text)
        if not quality_result.passes:
            return None

        # Step 4: Deduplication
        dedup_result = self.deduplicator.check(text, doc_id)
        if dedup_result.is_duplicate:
            return None
        self.deduplicator.add(text, doc_id)

        # Step 5: PII handling
        if self.config["stages"]["pii"]["enabled"]:
            text, pii_metadata = self.pii_processor.process(text)
            if not text:  # Filtered due to high PII
                return None

        return {
            "id": doc_id,
            "text": text,
            "language": lang,
            "language_confidence": conf,
            "quality_score": quality_result.score,
            "metadata": doc.get("metadata", {})
        }

    def run(self):
        """Execute preprocessing pipeline."""
        logger.info("Starting preprocessing pipeline")
        start_time = datetime.now()

        output_path = Path(self.config["output"]["path"])
        output_path.mkdir(parents=True, exist_ok=True)

        output_file = output_path / f"processed_{start_time.strftime('%Y%m%d_%H%M%S')}.jsonl"

        with open(output_file, "w") as f:
            for doc in self.read_input():
                self.stats["total_input"] += 1

                processed = self.process_document(doc)

                if processed:
                    f.write(json.dumps(processed) + "\n")
                    self.stats["final_output"] += 1

                # Log progress
                if self.stats["total_input"] % 10000 == 0:
                    logger.info(
                        f"Processed {self.stats['total_input']} documents, "
                        f"kept {self.stats['final_output']}"
                    )

        # Final stats
        duration = datetime.now() - start_time
        logger.info(f"Pipeline complete in {duration}")
        logger.info(f"Input: {self.stats['total_input']}")
        logger.info(f"Output: {self.stats['final_output']}")
        logger.info(
            f"Filtered: {self.stats['total_input'] - self.stats['final_output']} "
            f"({(1 - self.stats['final_output']/self.stats['total_input'])*100:.1f}%)"
        )

        return self.stats


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--config", default="config/pipeline.yaml")
    args = parser.parse_args()

    pipeline = PreprocessingPipeline(args.config)
    pipeline.run()
```

---

## Configuration Templates

### Quality Filter Configuration

```yaml
# config/quality_filters.yaml

# Gopher-style filters (DeepMind)
gopher:
  min_words: 50
  max_words: 100000
  min_mean_word_length: 3.0
  max_mean_word_length: 10.0
  max_symbol_word_ratio: 0.1
  max_bullet_line_ratio: 0.9
  max_ellipsis_ratio: 0.3
  min_alpha_ratio: 0.8
  max_duplicate_line_char_ratio: 0.3
  max_duplicate_paragraph_ratio: 0.3
  top_ngram_thresholds:
    2: 0.20
    3: 0.18
    4: 0.16

# C4-style filters
c4:
  min_words: 50
  min_sentences: 3
  require_sentence_ending: true
  filter_curly_braces: true
  filter_lorem_ipsum: true
  filter_javascript: true

# FineWeb-style filters (more permissive)
fineweb:
  min_words: 50
  max_words: 100000
  min_alpha_ratio: 0.6
  max_special_char_ratio: 0.3
  min_stopword_ratio: 0.05
  max_duplicate_ratio: 0.5
```

### Deduplication Configuration

```yaml
# config/deduplication.yaml

# Exact deduplication
exact:
  enabled: true
  normalize: true
  hash_algorithm: "sha256"

# MinHash LSH
minhash:
  enabled: true
  num_permutations: 128
  threshold: 0.8
  ngram_size: 5
  min_document_length: 100

# SimHash (alternative)
simhash:
  enabled: false
  hash_bits: 64
  hamming_threshold: 3

# Deduplication scope
scope:
  # "global" - across entire corpus
  # "per_source" - within each data source
  # "per_crawl" - within each crawl (for web data)
  type: "global"
```

---

## Troubleshooting

### Common Issues

#### Issue: Memory Errors During Deduplication

```python
"""
Solution: Use streaming MinHash with disk-based LSH.
"""

from datasketch import MinHash, MinHashLSH
import pickle
import os


class StreamingMinHashDeduplicator:
    """Memory-efficient MinHash deduplication using disk storage."""

    def __init__(
        self,
        threshold: float = 0.8,
        num_perm: int = 128,
        storage_path: str = "/tmp/minhash_index"
    ):
        self.threshold = threshold
        self.num_perm = num_perm
        self.storage_path = storage_path
        os.makedirs(storage_path, exist_ok=True)

        # Use Redis-backed LSH for large scale
        # Or implement custom disk-based storage
        self.lsh = MinHashLSH(
            threshold=threshold,
            num_perm=num_perm,
            storage_config={
                "type": "redis",
                "redis": {"host": "localhost", "port": 6379}
            }
        )

    def add_and_check(self, doc_id: str, text: str) -> bool:
        """Check for duplicate and add if unique."""
        minhash = self._compute_minhash(text)

        # Query first
        duplicates = self.lsh.query(minhash)

        if duplicates:
            return True  # Is duplicate

        # Add to index
        self.lsh.insert(doc_id, minhash)
        return False
```

#### Issue: Slow Language Detection

```python
"""
Solution: Batch processing with GPU acceleration.
"""

import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer


class FastLanguageDetector:
    """GPU-accelerated language detection."""

    def __init__(self, model_name: str = "papluca/xlm-roberta-base-language-detection"):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)
        self.model.to(self.device)
        self.model.eval()

    def detect_batch(self, texts: list[str], batch_size: int = 32) -> list[tuple]:
        """Detect language for batch of texts."""
        results = []

        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]

            # Truncate long texts
            batch = [t[:512] for t in batch]

            inputs = self.tokenizer(
                batch,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            ).to(self.device)

            with torch.no_grad():
                outputs = self.model(**inputs)
                predictions = torch.softmax(outputs.logits, dim=-1)
                top_predictions = predictions.argmax(dim=-1)
                confidences = predictions.max(dim=-1).values

            for pred, conf in zip(top_predictions.cpu().numpy(),
                                  confidences.cpu().numpy()):
                lang = self.model.config.id2label[pred]
                results.append((lang, float(conf)))

        return results
```

#### Issue: PII Detection Missing Custom Patterns

```python
"""
Solution: Add custom recognizers to Presidio.
"""

from presidio_analyzer import PatternRecognizer, Pattern


def add_custom_pii_patterns():
    """Add custom PII patterns to Presidio."""
    from presidio_analyzer import AnalyzerEngine

    analyzer = AnalyzerEngine()

    # Custom employee ID pattern
    employee_id_pattern = Pattern(
        name="employee_id_pattern",
        regex=r"\bEMP-\d{6}\b",
        score=0.9
    )
    employee_id_recognizer = PatternRecognizer(
        supported_entity="EMPLOYEE_ID",
        patterns=[employee_id_pattern]
    )
    analyzer.registry.add_recognizer(employee_id_recognizer)

    # Custom internal code pattern
    internal_code_pattern = Pattern(
        name="internal_code_pattern",
        regex=r"\bINT-[A-Z]{2}-\d{4}\b",
        score=0.85
    )
    internal_code_recognizer = PatternRecognizer(
        supported_entity="INTERNAL_CODE",
        patterns=[internal_code_pattern]
    )
    analyzer.registry.add_recognizer(internal_code_recognizer)

    return analyzer
```

---

## Appendices

### Appendix A: Preprocessing Pipeline Code

Complete reference implementation provided in the Code Examples sections above.

### Appendix B: Quality Filter Configurations

See Configuration Templates section for YAML configurations.

### Appendix C: Deduplication Threshold Tuning Guide

| Threshold | Precision | Recall | Use Case |
|-----------|-----------|--------|----------|
| 0.9 | High | Low | Conservative, exact near-duplicates only |
| 0.8 | Medium-High | Medium | Balanced, recommended default |
| 0.7 | Medium | High | Aggressive, catches paraphrases |
| 0.6 | Low | Very High | Very aggressive, may remove distinct docs |

### Appendix D: Common Preprocessing Pitfalls

1. **Over-filtering**: Too aggressive filters remove valuable training data
2. **Under-filtering**: Keeping too much low-quality data degrades model performance
3. **Encoding issues**: Not handling Unicode properly leads to mojibake
4. **PII leakage**: Incomplete PII detection leads to privacy issues
5. **Dedup scope**: Global vs local deduplication affects diversity
6. **Order effects**: Processing order can affect deduplication results

---

## Glossary

| Term | Definition |
|------|------------|
| **BPE** | Byte Pair Encoding - subword tokenization algorithm |
| **Deduplication** | Process of removing duplicate or near-duplicate content |
| **FineWeb** | High-quality web dataset from HuggingFace |
| **Gopher rules** | Quality filtering heuristics from DeepMind's Gopher paper |
| **MinHash** | Probabilistic algorithm for estimating Jaccard similarity |
| **Mojibake** | Garbled text from incorrect encoding interpretation |
| **NeMo Curator** | NVIDIA's GPU-accelerated data curation framework |
| **NFKC** | Unicode Normalization Form Compatibility Composition |
| **Perplexity** | Measure of how well a language model predicts text |
| **PII** | Personally Identifiable Information |
| **SimHash** | Algorithm for computing locality-sensitive hash |
| **Shingling** | Converting text to n-gram sets for similarity comparison |

---

## References

### Research Papers
1. Rae, J.W., et al. (2022). "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"
2. Raffel, C., et al. (2020). "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer" (C4)
3. Penedo, G., et al. (2023). "The RefinedWeb Dataset for Falcon LLM"
4. Soldaini, L., et al. (2024). "Dolma: an Open Corpus of Three Trillion Tokens"

### Tools and Frameworks
- [NVIDIA NeMo Curator](https://github.com/NVIDIA-NeMo/Curator) - GPU-accelerated data curation
- [datatrove](https://github.com/huggingface/datatrove) - HuggingFace data processing
- [trafilatura](https://github.com/adbar/trafilatura) - Web content extraction
- [ftfy](https://github.com/rspeer/python-ftfy) - Fix text encoding issues
- [datasketch](https://github.com/ekzhu/datasketch) - MinHash implementation
- [Presidio](https://github.com/microsoft/presidio) - PII detection

### External Resources
- [FineWeb Dataset](https://huggingface.co/datasets/HuggingFaceFW/fineweb) - 15T token web corpus
- [SlimPajama](https://huggingface.co/datasets/cerebras/SlimPajama-627B) - Cleaned RedPajama
- [NVIDIA Nemotron-CC Blog](https://developer.nvidia.com/blog/building-nemotron-cc-a-high-quality-trillion-token-dataset-for-llm-pretraining-from-common-crawl-using-nvidia-nemo-curator/)

---

*Document Version: 1.0 | Last Updated: December 2025 | Next Review: March 2026*

---

> **Navigation**
> [← 1.1 Data Collection](1.1_data_collection_sourcing.md) | **[Index](../README.md#15-repository-structure)** | [1.3 Data Labeling →](1.3_data_labeling_annotation.md)
