# Document 5.1: LLM Evaluation Framework Guide

## Executive Summary

This guide provides a comprehensive framework for evaluating language model capabilities across multiple dimensions including capability, safety, efficiency, usability, and reliability. It covers automated metrics, model-based evaluation, human evaluation protocols, benchmark suites, and production A/B testing methodologies. The framework enables systematic assessment of LLMs throughout the development lifecycle from pre-training checkpoints through production deployment.

## Prerequisites

- Understanding of LLM training pipelines (see Documents 2.1-2.6)
- Familiarity with statistical concepts (confidence intervals, significance testing)
- Python proficiency for implementing evaluation harnesses
- Access to compute resources for running evaluations
- Understanding of task-specific metrics (NLP background helpful)

---

## 5.1.1 Evaluation Dimensions

### Multi-Dimensional Evaluation Framework

Effective LLM evaluation requires assessing models across multiple orthogonal dimensions rather than relying on single metrics.

```python
"""
ABOUTME: Defines the evaluation dimension framework for comprehensive LLM assessment.
ABOUTME: Provides structured taxonomy of what aspects to measure and why.
"""

from dataclasses import dataclass, field
from enum import Enum
from typing import Dict, List, Optional, Any
import json


class EvaluationDimension(Enum):
    """Core dimensions for LLM evaluation."""
    CAPABILITY = "capability"
    SAFETY = "safety"
    EFFICIENCY = "efficiency"
    USABILITY = "usability"
    RELIABILITY = "reliability"


@dataclass
class DimensionSpec:
    """Specification for an evaluation dimension."""
    name: str
    description: str
    subdimensions: List[str]
    metrics: List[str]
    benchmarks: List[str]
    weight: float = 1.0  # Relative importance


class EvaluationFramework:
    """
    Comprehensive framework for multi-dimensional LLM evaluation.

    Organizes evaluation into structured dimensions with associated
    metrics, benchmarks, and relative importance weights.
    """

    def __init__(self):
        self.dimensions = self._initialize_dimensions()

    def _initialize_dimensions(self) -> Dict[EvaluationDimension, DimensionSpec]:
        """Initialize the five core evaluation dimensions."""
        return {
            EvaluationDimension.CAPABILITY: DimensionSpec(
                name="Capability",
                description="Task performance, knowledge, and reasoning abilities",
                subdimensions=[
                    "knowledge_recall",      # Factual knowledge retrieval
                    "reasoning",             # Logical and mathematical reasoning
                    "language_understanding", # Comprehension and semantics
                    "generation_quality",    # Fluency, coherence, relevance
                    "task_completion",       # End-to-end task success
                    "multilingual",          # Cross-language capabilities
                    "coding",                # Code generation and understanding
                ],
                metrics=[
                    "accuracy", "f1_score", "exact_match",
                    "bleu", "rouge", "bertscore", "pass@k"
                ],
                benchmarks=[
                    "MMLU", "HellaSwag", "ARC", "WinoGrande",
                    "GSM8K", "MATH", "HumanEval", "MBPP"
                ],
                weight=1.0
            ),
            EvaluationDimension.SAFETY: DimensionSpec(
                name="Safety",
                description="Harm avoidance, robustness, and alignment",
                subdimensions=[
                    "toxicity_avoidance",    # Harmful content prevention
                    "bias_fairness",         # Demographic fairness
                    "factuality",            # Hallucination prevention
                    "jailbreak_resistance",  # Adversarial robustness
                    "privacy_protection",    # PII handling
                    "instruction_boundaries", # Scope adherence
                ],
                metrics=[
                    "toxicity_score", "bias_score", "refusal_rate",
                    "attack_success_rate", "factuality_score"
                ],
                benchmarks=[
                    "ToxiGen", "RealToxicityPrompts", "TruthfulQA",
                    "BBQ", "HarmBench", "AdvBench"
                ],
                weight=1.2  # Higher weight for safety
            ),
            EvaluationDimension.EFFICIENCY: DimensionSpec(
                name="Efficiency",
                description="Latency, throughput, and cost metrics",
                subdimensions=[
                    "latency",               # Time to first token, total time
                    "throughput",            # Tokens per second
                    "memory_usage",          # GPU/CPU memory
                    "cost_per_token",        # Inference cost
                    "batch_efficiency",      # Batching effectiveness
                ],
                metrics=[
                    "ttft_ms", "tpot_ms", "tokens_per_second",
                    "gpu_memory_gb", "cost_per_1k_tokens"
                ],
                benchmarks=[
                    "custom_latency_suite", "throughput_benchmark"
                ],
                weight=0.8
            ),
            EvaluationDimension.USABILITY: DimensionSpec(
                name="Usability",
                description="Instruction following and format compliance",
                subdimensions=[
                    "instruction_following", # Adherence to instructions
                    "format_compliance",     # Output structure correctness
                    "tool_use",              # Function calling accuracy
                    "context_utilization",   # Long context handling
                    "conversation_coherence", # Multi-turn consistency
                ],
                metrics=[
                    "instruction_accuracy", "format_match_rate",
                    "tool_call_accuracy", "context_recall"
                ],
                benchmarks=[
                    "IFEval", "MT-Bench", "AlpacaEval",
                    "BFCL", "LongBench"
                ],
                weight=1.0
            ),
            EvaluationDimension.RELIABILITY: DimensionSpec(
                name="Reliability",
                description="Consistency and calibration",
                subdimensions=[
                    "output_consistency",    # Same input -> similar output
                    "calibration",           # Confidence accuracy
                    "degradation_patterns",  # Performance under load
                    "edge_case_handling",    # Unusual input robustness
                ],
                metrics=[
                    "consistency_score", "ece_score",
                    "variance_coefficient", "edge_case_accuracy"
                ],
                benchmarks=[
                    "consistency_suite", "calibration_benchmark"
                ],
                weight=0.9
            )
        }

    def get_evaluation_plan(
        self,
        focus_dimensions: Optional[List[EvaluationDimension]] = None,
        time_budget_hours: float = 24.0
    ) -> Dict[str, Any]:
        """
        Generate an evaluation plan based on dimension priorities.

        Args:
            focus_dimensions: Dimensions to prioritize (None = all)
            time_budget_hours: Available compute time

        Returns:
            Structured evaluation plan with benchmark selection
        """
        if focus_dimensions is None:
            focus_dimensions = list(EvaluationDimension)

        # Calculate time allocation based on weights
        total_weight = sum(
            self.dimensions[d].weight
            for d in focus_dimensions
        )

        plan = {
            "dimensions": {},
            "total_benchmarks": 0,
            "estimated_hours": 0
        }

        for dim in focus_dimensions:
            spec = self.dimensions[dim]
            time_allocation = (spec.weight / total_weight) * time_budget_hours

            plan["dimensions"][dim.value] = {
                "benchmarks": spec.benchmarks,
                "metrics": spec.metrics,
                "time_hours": round(time_allocation, 2),
                "subdimensions": spec.subdimensions
            }
            plan["total_benchmarks"] += len(spec.benchmarks)

        plan["estimated_hours"] = time_budget_hours
        return plan

    def compute_composite_score(
        self,
        dimension_scores: Dict[EvaluationDimension, float]
    ) -> float:
        """
        Compute weighted composite score across dimensions.

        Args:
            dimension_scores: Score (0-100) for each dimension

        Returns:
            Weighted average score
        """
        total_weight = 0
        weighted_sum = 0

        for dim, score in dimension_scores.items():
            weight = self.dimensions[dim].weight
            weighted_sum += score * weight
            total_weight += weight

        return weighted_sum / total_weight if total_weight > 0 else 0
```

### Capability Assessment Deep Dive

```python
"""
ABOUTME: Detailed capability assessment across knowledge, reasoning, and generation.
ABOUTME: Implements subdimension-specific evaluation strategies.
"""

@dataclass
class CapabilityProfile:
    """Detailed capability breakdown for a model."""
    knowledge_scores: Dict[str, float]  # Domain -> score
    reasoning_scores: Dict[str, float]  # Type -> score
    generation_metrics: Dict[str, float]
    task_completion_rates: Dict[str, float]

    def get_strengths(self, threshold: float = 0.8) -> List[str]:
        """Identify capability areas above threshold."""
        strengths = []
        for domain, score in self.knowledge_scores.items():
            if score >= threshold:
                strengths.append(f"knowledge:{domain}")
        for rtype, score in self.reasoning_scores.items():
            if score >= threshold:
                strengths.append(f"reasoning:{rtype}")
        return strengths

    def get_weaknesses(self, threshold: float = 0.5) -> List[str]:
        """Identify capability areas below threshold."""
        weaknesses = []
        for domain, score in self.knowledge_scores.items():
            if score < threshold:
                weaknesses.append(f"knowledge:{domain}")
        for rtype, score in self.reasoning_scores.items():
            if score < threshold:
                weaknesses.append(f"reasoning:{rtype}")
        return weaknesses


class CapabilityEvaluator:
    """
    Evaluates model capabilities across knowledge domains and reasoning types.
    """

    KNOWLEDGE_DOMAINS = [
        "stem", "humanities", "social_sciences",
        "professional", "other"
    ]

    REASONING_TYPES = [
        "logical", "mathematical", "commonsense",
        "causal", "analogical", "spatial"
    ]

    def __init__(self, model_interface):
        self.model = model_interface
        self.results_cache = {}

    def evaluate_knowledge(
        self,
        benchmark: str = "mmlu",
        domains: Optional[List[str]] = None
    ) -> Dict[str, float]:
        """
        Evaluate factual knowledge across domains.

        Uses MMLU-style multiple choice questions to assess
        knowledge recall and application.
        """
        if domains is None:
            domains = self.KNOWLEDGE_DOMAINS

        results = {}
        for domain in domains:
            questions = self._load_questions(benchmark, domain)
            correct = 0
            total = 0

            for q in questions:
                response = self.model.generate(
                    prompt=self._format_mcq(q),
                    max_tokens=1,
                    temperature=0
                )
                predicted = self._extract_answer(response)
                if predicted == q["answer"]:
                    correct += 1
                total += 1

            results[domain] = correct / total if total > 0 else 0

        return results

    def evaluate_reasoning(
        self,
        reasoning_types: Optional[List[str]] = None
    ) -> Dict[str, float]:
        """
        Evaluate reasoning capabilities by type.

        Tests different reasoning modalities with appropriate
        benchmark tasks.
        """
        if reasoning_types is None:
            reasoning_types = self.REASONING_TYPES

        # Benchmark mapping
        type_to_benchmark = {
            "logical": "logiqa",
            "mathematical": "gsm8k",
            "commonsense": "hellaswag",
            "causal": "copa",
            "analogical": "analogy_test",
            "spatial": "spatial_reasoning"
        }

        results = {}
        for rtype in reasoning_types:
            benchmark = type_to_benchmark.get(rtype)
            if benchmark:
                score = self._run_benchmark(benchmark)
                results[rtype] = score

        return results

    def _format_mcq(self, question: Dict) -> str:
        """Format multiple choice question for the model."""
        prompt = question["question"] + "\n\n"
        for i, choice in enumerate(question["choices"]):
            label = chr(65 + i)  # A, B, C, D
            prompt += f"{label}. {choice}\n"
        prompt += "\nAnswer:"
        return prompt

    def _extract_answer(self, response: str) -> str:
        """Extract answer letter from model response."""
        response = response.strip().upper()
        if response and response[0] in "ABCD":
            return response[0]
        return ""

    def _load_questions(self, benchmark: str, domain: str) -> List[Dict]:
        """Load questions for a specific benchmark and domain."""
        # Implementation would load from dataset
        pass

    def _run_benchmark(self, benchmark: str) -> float:
        """Run a specific benchmark and return score."""
        # Implementation would run full benchmark
        pass
```

---

## 5.1.2 Evaluation Methods

### Automated Metrics

```python
"""
ABOUTME: Implementation of automated evaluation metrics for LLM outputs.
ABOUTME: Covers lexical, semantic, and task-specific metrics.
"""

import numpy as np
from typing import List, Dict, Tuple, Optional, Callable
from collections import Counter
import re
import math


class AutomatedMetrics:
    """
    Collection of automated metrics for text evaluation.

    Includes lexical overlap metrics (BLEU, ROUGE), semantic
    similarity metrics (BERTScore), and task-specific metrics.
    """

    def __init__(self, embedding_model=None, tokenizer=None):
        self.embedding_model = embedding_model
        self.tokenizer = tokenizer

    # ==================== Lexical Metrics ====================

    def compute_bleu(
        self,
        hypothesis: str,
        references: List[str],
        max_n: int = 4,
        weights: Optional[List[float]] = None
    ) -> Dict[str, float]:
        """
        Compute BLEU score with configurable n-gram weights.

        Args:
            hypothesis: Generated text
            references: List of reference texts
            max_n: Maximum n-gram order
            weights: Weights for each n-gram order

        Returns:
            Dictionary with BLEU scores and precisions
        """
        if weights is None:
            weights = [1.0 / max_n] * max_n

        hyp_tokens = hypothesis.lower().split()
        ref_token_lists = [r.lower().split() for r in references]

        # Compute n-gram precisions
        precisions = []
        for n in range(1, max_n + 1):
            hyp_ngrams = self._get_ngrams(hyp_tokens, n)

            # Max count across references for each n-gram
            max_ref_counts = Counter()
            for ref_tokens in ref_token_lists:
                ref_ngrams = self._get_ngrams(ref_tokens, n)
                for ngram, count in ref_ngrams.items():
                    max_ref_counts[ngram] = max(
                        max_ref_counts[ngram], count
                    )

            # Clipped counts
            clipped = sum(
                min(count, max_ref_counts[ngram])
                for ngram, count in hyp_ngrams.items()
            )
            total = sum(hyp_ngrams.values())

            precision = clipped / total if total > 0 else 0
            precisions.append(precision)

        # Brevity penalty
        hyp_len = len(hyp_tokens)
        ref_lens = [len(r) for r in ref_token_lists]
        closest_ref_len = min(ref_lens, key=lambda x: abs(x - hyp_len))

        if hyp_len >= closest_ref_len:
            bp = 1.0
        else:
            bp = math.exp(1 - closest_ref_len / hyp_len)

        # Weighted geometric mean
        log_precisions = [
            w * math.log(p) if p > 0 else float('-inf')
            for w, p in zip(weights, precisions)
        ]

        if any(lp == float('-inf') for lp in log_precisions):
            bleu = 0.0
        else:
            bleu = bp * math.exp(sum(log_precisions))

        return {
            "bleu": bleu,
            "brevity_penalty": bp,
            "precisions": precisions,
            "hypothesis_length": hyp_len,
            "reference_length": closest_ref_len
        }

    def compute_rouge(
        self,
        hypothesis: str,
        reference: str,
        rouge_types: List[str] = ["rouge1", "rouge2", "rougeL"]
    ) -> Dict[str, Dict[str, float]]:
        """
        Compute ROUGE scores (recall-oriented).

        Args:
            hypothesis: Generated text
            reference: Reference text
            rouge_types: Which ROUGE variants to compute

        Returns:
            Dictionary with precision, recall, F1 for each type
        """
        hyp_tokens = hypothesis.lower().split()
        ref_tokens = reference.lower().split()

        results = {}

        for rouge_type in rouge_types:
            if rouge_type == "rouge1":
                scores = self._compute_rouge_n(hyp_tokens, ref_tokens, 1)
            elif rouge_type == "rouge2":
                scores = self._compute_rouge_n(hyp_tokens, ref_tokens, 2)
            elif rouge_type == "rougeL":
                scores = self._compute_rouge_l(hyp_tokens, ref_tokens)
            else:
                continue

            results[rouge_type] = scores

        return results

    def _compute_rouge_n(
        self,
        hyp_tokens: List[str],
        ref_tokens: List[str],
        n: int
    ) -> Dict[str, float]:
        """Compute ROUGE-N scores."""
        hyp_ngrams = self._get_ngrams(hyp_tokens, n)
        ref_ngrams = self._get_ngrams(ref_tokens, n)

        overlap = sum(
            min(hyp_ngrams[ng], ref_ngrams[ng])
            for ng in hyp_ngrams if ng in ref_ngrams
        )

        hyp_count = sum(hyp_ngrams.values())
        ref_count = sum(ref_ngrams.values())

        precision = overlap / hyp_count if hyp_count > 0 else 0
        recall = overlap / ref_count if ref_count > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

        return {"precision": precision, "recall": recall, "f1": f1}

    def _compute_rouge_l(
        self,
        hyp_tokens: List[str],
        ref_tokens: List[str]
    ) -> Dict[str, float]:
        """Compute ROUGE-L using longest common subsequence."""
        lcs_length = self._lcs_length(hyp_tokens, ref_tokens)

        precision = lcs_length / len(hyp_tokens) if hyp_tokens else 0
        recall = lcs_length / len(ref_tokens) if ref_tokens else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

        return {"precision": precision, "recall": recall, "f1": f1}

    def _lcs_length(self, x: List[str], y: List[str]) -> int:
        """Compute longest common subsequence length."""
        m, n = len(x), len(y)
        dp = [[0] * (n + 1) for _ in range(m + 1)]

        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if x[i-1] == y[j-1]:
                    dp[i][j] = dp[i-1][j-1] + 1
                else:
                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])

        return dp[m][n]

    def _get_ngrams(self, tokens: List[str], n: int) -> Counter:
        """Extract n-grams from token list."""
        ngrams = [
            tuple(tokens[i:i+n])
            for i in range(len(tokens) - n + 1)
        ]
        return Counter(ngrams)

    # ==================== Semantic Metrics ====================

    def compute_bertscore(
        self,
        hypotheses: List[str],
        references: List[str],
        model_name: str = "microsoft/deberta-xlarge-mnli"
    ) -> Dict[str, List[float]]:
        """
        Compute BERTScore for semantic similarity.

        Uses contextual embeddings to measure semantic overlap
        beyond lexical matching.

        Args:
            hypotheses: List of generated texts
            references: List of reference texts
            model_name: Embedding model to use

        Returns:
            Precision, recall, F1 scores for each pair
        """
        # Note: In production, use the bert_score library
        # This is a simplified implementation showing the concept

        results = {
            "precision": [],
            "recall": [],
            "f1": []
        }

        for hyp, ref in zip(hypotheses, references):
            # Get token embeddings
            hyp_embeddings = self._get_token_embeddings(hyp)
            ref_embeddings = self._get_token_embeddings(ref)

            if hyp_embeddings is None or ref_embeddings is None:
                results["precision"].append(0.0)
                results["recall"].append(0.0)
                results["f1"].append(0.0)
                continue

            # Compute pairwise cosine similarities
            similarity_matrix = self._cosine_similarity_matrix(
                hyp_embeddings, ref_embeddings
            )

            # Precision: max similarity for each hypothesis token
            precision = similarity_matrix.max(axis=1).mean()

            # Recall: max similarity for each reference token
            recall = similarity_matrix.max(axis=0).mean()

            # F1
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

            results["precision"].append(float(precision))
            results["recall"].append(float(recall))
            results["f1"].append(float(f1))

        return results

    def _get_token_embeddings(self, text: str) -> Optional[np.ndarray]:
        """Get contextual embeddings for each token."""
        if self.embedding_model is None:
            return None
        # Implementation would use transformer model
        pass

    def _cosine_similarity_matrix(
        self,
        embeddings_a: np.ndarray,
        embeddings_b: np.ndarray
    ) -> np.ndarray:
        """Compute pairwise cosine similarities."""
        # Normalize
        a_norm = embeddings_a / np.linalg.norm(embeddings_a, axis=1, keepdims=True)
        b_norm = embeddings_b / np.linalg.norm(embeddings_b, axis=1, keepdims=True)
        return np.dot(a_norm, b_norm.T)

    # ==================== Task-Specific Metrics ====================

    def compute_exact_match(
        self,
        predictions: List[str],
        references: List[str],
        normalize: bool = True
    ) -> float:
        """
        Compute exact match accuracy.

        Useful for QA and extraction tasks where exact
        string matching is appropriate.
        """
        correct = 0
        for pred, ref in zip(predictions, references):
            if normalize:
                pred = self._normalize_answer(pred)
                ref = self._normalize_answer(ref)
            if pred == ref:
                correct += 1
        return correct / len(predictions) if predictions else 0

    def _normalize_answer(self, text: str) -> str:
        """Normalize text for comparison."""
        text = text.lower()
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'[^\w\s]', '', text)
        return text.strip()

    def compute_pass_at_k(
        self,
        num_samples: int,
        num_correct: int,
        k: int
    ) -> float:
        """
        Compute pass@k for code generation.

        Probability that at least one of k samples passes.
        Uses unbiased estimator from Chen et al. (2021).
        """
        if num_samples - num_correct < k:
            return 1.0

        return 1.0 - np.prod(
            1.0 - k / np.arange(num_samples - num_correct + 1, num_samples + 1)
        )
```

### Model-Based Evaluation (LLM-as-Judge)

```python
"""
ABOUTME: LLM-as-Judge evaluation framework for scalable quality assessment.
ABOUTME: Implements pairwise comparison and direct scoring approaches.
"""

from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple
from enum import Enum
import json
import random


class JudgingMode(Enum):
    """Modes for LLM-based evaluation."""
    DIRECT_SCORE = "direct_score"      # Rate on a scale
    PAIRWISE = "pairwise"              # Compare two outputs
    REFERENCE = "reference"            # Compare to reference
    RUBRIC = "rubric"                  # Multi-criteria rubric


@dataclass
class JudgmentResult:
    """Result from LLM judge evaluation."""
    mode: JudgingMode
    score: Optional[float]             # For direct scoring
    winner: Optional[str]              # For pairwise (A/B/tie)
    criteria_scores: Dict[str, float]  # For rubric-based
    reasoning: str                     # Judge's explanation
    confidence: float                  # Self-assessed confidence
    raw_response: str                  # Original judge output


class LLMJudge:
    """
    LLM-as-Judge for scalable evaluation.

    Uses a capable LLM to evaluate outputs based on various
    criteria. Supports multiple judging modes and includes
    bias mitigation strategies.
    """

    def __init__(
        self,
        judge_model,
        temperature: float = 0.0,
        max_tokens: int = 1024
    ):
        self.judge = judge_model
        self.temperature = temperature
        self.max_tokens = max_tokens

    def evaluate_direct(
        self,
        prompt: str,
        response: str,
        criteria: str = "helpfulness",
        scale: Tuple[int, int] = (1, 10)
    ) -> JudgmentResult:
        """
        Direct scoring of a single response.

        Args:
            prompt: Original user prompt
            response: Model response to evaluate
            criteria: What aspect to judge
            scale: Min and max score values
        """
        judge_prompt = self._build_direct_prompt(
            prompt, response, criteria, scale
        )

        judge_response = self.judge.generate(
            prompt=judge_prompt,
            temperature=self.temperature,
            max_tokens=self.max_tokens
        )

        return self._parse_direct_response(
            judge_response, criteria, scale
        )

    def evaluate_pairwise(
        self,
        prompt: str,
        response_a: str,
        response_b: str,
        criteria: str = "overall_quality",
        swap_positions: bool = True
    ) -> JudgmentResult:
        """
        Pairwise comparison of two responses.

        Args:
            prompt: Original user prompt
            response_a: First response
            response_b: Second response
            criteria: Comparison criteria
            swap_positions: Run twice with swapped order (reduces position bias)
        """
        results = []

        # First comparison
        judge_prompt = self._build_pairwise_prompt(
            prompt, response_a, response_b, criteria
        )
        raw_result = self.judge.generate(
            prompt=judge_prompt,
            temperature=self.temperature,
            max_tokens=self.max_tokens
        )
        results.append(("A_first", self._parse_pairwise_response(raw_result)))

        # Second comparison with swapped positions (bias mitigation)
        if swap_positions:
            judge_prompt_swapped = self._build_pairwise_prompt(
                prompt, response_b, response_a, criteria
            )
            raw_result_swapped = self.judge.generate(
                prompt=judge_prompt_swapped,
                temperature=self.temperature,
                max_tokens=self.max_tokens
            )
            # Invert the result since positions were swapped
            parsed = self._parse_pairwise_response(raw_result_swapped)
            if parsed["winner"] == "A":
                parsed["winner"] = "B"
            elif parsed["winner"] == "B":
                parsed["winner"] = "A"
            results.append(("B_first", parsed))

        # Aggregate results
        return self._aggregate_pairwise_results(results)

    def evaluate_rubric(
        self,
        prompt: str,
        response: str,
        rubric: Dict[str, str]
    ) -> JudgmentResult:
        """
        Multi-criteria rubric evaluation.

        Args:
            prompt: Original user prompt
            response: Model response to evaluate
            rubric: Dictionary mapping criteria names to descriptions
        """
        judge_prompt = self._build_rubric_prompt(prompt, response, rubric)

        judge_response = self.judge.generate(
            prompt=judge_prompt,
            temperature=self.temperature,
            max_tokens=self.max_tokens
        )

        return self._parse_rubric_response(judge_response, rubric)

    def _build_direct_prompt(
        self,
        prompt: str,
        response: str,
        criteria: str,
        scale: Tuple[int, int]
    ) -> str:
        """Build prompt for direct scoring."""
        return """You are an expert evaluator. Rate the following response on {criteria}.

[User Prompt]
{prompt}

[Assistant Response]
{response}

Rate the response on a scale of {min_score} to {max_score} for {criteria}.

Provide your evaluation in the following JSON format:
{{
    "score": <number>,
    "reasoning": "<explanation>",
    "confidence": <0.0-1.0>
}}""".format(
            criteria=criteria,
            prompt=prompt,
            response=response,
            min_score=scale[0],
            max_score=scale[1]
        )

    def _build_pairwise_prompt(
        self,
        prompt: str,
        response_a: str,
        response_b: str,
        criteria: str
    ) -> str:
        """Build prompt for pairwise comparison."""
        return """You are an expert evaluator. Compare the following two responses.

[User Prompt]
{prompt}

[Response A]
{response_a}

[Response B]
{response_b}

Which response is better in terms of {criteria}?

Provide your evaluation in the following JSON format:
{{
    "winner": "A" or "B" or "tie",
    "reasoning": "<explanation>",
    "confidence": <0.0-1.0>
}}""".format(
            prompt=prompt,
            response_a=response_a,
            response_b=response_b,
            criteria=criteria
        )

    def _build_rubric_prompt(
        self,
        prompt: str,
        response: str,
        rubric: Dict[str, str]
    ) -> str:
        """Build prompt for rubric-based evaluation."""
        rubric_text = "\n".join(
            f"- {name}: {desc}" for name, desc in rubric.items()
        )

        return """You are an expert evaluator. Evaluate the response using the following rubric.

[User Prompt]
{prompt}

[Assistant Response]
{response}

[Evaluation Rubric]
{rubric_text}

For each criterion, provide a score from 1-5.

Provide your evaluation in the following JSON format:
{{
    "scores": {{"criterion_name": score, ...}},
    "reasoning": "<explanation for each criterion>",
    "overall_score": <average>,
    "confidence": <0.0-1.0>
}}""".format(
            prompt=prompt,
            response=response,
            rubric_text=rubric_text
        )

    def _parse_direct_response(
        self,
        response: str,
        criteria: str,
        scale: Tuple[int, int]
    ) -> JudgmentResult:
        """Parse direct scoring response."""
        try:
            # Extract JSON from response
            json_match = re.search(r'\{[^}]+\}', response, re.DOTALL)
            if json_match:
                data = json.loads(json_match.group())
                return JudgmentResult(
                    mode=JudgingMode.DIRECT_SCORE,
                    score=float(data.get("score", 0)),
                    winner=None,
                    criteria_scores={criteria: float(data.get("score", 0))},
                    reasoning=data.get("reasoning", ""),
                    confidence=float(data.get("confidence", 0.5)),
                    raw_response=response
                )
        except (json.JSONDecodeError, ValueError):
            pass

        # Fallback parsing
        return JudgmentResult(
            mode=JudgingMode.DIRECT_SCORE,
            score=None,
            winner=None,
            criteria_scores={},
            reasoning="Failed to parse response",
            confidence=0.0,
            raw_response=response
        )

    def _parse_pairwise_response(self, response: str) -> Dict:
        """Parse pairwise comparison response."""
        try:
            json_match = re.search(r'\{[^}]+\}', response, re.DOTALL)
            if json_match:
                data = json.loads(json_match.group())
                return {
                    "winner": data.get("winner", "tie"),
                    "reasoning": data.get("reasoning", ""),
                    "confidence": float(data.get("confidence", 0.5))
                }
        except (json.JSONDecodeError, ValueError):
            pass
        return {"winner": "tie", "reasoning": "Parse error", "confidence": 0.0}

    def _aggregate_pairwise_results(
        self,
        results: List[Tuple[str, Dict]]
    ) -> JudgmentResult:
        """Aggregate multiple pairwise comparisons."""
        winners = [r[1]["winner"] for r in results]
        confidences = [r[1]["confidence"] for r in results]

        # Determine final winner
        if len(set(winners)) == 1:
            final_winner = winners[0]
            final_confidence = sum(confidences) / len(confidences)
        else:
            # Disagreement - call it a tie with lower confidence
            final_winner = "tie"
            final_confidence = min(confidences) * 0.5

        return JudgmentResult(
            mode=JudgingMode.PAIRWISE,
            score=None,
            winner=final_winner,
            criteria_scores={},
            reasoning="; ".join(r[1]["reasoning"] for r in results),
            confidence=final_confidence,
            raw_response=str(results)
        )

    def _parse_rubric_response(
        self,
        response: str,
        rubric: Dict[str, str]
    ) -> JudgmentResult:
        """Parse rubric-based evaluation response."""
        try:
            json_match = re.search(r'\{[^}]+\}', response, re.DOTALL)
            if json_match:
                data = json.loads(json_match.group())
                scores = data.get("scores", {})
                return JudgmentResult(
                    mode=JudgingMode.RUBRIC,
                    score=float(data.get("overall_score", 0)),
                    winner=None,
                    criteria_scores={k: float(v) for k, v in scores.items()},
                    reasoning=data.get("reasoning", ""),
                    confidence=float(data.get("confidence", 0.5)),
                    raw_response=response
                )
        except (json.JSONDecodeError, ValueError):
            pass

        return JudgmentResult(
            mode=JudgingMode.RUBRIC,
            score=None,
            winner=None,
            criteria_scores={},
            reasoning="Failed to parse response",
            confidence=0.0,
            raw_response=response
        )


class GEval:
    """
    G-Eval: NLG Evaluation using GPT-4 with Chain-of-Thought.

    Implements the G-Eval framework for fine-grained evaluation
    with form-filling paradigm.
    """

    def __init__(self, judge_model, num_samples: int = 20):
        self.judge = judge_model
        self.num_samples = num_samples

    def evaluate(
        self,
        source: str,
        output: str,
        aspect: str,
        aspect_definition: str,
        steps: List[str]
    ) -> Dict[str, float]:
        """
        Evaluate using G-Eval methodology.

        Args:
            source: Source/input text
            output: Generated output to evaluate
            aspect: Aspect being evaluated (e.g., "coherence")
            aspect_definition: Definition of the aspect
            steps: Chain-of-thought evaluation steps

        Returns:
            Score and token probabilities
        """
        prompt = self._build_geval_prompt(
            source, output, aspect, aspect_definition, steps
        )

        # Sample multiple times with temperature
        scores = []
        for _ in range(self.num_samples):
            response = self.judge.generate(
                prompt=prompt,
                temperature=1.0,
                max_tokens=2,
                logprobs=True
            )
            score = self._extract_score(response)
            if score is not None:
                scores.append(score)

        if not scores:
            return {"score": 0.0, "std": 0.0, "samples": 0}

        return {
            "score": sum(scores) / len(scores),
            "std": np.std(scores) if len(scores) > 1 else 0.0,
            "samples": len(scores)
        }

    def _build_geval_prompt(
        self,
        source: str,
        output: str,
        aspect: str,
        aspect_definition: str,
        steps: List[str]
    ) -> str:
        """Build G-Eval prompt with CoT steps."""
        steps_text = "\n".join(f"{i+1}. {step}" for i, step in enumerate(steps))

        return """You will evaluate the quality of a generated text.

Task: Evaluate the {aspect} of the output.
Definition: {aspect_definition}

Evaluation Steps:
{steps_text}

Source: {source}

Output: {output}

Based on the above evaluation steps, rate the {aspect} on a scale of 1-5.
Score:""".format(
            aspect=aspect,
            aspect_definition=aspect_definition,
            steps_text=steps_text,
            source=source,
            output=output
        )

    def _extract_score(self, response: str) -> Optional[int]:
        """Extract numeric score from response."""
        match = re.search(r'[1-5]', response.strip())
        if match:
            return int(match.group())
        return None
```

### Human Evaluation Protocols

```python
"""
ABOUTME: Framework for structured human evaluation of LLM outputs.
ABOUTME: Implements annotation interfaces, quality control, and aggregation.
"""

from dataclasses import dataclass, field
from typing import List, Dict, Optional, Set
from enum import Enum
import statistics
from datetime import datetime


class AnnotationType(Enum):
    """Types of human annotation tasks."""
    LIKERT_SCALE = "likert"           # 1-5 or 1-7 rating
    BINARY = "binary"                  # Yes/no
    RANKING = "ranking"                # Order items
    PAIRWISE = "pairwise"             # Compare two items
    FREE_TEXT = "free_text"           # Open-ended feedback
    MULTI_LABEL = "multi_label"       # Select applicable labels


@dataclass
class AnnotationTask:
    """Definition of a human annotation task."""
    task_id: str
    task_type: AnnotationType
    prompt: str
    response: str
    reference: Optional[str] = None
    response_b: Optional[str] = None  # For pairwise
    criteria: str = ""
    options: List[str] = field(default_factory=list)
    metadata: Dict = field(default_factory=dict)


@dataclass
class Annotation:
    """A single human annotation."""
    task_id: str
    annotator_id: str
    value: any  # Type depends on AnnotationType
    timestamp: datetime
    duration_seconds: float
    confidence: Optional[float] = None
    comments: Optional[str] = None


class HumanEvaluationManager:
    """
    Manages human evaluation campaigns.

    Handles task distribution, quality control through redundancy
    and gold standards, and annotation aggregation.
    """

    def __init__(
        self,
        min_annotators: int = 3,
        gold_ratio: float = 0.1,
        agreement_threshold: float = 0.7
    ):
        self.min_annotators = min_annotators
        self.gold_ratio = gold_ratio
        self.agreement_threshold = agreement_threshold
        self.tasks: Dict[str, AnnotationTask] = {}
        self.annotations: Dict[str, List[Annotation]] = {}
        self.annotator_stats: Dict[str, Dict] = {}
        self.gold_standards: Dict[str, any] = {}

    def create_task(self, task: AnnotationTask) -> str:
        """Register a new annotation task."""
        self.tasks[task.task_id] = task
        self.annotations[task.task_id] = []
        return task.task_id

    def add_gold_standard(self, task_id: str, correct_value: any) -> None:
        """Add a gold standard answer for quality control."""
        self.gold_standards[task_id] = correct_value

    def submit_annotation(self, annotation: Annotation) -> Dict[str, any]:
        """
        Submit an annotation and check quality.

        Returns:
            Status including quality check results
        """
        task = self.tasks.get(annotation.task_id)
        if not task:
            return {"status": "error", "message": "Task not found"}

        # Check gold standard if applicable
        quality_result = None
        if annotation.task_id in self.gold_standards:
            correct = self.gold_standards[annotation.task_id]
            is_correct = self._check_gold_answer(annotation.value, correct, task.task_type)
            quality_result = {"is_gold": True, "correct": is_correct}
            self._update_annotator_quality(annotation.annotator_id, is_correct)

        # Store annotation
        self.annotations[annotation.task_id].append(annotation)

        # Update annotator statistics
        self._update_annotator_stats(annotation)

        return {
            "status": "success",
            "quality_result": quality_result,
            "task_complete": len(self.annotations[annotation.task_id]) >= self.min_annotators
        }

    def aggregate_annotations(
        self,
        task_id: str,
        method: str = "majority"
    ) -> Dict[str, any]:
        """
        Aggregate annotations for a task.

        Args:
            task_id: Task to aggregate
            method: Aggregation method (majority, weighted, dawid_skene)

        Returns:
            Aggregated result with agreement metrics
        """
        task = self.tasks.get(task_id)
        annotations = self.annotations.get(task_id, [])

        if not annotations:
            return {"status": "no_annotations"}

        values = [a.value for a in annotations]

        if task.task_type == AnnotationType.LIKERT_SCALE:
            return self._aggregate_likert(values, method)
        elif task.task_type == AnnotationType.BINARY:
            return self._aggregate_binary(values, method)
        elif task.task_type == AnnotationType.PAIRWISE:
            return self._aggregate_pairwise(values, method)
        elif task.task_type == AnnotationType.RANKING:
            return self._aggregate_ranking(values)
        else:
            return {"status": "unsupported_type"}

    def _aggregate_likert(
        self,
        values: List[int],
        method: str
    ) -> Dict[str, any]:
        """Aggregate Likert scale ratings."""
        if method == "majority":
            # Use median for ordinal data
            aggregated = statistics.median(values)
        elif method == "mean":
            aggregated = statistics.mean(values)
        else:
            aggregated = statistics.median(values)

        # Calculate agreement (Krippendorff's alpha approximation)
        if len(values) > 1:
            variance = statistics.variance(values)
            max_variance = ((max(values) - min(values)) / 2) ** 2
            agreement = 1 - (variance / max_variance) if max_variance > 0 else 1.0
        else:
            agreement = 1.0

        return {
            "value": aggregated,
            "mean": statistics.mean(values),
            "median": statistics.median(values),
            "std": statistics.stdev(values) if len(values) > 1 else 0,
            "agreement": agreement,
            "n_annotations": len(values)
        }

    def _aggregate_binary(
        self,
        values: List[bool],
        method: str
    ) -> Dict[str, any]:
        """Aggregate binary judgments."""
        yes_count = sum(1 for v in values if v)
        no_count = len(values) - yes_count

        if method == "majority":
            aggregated = yes_count > no_count
        else:
            aggregated = yes_count >= len(values) / 2

        # Agreement is proportion agreeing with majority
        majority_count = max(yes_count, no_count)
        agreement = majority_count / len(values)

        return {
            "value": aggregated,
            "yes_ratio": yes_count / len(values),
            "agreement": agreement,
            "n_annotations": len(values)
        }

    def _aggregate_pairwise(
        self,
        values: List[str],
        method: str
    ) -> Dict[str, any]:
        """Aggregate pairwise comparisons."""
        from collections import Counter
        counts = Counter(values)

        if method == "majority":
            aggregated = counts.most_common(1)[0][0]
        else:
            aggregated = counts.most_common(1)[0][0]

        agreement = counts[aggregated] / len(values)

        return {
            "winner": aggregated,
            "counts": dict(counts),
            "agreement": agreement,
            "n_annotations": len(values)
        }

    def _aggregate_ranking(self, values: List[List[str]]) -> Dict[str, any]:
        """Aggregate rankings using Borda count."""
        scores = {}
        n_items = len(values[0]) if values else 0

        for ranking in values:
            for position, item in enumerate(ranking):
                points = n_items - position
                scores[item] = scores.get(item, 0) + points

        # Sort by score
        aggregated_ranking = sorted(
            scores.keys(),
            key=lambda x: scores[x],
            reverse=True
        )

        return {
            "ranking": aggregated_ranking,
            "scores": scores,
            "n_annotations": len(values)
        }

    def _check_gold_answer(
        self,
        submitted: any,
        correct: any,
        task_type: AnnotationType
    ) -> bool:
        """Check if submitted answer matches gold standard."""
        if task_type == AnnotationType.LIKERT_SCALE:
            return abs(submitted - correct) <= 1  # Allow 1 point tolerance
        elif task_type == AnnotationType.BINARY:
            return submitted == correct
        elif task_type == AnnotationType.PAIRWISE:
            return submitted == correct
        return False

    def _update_annotator_quality(
        self,
        annotator_id: str,
        is_correct: bool
    ) -> None:
        """Update annotator quality metrics."""
        if annotator_id not in self.annotator_stats:
            self.annotator_stats[annotator_id] = {
                "gold_correct": 0,
                "gold_total": 0,
                "total_annotations": 0
            }

        stats = self.annotator_stats[annotator_id]
        stats["gold_total"] += 1
        if is_correct:
            stats["gold_correct"] += 1

    def _update_annotator_stats(self, annotation: Annotation) -> None:
        """Update general annotator statistics."""
        if annotation.annotator_id not in self.annotator_stats:
            self.annotator_stats[annotation.annotator_id] = {
                "gold_correct": 0,
                "gold_total": 0,
                "total_annotations": 0,
                "avg_duration": 0,
                "durations": []
            }

        stats = self.annotator_stats[annotation.annotator_id]
        stats["total_annotations"] += 1
        stats["durations"].append(annotation.duration_seconds)
        stats["avg_duration"] = statistics.mean(stats["durations"])

    def get_annotator_quality(self, annotator_id: str) -> Optional[float]:
        """Get annotator quality score based on gold standards."""
        stats = self.annotator_stats.get(annotator_id)
        if not stats or stats["gold_total"] == 0:
            return None
        return stats["gold_correct"] / stats["gold_total"]

    def compute_inter_annotator_agreement(
        self,
        metric: str = "fleiss_kappa"
    ) -> float:
        """
        Compute overall inter-annotator agreement.

        Args:
            metric: Agreement metric (fleiss_kappa, krippendorff_alpha)

        Returns:
            Agreement score
        """
        # Collect all annotations into matrix form
        # Implementation would compute the specified metric
        pass
```

---

## 5.1.3 Evaluation Infrastructure

### Evaluation Harness Setup

```python
"""
ABOUTME: Evaluation harness infrastructure for systematic LLM assessment.
ABOUTME: Wraps lm-evaluation-harness with extensions for custom tasks.
"""

import os
import json
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Any, Callable
from pathlib import Path
import subprocess
import yaml


@dataclass
class EvaluationConfig:
    """Configuration for an evaluation run."""
    model_path: str
    tasks: List[str]
    num_fewshot: int = 0
    batch_size: int = 8
    device: str = "cuda"
    output_dir: str = "./eval_results"
    limit: Optional[int] = None
    seed: int = 42
    trust_remote_code: bool = False
    extra_args: Dict[str, Any] = field(default_factory=dict)


class EvaluationHarness:
    """
    Wrapper for lm-evaluation-harness with extensions.

    Provides a unified interface for running evaluations,
    supporting both standard benchmarks and custom tasks.
    """

    def __init__(
        self,
        harness_path: str = "lm-evaluation-harness",
        custom_tasks_dir: Optional[str] = None
    ):
        self.harness_path = harness_path
        self.custom_tasks_dir = custom_tasks_dir
        self.registered_tasks: Dict[str, Dict] = {}

        if custom_tasks_dir:
            self._load_custom_tasks(custom_tasks_dir)

    def run_evaluation(
        self,
        config: EvaluationConfig
    ) -> Dict[str, Any]:
        """
        Run evaluation with the specified configuration.

        Args:
            config: Evaluation configuration

        Returns:
            Dictionary containing results for each task
        """
        os.makedirs(config.output_dir, exist_ok=True)

        # Build command for lm-evaluation-harness
        cmd = [
            "python", "-m", "lm_eval",
            "--model", "hf",
            "--model_args", f"pretrained={config.model_path}",
            "--tasks", ",".join(config.tasks),
            "--num_fewshot", str(config.num_fewshot),
            "--batch_size", str(config.batch_size),
            "--device", config.device,
            "--output_path", config.output_dir,
            "--seed", str(config.seed),
        ]

        if config.limit:
            cmd.extend(["--limit", str(config.limit)])

        if config.trust_remote_code:
            cmd.append("--trust_remote_code")

        if self.custom_tasks_dir:
            cmd.extend(["--include_path", self.custom_tasks_dir])

        # Run evaluation
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            cwd=self.harness_path
        )

        if result.returncode != 0:
            raise RuntimeError(f"Evaluation failed: {result.stderr}")

        # Load and return results
        return self._load_results(config.output_dir)

    def _load_results(self, output_dir: str) -> Dict[str, Any]:
        """Load evaluation results from output directory."""
        results_file = Path(output_dir) / "results.json"
        if results_file.exists():
            with open(results_file) as f:
                return json.load(f)
        return {}

    def _load_custom_tasks(self, tasks_dir: str) -> None:
        """Load custom task definitions."""
        tasks_path = Path(tasks_dir)
        for yaml_file in tasks_path.glob("*.yaml"):
            with open(yaml_file) as f:
                task_config = yaml.safe_load(f)
                task_name = yaml_file.stem
                self.registered_tasks[task_name] = task_config

    def register_task(
        self,
        name: str,
        dataset_path: str,
        metric: str,
        prompt_template: str,
        doc_to_text: Callable,
        doc_to_target: Callable,
        num_fewshot: int = 0
    ) -> None:
        """
        Register a custom evaluation task.

        Args:
            name: Task identifier
            dataset_path: HuggingFace dataset path
            metric: Primary metric (accuracy, f1, etc.)
            prompt_template: Template for formatting prompts
            doc_to_text: Function to convert doc to input text
            doc_to_target: Function to extract target answer
            num_fewshot: Default few-shot examples
        """
        task_config = {
            "dataset_path": dataset_path,
            "metric": metric,
            "prompt_template": prompt_template,
            "num_fewshot": num_fewshot,
            "doc_to_text": doc_to_text,
            "doc_to_target": doc_to_target
        }

        self.registered_tasks[name] = task_config

        # Write to custom tasks directory if specified
        if self.custom_tasks_dir:
            self._write_task_yaml(name, task_config)

    def _write_task_yaml(self, name: str, config: Dict) -> None:
        """Write task configuration as YAML for lm-eval-harness."""
        yaml_config = {
            "task": name,
            "dataset_path": config["dataset_path"],
            "output_type": "generate_until",
            "metric_list": [{"metric": config["metric"]}],
            "num_fewshot": config["num_fewshot"]
        }

        yaml_path = Path(self.custom_tasks_dir) / f"{name}.yaml"
        with open(yaml_path, "w") as f:
            yaml.dump(yaml_config, f)

    def list_available_tasks(self) -> List[str]:
        """List all available evaluation tasks."""
        # Standard tasks from lm-evaluation-harness
        standard_tasks = [
            "mmlu", "hellaswag", "arc_easy", "arc_challenge",
            "winogrande", "truthfulqa", "gsm8k", "humaneval"
        ]

        # Add custom tasks
        all_tasks = standard_tasks + list(self.registered_tasks.keys())
        return sorted(set(all_tasks))


@dataclass
class CustomTaskDefinition:
    """Definition for a custom evaluation task."""
    name: str
    description: str
    dataset_config: Dict[str, Any]
    prompt_config: Dict[str, Any]
    metric_config: Dict[str, Any]

    def to_yaml(self) -> str:
        """Convert to YAML format for lm-eval-harness."""
        config = {
            "task": self.name,
            "description": self.description,
            **self.dataset_config,
            **self.prompt_config,
            **self.metric_config
        }
        return yaml.dump(config)


class CustomTaskBuilder:
    """Builder for creating custom evaluation tasks."""

    def __init__(self):
        self.name = ""
        self.description = ""
        self.dataset_config = {}
        self.prompt_config = {}
        self.metric_config = {}

    def set_name(self, name: str) -> "CustomTaskBuilder":
        self.name = name
        return self

    def set_description(self, desc: str) -> "CustomTaskBuilder":
        self.description = desc
        return self

    def set_dataset(
        self,
        path: str,
        name: Optional[str] = None,
        split: str = "test"
    ) -> "CustomTaskBuilder":
        self.dataset_config = {
            "dataset_path": path,
            "dataset_name": name,
            "test_split": split
        }
        return self

    def set_prompt(
        self,
        template: str,
        doc_to_text: str,
        doc_to_target: str
    ) -> "CustomTaskBuilder":
        self.prompt_config = {
            "doc_to_text": doc_to_text,
            "doc_to_target": doc_to_target,
            "prompt_template": template
        }
        return self

    def set_metrics(
        self,
        primary: str,
        additional: Optional[List[str]] = None
    ) -> "CustomTaskBuilder":
        metrics = [{"metric": primary}]
        if additional:
            metrics.extend([{"metric": m} for m in additional])
        self.metric_config = {"metric_list": metrics}
        return self

    def build(self) -> CustomTaskDefinition:
        return CustomTaskDefinition(
            name=self.name,
            description=self.description,
            dataset_config=self.dataset_config,
            prompt_config=self.prompt_config,
            metric_config=self.metric_config
        )
```

### Distributed Evaluation

```python
"""
ABOUTME: Distributed evaluation infrastructure for large-scale benchmarking.
ABOUTME: Implements parallel evaluation across multiple workers and GPUs.
"""

import ray
from dataclasses import dataclass
from typing import List, Dict, Any, Optional
import torch
from concurrent.futures import ThreadPoolExecutor, as_completed


@dataclass
class DistributedEvalConfig:
    """Configuration for distributed evaluation."""
    num_workers: int = 4
    gpus_per_worker: float = 1.0
    tasks_per_worker: int = 1
    batch_size: int = 8
    timeout_seconds: int = 3600
    retry_failed: bool = True
    max_retries: int = 3


@ray.remote
class EvaluationWorker:
    """
    Ray actor for distributed evaluation.

    Each worker handles a subset of evaluation tasks
    on its assigned GPU(s).
    """

    def __init__(
        self,
        model_path: str,
        device_id: int = 0
    ):
        self.model_path = model_path
        self.device = f"cuda:{device_id}" if torch.cuda.is_available() else "cpu"
        self.model = None
        self._load_model()

    def _load_model(self):
        """Load model onto assigned device."""
        from transformers import AutoModelForCausalLM, AutoTokenizer

        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            torch_dtype=torch.float16,
            device_map=self.device
        )
        self.model.eval()

    def evaluate_task(
        self,
        task_name: str,
        examples: List[Dict],
        batch_size: int = 8
    ) -> Dict[str, Any]:
        """
        Evaluate a single task on assigned examples.

        Args:
            task_name: Name of the evaluation task
            examples: List of examples to evaluate
            batch_size: Batch size for inference

        Returns:
            Results dictionary with metrics
        """
        results = {
            "task": task_name,
            "predictions": [],
            "correct": 0,
            "total": 0
        }

        for i in range(0, len(examples), batch_size):
            batch = examples[i:i + batch_size]
            predictions = self._process_batch(batch, task_name)

            for pred, example in zip(predictions, batch):
                is_correct = self._check_answer(pred, example.get("answer"))
                results["predictions"].append({
                    "input": example.get("input"),
                    "prediction": pred,
                    "expected": example.get("answer"),
                    "correct": is_correct
                })
                results["total"] += 1
                if is_correct:
                    results["correct"] += 1

        results["accuracy"] = results["correct"] / results["total"] if results["total"] > 0 else 0
        return results

    def _process_batch(
        self,
        batch: List[Dict],
        task_name: str
    ) -> List[str]:
        """Process a batch of examples."""
        inputs = [ex.get("input", "") for ex in batch]

        encoded = self.tokenizer(
            inputs,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=2048
        ).to(self.device)

        with torch.no_grad():
            outputs = self.model.generate(
                **encoded,
                max_new_tokens=256,
                temperature=0.0,
                do_sample=False
            )

        predictions = self.tokenizer.batch_decode(
            outputs[:, encoded["input_ids"].shape[1]:],
            skip_special_tokens=True
        )

        return predictions

    def _check_answer(
        self,
        prediction: str,
        expected: Optional[str]
    ) -> bool:
        """Check if prediction matches expected answer."""
        if expected is None:
            return False
        pred_clean = prediction.strip().lower()
        exp_clean = expected.strip().lower()
        return pred_clean == exp_clean or pred_clean.startswith(exp_clean)


class DistributedEvaluator:
    """
    Coordinator for distributed evaluation.

    Manages worker pool, task distribution, and result aggregation.
    """

    def __init__(
        self,
        model_path: str,
        config: DistributedEvalConfig
    ):
        self.model_path = model_path
        self.config = config
        self.workers: List[EvaluationWorker] = []

        self._initialize_workers()

    def _initialize_workers(self):
        """Initialize Ray workers."""
        if not ray.is_initialized():
            ray.init()

        num_gpus = torch.cuda.device_count()
        workers_per_gpu = max(1, self.config.num_workers // num_gpus)

        for gpu_id in range(min(num_gpus, self.config.num_workers)):
            for _ in range(workers_per_gpu):
                worker = EvaluationWorker.options(
                    num_gpus=self.config.gpus_per_worker
                ).remote(self.model_path, gpu_id)
                self.workers.append(worker)

    def evaluate(
        self,
        tasks: Dict[str, List[Dict]]
    ) -> Dict[str, Dict[str, Any]]:
        """
        Run distributed evaluation across all tasks.

        Args:
            tasks: Dictionary mapping task names to example lists

        Returns:
            Aggregated results for each task
        """
        # Distribute tasks across workers
        task_assignments = self._assign_tasks(tasks)

        # Launch evaluations
        futures = []
        for worker, (task_name, examples) in task_assignments:
            future = worker.evaluate_task.remote(
                task_name,
                examples,
                self.config.batch_size
            )
            futures.append((task_name, future))

        # Collect results
        results = {}
        for task_name, future in futures:
            try:
                task_result = ray.get(future, timeout=self.config.timeout_seconds)
                if task_name not in results:
                    results[task_name] = {
                        "predictions": [],
                        "correct": 0,
                        "total": 0
                    }
                results[task_name]["predictions"].extend(task_result["predictions"])
                results[task_name]["correct"] += task_result["correct"]
                results[task_name]["total"] += task_result["total"]
            except Exception as e:
                print(f"Error evaluating {task_name}: {e}")
                if self.config.retry_failed:
                    # Retry logic would go here
                    pass

        # Compute final metrics
        for task_name in results:
            r = results[task_name]
            r["accuracy"] = r["correct"] / r["total"] if r["total"] > 0 else 0

        return results

    def _assign_tasks(
        self,
        tasks: Dict[str, List[Dict]]
    ) -> List[tuple]:
        """Distribute tasks to workers using round-robin."""
        assignments = []
        worker_idx = 0

        for task_name, examples in tasks.items():
            # Split examples across workers
            chunk_size = len(examples) // len(self.workers) + 1

            for i in range(0, len(examples), chunk_size):
                chunk = examples[i:i + chunk_size]
                worker = self.workers[worker_idx % len(self.workers)]
                assignments.append((worker, (task_name, chunk)))
                worker_idx += 1

        return assignments

    def shutdown(self):
        """Shutdown Ray workers."""
        ray.shutdown()
```

### Result Storage and Tracking

```python
"""
ABOUTME: Evaluation result storage and experiment tracking infrastructure.
ABOUTME: Implements versioned storage with comparison and trend analysis.
"""

import json
import hashlib
from dataclasses import dataclass, field, asdict
from datetime import datetime
from typing import List, Dict, Any, Optional
from pathlib import Path
import sqlite3


@dataclass
class EvaluationRun:
    """Record of a single evaluation run."""
    run_id: str
    model_name: str
    model_version: str
    timestamp: datetime
    config: Dict[str, Any]
    results: Dict[str, Any]
    metadata: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> Dict:
        d = asdict(self)
        d["timestamp"] = self.timestamp.isoformat()
        return d

    @classmethod
    def from_dict(cls, d: Dict) -> "EvaluationRun":
        d["timestamp"] = datetime.fromisoformat(d["timestamp"])
        return cls(**d)


class EvaluationStore:
    """
    Persistent storage for evaluation results.

    Supports versioning, querying, and comparison of
    evaluation runs across model versions.
    """

    def __init__(self, db_path: str = "evaluations.db"):
        self.db_path = db_path
        self._init_db()

    def _init_db(self):
        """Initialize SQLite database."""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute("""
            CREATE TABLE IF NOT EXISTS runs (
                run_id TEXT PRIMARY KEY,
                model_name TEXT NOT NULL,
                model_version TEXT NOT NULL,
                timestamp TEXT NOT NULL,
                config_json TEXT NOT NULL,
                results_json TEXT NOT NULL,
                metadata_json TEXT
            )
        """)

        cursor.execute("""
            CREATE TABLE IF NOT EXISTS metrics (
                run_id TEXT NOT NULL,
                task TEXT NOT NULL,
                metric_name TEXT NOT NULL,
                metric_value REAL NOT NULL,
                FOREIGN KEY (run_id) REFERENCES runs (run_id)
            )
        """)

        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_model_version
            ON runs (model_name, model_version)
        """)

        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_task_metric
            ON metrics (task, metric_name)
        """)

        conn.commit()
        conn.close()

    def store_run(self, run: EvaluationRun) -> str:
        """Store an evaluation run."""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute("""
            INSERT INTO runs (run_id, model_name, model_version, timestamp,
                            config_json, results_json, metadata_json)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (
            run.run_id,
            run.model_name,
            run.model_version,
            run.timestamp.isoformat(),
            json.dumps(run.config),
            json.dumps(run.results),
            json.dumps(run.metadata)
        ))

        # Store individual metrics for efficient querying
        for task, task_results in run.results.items():
            if isinstance(task_results, dict):
                for metric_name, value in task_results.items():
                    if isinstance(value, (int, float)):
                        cursor.execute("""
                            INSERT INTO metrics (run_id, task, metric_name, metric_value)
                            VALUES (?, ?, ?, ?)
                        """, (run.run_id, task, metric_name, value))

        conn.commit()
        conn.close()
        return run.run_id

    def get_run(self, run_id: str) -> Optional[EvaluationRun]:
        """Retrieve a specific run by ID."""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute("""
            SELECT * FROM runs WHERE run_id = ?
        """, (run_id,))

        row = cursor.fetchone()
        conn.close()

        if row:
            return EvaluationRun(
                run_id=row[0],
                model_name=row[1],
                model_version=row[2],
                timestamp=datetime.fromisoformat(row[3]),
                config=json.loads(row[4]),
                results=json.loads(row[5]),
                metadata=json.loads(row[6]) if row[6] else {}
            )
        return None

    def query_runs(
        self,
        model_name: Optional[str] = None,
        model_version: Optional[str] = None,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
        limit: int = 100
    ) -> List[EvaluationRun]:
        """Query runs with filters."""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        query = "SELECT * FROM runs WHERE 1=1"
        params = []

        if model_name:
            query += " AND model_name = ?"
            params.append(model_name)

        if model_version:
            query += " AND model_version = ?"
            params.append(model_version)

        if start_date:
            query += " AND timestamp >= ?"
            params.append(start_date.isoformat())

        if end_date:
            query += " AND timestamp <= ?"
            params.append(end_date.isoformat())

        query += " ORDER BY timestamp DESC LIMIT ?"
        params.append(limit)

        cursor.execute(query, params)
        rows = cursor.fetchall()
        conn.close()

        return [
            EvaluationRun(
                run_id=row[0],
                model_name=row[1],
                model_version=row[2],
                timestamp=datetime.fromisoformat(row[3]),
                config=json.loads(row[4]),
                results=json.loads(row[5]),
                metadata=json.loads(row[6]) if row[6] else {}
            )
            for row in rows
        ]

    def compare_runs(
        self,
        run_ids: List[str],
        tasks: Optional[List[str]] = None
    ) -> Dict[str, Dict[str, Dict[str, float]]]:
        """
        Compare metrics across multiple runs.

        Returns:
            Nested dict: run_id -> task -> metric -> value
        """
        comparison = {}

        for run_id in run_ids:
            run = self.get_run(run_id)
            if run:
                comparison[run_id] = {}
                for task, task_results in run.results.items():
                    if tasks and task not in tasks:
                        continue
                    if isinstance(task_results, dict):
                        comparison[run_id][task] = {
                            k: v for k, v in task_results.items()
                            if isinstance(v, (int, float))
                        }

        return comparison

    def get_metric_trend(
        self,
        model_name: str,
        task: str,
        metric: str,
        limit: int = 20
    ) -> List[Dict[str, Any]]:
        """
        Get metric values over time for trend analysis.

        Returns:
            List of (version, timestamp, value) tuples
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute("""
            SELECT r.model_version, r.timestamp, m.metric_value
            FROM runs r
            JOIN metrics m ON r.run_id = m.run_id
            WHERE r.model_name = ? AND m.task = ? AND m.metric_name = ?
            ORDER BY r.timestamp DESC
            LIMIT ?
        """, (model_name, task, metric, limit))

        rows = cursor.fetchall()
        conn.close()

        return [
            {
                "version": row[0],
                "timestamp": datetime.fromisoformat(row[1]),
                "value": row[2]
            }
            for row in rows
        ]


def generate_run_id(
    model_name: str,
    model_version: str,
    config: Dict[str, Any]
) -> str:
    """Generate deterministic run ID from configuration."""
    content = json.dumps({
        "model_name": model_name,
        "model_version": model_version,
        "config": config,
        "timestamp": datetime.now().isoformat()
    }, sort_keys=True)
    return hashlib.sha256(content.encode()).hexdigest()[:16]
```

---

## 5.1.4 Evaluation Timing

### Checkpoint Evaluation Strategy

```python
"""
ABOUTME: Strategies for when to evaluate during training lifecycle.
ABOUTME: Implements checkpoint selection and continuous evaluation.
"""

from dataclasses import dataclass
from typing import List, Dict, Optional, Callable
from enum import Enum
from datetime import datetime


class TrainingPhase(Enum):
    """Phases in the LLM training lifecycle."""
    PRETRAINING = "pretraining"
    CONTINUED_PRETRAINING = "continued_pretraining"
    SFT = "supervised_finetuning"
    RLHF = "rlhf"
    DPO = "dpo"
    DEPLOYMENT = "deployment"


@dataclass
class EvaluationSchedule:
    """Schedule for when to run evaluations."""
    phase: TrainingPhase
    eval_frequency_steps: int
    eval_tasks: List[str]
    blocking: bool = False  # Whether to pause training for eval
    early_stop_metric: Optional[str] = None
    early_stop_patience: int = 3


class CheckpointEvaluator:
    """
    Manages evaluation timing during training.

    Determines which checkpoints to evaluate based on
    training phase, step count, and resource availability.
    """

    DEFAULT_SCHEDULES = {
        TrainingPhase.PRETRAINING: EvaluationSchedule(
            phase=TrainingPhase.PRETRAINING,
            eval_frequency_steps=10000,
            eval_tasks=["hellaswag", "arc_easy", "winogrande"],
            blocking=False
        ),
        TrainingPhase.SFT: EvaluationSchedule(
            phase=TrainingPhase.SFT,
            eval_frequency_steps=500,
            eval_tasks=["mmlu", "truthfulqa", "mt_bench"],
            blocking=True,
            early_stop_metric="mmlu_accuracy",
            early_stop_patience=5
        ),
        TrainingPhase.RLHF: EvaluationSchedule(
            phase=TrainingPhase.RLHF,
            eval_frequency_steps=100,
            eval_tasks=["reward_model_eval", "safety_eval", "helpfulness_eval"],
            blocking=True,
            early_stop_metric="reward_accuracy"
        ),
        TrainingPhase.DEPLOYMENT: EvaluationSchedule(
            phase=TrainingPhase.DEPLOYMENT,
            eval_frequency_steps=1,  # Every checkpoint
            eval_tasks=["full_benchmark_suite"],
            blocking=True
        )
    }

    def __init__(
        self,
        schedules: Optional[Dict[TrainingPhase, EvaluationSchedule]] = None,
        eval_runner: Optional[Callable] = None
    ):
        self.schedules = schedules or self.DEFAULT_SCHEDULES
        self.eval_runner = eval_runner
        self.eval_history: List[Dict] = []
        self.metric_history: Dict[str, List[float]] = {}

    def should_evaluate(
        self,
        phase: TrainingPhase,
        step: int,
        last_eval_step: int
    ) -> bool:
        """
        Determine if evaluation should run at current step.

        Args:
            phase: Current training phase
            step: Current training step
            last_eval_step: Last step when evaluation ran

        Returns:
            True if evaluation should run
        """
        schedule = self.schedules.get(phase)
        if not schedule:
            return False

        steps_since_eval = step - last_eval_step
        return steps_since_eval >= schedule.eval_frequency_steps

    def get_eval_tasks(self, phase: TrainingPhase) -> List[str]:
        """Get evaluation tasks for current phase."""
        schedule = self.schedules.get(phase)
        return schedule.eval_tasks if schedule else []

    def record_evaluation(
        self,
        phase: TrainingPhase,
        step: int,
        results: Dict[str, float]
    ) -> Dict[str, any]:
        """
        Record evaluation results and check stopping criteria.

        Args:
            phase: Current training phase
            step: Training step
            results: Evaluation results

        Returns:
            Dictionary with early stopping recommendation
        """
        record = {
            "phase": phase.value,
            "step": step,
            "timestamp": datetime.now().isoformat(),
            "results": results
        }
        self.eval_history.append(record)

        # Update metric history
        for metric, value in results.items():
            if metric not in self.metric_history:
                self.metric_history[metric] = []
            self.metric_history[metric].append(value)

        # Check early stopping
        schedule = self.schedules.get(phase)
        should_stop = False
        stop_reason = None

        if schedule and schedule.early_stop_metric:
            should_stop, stop_reason = self._check_early_stop(
                schedule.early_stop_metric,
                schedule.early_stop_patience
            )

        return {
            "recorded": True,
            "should_stop": should_stop,
            "stop_reason": stop_reason
        }

    def _check_early_stop(
        self,
        metric: str,
        patience: int
    ) -> tuple:
        """Check if early stopping criteria met."""
        history = self.metric_history.get(metric, [])

        if len(history) < patience + 1:
            return False, None

        recent = history[-patience:]
        best_recent = max(recent)
        best_ever = max(history[:-patience]) if len(history) > patience else 0

        if best_recent <= best_ever:
            return True, f"No improvement in {metric} for {patience} evaluations"

        return False, None

    def get_best_checkpoint(
        self,
        metric: str,
        phase: Optional[TrainingPhase] = None
    ) -> Optional[Dict]:
        """Find the checkpoint with best metric value."""
        best_value = float("-inf")
        best_record = None

        for record in self.eval_history:
            if phase and record["phase"] != phase.value:
                continue

            value = record["results"].get(metric)
            if value and value > best_value:
                best_value = value
                best_record = record

        return best_record


class ContinuousEvaluator:
    """
    Runs continuous evaluation in production.

    Monitors model performance over time with sampling
    and drift detection.
    """

    def __init__(
        self,
        sample_rate: float = 0.01,
        eval_batch_size: int = 100,
        alert_threshold: float = 0.05
    ):
        self.sample_rate = sample_rate
        self.eval_batch_size = eval_batch_size
        self.alert_threshold = alert_threshold
        self.baseline_metrics: Dict[str, float] = {}
        self.current_metrics: Dict[str, List[float]] = {}

    def set_baseline(self, metrics: Dict[str, float]) -> None:
        """Set baseline metrics for drift detection."""
        self.baseline_metrics = metrics.copy()

    def record_sample(
        self,
        request_id: str,
        input_text: str,
        output_text: str,
        metrics: Dict[str, float]
    ) -> Optional[Dict]:
        """
        Record a sampled request for continuous evaluation.

        Returns alert if drift detected.
        """
        import random

        # Probabilistic sampling
        if random.random() > self.sample_rate:
            return None

        # Update running metrics
        for metric, value in metrics.items():
            if metric not in self.current_metrics:
                self.current_metrics[metric] = []
            self.current_metrics[metric].append(value)

        # Check for drift if enough samples
        alerts = []
        for metric, values in self.current_metrics.items():
            if len(values) >= self.eval_batch_size:
                drift = self._check_drift(metric, values)
                if drift:
                    alerts.append(drift)
                # Reset window
                self.current_metrics[metric] = []

        return {"alerts": alerts} if alerts else None

    def _check_drift(
        self,
        metric: str,
        values: List[float]
    ) -> Optional[Dict]:
        """Check if metric has drifted from baseline."""
        if metric not in self.baseline_metrics:
            return None

        baseline = self.baseline_metrics[metric]
        current = sum(values) / len(values)

        relative_change = abs(current - baseline) / baseline if baseline != 0 else 0

        if relative_change > self.alert_threshold:
            return {
                "metric": metric,
                "baseline": baseline,
                "current": current,
                "change_pct": relative_change * 100,
                "severity": "high" if relative_change > 0.1 else "medium"
            }

        return None
```

---

## 5.1.5 Evaluation Reporting

### Report Generation

```python
"""
ABOUTME: Evaluation report generation with visualizations and comparisons.
ABOUTME: Produces markdown reports, HTML dashboards, and comparison tables.
"""

from dataclasses import dataclass
from typing import List, Dict, Any, Optional
from datetime import datetime
import json


@dataclass
class BenchmarkResult:
    """Result for a single benchmark."""
    name: str
    score: float
    score_stderr: Optional[float] = None
    baseline_score: Optional[float] = None
    num_samples: int = 0
    metadata: Dict[str, Any] = None


@dataclass
class EvaluationReport:
    """Complete evaluation report."""
    model_name: str
    model_version: str
    timestamp: datetime
    results: List[BenchmarkResult]
    config: Dict[str, Any]
    summary: str = ""


class ReportGenerator:
    """
    Generates evaluation reports in multiple formats.

    Supports markdown, HTML, JSON, and comparison reports.
    """

    def __init__(
        self,
        baseline_results: Optional[Dict[str, float]] = None,
        template_dir: Optional[str] = None
    ):
        self.baseline = baseline_results or {}
        self.template_dir = template_dir

    def generate_markdown(
        self,
        report: EvaluationReport,
        include_analysis: bool = True
    ) -> str:
        """Generate markdown report."""
        lines = [
            f"# Evaluation Report: {report.model_name}",
            f"",
            f"**Version:** {report.model_version}",
            f"**Date:** {report.timestamp.strftime('%Y-%m-%d %H:%M:%S')}",
            f"",
            "## Summary",
            f"",
            report.summary or self._generate_summary(report),
            f"",
            "## Benchmark Results",
            f"",
            "| Benchmark | Score | Std Err | Baseline | Delta |",
            "|-----------|-------|---------|----------|-------|"
        ]

        for result in report.results:
            baseline = result.baseline_score or self.baseline.get(result.name, None)
            delta = ""
            if baseline:
                diff = result.score - baseline
                delta = f"+{diff:.2f}" if diff > 0 else f"{diff:.2f}"

            stderr = f"{result.score_stderr:.3f}" if result.score_stderr else "-"
            baseline_str = f"{baseline:.2f}" if baseline else "-"

            lines.append(
                f"| {result.name} | {result.score:.2f} | {stderr} | {baseline_str} | {delta} |"
            )

        if include_analysis:
            lines.extend([
                "",
                "## Analysis",
                "",
                self._generate_analysis(report)
            ])

        lines.extend([
            "",
            "## Configuration",
            "",
            "```json",
            json.dumps(report.config, indent=2),
            "```"
        ])

        return "\n".join(lines)

    def generate_html(
        self,
        report: EvaluationReport,
        include_charts: bool = True
    ) -> str:
        """Generate HTML report with optional visualizations."""
        html_parts = [
            "<!DOCTYPE html>",
            "<html><head>",
            "<title>Evaluation Report</title>",
            "<style>",
            self._get_css(),
            "</style>",
            "</head><body>",
            f"<h1>Evaluation Report: {report.model_name}</h1>",
            f"<p><strong>Version:</strong> {report.model_version}</p>",
            f"<p><strong>Date:</strong> {report.timestamp.strftime('%Y-%m-%d %H:%M:%S')}</p>",
        ]

        if include_charts:
            html_parts.extend([
                "<h2>Performance Overview</h2>",
                self._generate_radar_chart(report),
                self._generate_bar_chart(report)
            ])

        # Results table
        html_parts.extend([
            "<h2>Benchmark Results</h2>",
            "<table>",
            "<tr><th>Benchmark</th><th>Score</th><th>Baseline</th><th>Change</th></tr>"
        ])

        for result in report.results:
            baseline = result.baseline_score or self.baseline.get(result.name)
            change_class = ""
            change_text = "-"

            if baseline:
                diff = result.score - baseline
                change_class = "positive" if diff > 0 else "negative"
                change_text = f"+{diff:.2f}" if diff > 0 else f"{diff:.2f}"

            html_parts.append(
                f"<tr><td>{result.name}</td><td>{result.score:.2f}</td>"
                f"<td>{baseline:.2f if baseline else '-'}</td>"
                f"<td class='{change_class}'>{change_text}</td></tr>"
            )

        html_parts.extend([
            "</table>",
            "</body></html>"
        ])

        return "\n".join(html_parts)

    def generate_comparison(
        self,
        reports: List[EvaluationReport],
        highlight_best: bool = True
    ) -> str:
        """Generate comparison table across multiple reports."""
        if not reports:
            return "No reports to compare."

        # Collect all benchmarks
        all_benchmarks = set()
        for report in reports:
            for result in report.results:
                all_benchmarks.add(result.name)

        # Build comparison table
        lines = [
            "# Model Comparison",
            "",
            "| Benchmark | " + " | ".join(r.model_name for r in reports) + " |",
            "|-----------|" + "|".join("-" * 10 for _ in reports) + "|"
        ]

        for benchmark in sorted(all_benchmarks):
            scores = []
            for report in reports:
                result = next(
                    (r for r in report.results if r.name == benchmark),
                    None
                )
                scores.append(result.score if result else None)

            # Find best score
            best_idx = None
            if highlight_best:
                valid_scores = [(i, s) for i, s in enumerate(scores) if s is not None]
                if valid_scores:
                    best_idx = max(valid_scores, key=lambda x: x[1])[0]

            # Format scores
            formatted = []
            for i, score in enumerate(scores):
                if score is None:
                    formatted.append("-")
                elif i == best_idx:
                    formatted.append(f"**{score:.2f}**")
                else:
                    formatted.append(f"{score:.2f}")

            lines.append(f"| {benchmark} | " + " | ".join(formatted) + " |")

        return "\n".join(lines)

    def _generate_summary(self, report: EvaluationReport) -> str:
        """Auto-generate summary from results."""
        if not report.results:
            return "No results available."

        avg_score = sum(r.score for r in report.results) / len(report.results)
        best = max(report.results, key=lambda r: r.score)
        worst = min(report.results, key=lambda r: r.score)

        return (
            f"Average score across {len(report.results)} benchmarks: {avg_score:.2f}. "
            f"Best performance on {best.name} ({best.score:.2f}), "
            f"weakest on {worst.name} ({worst.score:.2f})."
        )

    def _generate_analysis(self, report: EvaluationReport) -> str:
        """Generate analysis of results vs baseline."""
        if not self.baseline:
            return "No baseline available for comparison."

        improvements = []
        regressions = []

        for result in report.results:
            baseline = self.baseline.get(result.name)
            if baseline:
                diff = result.score - baseline
                if diff > 0.01:
                    improvements.append((result.name, diff))
                elif diff < -0.01:
                    regressions.append((result.name, diff))

        analysis = []

        if improvements:
            analysis.append("**Improvements:**")
            for name, diff in sorted(improvements, key=lambda x: -x[1]):
                analysis.append(f"- {name}: +{diff:.2f}")

        if regressions:
            analysis.append("")
            analysis.append("**Regressions:**")
            for name, diff in sorted(regressions, key=lambda x: x[1]):
                analysis.append(f"- {name}: {diff:.2f}")

        if not improvements and not regressions:
            analysis.append("No significant changes from baseline.")

        return "\n".join(analysis)

    def _generate_radar_chart(self, report: EvaluationReport) -> str:
        """Generate SVG radar chart."""
        # Simplified radar chart generation
        # In production, use a proper charting library
        return "<div class='chart'>Radar chart placeholder</div>"

    def _generate_bar_chart(self, report: EvaluationReport) -> str:
        """Generate SVG bar chart."""
        return "<div class='chart'>Bar chart placeholder</div>"

    def _get_css(self) -> str:
        """Get CSS styles for HTML report."""
        return """
            body { font-family: Arial, sans-serif; margin: 40px; }
            h1 { color: #333; }
            table { border-collapse: collapse; width: 100%; margin: 20px 0; }
            th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
            th { background-color: #4CAF50; color: white; }
            tr:nth-child(even) { background-color: #f2f2f2; }
            .positive { color: green; }
            .negative { color: red; }
            .chart { border: 1px solid #ddd; padding: 20px; margin: 20px 0; }
        """


class ConfidenceIntervalCalculator:
    """
    Calculate confidence intervals for evaluation metrics.

    Supports bootstrap and normal approximation methods.
    """

    @staticmethod
    def bootstrap_ci(
        scores: List[float],
        confidence: float = 0.95,
        n_bootstrap: int = 1000
    ) -> tuple:
        """
        Calculate bootstrap confidence interval.

        Args:
            scores: List of individual scores
            confidence: Confidence level
            n_bootstrap: Number of bootstrap samples

        Returns:
            (lower, upper) bounds
        """
        import numpy as np

        scores = np.array(scores)
        n = len(scores)

        bootstrap_means = []
        for _ in range(n_bootstrap):
            sample = np.random.choice(scores, size=n, replace=True)
            bootstrap_means.append(np.mean(sample))

        alpha = 1 - confidence
        lower = np.percentile(bootstrap_means, alpha / 2 * 100)
        upper = np.percentile(bootstrap_means, (1 - alpha / 2) * 100)

        return float(lower), float(upper)

    @staticmethod
    def normal_ci(
        mean: float,
        stderr: float,
        confidence: float = 0.95
    ) -> tuple:
        """
        Calculate confidence interval using normal approximation.

        Args:
            mean: Sample mean
            stderr: Standard error
            confidence: Confidence level

        Returns:
            (lower, upper) bounds
        """
        from scipy import stats

        alpha = 1 - confidence
        z = stats.norm.ppf(1 - alpha / 2)

        lower = mean - z * stderr
        upper = mean + z * stderr

        return lower, upper

    @staticmethod
    def wilson_ci(
        successes: int,
        total: int,
        confidence: float = 0.95
    ) -> tuple:
        """
        Wilson score interval for proportions.

        Better coverage for accuracy metrics near 0 or 1.
        """
        from scipy import stats

        if total == 0:
            return 0.0, 0.0

        p = successes / total
        alpha = 1 - confidence
        z = stats.norm.ppf(1 - alpha / 2)

        denominator = 1 + z**2 / total
        center = (p + z**2 / (2 * total)) / denominator
        spread = z * (p * (1 - p) / total + z**2 / (4 * total**2))**0.5 / denominator

        return max(0, center - spread), min(1, center + spread)
```

---

## Appendix A: lm-evaluation-harness Configuration

```yaml
# Example configuration for lm-evaluation-harness
# Save as eval_config.yaml

# Model configuration
model: hf
model_args:
  pretrained: meta-llama/Llama-2-7b-hf
  dtype: float16
  device_map: auto

# Task configuration
tasks:
  - mmlu
  - hellaswag
  - arc_challenge
  - winogrande
  - truthfulqa_mc

# Generation settings
num_fewshot: 5
batch_size: auto
max_batch_size: 32

# Output settings
output_path: ./results
log_samples: true
show_config: true

# Reproducibility
seed: 42
```

```bash
#!/bin/bash
# run_eval.sh - Script to run evaluations

MODEL_PATH=$1
OUTPUT_DIR=$2

python -m lm_eval \
    --model hf \
    --model_args pretrained=$MODEL_PATH,dtype=float16 \
    --tasks mmlu,hellaswag,arc_challenge,winogrande \
    --num_fewshot 5 \
    --batch_size auto \
    --output_path $OUTPUT_DIR \
    --seed 42 \
    --log_samples
```

---

## Appendix B: Custom Evaluation Task Templates

```yaml
# custom_task_template.yaml
# Template for creating custom evaluation tasks

task: my_custom_task
dataset_path: my_org/my_dataset
dataset_name: default
output_type: generate_until
test_split: test

# Document processing
doc_to_text: "Question: {{question}}\nAnswer:"
doc_to_target: "{{answer}}"

# Generation settings
generation_kwargs:
  max_gen_toks: 256
  temperature: 0
  do_sample: false
  until:
    - "\n"
    - "Question:"

# Metrics
metric_list:
  - metric: exact_match
    aggregation: mean
    higher_is_better: true
  - metric: f1
    aggregation: mean
    higher_is_better: true

# Few-shot configuration
num_fewshot: 5
fewshot_delimiter: "\n\n"
```

---

## Appendix C: Statistical Significance Testing

```python
"""
ABOUTME: Statistical significance testing utilities for evaluation comparison.
ABOUTME: Implements paired tests, multiple comparison correction, and effect sizes.
"""

from typing import List, Dict, Tuple, Optional
import numpy as np
from scipy import stats
from dataclasses import dataclass


@dataclass
class SignificanceResult:
    """Result of a significance test."""
    test_name: str
    statistic: float
    p_value: float
    significant: bool
    effect_size: Optional[float] = None
    confidence_interval: Optional[Tuple[float, float]] = None


class SignificanceTester:
    """Statistical significance testing for evaluation comparisons."""

    def __init__(self, alpha: float = 0.05):
        self.alpha = alpha

    def paired_bootstrap_test(
        self,
        scores_a: List[float],
        scores_b: List[float],
        n_bootstrap: int = 10000
    ) -> SignificanceResult:
        """
        Paired bootstrap test for comparing two systems.

        Recommended for most NLP evaluation comparisons.
        """
        scores_a = np.array(scores_a)
        scores_b = np.array(scores_b)

        observed_diff = np.mean(scores_a) - np.mean(scores_b)
        n = len(scores_a)

        # Bootstrap
        diffs = []
        for _ in range(n_bootstrap):
            indices = np.random.randint(0, n, size=n)
            boot_a = scores_a[indices]
            boot_b = scores_b[indices]
            diffs.append(np.mean(boot_a) - np.mean(boot_b))

        diffs = np.array(diffs)

        # Two-tailed p-value
        p_value = 2 * min(
            np.mean(diffs >= 0),
            np.mean(diffs <= 0)
        )

        # Confidence interval
        ci_lower = np.percentile(diffs, 2.5)
        ci_upper = np.percentile(diffs, 97.5)

        # Effect size (Cohen's d)
        pooled_std = np.sqrt((np.std(scores_a)**2 + np.std(scores_b)**2) / 2)
        effect_size = observed_diff / pooled_std if pooled_std > 0 else 0

        return SignificanceResult(
            test_name="paired_bootstrap",
            statistic=observed_diff,
            p_value=p_value,
            significant=p_value < self.alpha,
            effect_size=effect_size,
            confidence_interval=(ci_lower, ci_upper)
        )

    def mcnemar_test(
        self,
        correct_a: List[bool],
        correct_b: List[bool]
    ) -> SignificanceResult:
        """
        McNemar's test for comparing binary outcomes.

        Good for accuracy comparisons on the same test set.
        """
        # Build contingency table
        b = sum(a and not b for a, b in zip(correct_a, correct_b))  # A right, B wrong
        c = sum(not a and b for a, b in zip(correct_a, correct_b))  # A wrong, B right

        # McNemar's test with continuity correction
        if b + c == 0:
            return SignificanceResult(
                test_name="mcnemar",
                statistic=0,
                p_value=1.0,
                significant=False
            )

        statistic = (abs(b - c) - 1)**2 / (b + c)
        p_value = 1 - stats.chi2.cdf(statistic, df=1)

        return SignificanceResult(
            test_name="mcnemar",
            statistic=statistic,
            p_value=p_value,
            significant=p_value < self.alpha
        )

    def bonferroni_correction(
        self,
        p_values: List[float]
    ) -> List[float]:
        """Apply Bonferroni correction for multiple comparisons."""
        n = len(p_values)
        return [min(p * n, 1.0) for p in p_values]

    def benjamini_hochberg(
        self,
        p_values: List[float]
    ) -> List[Tuple[float, bool]]:
        """
        Benjamini-Hochberg procedure for FDR control.

        Returns adjusted p-values and significance decisions.
        """
        n = len(p_values)
        sorted_indices = np.argsort(p_values)
        sorted_p = np.array(p_values)[sorted_indices]

        # Calculate adjusted p-values
        adjusted = np.zeros(n)
        for i in range(n - 1, -1, -1):
            if i == n - 1:
                adjusted[i] = sorted_p[i]
            else:
                adjusted[i] = min(adjusted[i + 1], sorted_p[i] * n / (i + 1))

        # Map back to original order
        result = [(0.0, False)] * n
        for i, orig_idx in enumerate(sorted_indices):
            result[orig_idx] = (adjusted[i], adjusted[i] < self.alpha)

        return result
```

---

## Appendix D: Evaluation Report Template

```markdown
# Model Evaluation Report

## Model Information
- **Model Name:** [MODEL_NAME]
- **Model Version:** [VERSION]
- **Evaluation Date:** [DATE]
- **Evaluator:** [NAME]

## Executive Summary
[Brief summary of key findings - 2-3 sentences]

## Benchmark Results

### Capability Benchmarks
| Benchmark | Score | 95% CI | Baseline | Change |
|-----------|-------|--------|----------|--------|
| MMLU | | | | |
| HellaSwag | | | | |
| ARC-Challenge | | | | |
| WinoGrande | | | | |
| GSM8K | | | | |

### Safety Benchmarks
| Benchmark | Score | Baseline | Status |
|-----------|-------|----------|--------|
| ToxiGen | | | |
| TruthfulQA | | | |
| BBQ | | | |

### Efficiency Metrics
| Metric | Value | Target | Status |
|--------|-------|--------|--------|
| TTFT (p50) | | | |
| TTFT (p99) | | | |
| Throughput | | | |

## Analysis

### Strengths
- [Strength 1]
- [Strength 2]

### Areas for Improvement
- [Area 1]
- [Area 2]

### Regressions (if any)
- [Regression 1]

## Recommendations
1. [Recommendation 1]
2. [Recommendation 2]

## Appendix
- Raw results: [LINK]
- Configuration: [LINK]
- Logs: [LINK]
```

---

## Troubleshooting

### Common Issues

**Issue: Out of memory during evaluation**
```python
# Solution: Reduce batch size and enable gradient checkpointing
config = EvaluationConfig(
    batch_size=1,  # Reduce batch size
    extra_args={"gradient_checkpointing": True}
)

# Or use model offloading
model_args = "pretrained=model_path,device_map=auto,offload_folder=./offload"
```

**Issue: Inconsistent results across runs**
```python
# Solution: Fix all random seeds
import random
import numpy as np
import torch

def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    # Ensure deterministic operations
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
```

**Issue: Custom task not loading**
```yaml
# Ensure task YAML has required fields
task: my_task
dataset_path: my_org/dataset  # Must be valid HF path
output_type: generate_until    # Required
test_split: test              # Must match dataset
```

**Issue: Low agreement in human evaluation**
```python
# Solution: Add gold standards and annotator training
manager = HumanEvaluationManager(
    min_annotators=5,        # Increase annotators
    gold_ratio=0.15,         # More gold standards
    agreement_threshold=0.6  # Lower initial threshold
)

# Add calibration examples
for example in calibration_set:
    task = AnnotationTask(...)
    manager.create_task(task)
    manager.add_gold_standard(task.task_id, example["answer"])
```

---

## Glossary

| Term | Definition |
|------|------------|
| **BLEU** | Bilingual Evaluation Understudy - n-gram precision metric for generation |
| **ROUGE** | Recall-Oriented Understudy for Gisting Evaluation - recall-based metric |
| **BERTScore** | Semantic similarity metric using contextual embeddings |
| **Pass@k** | Probability at least one of k samples solves a coding problem |
| **LLM-as-Judge** | Using an LLM to evaluate other LLM outputs |
| **G-Eval** | Evaluation framework using chain-of-thought prompting |
| **Confidence Interval** | Range likely to contain the true parameter value |
| **Inter-Annotator Agreement** | Measure of consistency between human annotators |
| **Krippendorff's Alpha** | Agreement metric for multiple annotators and data types |
| **McNemar's Test** | Statistical test for paired binary outcomes |
| **Bonferroni Correction** | Method to control family-wise error rate |
| **FDR** | False Discovery Rate - expected proportion of false positives |

---

## References

1. Gao, L., et al. (2023). "A Framework for Few-shot Language Model Evaluation." [lm-evaluation-harness]
2. Zhang, T., et al. (2020). "BERTScore: Evaluating Text Generation with BERT."
3. Liu, Y., et al. (2023). "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment."
4. Zheng, L., et al. (2023). "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena."
5. Hendrycks, D., et al. (2021). "Measuring Massive Multitask Language Understanding." [MMLU]
6. Chen, M., et al. (2021). "Evaluating Large Language Models Trained on Code." [HumanEval]
7. Papineni, K., et al. (2002). "BLEU: a Method for Automatic Evaluation of Machine Translation."
8. Lin, C.Y. (2004). "ROUGE: A Package for Automatic Evaluation of Summaries."
