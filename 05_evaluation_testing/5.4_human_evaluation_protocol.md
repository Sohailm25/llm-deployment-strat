> **Navigation** | [← 5.3 LLM-as-Judge](5.3_llm_as_judge_evaluation.md) | [6.1 Quantization →](../06_model_optimization/6.1_quantization_guide.md)
>
> | | |
> |---|---|
> | **Prerequisites** | [5.1 Evaluation Framework](5.1_llm_evaluation_framework.md) &#124; Statistics |
> | **Related** | [5.3 LLM-as-Judge](5.3_llm_as_judge_evaluation.md) &#124; [1.3 Data Labeling](../01_data_pipeline/1.3_data_labeling_annotation.md) |
> | **Next** | [6.1 Quantization](../06_model_optimization/6.1_quantization_guide.md) |

# Document 5.4: Human Evaluation Protocol Guide

## Executive Summary

This guide provides standardized protocols for conducting human evaluations of LLM outputs. It covers evaluation task design, evaluator selection and training, interface design, quality control mechanisms, statistical analysis methods, and ethical considerations. The framework ensures reliable, reproducible human evaluations that serve as ground truth for model development and as validation for automated evaluation systems.

## Prerequisites

- Understanding of evaluation fundamentals (see Document 5.1)
- Basic statistics knowledge (significance testing, agreement metrics)
- Access to annotation platforms or tools
- Budget for evaluator compensation
- IRB/ethics approval where required

---

## 5.4.1 Evaluation Task Types

### Task Type Selection Framework

```python
"""
ABOUTME: Framework for selecting appropriate human evaluation task types.
ABOUTME: Maps evaluation needs to optimal task designs.
"""

from dataclasses import dataclass, field
from typing import List, Dict, Optional
from enum import Enum


class EvaluationTaskType(Enum):
    """Types of human evaluation tasks."""
    PAIRWISE_PREFERENCE = "pairwise"       # A vs B comparison
    ABSOLUTE_RATING = "absolute"           # Likert scale rating
    MULTI_DIMENSIONAL = "multi_dim"        # Rate multiple aspects
    ERROR_ANNOTATION = "error"             # Identify specific issues
    FREE_FORM_FEEDBACK = "free_form"       # Open-ended comments
    RANKING = "ranking"                    # Order multiple items
    BINARY_JUDGMENT = "binary"             # Yes/no decisions


class EvaluationGoal(Enum):
    """Goals for human evaluation."""
    MODEL_COMPARISON = "comparison"        # Which model is better
    QUALITY_ASSESSMENT = "quality"         # How good is output
    ERROR_DETECTION = "errors"             # What's wrong
    USER_PREFERENCE = "preference"         # What users prefer
    SAFETY_AUDIT = "safety"                # Safety review


@dataclass
class TaskTypeSpec:
    """Specification for an evaluation task type."""
    task_type: EvaluationTaskType
    description: str
    best_for: List[EvaluationGoal]
    pros: List[str]
    cons: List[str]
    min_annotators: int
    typical_time_per_item: float  # seconds
    agreement_metric: str


class TaskTypeSelector:
    """
    Selects optimal evaluation task type based on goals.

    Considers evaluation objectives, resource constraints,
    and reliability requirements.
    """

    TASK_SPECS = {
        EvaluationTaskType.PAIRWISE_PREFERENCE: TaskTypeSpec(
            task_type=EvaluationTaskType.PAIRWISE_PREFERENCE,
            description="Compare two responses side by side",
            best_for=[
                EvaluationGoal.MODEL_COMPARISON,
                EvaluationGoal.USER_PREFERENCE
            ],
            pros=[
                "Easier for annotators than absolute judgments",
                "Higher inter-annotator agreement",
                "Directly answers 'which is better'",
                "Less calibration required"
            ],
            cons=[
                "Only provides relative ranking",
                "Quadratic scaling for many models",
                "Position bias possible"
            ],
            min_annotators=3,
            typical_time_per_item=45,
            agreement_metric="cohens_kappa"
        ),
        EvaluationTaskType.ABSOLUTE_RATING: TaskTypeSpec(
            task_type=EvaluationTaskType.ABSOLUTE_RATING,
            description="Rate response on numeric scale (e.g., 1-5)",
            best_for=[
                EvaluationGoal.QUALITY_ASSESSMENT
            ],
            pros=[
                "Provides absolute quality measure",
                "Can compare any number of models",
                "Enables trend analysis over time"
            ],
            cons=[
                "Requires calibration across annotators",
                "Scale interpretation varies",
                "Lower inter-annotator agreement"
            ],
            min_annotators=5,
            typical_time_per_item=30,
            agreement_metric="krippendorff_alpha"
        ),
        EvaluationTaskType.MULTI_DIMENSIONAL: TaskTypeSpec(
            task_type=EvaluationTaskType.MULTI_DIMENSIONAL,
            description="Rate multiple quality dimensions separately",
            best_for=[
                EvaluationGoal.QUALITY_ASSESSMENT,
                EvaluationGoal.ERROR_DETECTION
            ],
            pros=[
                "Detailed quality breakdown",
                "Identifies specific strengths/weaknesses",
                "Enables targeted improvements"
            ],
            cons=[
                "Higher annotator cognitive load",
                "More time per annotation",
                "Potential dimension correlation issues"
            ],
            min_annotators=3,
            typical_time_per_item=90,
            agreement_metric="krippendorff_alpha"
        ),
        EvaluationTaskType.ERROR_ANNOTATION: TaskTypeSpec(
            task_type=EvaluationTaskType.ERROR_ANNOTATION,
            description="Identify and classify specific errors",
            best_for=[
                EvaluationGoal.ERROR_DETECTION,
                EvaluationGoal.SAFETY_AUDIT
            ],
            pros=[
                "Actionable feedback for improvement",
                "Precise error categorization",
                "Enables root cause analysis"
            ],
            cons=[
                "Requires trained annotators",
                "Time-intensive",
                "Error taxonomy must be predefined"
            ],
            min_annotators=2,
            typical_time_per_item=120,
            agreement_metric="f1_overlap"
        ),
        EvaluationTaskType.BINARY_JUDGMENT: TaskTypeSpec(
            task_type=EvaluationTaskType.BINARY_JUDGMENT,
            description="Yes/no decision on specific criteria",
            best_for=[
                EvaluationGoal.SAFETY_AUDIT,
                EvaluationGoal.ERROR_DETECTION
            ],
            pros=[
                "Simple, fast annotations",
                "High throughput",
                "Clear criteria"
            ],
            cons=[
                "Loses nuance",
                "Threshold ambiguity",
                "May miss gradations"
            ],
            min_annotators=3,
            typical_time_per_item=15,
            agreement_metric="fleiss_kappa"
        )
    }

    def select_task_type(
        self,
        goal: EvaluationGoal,
        num_models: int = 2,
        budget_per_item: float = 1.0,
        time_constraint: str = "flexible"
    ) -> TaskTypeSpec:
        """
        Select optimal task type for evaluation goal.

        Args:
            goal: Primary evaluation goal
            num_models: Number of models being evaluated
            budget_per_item: Available budget per evaluation item
            time_constraint: "urgent", "normal", "flexible"
        """
        # Filter by goal compatibility
        candidates = [
            spec for spec in self.TASK_SPECS.values()
            if goal in spec.best_for
        ]

        if not candidates:
            # Fallback to pairwise for comparison, absolute for quality
            if goal == EvaluationGoal.MODEL_COMPARISON:
                return self.TASK_SPECS[EvaluationTaskType.PAIRWISE_PREFERENCE]
            return self.TASK_SPECS[EvaluationTaskType.ABSOLUTE_RATING]

        # Score candidates
        scored = []
        for spec in candidates:
            score = self._score_task_type(
                spec, num_models, budget_per_item, time_constraint
            )
            scored.append((spec, score))

        # Return highest scoring
        scored.sort(key=lambda x: x[1], reverse=True)
        return scored[0][0]

    def _score_task_type(
        self,
        spec: TaskTypeSpec,
        num_models: int,
        budget_per_item: float,
        time_constraint: str
    ) -> float:
        """Score task type suitability."""
        score = 0.5

        # Time efficiency (important if constrained)
        time_multipliers = {"urgent": 2.0, "normal": 1.0, "flexible": 0.5}
        time_weight = time_multipliers.get(time_constraint, 1.0)
        time_score = 1.0 - (spec.typical_time_per_item / 120)  # Normalize to 2min
        score += time_score * 0.2 * time_weight

        # Budget fit
        estimated_cost = (spec.typical_time_per_item / 3600) * 15 * spec.min_annotators
        if estimated_cost <= budget_per_item:
            score += 0.2
        else:
            score -= 0.1

        # Scalability for multiple models
        if num_models > 5 and spec.task_type == EvaluationTaskType.PAIRWISE_PREFERENCE:
            score -= 0.2  # Pairwise doesn't scale well
        elif num_models > 5 and spec.task_type == EvaluationTaskType.ABSOLUTE_RATING:
            score += 0.1  # Absolute scales better

        return score

    def get_task_design(
        self,
        task_type: EvaluationTaskType
    ) -> Dict:
        """Get detailed task design template."""
        designs = {
            EvaluationTaskType.PAIRWISE_PREFERENCE: {
                "instruction_template": (
                    "Compare the following two AI assistant responses to the "
                    "user's question. Select the response that better addresses "
                    "the question in terms of helpfulness, accuracy, and clarity."
                ),
                "response_format": {
                    "type": "single_choice",
                    "options": ["Response A is better", "Response B is better", "Tie"]
                },
                "layout": "side_by_side",
                "randomize_order": True
            },
            EvaluationTaskType.ABSOLUTE_RATING: {
                "instruction_template": (
                    "Rate the quality of the following AI assistant response "
                    "on a scale of 1-5, where 1 is poor and 5 is excellent."
                ),
                "response_format": {
                    "type": "likert",
                    "scale": [1, 2, 3, 4, 5],
                    "labels": {
                        1: "Poor",
                        2: "Below Average",
                        3: "Average",
                        4: "Good",
                        5: "Excellent"
                    }
                },
                "layout": "single",
                "include_justification": True
            },
            EvaluationTaskType.MULTI_DIMENSIONAL: {
                "instruction_template": (
                    "Evaluate the response on each of the following dimensions. "
                    "Rate each dimension independently on a scale of 1-5."
                ),
                "dimensions": [
                    {"name": "Helpfulness", "description": "Does it answer the question?"},
                    {"name": "Accuracy", "description": "Is the information correct?"},
                    {"name": "Clarity", "description": "Is it easy to understand?"},
                    {"name": "Completeness", "description": "Does it cover all aspects?"}
                ],
                "response_format": {
                    "type": "multi_likert",
                    "scale": [1, 2, 3, 4, 5]
                }
            },
            EvaluationTaskType.ERROR_ANNOTATION: {
                "instruction_template": (
                    "Review the response and identify any errors. For each error, "
                    "select the error type and highlight the relevant text."
                ),
                "error_types": [
                    "Factual Error",
                    "Logical Error",
                    "Incompleteness",
                    "Irrelevance",
                    "Harmful Content",
                    "Formatting Issue"
                ],
                "response_format": {
                    "type": "span_annotation",
                    "allow_multiple": True
                }
            },
            EvaluationTaskType.BINARY_JUDGMENT: {
                "instruction_template": (
                    "Determine whether the response meets the specified criterion."
                ),
                "response_format": {
                    "type": "binary",
                    "options": ["Yes", "No"]
                },
                "include_confidence": True
            }
        }

        return designs.get(task_type, {})
```

---

## 5.4.2 Evaluator Selection

### Selection Criteria

```python
"""
ABOUTME: Framework for selecting and qualifying human evaluators.
ABOUTME: Implements screening, qualification, and diversity requirements.
"""

from dataclasses import dataclass
from typing import List, Dict, Optional, Set
from enum import Enum


class ExpertiseLevel(Enum):
    """Required expertise levels."""
    GENERAL_PUBLIC = "general"        # No special expertise
    DOMAIN_FAMILIAR = "familiar"      # Basic domain knowledge
    DOMAIN_EXPERT = "expert"          # Professional expertise
    HIGHLY_SPECIALIZED = "specialized" # Deep specialist knowledge


@dataclass
class EvaluatorRequirements:
    """Requirements for evaluator selection."""
    expertise_level: ExpertiseLevel
    languages: List[str]
    min_education: Optional[str] = None
    domain_requirements: List[str] = None
    demographic_diversity: bool = True
    geographic_requirements: List[str] = None
    min_age: int = 18
    prior_annotation_experience: bool = False


@dataclass
class EvaluatorProfile:
    """Profile of a potential evaluator."""
    evaluator_id: str
    expertise_areas: List[str]
    languages: List[str]
    education: str
    demographics: Dict[str, str]
    location: str
    age: int
    prior_experience: bool
    qualification_scores: Dict[str, float]


class EvaluatorSelector:
    """
    Selects qualified evaluators based on task requirements.

    Handles screening, qualification testing, and diversity balancing.
    """

    def __init__(
        self,
        evaluator_pool: List[EvaluatorProfile],
        diversity_targets: Optional[Dict[str, Dict[str, float]]] = None
    ):
        self.pool = evaluator_pool
        self.diversity_targets = diversity_targets or {}

    def select_evaluators(
        self,
        requirements: EvaluatorRequirements,
        num_needed: int,
        balance_diversity: bool = True
    ) -> List[EvaluatorProfile]:
        """
        Select qualified evaluators for a task.

        Args:
            requirements: Evaluator requirements
            num_needed: Number of evaluators needed
            balance_diversity: Whether to balance demographics
        """
        # Filter by basic requirements
        qualified = self._filter_qualified(requirements)

        if len(qualified) < num_needed:
            raise ValueError(
                f"Only {len(qualified)} qualified evaluators found, "
                f"need {num_needed}"
            )

        # Select with diversity if requested
        if balance_diversity and requirements.demographic_diversity:
            selected = self._select_diverse(qualified, num_needed)
        else:
            # Select by qualification score
            qualified.sort(
                key=lambda e: sum(e.qualification_scores.values()),
                reverse=True
            )
            selected = qualified[:num_needed]

        return selected

    def _filter_qualified(
        self,
        requirements: EvaluatorRequirements
    ) -> List[EvaluatorProfile]:
        """Filter evaluators by requirements."""
        qualified = []

        for evaluator in self.pool:
            # Check expertise level
            if not self._meets_expertise(evaluator, requirements.expertise_level):
                continue

            # Check language requirements
            if requirements.languages:
                if not any(lang in evaluator.languages for lang in requirements.languages):
                    continue

            # Check domain requirements
            if requirements.domain_requirements:
                if not any(
                    domain in evaluator.expertise_areas
                    for domain in requirements.domain_requirements
                ):
                    continue

            # Check geographic requirements
            if requirements.geographic_requirements:
                if evaluator.location not in requirements.geographic_requirements:
                    continue

            # Check age
            if evaluator.age < requirements.min_age:
                continue

            # Check prior experience if required
            if requirements.prior_annotation_experience and not evaluator.prior_experience:
                continue

            qualified.append(evaluator)

        return qualified

    def _meets_expertise(
        self,
        evaluator: EvaluatorProfile,
        required_level: ExpertiseLevel
    ) -> bool:
        """Check if evaluator meets expertise requirement."""
        # Simplified check based on qualification scores
        expertise_thresholds = {
            ExpertiseLevel.GENERAL_PUBLIC: 0.0,
            ExpertiseLevel.DOMAIN_FAMILIAR: 0.5,
            ExpertiseLevel.DOMAIN_EXPERT: 0.75,
            ExpertiseLevel.HIGHLY_SPECIALIZED: 0.9
        }

        threshold = expertise_thresholds.get(required_level, 0.0)
        avg_score = (
            sum(evaluator.qualification_scores.values()) /
            len(evaluator.qualification_scores)
            if evaluator.qualification_scores else 0
        )

        return avg_score >= threshold

    def _select_diverse(
        self,
        candidates: List[EvaluatorProfile],
        num_needed: int
    ) -> List[EvaluatorProfile]:
        """Select evaluators with demographic diversity."""
        selected = []
        demographic_counts: Dict[str, Dict[str, int]] = {}

        # Initialize counts
        for dim in self.diversity_targets:
            demographic_counts[dim] = {}

        for _ in range(num_needed):
            best_candidate = None
            best_diversity_score = -1

            for candidate in candidates:
                if candidate in selected:
                    continue

                # Score based on improving diversity
                diversity_score = self._compute_diversity_score(
                    candidate, demographic_counts
                )

                if diversity_score > best_diversity_score:
                    best_diversity_score = diversity_score
                    best_candidate = candidate

            if best_candidate:
                selected.append(best_candidate)
                # Update demographic counts
                for dim, value in best_candidate.demographics.items():
                    if dim in demographic_counts:
                        demographic_counts[dim][value] = (
                            demographic_counts[dim].get(value, 0) + 1
                        )

        return selected

    def _compute_diversity_score(
        self,
        candidate: EvaluatorProfile,
        current_counts: Dict[str, Dict[str, int]]
    ) -> float:
        """Score how much a candidate improves diversity."""
        score = 0.0

        for dim, targets in self.diversity_targets.items():
            candidate_value = candidate.demographics.get(dim)
            if not candidate_value:
                continue

            target_ratio = targets.get(candidate_value, 1.0 / len(targets))
            current_count = current_counts.get(dim, {}).get(candidate_value, 0)
            total_count = sum(current_counts.get(dim, {}).values()) + 1

            current_ratio = (current_count + 1) / total_count

            # Higher score for underrepresented groups
            if current_ratio < target_ratio:
                score += 1.0
            elif current_ratio > target_ratio * 1.5:
                score -= 0.5

        return score


class QualificationTester:
    """
    Creates and scores qualification tests for evaluators.

    Ensures evaluators understand task and provide quality annotations.
    """

    def __init__(self, pass_threshold: float = 0.8):
        self.pass_threshold = pass_threshold
        self.gold_standards: Dict[str, Dict] = {}

    def create_qualification_test(
        self,
        task_type: EvaluationTaskType,
        num_questions: int = 10
    ) -> List[Dict]:
        """Create qualification test for task type."""
        test_items = []

        # Include gold standard items with known answers
        gold_items = self._get_gold_standards(task_type, num_questions)
        test_items.extend(gold_items)

        return test_items

    def _get_gold_standards(
        self,
        task_type: EvaluationTaskType,
        num_items: int
    ) -> List[Dict]:
        """Get gold standard items for qualification."""
        # In practice, these would be pre-verified examples
        if task_type == EvaluationTaskType.PAIRWISE_PREFERENCE:
            return self._get_pairwise_gold_standards(num_items)
        elif task_type == EvaluationTaskType.ABSOLUTE_RATING:
            return self._get_rating_gold_standards(num_items)
        return []

    def _get_pairwise_gold_standards(self, num_items: int) -> List[Dict]:
        """Get gold standards for pairwise tasks."""
        # Example gold standards - in practice, load from database
        standards = [
            {
                "id": "gold_1",
                "question": "What is 2+2?",
                "response_a": "2+2 equals 4.",
                "response_b": "The answer might be 5 or maybe 4.",
                "gold_answer": "A",  # A is clearly better
                "difficulty": "easy"
            },
            {
                "id": "gold_2",
                "question": "Explain photosynthesis",
                "response_a": "Plants make food.",
                "response_b": "Photosynthesis is the process by which plants convert sunlight, water, and CO2 into glucose and oxygen.",
                "gold_answer": "B",  # B is clearly better
                "difficulty": "easy"
            }
        ]
        return standards[:num_items]

    def _get_rating_gold_standards(self, num_items: int) -> List[Dict]:
        """Get gold standards for rating tasks."""
        standards = [
            {
                "id": "gold_1",
                "question": "What is the capital of France?",
                "response": "Paris is the capital of France.",
                "gold_rating": 5,  # Perfect answer
                "acceptable_range": (4, 5)
            },
            {
                "id": "gold_2",
                "question": "Explain quantum mechanics",
                "response": "It's about tiny particles.",
                "gold_rating": 2,  # Too vague
                "acceptable_range": (1, 3)
            }
        ]
        return standards[:num_items]

    def score_qualification(
        self,
        responses: List[Dict],
        gold_standards: List[Dict],
        task_type: EvaluationTaskType
    ) -> Dict:
        """
        Score evaluator's qualification test.

        Returns pass/fail and detailed breakdown.
        """
        correct = 0
        details = []

        for response, gold in zip(responses, gold_standards):
            is_correct = self._check_response(response, gold, task_type)
            if is_correct:
                correct += 1
            details.append({
                "item_id": gold["id"],
                "correct": is_correct,
                "submitted": response.get("answer"),
                "expected": gold.get("gold_answer") or gold.get("gold_rating")
            })

        score = correct / len(gold_standards) if gold_standards else 0

        return {
            "score": score,
            "passed": score >= self.pass_threshold,
            "correct_count": correct,
            "total_count": len(gold_standards),
            "details": details
        }

    def _check_response(
        self,
        response: Dict,
        gold: Dict,
        task_type: EvaluationTaskType
    ) -> bool:
        """Check if response matches gold standard."""
        if task_type == EvaluationTaskType.PAIRWISE_PREFERENCE:
            return response.get("answer") == gold.get("gold_answer")

        elif task_type == EvaluationTaskType.ABSOLUTE_RATING:
            rating = response.get("rating")
            acceptable = gold.get("acceptable_range", (gold["gold_rating"], gold["gold_rating"]))
            return acceptable[0] <= rating <= acceptable[1]

        return False
```

---

## 5.4.3 Evaluation Interface Design

### Interface Components

```python
"""
ABOUTME: Design patterns for human evaluation interfaces.
ABOUTME: Implements task presentation, response collection, and UX optimization.
"""

from dataclasses import dataclass, field
from typing import List, Dict, Optional, Any
from enum import Enum


class LayoutType(Enum):
    """Interface layout types."""
    SIDE_BY_SIDE = "side_by_side"    # For pairwise comparison
    SINGLE_ITEM = "single"           # For single response evaluation
    LIST_VIEW = "list"               # For ranking tasks
    TABBED = "tabbed"                # Multiple items in tabs


@dataclass
class InterfaceConfig:
    """Configuration for evaluation interface."""
    layout: LayoutType
    show_instructions: bool = True
    show_progress: bool = True
    allow_skip: bool = False
    require_justification: bool = False
    time_tracking: bool = True
    attention_checks_enabled: bool = True
    attention_check_frequency: float = 0.1  # 10% of items
    max_time_per_item: Optional[int] = None  # seconds
    randomize_order: bool = True
    show_context: bool = True


@dataclass
class UIComponent:
    """UI component specification."""
    component_type: str
    label: str
    required: bool = True
    options: Optional[List] = None
    validation: Optional[Dict] = None
    help_text: Optional[str] = None


class InterfaceBuilder:
    """
    Builds evaluation interfaces based on task requirements.

    Creates configurable, accessible interfaces for various
    evaluation task types.
    """

    def build_interface(
        self,
        task_type: EvaluationTaskType,
        config: InterfaceConfig,
        custom_components: Optional[List[UIComponent]] = None
    ) -> Dict[str, Any]:
        """
        Build interface specification for task type.

        Returns:
            Interface specification as JSON-serializable dict
        """
        base = self._get_base_interface(task_type, config)

        # Add custom components
        if custom_components:
            base["components"].extend(
                self._serialize_component(c) for c in custom_components
            )

        # Add attention checks if enabled
        if config.attention_checks_enabled:
            base["attention_checks"] = self._generate_attention_checks(task_type)

        return base

    def _get_base_interface(
        self,
        task_type: EvaluationTaskType,
        config: InterfaceConfig
    ) -> Dict:
        """Get base interface for task type."""
        interfaces = {
            EvaluationTaskType.PAIRWISE_PREFERENCE: {
                "layout": "side_by_side",
                "components": [
                    {
                        "type": "instruction_panel",
                        "content": self._get_pairwise_instructions()
                    },
                    {
                        "type": "context_display",
                        "label": "User Question",
                        "field": "question"
                    },
                    {
                        "type": "response_panel",
                        "position": "left",
                        "label": "Response A",
                        "field": "response_a"
                    },
                    {
                        "type": "response_panel",
                        "position": "right",
                        "label": "Response B",
                        "field": "response_b"
                    },
                    {
                        "type": "radio_group",
                        "name": "preference",
                        "required": True,
                        "options": [
                            {"value": "A", "label": "Response A is better"},
                            {"value": "B", "label": "Response B is better"},
                            {"value": "tie", "label": "About the same (tie)"}
                        ]
                    }
                ]
            },
            EvaluationTaskType.ABSOLUTE_RATING: {
                "layout": "single",
                "components": [
                    {
                        "type": "instruction_panel",
                        "content": self._get_rating_instructions()
                    },
                    {
                        "type": "context_display",
                        "label": "User Question",
                        "field": "question"
                    },
                    {
                        "type": "response_panel",
                        "label": "AI Response",
                        "field": "response"
                    },
                    {
                        "type": "likert_scale",
                        "name": "rating",
                        "required": True,
                        "min": 1,
                        "max": 5,
                        "labels": {
                            1: "Poor",
                            2: "Below Average",
                            3: "Average",
                            4: "Good",
                            5: "Excellent"
                        }
                    }
                ]
            },
            EvaluationTaskType.MULTI_DIMENSIONAL: {
                "layout": "single",
                "components": [
                    {
                        "type": "instruction_panel",
                        "content": self._get_multidim_instructions()
                    },
                    {
                        "type": "context_display",
                        "label": "User Question",
                        "field": "question"
                    },
                    {
                        "type": "response_panel",
                        "label": "AI Response",
                        "field": "response"
                    },
                    {
                        "type": "dimension_ratings",
                        "dimensions": [
                            {"name": "helpfulness", "label": "Helpfulness"},
                            {"name": "accuracy", "label": "Accuracy"},
                            {"name": "clarity", "label": "Clarity"},
                            {"name": "completeness", "label": "Completeness"}
                        ],
                        "scale": {"min": 1, "max": 5}
                    }
                ]
            },
            EvaluationTaskType.ERROR_ANNOTATION: {
                "layout": "single",
                "components": [
                    {
                        "type": "instruction_panel",
                        "content": self._get_error_instructions()
                    },
                    {
                        "type": "context_display",
                        "label": "User Question",
                        "field": "question"
                    },
                    {
                        "type": "annotatable_text",
                        "label": "AI Response (select text to annotate errors)",
                        "field": "response",
                        "annotation_types": [
                            "Factual Error",
                            "Logical Error",
                            "Incompleteness",
                            "Irrelevance",
                            "Harmful Content"
                        ]
                    },
                    {
                        "type": "checkbox",
                        "name": "no_errors",
                        "label": "No errors found in this response"
                    }
                ]
            }
        }

        base = interfaces.get(task_type, interfaces[EvaluationTaskType.ABSOLUTE_RATING])

        # Add justification if required
        if config.require_justification:
            base["components"].append({
                "type": "textarea",
                "name": "justification",
                "label": "Please explain your rating",
                "required": True,
                "min_length": 50
            })

        # Add progress indicator
        if config.show_progress:
            base["components"].insert(0, {
                "type": "progress_bar",
                "show_count": True
            })

        return base

    def _get_pairwise_instructions(self) -> str:
        """Get instructions for pairwise comparison."""
        return """
        ## Instructions

        Compare the two AI responses to the user's question below.

        Consider the following criteria:
        - **Helpfulness**: Does the response address what the user asked?
        - **Accuracy**: Is the information provided correct?
        - **Clarity**: Is the response easy to understand?

        Select the response that you believe is better overall. If both responses
        are roughly equal in quality, you may select "About the same (tie)".

        **Important**: Focus on the content and quality, not the length.
        """

    def _get_rating_instructions(self) -> str:
        """Get instructions for absolute rating."""
        return """
        ## Instructions

        Rate the quality of the AI response on a scale of 1-5.

        Rating Guide:
        - **1 (Poor)**: Major issues, unhelpful, or incorrect
        - **2 (Below Average)**: Significant problems but some value
        - **3 (Average)**: Acceptable but nothing special
        - **4 (Good)**: Helpful and mostly correct
        - **5 (Excellent)**: Outstanding response
        """

    def _get_multidim_instructions(self) -> str:
        """Get instructions for multi-dimensional rating."""
        return """
        ## Instructions

        Rate the AI response on each dimension separately (1-5 scale).

        **Helpfulness**: Does it help answer the user's question?
        **Accuracy**: Is the information factually correct?
        **Clarity**: Is it well-written and easy to understand?
        **Completeness**: Does it fully address the question?
        """

    def _get_error_instructions(self) -> str:
        """Get instructions for error annotation."""
        return """
        ## Instructions

        Review the AI response and identify any errors.

        To annotate an error:
        1. Select the problematic text
        2. Choose the error type
        3. Optionally add a note explaining the issue

        If the response has no errors, check "No errors found".
        """

    def _serialize_component(self, component: UIComponent) -> Dict:
        """Convert UIComponent to dict."""
        return {
            "type": component.component_type,
            "label": component.label,
            "required": component.required,
            "options": component.options,
            "validation": component.validation,
            "help_text": component.help_text
        }

    def _generate_attention_checks(
        self,
        task_type: EvaluationTaskType
    ) -> List[Dict]:
        """Generate attention check items."""
        checks = {
            EvaluationTaskType.PAIRWISE_PREFERENCE: [
                {
                    "type": "obvious_winner",
                    "question": "What is 1+1?",
                    "response_a": "1+1 equals 2.",
                    "response_b": "I don't know math.",
                    "expected": "A"
                }
            ],
            EvaluationTaskType.ABSOLUTE_RATING: [
                {
                    "type": "instruction_check",
                    "question": "Please select rating 3 for this attention check.",
                    "response": "This is an attention check. Please select rating 3.",
                    "expected": 3
                }
            ]
        }
        return checks.get(task_type, [])
```

---

## 5.4.4 Quality Control

### Quality Metrics and Monitoring

```python
"""
ABOUTME: Quality control mechanisms for human evaluation.
ABOUTME: Implements agreement metrics, gold standards, and outlier detection.
"""

from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple
import numpy as np
from collections import Counter


@dataclass
class QualityMetrics:
    """Quality metrics for annotation task."""
    inter_annotator_agreement: float
    gold_accuracy: float
    time_per_item_avg: float
    time_per_item_std: float
    attention_check_pass_rate: float
    annotator_count: int


class QualityController:
    """
    Monitors and controls annotation quality.

    Implements multiple quality assurance mechanisms including
    inter-annotator agreement, gold standards, and time analysis.
    """

    def __init__(
        self,
        min_agreement: float = 0.7,
        min_gold_accuracy: float = 0.8,
        min_time_per_item: float = 5.0,  # seconds
        max_time_per_item: float = 300.0
    ):
        self.min_agreement = min_agreement
        self.min_gold_accuracy = min_gold_accuracy
        self.min_time = min_time_per_item
        self.max_time = max_time_per_item

    def compute_inter_annotator_agreement(
        self,
        annotations: Dict[str, List],
        metric: str = "fleiss_kappa"
    ) -> float:
        """
        Compute inter-annotator agreement.

        Args:
            annotations: item_id -> list of annotations from different annotators
            metric: Agreement metric (fleiss_kappa, krippendorff_alpha, percent_agreement)
        """
        if metric == "fleiss_kappa":
            return self._fleiss_kappa(annotations)
        elif metric == "krippendorff_alpha":
            return self._krippendorff_alpha(annotations)
        elif metric == "percent_agreement":
            return self._percent_agreement(annotations)
        else:
            raise ValueError(f"Unknown metric: {metric}")

    def _fleiss_kappa(self, annotations: Dict[str, List]) -> float:
        """Compute Fleiss' kappa for multiple raters."""
        # Convert to matrix format
        # Rows = items, columns = categories, values = count of annotators choosing that category
        all_values = set()
        for item_annotations in annotations.values():
            all_values.update(item_annotations)

        categories = sorted(all_values)
        n_items = len(annotations)
        n_raters = len(next(iter(annotations.values())))

        if n_items == 0 or n_raters < 2:
            return 0.0

        # Build count matrix
        matrix = np.zeros((n_items, len(categories)))
        for i, (item_id, item_annotations) in enumerate(annotations.items()):
            for annotation in item_annotations:
                cat_idx = categories.index(annotation)
                matrix[i, cat_idx] += 1

        # Compute Fleiss' kappa
        # P_i for each item
        P_i = (np.sum(matrix ** 2, axis=1) - n_raters) / (n_raters * (n_raters - 1))
        P_bar = np.mean(P_i)

        # P_j for each category
        P_j = np.sum(matrix, axis=0) / (n_items * n_raters)
        P_e = np.sum(P_j ** 2)

        if P_e == 1:
            return 1.0

        kappa = (P_bar - P_e) / (1 - P_e)
        return kappa

    def _krippendorff_alpha(self, annotations: Dict[str, List]) -> float:
        """Compute Krippendorff's alpha."""
        # Simplified implementation for nominal data
        # For full implementation, use krippendorff library
        # This is a proxy using percent agreement adjusted for chance

        agreement = self._percent_agreement(annotations)

        # Estimate chance agreement
        all_annotations = []
        for item_annotations in annotations.values():
            all_annotations.extend(item_annotations)

        counts = Counter(all_annotations)
        total = len(all_annotations)
        chance_agreement = sum((c / total) ** 2 for c in counts.values())

        if chance_agreement == 1:
            return 1.0

        alpha = (agreement - chance_agreement) / (1 - chance_agreement)
        return alpha

    def _percent_agreement(self, annotations: Dict[str, List]) -> float:
        """Compute simple percent agreement."""
        agreements = 0
        comparisons = 0

        for item_annotations in annotations.values():
            n = len(item_annotations)
            for i in range(n):
                for j in range(i + 1, n):
                    if item_annotations[i] == item_annotations[j]:
                        agreements += 1
                    comparisons += 1

        return agreements / comparisons if comparisons > 0 else 0

    def evaluate_annotator(
        self,
        annotator_id: str,
        annotations: List[Dict],
        gold_standards: Dict[str, any],
        time_data: List[float]
    ) -> Dict:
        """
        Evaluate individual annotator quality.

        Returns quality assessment with pass/fail.
        """
        # Gold standard accuracy
        gold_correct = 0
        gold_total = 0
        for annotation in annotations:
            item_id = annotation.get("item_id")
            if item_id in gold_standards:
                gold_total += 1
                if annotation.get("value") == gold_standards[item_id]:
                    gold_correct += 1

        gold_accuracy = gold_correct / gold_total if gold_total > 0 else 0

        # Time analysis
        avg_time = np.mean(time_data) if time_data else 0
        std_time = np.std(time_data) if time_data else 0

        # Detect suspicious patterns
        flags = []

        if gold_accuracy < self.min_gold_accuracy:
            flags.append(f"Gold accuracy too low: {gold_accuracy:.2f}")

        if avg_time < self.min_time:
            flags.append(f"Average time too fast: {avg_time:.1f}s")

        if avg_time > self.max_time:
            flags.append(f"Average time too slow: {avg_time:.1f}s")

        # Check for response patterns (always same answer)
        values = [a.get("value") for a in annotations]
        if len(set(values)) == 1 and len(values) > 5:
            flags.append("All responses identical")

        return {
            "annotator_id": annotator_id,
            "gold_accuracy": gold_accuracy,
            "avg_time": avg_time,
            "std_time": std_time,
            "num_annotations": len(annotations),
            "flags": flags,
            "passed": len(flags) == 0 and gold_accuracy >= self.min_gold_accuracy
        }

    def detect_outliers(
        self,
        all_annotations: Dict[str, Dict[str, any]]
    ) -> List[Dict]:
        """
        Detect outlier annotations that deviate from consensus.

        Args:
            all_annotations: item_id -> annotator_id -> annotation

        Returns:
            List of outlier annotations with details
        """
        outliers = []

        for item_id, item_annotations in all_annotations.items():
            # Get majority vote
            values = list(item_annotations.values())
            value_counts = Counter(values)
            majority = value_counts.most_common(1)[0]
            majority_value, majority_count = majority

            # Annotators who disagreed with majority
            total = len(values)
            agreement_rate = majority_count / total

            # If there's strong consensus, flag outliers
            if agreement_rate >= 0.7:
                for annotator_id, annotation in item_annotations.items():
                    if annotation != majority_value:
                        outliers.append({
                            "item_id": item_id,
                            "annotator_id": annotator_id,
                            "annotation": annotation,
                            "majority": majority_value,
                            "agreement_rate": agreement_rate
                        })

        return outliers


class DisagreementResolver:
    """
    Resolves annotation disagreements through adjudication.

    Implements strategies for handling cases where annotators disagree.
    """

    def __init__(self, resolution_strategy: str = "majority"):
        self.strategy = resolution_strategy

    def resolve(
        self,
        annotations: List[any],
        annotator_weights: Optional[Dict[str, float]] = None
    ) -> Tuple[any, float]:
        """
        Resolve disagreement among annotations.

        Args:
            annotations: List of annotations for same item
            annotator_weights: Optional weights by annotator quality

        Returns:
            (resolved_value, confidence)
        """
        if self.strategy == "majority":
            return self._majority_vote(annotations)
        elif self.strategy == "weighted":
            return self._weighted_vote(annotations, annotator_weights)
        elif self.strategy == "adjudication":
            return self._flag_for_adjudication(annotations)
        else:
            return self._majority_vote(annotations)

    def _majority_vote(self, annotations: List) -> Tuple[any, float]:
        """Simple majority vote resolution."""
        counts = Counter(annotations)
        winner, win_count = counts.most_common(1)[0]
        confidence = win_count / len(annotations)
        return winner, confidence

    def _weighted_vote(
        self,
        annotations: List,
        weights: Optional[Dict[str, float]]
    ) -> Tuple[any, float]:
        """Weighted voting based on annotator quality."""
        if not weights:
            return self._majority_vote(annotations)

        # Sum weights for each value
        value_weights = {}
        total_weight = 0

        for annotation, weight in zip(annotations, weights.values()):
            value_weights[annotation] = value_weights.get(annotation, 0) + weight
            total_weight += weight

        winner = max(value_weights, key=value_weights.get)
        confidence = value_weights[winner] / total_weight
        return winner, confidence

    def _flag_for_adjudication(self, annotations: List) -> Tuple[any, float]:
        """Flag for expert adjudication when disagreement is high."""
        counts = Counter(annotations)
        winner, win_count = counts.most_common(1)[0]
        agreement = win_count / len(annotations)

        if agreement < 0.6:
            # No clear winner - needs adjudication
            return None, 0.0
        return winner, agreement
```

---

## 5.4.5 Statistical Analysis

### Analysis Framework

```python
"""
ABOUTME: Statistical analysis methods for human evaluation data.
ABOUTME: Implements sample size calculation, significance testing, and CI estimation.
"""

from typing import List, Dict, Tuple, Optional
import numpy as np
from scipy import stats
from dataclasses import dataclass


@dataclass
class EvaluationResults:
    """Results from human evaluation."""
    win_rates: Dict[str, float]
    confidence_intervals: Dict[str, Tuple[float, float]]
    sample_size: int
    agreement_metrics: Dict[str, float]
    significance_tests: Dict[str, Dict]


class StatisticalAnalyzer:
    """
    Statistical analysis for human evaluation results.

    Provides sample size planning, significance testing,
    and confidence interval estimation.
    """

    def __init__(self, confidence_level: float = 0.95):
        self.confidence = confidence_level
        self.alpha = 1 - confidence_level

    def calculate_sample_size(
        self,
        expected_effect_size: float,
        power: float = 0.8,
        test_type: str = "pairwise"
    ) -> int:
        """
        Calculate required sample size for desired power.

        Args:
            expected_effect_size: Expected difference to detect
            power: Statistical power (usually 0.8)
            test_type: "pairwise" or "absolute"

        Returns:
            Required sample size per condition
        """
        if test_type == "pairwise":
            # For binomial proportion (win rate)
            # Using normal approximation
            p0 = 0.5  # Null hypothesis: equal preference
            p1 = 0.5 + expected_effect_size / 2

            z_alpha = stats.norm.ppf(1 - self.alpha / 2)
            z_beta = stats.norm.ppf(power)

            # Sample size formula for proportion
            n = ((z_alpha * np.sqrt(2 * p0 * (1 - p0)) +
                  z_beta * np.sqrt(p1 * (1 - p1) + p0 * (1 - p0))) /
                 (p1 - p0)) ** 2

            return int(np.ceil(n))

        elif test_type == "absolute":
            # For comparing means
            # Assuming standardized effect size (Cohen's d)
            z_alpha = stats.norm.ppf(1 - self.alpha / 2)
            z_beta = stats.norm.ppf(power)

            n = 2 * ((z_alpha + z_beta) / expected_effect_size) ** 2
            return int(np.ceil(n))

        return 100  # Default

    def analyze_pairwise(
        self,
        comparisons: List[Dict],
        model_a: str,
        model_b: str
    ) -> Dict:
        """
        Analyze pairwise comparison results.

        Args:
            comparisons: List of comparison results
            model_a: Name of first model
            model_b: Name of second model
        """
        # Count wins
        a_wins = sum(1 for c in comparisons if c.get("winner") == "A")
        b_wins = sum(1 for c in comparisons if c.get("winner") == "B")
        ties = sum(1 for c in comparisons if c.get("winner") == "tie")
        total = len(comparisons)

        # Win rates
        a_rate = a_wins / total
        b_rate = b_wins / total
        tie_rate = ties / total

        # Confidence intervals (Wilson score)
        a_ci = self._wilson_ci(a_wins, total)
        b_ci = self._wilson_ci(b_wins, total)

        # Significance test (binomial test against 0.5)
        # Excluding ties for head-to-head comparison
        decisive = a_wins + b_wins
        if decisive > 0:
            p_value = stats.binomtest(a_wins, decisive, 0.5).pvalue
        else:
            p_value = 1.0

        return {
            "model_a": model_a,
            "model_b": model_b,
            "total_comparisons": total,
            "results": {
                "a_wins": a_wins,
                "b_wins": b_wins,
                "ties": ties
            },
            "win_rates": {
                model_a: a_rate,
                model_b: b_rate,
                "tie": tie_rate
            },
            "confidence_intervals": {
                model_a: a_ci,
                model_b: b_ci
            },
            "significance": {
                "p_value": p_value,
                "significant": p_value < self.alpha,
                "winner": model_a if a_wins > b_wins else (model_b if b_wins > a_wins else "tie")
            }
        }

    def analyze_ratings(
        self,
        ratings: Dict[str, List[float]]
    ) -> Dict:
        """
        Analyze absolute rating results.

        Args:
            ratings: model_name -> list of ratings
        """
        results = {}

        for model, model_ratings in ratings.items():
            mean = np.mean(model_ratings)
            std = np.std(model_ratings)
            sem = std / np.sqrt(len(model_ratings))

            # Confidence interval
            ci = stats.t.interval(
                self.confidence,
                len(model_ratings) - 1,
                loc=mean,
                scale=sem
            )

            results[model] = {
                "mean": mean,
                "std": std,
                "sem": sem,
                "confidence_interval": ci,
                "n": len(model_ratings),
                "median": np.median(model_ratings),
                "min": min(model_ratings),
                "max": max(model_ratings)
            }

        # Pairwise significance tests
        model_names = list(ratings.keys())
        comparisons = {}

        for i, model_a in enumerate(model_names):
            for model_b in model_names[i+1:]:
                stat, p_value = stats.ttest_ind(
                    ratings[model_a],
                    ratings[model_b],
                    equal_var=False  # Welch's t-test
                )
                comparisons[f"{model_a}_vs_{model_b}"] = {
                    "t_statistic": stat,
                    "p_value": p_value,
                    "significant": p_value < self.alpha,
                    "effect_size": self._cohens_d(ratings[model_a], ratings[model_b])
                }

        return {
            "per_model": results,
            "pairwise_comparisons": comparisons
        }

    def _wilson_ci(
        self,
        successes: int,
        total: int
    ) -> Tuple[float, float]:
        """Wilson score confidence interval for proportions."""
        if total == 0:
            return (0.0, 0.0)

        p = successes / total
        z = stats.norm.ppf(1 - self.alpha / 2)

        denominator = 1 + z**2 / total
        center = (p + z**2 / (2 * total)) / denominator
        spread = z * np.sqrt(p * (1 - p) / total + z**2 / (4 * total**2)) / denominator

        return (max(0, center - spread), min(1, center + spread))

    def _cohens_d(self, group1: List[float], group2: List[float]) -> float:
        """Compute Cohen's d effect size."""
        n1, n2 = len(group1), len(group2)
        var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)

        # Pooled standard deviation
        pooled_std = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))

        if pooled_std == 0:
            return 0.0

        return (np.mean(group1) - np.mean(group2)) / pooled_std

    def bootstrap_analysis(
        self,
        data: List[float],
        statistic: str = "mean",
        n_bootstrap: int = 10000
    ) -> Dict:
        """
        Bootstrap analysis for robust statistics.

        Args:
            data: Data points
            statistic: Statistic to compute (mean, median, etc.)
            n_bootstrap: Number of bootstrap samples
        """
        data = np.array(data)
        n = len(data)

        stat_func = {
            "mean": np.mean,
            "median": np.median,
            "std": np.std
        }.get(statistic, np.mean)

        # Generate bootstrap samples
        bootstrap_stats = []
        for _ in range(n_bootstrap):
            sample = np.random.choice(data, size=n, replace=True)
            bootstrap_stats.append(stat_func(sample))

        bootstrap_stats = np.array(bootstrap_stats)

        # Confidence interval
        ci_lower = np.percentile(bootstrap_stats, (1 - self.confidence) / 2 * 100)
        ci_upper = np.percentile(bootstrap_stats, (1 + self.confidence) / 2 * 100)

        return {
            "point_estimate": stat_func(data),
            "bootstrap_mean": np.mean(bootstrap_stats),
            "bootstrap_std": np.std(bootstrap_stats),
            "confidence_interval": (ci_lower, ci_upper),
            "n_bootstrap": n_bootstrap
        }
```

---

## 5.4.6 Ethical Considerations

### Ethical Framework

```python
"""
ABOUTME: Ethical framework for human evaluation studies.
ABOUTME: Implements compensation, consent, and safety guidelines.
"""

from dataclasses import dataclass
from typing import List, Dict, Optional
from datetime import timedelta


@dataclass
class CompensationGuidelines:
    """Guidelines for fair annotator compensation."""
    min_hourly_rate: float = 15.0  # USD
    recommended_hourly_rate: float = 20.0
    bonus_rate: float = 0.10  # 10% quality bonus
    payment_schedule: str = "weekly"


@dataclass
class ConsentRequirements:
    """Requirements for informed consent."""
    purpose_disclosure: bool = True
    data_usage_disclosure: bool = True
    right_to_withdraw: bool = True
    anonymization_policy: bool = True
    contact_information: bool = True
    irb_approval_number: Optional[str] = None


@dataclass
class ContentSafetyGuidelines:
    """Guidelines for handling sensitive content."""
    content_warnings_required: bool = True
    opt_out_allowed: bool = True
    max_sensitive_content_ratio: float = 0.2
    mandatory_breaks: bool = True
    break_frequency_minutes: int = 60
    counseling_resources_provided: bool = True


class EthicsChecker:
    """
    Ensures human evaluation studies meet ethical standards.

    Validates compensation, consent, and content safety.
    """

    def __init__(
        self,
        compensation: CompensationGuidelines = None,
        consent: ConsentRequirements = None,
        safety: ContentSafetyGuidelines = None
    ):
        self.compensation = compensation or CompensationGuidelines()
        self.consent = consent or ConsentRequirements()
        self.safety = safety or ContentSafetyGuidelines()

    def validate_study_design(
        self,
        study_config: Dict
    ) -> Dict:
        """
        Validate study design against ethical guidelines.

        Returns validation results with issues and recommendations.
        """
        issues = []
        warnings = []
        recommendations = []

        # Check compensation
        comp_issues = self._check_compensation(study_config)
        issues.extend(comp_issues.get("issues", []))
        warnings.extend(comp_issues.get("warnings", []))

        # Check consent
        consent_issues = self._check_consent(study_config)
        issues.extend(consent_issues.get("issues", []))
        warnings.extend(consent_issues.get("warnings", []))

        # Check content safety
        safety_issues = self._check_content_safety(study_config)
        issues.extend(safety_issues.get("issues", []))
        warnings.extend(safety_issues.get("warnings", []))

        return {
            "approved": len(issues) == 0,
            "issues": issues,
            "warnings": warnings,
            "recommendations": recommendations
        }

    def _check_compensation(self, config: Dict) -> Dict:
        """Check compensation meets guidelines."""
        issues = []
        warnings = []

        estimated_time = config.get("estimated_time_per_task", 60)  # seconds
        payment_per_task = config.get("payment_per_task", 0)

        hourly_rate = (payment_per_task / estimated_time) * 3600

        if hourly_rate < self.compensation.min_hourly_rate:
            issues.append(
                f"Effective hourly rate (${hourly_rate:.2f}) is below minimum "
                f"(${self.compensation.min_hourly_rate:.2f})"
            )
        elif hourly_rate < self.compensation.recommended_hourly_rate:
            warnings.append(
                f"Hourly rate (${hourly_rate:.2f}) is below recommended "
                f"(${self.compensation.recommended_hourly_rate:.2f})"
            )

        return {"issues": issues, "warnings": warnings}

    def _check_consent(self, config: Dict) -> Dict:
        """Check consent requirements."""
        issues = []
        warnings = []

        consent_form = config.get("consent_form", {})

        if self.consent.purpose_disclosure and not consent_form.get("purpose"):
            issues.append("Consent form must disclose study purpose")

        if self.consent.data_usage_disclosure and not consent_form.get("data_usage"):
            issues.append("Consent form must explain how data will be used")

        if self.consent.right_to_withdraw and not consent_form.get("withdrawal_info"):
            issues.append("Consent form must explain right to withdraw")

        if self.consent.irb_approval_number and not config.get("irb_approval"):
            warnings.append("Consider obtaining IRB approval for human subjects research")

        return {"issues": issues, "warnings": warnings}

    def _check_content_safety(self, config: Dict) -> Dict:
        """Check content safety measures."""
        issues = []
        warnings = []

        contains_sensitive = config.get("contains_sensitive_content", False)

        if contains_sensitive:
            if self.safety.content_warnings_required and not config.get("content_warnings"):
                issues.append("Content warnings required for sensitive material")

            if self.safety.opt_out_allowed and not config.get("opt_out_mechanism"):
                issues.append("Must provide opt-out mechanism for sensitive content")

            if self.safety.counseling_resources_provided and not config.get("support_resources"):
                warnings.append("Consider providing mental health support resources")

        return {"issues": issues, "warnings": warnings}

    def generate_consent_form(
        self,
        study_info: Dict
    ) -> str:
        """Generate consent form template."""
        template = """
# Informed Consent for Research Participation

## Study Title
{title}

## Purpose
{purpose}

## What You Will Do
{procedure}

## Time Required
Approximately {duration}

## Compensation
{compensation}

## Risks and Benefits
{risks_benefits}

## Data Usage
{data_usage}

## Confidentiality
Your responses will be kept confidential and stored securely.
{anonymization}

## Voluntary Participation
Your participation is voluntary. You may stop at any time without penalty.
You may skip any questions you do not wish to answer.

## Contact Information
If you have questions about this study, contact:
{contact}

## Consent
By proceeding, you confirm that:
- You have read and understood this consent form
- You are at least 18 years old
- You agree to participate voluntarily

{irb_statement}
"""
        return template.format(
            title=study_info.get("title", "[Study Title]"),
            purpose=study_info.get("purpose", "[Study Purpose]"),
            procedure=study_info.get("procedure", "[What participants will do]"),
            duration=study_info.get("duration", "[Estimated time]"),
            compensation=study_info.get("compensation", "[Payment details]"),
            risks_benefits=study_info.get("risks_benefits", "[Risks and benefits]"),
            data_usage=study_info.get("data_usage", "[How data will be used]"),
            anonymization=study_info.get("anonymization", "Responses will be anonymized."),
            contact=study_info.get("contact", "[Researcher contact info]"),
            irb_statement=study_info.get("irb_statement", "")
        )

    def calculate_fair_payment(
        self,
        task_time_seconds: float,
        task_complexity: str = "medium"
    ) -> Dict:
        """Calculate fair payment for task."""
        complexity_multipliers = {
            "simple": 1.0,
            "medium": 1.2,
            "complex": 1.5,
            "expert": 2.0
        }

        multiplier = complexity_multipliers.get(task_complexity, 1.0)
        base_rate = self.compensation.recommended_hourly_rate

        hourly_with_complexity = base_rate * multiplier
        payment = (task_time_seconds / 3600) * hourly_with_complexity

        return {
            "recommended_payment": round(payment, 2),
            "effective_hourly_rate": hourly_with_complexity,
            "min_payment": round((task_time_seconds / 3600) * self.compensation.min_hourly_rate, 2),
            "with_bonus": round(payment * (1 + self.compensation.bonus_rate), 2)
        }
```

---

## Appendix A: Evaluation Interface Templates

### Pairwise Comparison Template (HTML)

```html
<div class="evaluation-container">
  <div class="instructions">
    <h2>Instructions</h2>
    <p>Compare the two AI responses and select the one that better answers the question.</p>
  </div>

  <div class="context">
    <h3>User Question</h3>
    <p class="question">{{question}}</p>
  </div>

  <div class="responses-container">
    <div class="response-panel" id="response-a">
      <h4>Response A</h4>
      <div class="response-content">{{response_a}}</div>
    </div>
    <div class="response-panel" id="response-b">
      <h4>Response B</h4>
      <div class="response-content">{{response_b}}</div>
    </div>
  </div>

  <div class="selection">
    <label>
      <input type="radio" name="preference" value="A">
      Response A is better
    </label>
    <label>
      <input type="radio" name="preference" value="B">
      Response B is better
    </label>
    <label>
      <input type="radio" name="preference" value="tie">
      About the same (tie)
    </label>
  </div>

  <div class="justification">
    <label for="reasoning">Please explain your choice (optional):</label>
    <textarea id="reasoning" name="reasoning"></textarea>
  </div>

  <button type="submit" class="submit-btn">Submit</button>
</div>
```

---

## Appendix B: Annotator Training Materials

### Training Checklist

1. **Understanding the Task**
   - [ ] Read task instructions completely
   - [ ] Review evaluation criteria definitions
   - [ ] Study example annotations

2. **Calibration**
   - [ ] Complete calibration examples
   - [ ] Review feedback on calibration
   - [ ] Discuss disagreements with trainer

3. **Practice**
   - [ ] Complete practice batch
   - [ ] Achieve >80% agreement with gold standards
   - [ ] Pass qualification test

4. **Quality Standards**
   - [ ] Understand time expectations
   - [ ] Know how to flag unclear items
   - [ ] Understand quality monitoring process

---

## Appendix C: Agreement Metric Reference

| Metric | Use Case | Interpretation |
|--------|----------|----------------|
| **Percent Agreement** | Simple check | >80% good, >90% excellent |
| **Cohen's Kappa** | 2 annotators, nominal data | >0.6 substantial, >0.8 perfect |
| **Fleiss' Kappa** | 3+ annotators, nominal | >0.6 substantial, >0.8 perfect |
| **Krippendorff's Alpha** | Any data type, missing values OK | >0.8 reliable, >0.67 tentative |
| **ICC** | Continuous ratings | >0.75 good, >0.9 excellent |

---

## Troubleshooting

**Issue: Low inter-annotator agreement**
```
Diagnosis:
1. Check if guidelines are clear
2. Analyze patterns of disagreement
3. Review annotator qualifications

Solutions:
- Add calibration examples
- Revise unclear guidelines
- Provide additional training
- Add gold standard checkpoints
```

**Issue: Annotators rushing through tasks**
```
Diagnosis:
1. Check time-per-task distribution
2. Look for pattern responses
3. Review gold standard accuracy

Solutions:
- Add attention checks
- Implement minimum time requirements
- Increase payment for quality
- Remove low-quality annotators
```

**Issue: Annotation fatigue/quality decline**
```
Diagnosis:
1. Track quality over session duration
2. Check for time-of-day effects
3. Monitor break compliance

Solutions:
- Enforce mandatory breaks
- Limit session length
- Vary task difficulty
- Provide feedback/motivation
```

---

## Glossary

| Term | Definition |
|------|------------|
| **IAA** | Inter-Annotator Agreement - consistency between annotators |
| **Gold Standard** | Pre-verified correct answer for quality control |
| **Calibration** | Training phase to align annotators |
| **Attention Check** | Hidden test to verify engagement |
| **Adjudication** | Expert resolution of disagreements |
| **Cohen's Kappa** | Agreement metric accounting for chance |
| **Likert Scale** | Ordered rating scale (e.g., 1-5) |
| **IRB** | Institutional Review Board for research ethics |

---

## References

1. Artstein, R., & Poesio, M. (2008). "Inter-Coder Agreement for Computational Linguistics."
2. Krippendorff, K. (2004). "Content Analysis: An Introduction to Its Methodology."
3. Snow, R., et al. (2008). "Cheap and Fast - But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks."
4. Daniel, F., et al. (2018). "Quality Control in Crowdsourcing: A Survey of Quality Attributes, Assessment Techniques and Assurance Actions."
5. Chiang, C.H., & Lee, H.Y. (2023). "Can Large Language Models Be an Alternative to Human Evaluations?"
6. Clark, E., et al. (2021). "All That's 'Human' Is Not Gold: Evaluating Human Evaluation of Generated Text."
