> **Navigation** | [← 5.1 Evaluation Framework](5.1_llm_evaluation_framework.md) | [5.3 LLM-as-Judge →](5.3_llm_as_judge_evaluation.md)
>
> | | |
> |---|---|
> | **Prerequisites** | [5.1 Evaluation Framework](5.1_llm_evaluation_framework.md) |
> | **Related** | [5.3 LLM-as-Judge](5.3_llm_as_judge_evaluation.md) &#124; [1.6 Data Quality](../01_data_pipeline/1.6_data_quality_assurance.md) |
> | **Next** | [5.3 LLM-as-Judge Evaluation](5.3_llm_as_judge_evaluation.md) |

# Document 5.2: Benchmark Selection & Interpretation Guide

## Executive Summary

This guide provides comprehensive guidance on selecting appropriate benchmarks for LLM evaluation and correctly interpreting results. It covers major benchmark categories (knowledge, reasoning, math, coding, long context, instruction following, multilingual), addresses critical contamination concerns, and establishes interpretation guidelines that account for statistical uncertainty, benchmark limitations, and gaming risks. The goal is to enable informed benchmark selection that yields meaningful, actionable insights about model capabilities.

## Prerequisites

- Understanding of LLM evaluation fundamentals (see Document 5.1)
- Familiarity with basic statistics (confidence intervals, significance testing)
- Knowledge of model training pipelines (for contamination analysis)
- Access to evaluation infrastructure

---

## 5.2.1 Benchmark Categories

### Knowledge & Reasoning Benchmarks

```python
"""
ABOUTME: Catalog of knowledge and reasoning benchmarks with metadata.
ABOUTME: Provides structured information for benchmark selection decisions.
"""

from dataclasses import dataclass, field
from typing import List, Dict, Optional, Set
from enum import Enum


class BenchmarkCategory(Enum):
    """High-level benchmark categories."""
    KNOWLEDGE = "knowledge"
    REASONING = "reasoning"
    MATHEMATICS = "mathematics"
    CODING = "coding"
    LONG_CONTEXT = "long_context"
    INSTRUCTION_FOLLOWING = "instruction_following"
    MULTILINGUAL = "multilingual"
    SAFETY = "safety"


class DifficultyLevel(Enum):
    """Benchmark difficulty levels."""
    EASY = "easy"           # Saturated by frontier models
    MEDIUM = "medium"       # Challenging but achievable
    HARD = "hard"           # Frontier models score < 70%
    EXPERT = "expert"       # Requires specialist knowledge


@dataclass
class BenchmarkMetadata:
    """Comprehensive metadata for a benchmark."""
    name: str
    category: BenchmarkCategory
    description: str
    num_examples: int
    metric: str
    difficulty: DifficultyLevel
    contamination_risk: str  # low, medium, high, critical
    publication_year: int
    maintainer: str
    url: str
    subsets: List[str] = field(default_factory=list)
    requires_generation: bool = False
    human_baseline: Optional[float] = None
    random_baseline: Optional[float] = None
    saturation_threshold: float = 0.95  # Score at which benchmark is "solved"
    known_issues: List[str] = field(default_factory=list)


class BenchmarkCatalog:
    """
    Comprehensive catalog of LLM benchmarks.

    Provides structured access to benchmark metadata for
    informed selection decisions.
    """

    def __init__(self):
        self.benchmarks = self._initialize_catalog()

    def _initialize_catalog(self) -> Dict[str, BenchmarkMetadata]:
        """Initialize the benchmark catalog."""
        return {
            # ============ Knowledge & Reasoning ============
            "mmlu": BenchmarkMetadata(
                name="MMLU",
                category=BenchmarkCategory.KNOWLEDGE,
                description="Massive Multitask Language Understanding - 57 subjects",
                num_examples=14042,
                metric="accuracy",
                difficulty=DifficultyLevel.MEDIUM,
                contamination_risk="critical",
                publication_year=2021,
                maintainer="UC Berkeley",
                url="https://github.com/hendrycks/test",
                subsets=[
                    "stem", "humanities", "social_sciences",
                    "professional", "other"
                ],
                human_baseline=0.898,
                random_baseline=0.25,
                known_issues=[
                    "Severe contamination in many models",
                    "Some questions have errors",
                    "Format sensitivity"
                ]
            ),
            "mmlu_pro": BenchmarkMetadata(
                name="MMLU-Pro",
                category=BenchmarkCategory.KNOWLEDGE,
                description="Harder MMLU with 10 choices and reasoning focus",
                num_examples=12032,
                metric="accuracy",
                difficulty=DifficultyLevel.HARD,
                contamination_risk="low",
                publication_year=2024,
                maintainer="TIGER-Lab",
                url="https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro",
                human_baseline=None,
                random_baseline=0.10,
                known_issues=[]
            ),
            "arc_challenge": BenchmarkMetadata(
                name="ARC-Challenge",
                category=BenchmarkCategory.REASONING,
                description="AI2 Reasoning Challenge - grade school science",
                num_examples=1172,
                metric="accuracy",
                difficulty=DifficultyLevel.MEDIUM,
                contamination_risk="high",
                publication_year=2018,
                maintainer="AI2",
                url="https://allenai.org/data/arc",
                human_baseline=0.80,
                random_baseline=0.25,
                known_issues=["Small test set", "Some annotation errors"]
            ),
            "hellaswag": BenchmarkMetadata(
                name="HellaSwag",
                category=BenchmarkCategory.REASONING,
                description="Commonsense NLI about everyday situations",
                num_examples=10042,
                metric="accuracy",
                difficulty=DifficultyLevel.EASY,
                contamination_risk="high",
                publication_year=2019,
                maintainer="AI2",
                url="https://rowanzellers.com/hellaswag/",
                human_baseline=0.954,
                random_baseline=0.25,
                saturation_threshold=0.90,
                known_issues=["Near saturation", "Adversarial filtering artifacts"]
            ),
            "winogrande": BenchmarkMetadata(
                name="WinoGrande",
                category=BenchmarkCategory.REASONING,
                description="Commonsense pronoun resolution",
                num_examples=1767,
                metric="accuracy",
                difficulty=DifficultyLevel.MEDIUM,
                contamination_risk="high",
                publication_year=2020,
                maintainer="AI2",
                url="https://winogrande.allenai.org/",
                human_baseline=0.94,
                random_baseline=0.50,
                known_issues=["Small test set", "Binary choice limits discrimination"]
            ),
            "gpqa": BenchmarkMetadata(
                name="GPQA",
                category=BenchmarkCategory.KNOWLEDGE,
                description="Graduate-level science questions",
                num_examples=448,
                metric="accuracy",
                difficulty=DifficultyLevel.EXPERT,
                contamination_risk="low",
                publication_year=2023,
                maintainer="NYU",
                url="https://github.com/idavidrein/gpqa",
                subsets=["diamond", "main", "extended"],
                human_baseline=0.34,  # Non-expert humans
                random_baseline=0.25,
                known_issues=["Small dataset", "Domain-specific"]
            ),

            # ============ Mathematics ============
            "gsm8k": BenchmarkMetadata(
                name="GSM8K",
                category=BenchmarkCategory.MATHEMATICS,
                description="Grade school math word problems",
                num_examples=1319,
                metric="accuracy",
                difficulty=DifficultyLevel.MEDIUM,
                contamination_risk="critical",
                publication_year=2021,
                maintainer="OpenAI",
                url="https://github.com/openai/grade-school-math",
                requires_generation=True,
                human_baseline=0.95,
                random_baseline=0.0,
                known_issues=[
                    "Severe contamination - rephrased samples achieve 95%+",
                    "Chain-of-thought sensitivity",
                    "Calculator usage ambiguity"
                ]
            ),
            "math": BenchmarkMetadata(
                name="MATH",
                category=BenchmarkCategory.MATHEMATICS,
                description="Competition mathematics problems",
                num_examples=5000,
                metric="accuracy",
                difficulty=DifficultyLevel.HARD,
                contamination_risk="high",
                publication_year=2021,
                maintainer="UC Berkeley",
                url="https://github.com/hendrycks/math",
                subsets=[
                    "algebra", "counting", "geometry", "intermediate_algebra",
                    "number_theory", "prealgebra", "precalculus"
                ],
                requires_generation=True,
                human_baseline=0.40,  # AMC competitor average
                random_baseline=0.0,
                known_issues=["LaTeX parsing issues", "Answer format sensitivity"]
            ),
            "mathvista": BenchmarkMetadata(
                name="MathVista",
                category=BenchmarkCategory.MATHEMATICS,
                description="Visual math reasoning",
                num_examples=6141,
                metric="accuracy",
                difficulty=DifficultyLevel.HARD,
                contamination_risk="medium",
                publication_year=2023,
                maintainer="UCLA",
                url="https://mathvista.github.io/",
                requires_generation=True,
                known_issues=["Requires vision capabilities"]
            ),

            # ============ Coding ============
            "humaneval": BenchmarkMetadata(
                name="HumanEval",
                category=BenchmarkCategory.CODING,
                description="Python function completion",
                num_examples=164,
                metric="pass@k",
                difficulty=DifficultyLevel.MEDIUM,
                contamination_risk="critical",
                publication_year=2021,
                maintainer="OpenAI",
                url="https://github.com/openai/human-eval",
                requires_generation=True,
                known_issues=[
                    "8-18% overlap with training data",
                    "Small test set",
                    "Python-only"
                ]
            ),
            "mbpp": BenchmarkMetadata(
                name="MBPP",
                category=BenchmarkCategory.CODING,
                description="Mostly Basic Programming Problems",
                num_examples=500,
                metric="pass@k",
                difficulty=DifficultyLevel.EASY,
                contamination_risk="high",
                publication_year=2021,
                maintainer="Google",
                url="https://github.com/google-research/mbpp",
                requires_generation=True,
                known_issues=["Many problems are trivial", "Test coverage varies"]
            ),
            "livecodebench": BenchmarkMetadata(
                name="LiveCodeBench",
                category=BenchmarkCategory.CODING,
                description="Continuously updated coding problems",
                num_examples=500,  # Varies by time window
                metric="pass@k",
                difficulty=DifficultyLevel.HARD,
                contamination_risk="low",
                publication_year=2024,
                maintainer="LiveCodeBench Team",
                url="https://livecodebench.github.io/",
                requires_generation=True,
                known_issues=["Problems change over time"]
            ),
            "swe_bench": BenchmarkMetadata(
                name="SWE-bench",
                category=BenchmarkCategory.CODING,
                description="Real-world GitHub issue resolution",
                num_examples=2294,
                metric="resolve_rate",
                difficulty=DifficultyLevel.EXPERT,
                contamination_risk="medium",
                publication_year=2023,
                maintainer="Princeton",
                url="https://www.swebench.com/",
                subsets=["lite", "verified", "full"],
                requires_generation=True,
                known_issues=["Requires agentic capabilities", "High variance"]
            ),

            # ============ Long Context ============
            "ruler": BenchmarkMetadata(
                name="RULER",
                category=BenchmarkCategory.LONG_CONTEXT,
                description="Synthetic long-context retrieval tasks",
                num_examples=4000,  # Per length
                metric="accuracy",
                difficulty=DifficultyLevel.MEDIUM,
                contamination_risk="low",
                publication_year=2024,
                maintainer="NVIDIA",
                url="https://github.com/hsiehjackson/RULER",
                known_issues=["Synthetic tasks may not reflect real usage"]
            ),
            "needle_in_haystack": BenchmarkMetadata(
                name="Needle-in-a-Haystack",
                category=BenchmarkCategory.LONG_CONTEXT,
                description="Fact retrieval from long documents",
                num_examples=100,
                metric="accuracy",
                difficulty=DifficultyLevel.EASY,
                contamination_risk="low",
                publication_year=2023,
                maintainer="Greg Kamradt",
                url="https://github.com/gkamradt/LLMTest_NeedleInAHaystack",
                known_issues=["Too simple for frontier models", "Position-dependent"]
            ),
            "longbench": BenchmarkMetadata(
                name="LongBench",
                category=BenchmarkCategory.LONG_CONTEXT,
                description="Multi-task long context benchmark",
                num_examples=4750,
                metric="f1",
                difficulty=DifficultyLevel.HARD,
                contamination_risk="medium",
                publication_year=2023,
                maintainer="THUDM",
                url="https://github.com/THUDM/LongBench",
                known_issues=["Metric varies by task"]
            ),

            # ============ Instruction Following ============
            "ifeval": BenchmarkMetadata(
                name="IFEval",
                category=BenchmarkCategory.INSTRUCTION_FOLLOWING,
                description="Verifiable instruction following",
                num_examples=541,
                metric="accuracy",
                difficulty=DifficultyLevel.MEDIUM,
                contamination_risk="medium",
                publication_year=2023,
                maintainer="Google",
                url="https://github.com/google-research/google-research/tree/master/instruction_following_eval",
                known_issues=["Limited instruction types"]
            ),
            "mt_bench": BenchmarkMetadata(
                name="MT-Bench",
                category=BenchmarkCategory.INSTRUCTION_FOLLOWING,
                description="Multi-turn conversation evaluation",
                num_examples=80,
                metric="llm_judge_score",
                difficulty=DifficultyLevel.MEDIUM,
                contamination_risk="medium",
                publication_year=2023,
                maintainer="LMSYS",
                url="https://github.com/lm-sys/FastChat",
                requires_generation=True,
                known_issues=["Small set", "LLM judge variability"]
            ),
            "alpaca_eval": BenchmarkMetadata(
                name="AlpacaEval",
                category=BenchmarkCategory.INSTRUCTION_FOLLOWING,
                description="Single-turn instruction following",
                num_examples=805,
                metric="win_rate",
                difficulty=DifficultyLevel.MEDIUM,
                contamination_risk="medium",
                publication_year=2023,
                maintainer="Stanford",
                url="https://tatsu-lab.github.io/alpaca_eval/",
                requires_generation=True,
                known_issues=[
                    "Length bias in original version",
                    "Gaming via verbose outputs"
                ]
            ),

            # ============ Multilingual ============
            "mgsm": BenchmarkMetadata(
                name="MGSM",
                category=BenchmarkCategory.MULTILINGUAL,
                description="Multilingual GSM8K",
                num_examples=250,  # Per language
                metric="accuracy",
                difficulty=DifficultyLevel.MEDIUM,
                contamination_risk="medium",
                publication_year=2022,
                maintainer="Google",
                url="https://github.com/google-research/mgsm",
                subsets=[
                    "bn", "de", "es", "fr", "ja",
                    "ru", "sw", "te", "th", "zh"
                ],
                requires_generation=True,
                known_issues=["Translation artifacts"]
            ),
        }

    def get_by_category(
        self,
        category: BenchmarkCategory
    ) -> List[BenchmarkMetadata]:
        """Get all benchmarks in a category."""
        return [
            b for b in self.benchmarks.values()
            if b.category == category
        ]

    def get_low_contamination(
        self,
        max_risk: str = "medium"
    ) -> List[BenchmarkMetadata]:
        """Get benchmarks with low contamination risk."""
        risk_levels = {"low": 0, "medium": 1, "high": 2, "critical": 3}
        max_level = risk_levels.get(max_risk, 1)
        return [
            b for b in self.benchmarks.values()
            if risk_levels.get(b.contamination_risk, 3) <= max_level
        ]

    def get_unsaturated(self) -> List[BenchmarkMetadata]:
        """Get benchmarks that haven't been solved."""
        # Would need current SOTA scores to implement properly
        return [
            b for b in self.benchmarks.values()
            if b.difficulty in [DifficultyLevel.HARD, DifficultyLevel.EXPERT]
        ]

    def recommend_for_use_case(
        self,
        use_case: str,
        contamination_sensitive: bool = True
    ) -> List[str]:
        """
        Recommend benchmarks for a specific use case.

        Args:
            use_case: Description of intended model use
            contamination_sensitive: Whether to avoid high-contamination benchmarks
        """
        recommendations = {
            "general_assistant": [
                "mmlu_pro", "mt_bench", "alpaca_eval", "ifeval"
            ],
            "coding": [
                "livecodebench", "swe_bench", "humaneval"
            ],
            "math_tutor": [
                "math", "gsm8k", "mathvista"
            ],
            "research_assistant": [
                "gpqa", "mmlu_pro", "longbench"
            ],
            "multilingual": [
                "mgsm", "mmlu"  # With language-specific subsets
            ],
            "document_qa": [
                "longbench", "ruler", "needle_in_haystack"
            ]
        }

        base_recs = recommendations.get(use_case, ["mmlu_pro", "mt_bench"])

        if contamination_sensitive:
            low_contam = {b.name.lower() for b in self.get_low_contamination()}
            base_recs = [r for r in base_recs if r in low_contam]

        return base_recs
```

---

## 5.2.2 Benchmark Selection Criteria

### Selection Framework

```python
"""
ABOUTME: Framework for principled benchmark selection based on multiple criteria.
ABOUTME: Implements scoring and trade-off analysis for benchmark choices.
"""

from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple
from enum import Enum


@dataclass
class SelectionCriteria:
    """Criteria weights for benchmark selection."""
    relevance_to_use_case: float = 0.30
    contamination_status: float = 0.25
    difficulty_appropriateness: float = 0.15
    metric_reliability: float = 0.15
    community_adoption: float = 0.10
    evaluation_cost: float = 0.05

    def validate(self):
        """Ensure weights sum to 1.0."""
        total = (
            self.relevance_to_use_case +
            self.contamination_status +
            self.difficulty_appropriateness +
            self.metric_reliability +
            self.community_adoption +
            self.evaluation_cost
        )
        if abs(total - 1.0) > 0.01:
            raise ValueError(f"Weights must sum to 1.0, got {total}")


class BenchmarkSelector:
    """
    Selects optimal benchmarks based on weighted criteria.

    Considers relevance, contamination risk, difficulty, metric
    quality, community adoption, and evaluation cost.
    """

    def __init__(
        self,
        catalog: BenchmarkCatalog,
        criteria: Optional[SelectionCriteria] = None
    ):
        self.catalog = catalog
        self.criteria = criteria or SelectionCriteria()
        self.criteria.validate()

    def score_benchmark(
        self,
        benchmark_name: str,
        use_case: str,
        model_capability_estimate: float,  # 0-1 scale
        budget_hours: float = 10.0
    ) -> Dict[str, float]:
        """
        Score a benchmark for a specific evaluation need.

        Args:
            benchmark_name: Name of benchmark to score
            use_case: Intended model application
            model_capability_estimate: Expected model performance level
            budget_hours: Available compute time

        Returns:
            Dictionary of criterion scores and total
        """
        benchmark = self.catalog.benchmarks.get(benchmark_name)
        if not benchmark:
            return {"error": f"Unknown benchmark: {benchmark_name}"}

        scores = {}

        # Relevance to use case
        scores["relevance"] = self._score_relevance(benchmark, use_case)

        # Contamination status (higher = less contaminated = better)
        scores["contamination"] = self._score_contamination(benchmark)

        # Difficulty appropriateness
        scores["difficulty"] = self._score_difficulty(
            benchmark, model_capability_estimate
        )

        # Metric reliability
        scores["metric_reliability"] = self._score_metric_reliability(benchmark)

        # Community adoption
        scores["community"] = self._score_community_adoption(benchmark)

        # Evaluation cost
        scores["cost"] = self._score_cost(benchmark, budget_hours)

        # Weighted total
        scores["total"] = (
            scores["relevance"] * self.criteria.relevance_to_use_case +
            scores["contamination"] * self.criteria.contamination_status +
            scores["difficulty"] * self.criteria.difficulty_appropriateness +
            scores["metric_reliability"] * self.criteria.metric_reliability +
            scores["community"] * self.criteria.community_adoption +
            scores["cost"] * self.criteria.evaluation_cost
        )

        return scores

    def _score_relevance(
        self,
        benchmark: BenchmarkMetadata,
        use_case: str
    ) -> float:
        """Score relevance to use case (0-1)."""
        # Mapping of use cases to relevant categories
        use_case_categories = {
            "general_assistant": [
                BenchmarkCategory.KNOWLEDGE,
                BenchmarkCategory.REASONING,
                BenchmarkCategory.INSTRUCTION_FOLLOWING
            ],
            "coding": [BenchmarkCategory.CODING],
            "math_tutor": [BenchmarkCategory.MATHEMATICS],
            "research": [
                BenchmarkCategory.KNOWLEDGE,
                BenchmarkCategory.LONG_CONTEXT
            ],
            "customer_service": [
                BenchmarkCategory.INSTRUCTION_FOLLOWING,
                BenchmarkCategory.SAFETY
            ]
        }

        relevant_categories = use_case_categories.get(
            use_case,
            list(BenchmarkCategory)
        )

        if benchmark.category in relevant_categories:
            return 1.0
        return 0.3  # Some cross-category relevance

    def _score_contamination(self, benchmark: BenchmarkMetadata) -> float:
        """Score based on contamination risk (higher = safer)."""
        risk_scores = {
            "low": 1.0,
            "medium": 0.7,
            "high": 0.4,
            "critical": 0.1
        }
        return risk_scores.get(benchmark.contamination_risk, 0.5)

    def _score_difficulty(
        self,
        benchmark: BenchmarkMetadata,
        capability_estimate: float
    ) -> float:
        """
        Score difficulty appropriateness.

        Benchmarks should be challenging but not impossible.
        Saturated benchmarks provide little signal.
        """
        difficulty_ranges = {
            DifficultyLevel.EASY: (0.8, 1.0),
            DifficultyLevel.MEDIUM: (0.5, 0.85),
            DifficultyLevel.HARD: (0.2, 0.6),
            DifficultyLevel.EXPERT: (0.0, 0.4)
        }

        min_cap, max_cap = difficulty_ranges.get(
            benchmark.difficulty, (0.3, 0.7)
        )

        # Best score when capability is in range
        if min_cap <= capability_estimate <= max_cap:
            return 1.0

        # Penalize mismatch
        if capability_estimate < min_cap:
            return 0.5  # Too hard
        else:
            return 0.3  # Too easy / saturated

    def _score_metric_reliability(
        self,
        benchmark: BenchmarkMetadata
    ) -> float:
        """Score metric reliability."""
        # Prefer accuracy over subjective metrics
        reliable_metrics = {"accuracy", "exact_match", "pass@k"}
        moderate_metrics = {"f1", "bleu", "rouge"}

        if benchmark.metric in reliable_metrics:
            return 1.0
        elif benchmark.metric in moderate_metrics:
            return 0.8
        else:  # LLM-judge, win_rate, etc.
            return 0.6

    def _score_community_adoption(
        self,
        benchmark: BenchmarkMetadata
    ) -> float:
        """Score based on community adoption."""
        # High adoption benchmarks (based on leaderboard presence)
        high_adoption = {
            "mmlu", "gsm8k", "humaneval", "hellaswag",
            "arc_challenge", "winogrande", "mt_bench"
        }

        medium_adoption = {
            "math", "mbpp", "ifeval", "alpaca_eval",
            "truthfulqa", "gpqa"
        }

        name_lower = benchmark.name.lower()
        if name_lower in high_adoption:
            return 1.0
        elif name_lower in medium_adoption:
            return 0.7
        else:
            return 0.4

    def _score_cost(
        self,
        benchmark: BenchmarkMetadata,
        budget_hours: float
    ) -> float:
        """Score based on evaluation cost vs budget."""
        # Estimate hours based on example count and generation requirement
        base_time_per_1k = 0.5 if not benchmark.requires_generation else 2.0
        estimated_hours = (benchmark.num_examples / 1000) * base_time_per_1k

        if estimated_hours <= budget_hours * 0.5:
            return 1.0  # Well within budget
        elif estimated_hours <= budget_hours:
            return 0.7  # Fits budget
        else:
            return 0.3  # Over budget

    def select_benchmark_suite(
        self,
        use_case: str,
        model_capability: float,
        max_benchmarks: int = 5,
        budget_hours: float = 24.0,
        required_categories: Optional[List[BenchmarkCategory]] = None
    ) -> List[Tuple[str, Dict[str, float]]]:
        """
        Select optimal benchmark suite for evaluation.

        Returns ranked list of (benchmark_name, scores) tuples.
        """
        scored = []

        for name in self.catalog.benchmarks:
            scores = self.score_benchmark(
                name, use_case, model_capability, budget_hours
            )
            if "error" not in scores:
                scored.append((name, scores))

        # Sort by total score
        scored.sort(key=lambda x: x[1]["total"], reverse=True)

        # Ensure category coverage if required
        if required_categories:
            return self._ensure_category_coverage(
                scored, required_categories, max_benchmarks
            )

        return scored[:max_benchmarks]

    def _ensure_category_coverage(
        self,
        scored: List[Tuple[str, Dict]],
        required_categories: List[BenchmarkCategory],
        max_benchmarks: int
    ) -> List[Tuple[str, Dict]]:
        """Ensure at least one benchmark per required category."""
        selected = []
        covered_categories = set()

        # First pass: cover required categories
        for name, scores in scored:
            benchmark = self.catalog.benchmarks[name]
            if benchmark.category in required_categories:
                if benchmark.category not in covered_categories:
                    selected.append((name, scores))
                    covered_categories.add(benchmark.category)

        # Second pass: fill remaining slots with highest scores
        for name, scores in scored:
            if len(selected) >= max_benchmarks:
                break
            if name not in [s[0] for s in selected]:
                selected.append((name, scores))

        return selected[:max_benchmarks]
```

### Contamination Detection

```python
"""
ABOUTME: Contamination detection and mitigation for benchmark evaluation.
ABOUTME: Implements statistical and n-gram based contamination checks.
"""

from typing import List, Dict, Set, Tuple
from collections import Counter
import hashlib


class ContaminationDetector:
    """
    Detects potential data contamination in model evaluations.

    Uses multiple signals to identify if a model may have
    seen benchmark data during training.
    """

    def __init__(
        self,
        ngram_size: int = 5,
        contamination_threshold: float = 0.8
    ):
        self.ngram_size = ngram_size
        self.contamination_threshold = contamination_threshold
        self.benchmark_ngrams: Dict[str, Set[str]] = {}

    def register_benchmark(
        self,
        benchmark_name: str,
        examples: List[str]
    ) -> None:
        """Register benchmark examples for contamination checking."""
        ngrams = set()
        for example in examples:
            example_ngrams = self._extract_ngrams(example)
            ngrams.update(example_ngrams)
        self.benchmark_ngrams[benchmark_name] = ngrams

    def check_model_contamination(
        self,
        model,
        benchmark_name: str,
        examples: List[str],
        num_samples: int = 100
    ) -> Dict[str, any]:
        """
        Check for contamination by measuring n-gram prediction accuracy.

        High accuracy in predicting exact n-grams from benchmark
        suggests the model may have seen the data.
        """
        if benchmark_name not in self.benchmark_ngrams:
            self.register_benchmark(benchmark_name, examples)

        # Sample examples for testing
        import random
        test_examples = random.sample(
            examples,
            min(num_samples, len(examples))
        )

        exact_matches = 0
        high_confidence_predictions = 0

        for example in test_examples:
            ngrams = self._extract_ngrams(example)

            for ngram in ngrams:
                # Test if model can complete the n-gram
                prefix = " ".join(ngram[:-1])
                true_completion = ngram[-1]

                predicted = self._get_model_completion(model, prefix)

                if predicted == true_completion:
                    exact_matches += 1

                # Check confidence
                confidence = self._get_completion_confidence(
                    model, prefix, true_completion
                )
                if confidence > 0.9:
                    high_confidence_predictions += 1

        total_ngrams = sum(
            len(self._extract_ngrams(ex))
            for ex in test_examples
        )

        contamination_score = exact_matches / total_ngrams if total_ngrams > 0 else 0

        return {
            "benchmark": benchmark_name,
            "contamination_score": contamination_score,
            "exact_matches": exact_matches,
            "high_confidence_ratio": high_confidence_predictions / total_ngrams,
            "likely_contaminated": contamination_score > self.contamination_threshold,
            "recommendation": self._get_recommendation(contamination_score)
        }

    def _extract_ngrams(self, text: str) -> List[Tuple[str, ...]]:
        """Extract n-grams from text."""
        tokens = text.lower().split()
        ngrams = []
        for i in range(len(tokens) - self.ngram_size + 1):
            ngram = tuple(tokens[i:i + self.ngram_size])
            ngrams.append(ngram)
        return ngrams

    def _get_model_completion(self, model, prefix: str) -> str:
        """Get model's completion for prefix."""
        # Implementation would call model
        pass

    def _get_completion_confidence(
        self,
        model,
        prefix: str,
        completion: str
    ) -> float:
        """Get model's confidence in specific completion."""
        # Implementation would get logprobs
        pass

    def _get_recommendation(self, score: float) -> str:
        """Get recommendation based on contamination score."""
        if score > 0.9:
            return "CRITICAL: Do not use this benchmark for this model"
        elif score > 0.7:
            return "HIGH RISK: Consider using contamination-free alternative"
        elif score > 0.5:
            return "MEDIUM RISK: Results may be inflated, interpret cautiously"
        elif score > 0.3:
            return "LOW RISK: Some contamination possible, minor adjustment needed"
        else:
            return "MINIMAL RISK: Benchmark results likely reliable"

    def suggest_alternatives(
        self,
        contaminated_benchmark: str,
        catalog: BenchmarkCatalog
    ) -> List[str]:
        """Suggest contamination-free alternatives."""
        alternatives_map = {
            "mmlu": ["mmlu_pro", "gpqa"],
            "gsm8k": ["math", "mathvista"],
            "humaneval": ["livecodebench", "swe_bench"],
            "hellaswag": ["arc_challenge", "winogrande"],
        }

        return alternatives_map.get(
            contaminated_benchmark.lower(),
            catalog.get_low_contamination("medium")
        )
```

---

## 5.2.3 Interpretation Guidelines

### Statistical Interpretation

```python
"""
ABOUTME: Guidelines for statistically sound benchmark interpretation.
ABOUTME: Implements confidence intervals, significance testing, and variance analysis.
"""

from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple
import numpy as np
from scipy import stats


@dataclass
class BenchmarkResult:
    """Individual benchmark result with uncertainty."""
    benchmark_name: str
    score: float
    confidence_interval: Tuple[float, float]
    num_samples: int
    standard_error: float
    per_subset_scores: Optional[Dict[str, float]] = None


class ResultInterpreter:
    """
    Interprets benchmark results with appropriate statistical rigor.

    Accounts for uncertainty, multiple testing, and benchmark
    limitations when drawing conclusions.
    """

    def __init__(self, confidence_level: float = 0.95):
        self.confidence_level = confidence_level

    def compute_confidence_interval(
        self,
        scores: List[float],
        method: str = "wilson"
    ) -> Tuple[float, float]:
        """
        Compute confidence interval for accuracy scores.

        Args:
            scores: List of binary (0/1) scores or proportions
            method: CI method (wilson, normal, bootstrap)
        """
        n = len(scores)
        if n == 0:
            return (0.0, 0.0)

        mean = np.mean(scores)

        if method == "wilson":
            # Wilson score interval - better for proportions
            return self._wilson_interval(sum(scores), n)
        elif method == "normal":
            stderr = np.std(scores) / np.sqrt(n)
            z = stats.norm.ppf(1 - (1 - self.confidence_level) / 2)
            return (mean - z * stderr, mean + z * stderr)
        elif method == "bootstrap":
            return self._bootstrap_interval(scores)
        else:
            raise ValueError(f"Unknown method: {method}")

    def _wilson_interval(
        self,
        successes: int,
        total: int
    ) -> Tuple[float, float]:
        """Wilson score interval for proportions."""
        if total == 0:
            return (0.0, 0.0)

        p = successes / total
        z = stats.norm.ppf(1 - (1 - self.confidence_level) / 2)

        denominator = 1 + z**2 / total
        center = (p + z**2 / (2 * total)) / denominator
        spread = z * np.sqrt(p * (1 - p) / total + z**2 / (4 * total**2)) / denominator

        return (max(0, center - spread), min(1, center + spread))

    def _bootstrap_interval(
        self,
        scores: List[float],
        n_bootstrap: int = 10000
    ) -> Tuple[float, float]:
        """Bootstrap confidence interval."""
        scores = np.array(scores)
        bootstrap_means = []

        for _ in range(n_bootstrap):
            sample = np.random.choice(scores, size=len(scores), replace=True)
            bootstrap_means.append(np.mean(sample))

        alpha = 1 - self.confidence_level
        lower = np.percentile(bootstrap_means, alpha / 2 * 100)
        upper = np.percentile(bootstrap_means, (1 - alpha / 2) * 100)

        return (lower, upper)

    def is_significant_difference(
        self,
        scores_a: List[float],
        scores_b: List[float],
        paired: bool = True
    ) -> Dict[str, any]:
        """
        Test if difference between two models is significant.

        Args:
            scores_a: Scores from model A (per example)
            scores_b: Scores from model B (per example)
            paired: Whether scores are paired (same examples)
        """
        scores_a = np.array(scores_a)
        scores_b = np.array(scores_b)

        mean_diff = np.mean(scores_a) - np.mean(scores_b)

        if paired:
            # Paired bootstrap test
            diffs = scores_a - scores_b
            observed_mean_diff = np.mean(diffs)

            # Bootstrap under null (shift to mean 0)
            centered_diffs = diffs - observed_mean_diff
            n_bootstrap = 10000
            null_diffs = []

            for _ in range(n_bootstrap):
                sample = np.random.choice(centered_diffs, size=len(diffs), replace=True)
                null_diffs.append(np.mean(sample))

            # Two-tailed p-value
            null_diffs = np.array(null_diffs)
            p_value = 2 * min(
                np.mean(null_diffs >= observed_mean_diff),
                np.mean(null_diffs <= observed_mean_diff)
            )

            test_name = "paired_bootstrap"
        else:
            # Welch's t-test for independent samples
            statistic, p_value = stats.ttest_ind(scores_a, scores_b, equal_var=False)
            test_name = "welch_t_test"

        # Effect size (Cohen's d)
        pooled_std = np.sqrt((np.var(scores_a) + np.var(scores_b)) / 2)
        effect_size = mean_diff / pooled_std if pooled_std > 0 else 0

        return {
            "test": test_name,
            "mean_difference": mean_diff,
            "p_value": p_value,
            "significant": p_value < (1 - self.confidence_level),
            "effect_size": effect_size,
            "effect_interpretation": self._interpret_effect_size(effect_size)
        }

    def _interpret_effect_size(self, d: float) -> str:
        """Interpret Cohen's d effect size."""
        d = abs(d)
        if d < 0.2:
            return "negligible"
        elif d < 0.5:
            return "small"
        elif d < 0.8:
            return "medium"
        else:
            return "large"

    def interpret_result(
        self,
        result: BenchmarkResult,
        baseline: Optional[float] = None,
        human_performance: Optional[float] = None
    ) -> Dict[str, any]:
        """
        Provide interpretation of benchmark result.

        Returns structured interpretation with context.
        """
        interpretation = {
            "benchmark": result.benchmark_name,
            "score": result.score,
            "confidence_interval": result.confidence_interval,
            "interpretation": []
        }

        # CI width indicates reliability
        ci_width = result.confidence_interval[1] - result.confidence_interval[0]
        if ci_width > 0.1:
            interpretation["interpretation"].append(
                f"Wide confidence interval ({ci_width:.2f}) suggests high uncertainty. "
                f"Consider larger sample or more trials."
            )

        # Comparison to baseline
        if baseline is not None:
            if result.score > baseline:
                pct_improvement = ((result.score - baseline) / baseline) * 100
                interpretation["baseline_comparison"] = {
                    "baseline": baseline,
                    "improvement_pct": pct_improvement,
                    "significant": result.confidence_interval[0] > baseline
                }
                if result.confidence_interval[0] > baseline:
                    interpretation["interpretation"].append(
                        f"Significantly above baseline ({pct_improvement:.1f}% improvement)."
                    )
                else:
                    interpretation["interpretation"].append(
                        f"Above baseline but not statistically significant."
                    )

        # Comparison to human performance
        if human_performance is not None:
            gap = human_performance - result.score
            interpretation["human_comparison"] = {
                "human_baseline": human_performance,
                "gap": gap,
                "superhuman": result.score > human_performance
            }
            if result.score >= human_performance:
                interpretation["interpretation"].append(
                    "Matches or exceeds human performance on this benchmark."
                )
            else:
                interpretation["interpretation"].append(
                    f"Gap to human performance: {gap:.2f}"
                )

        # Subset analysis if available
        if result.per_subset_scores:
            best_subset = max(result.per_subset_scores.items(), key=lambda x: x[1])
            worst_subset = min(result.per_subset_scores.items(), key=lambda x: x[1])

            interpretation["subset_analysis"] = {
                "strongest": best_subset,
                "weakest": worst_subset,
                "variance": np.var(list(result.per_subset_scores.values()))
            }

            if best_subset[1] - worst_subset[1] > 0.2:
                interpretation["interpretation"].append(
                    f"Large performance gap between subsets: "
                    f"best on {best_subset[0]} ({best_subset[1]:.2f}), "
                    f"weakest on {worst_subset[0]} ({worst_subset[1]:.2f})."
                )

        return interpretation


class LeaderboardInterpreter:
    """
    Interprets leaderboard rankings with appropriate skepticism.

    Accounts for benchmark gaming, cherry-picking, and
    statistical noise in rankings.
    """

    def analyze_ranking(
        self,
        model_scores: Dict[str, Dict[str, float]],
        catalog: BenchmarkCatalog
    ) -> Dict[str, any]:
        """
        Analyze leaderboard ranking critically.

        Args:
            model_scores: model_name -> benchmark -> score
            catalog: Benchmark catalog for metadata
        """
        analysis = {
            "warnings": [],
            "adjusted_rankings": {},
            "benchmark_weights": {}
        }

        # Check for suspicious patterns
        for model, scores in model_scores.items():
            patterns = self._detect_suspicious_patterns(scores, catalog)
            if patterns:
                analysis["warnings"].append({
                    "model": model,
                    "patterns": patterns
                })

        # Compute adjusted rankings (downweight contaminated benchmarks)
        for benchmark_name in next(iter(model_scores.values())).keys():
            metadata = catalog.benchmarks.get(benchmark_name)
            if metadata:
                weight = self._compute_benchmark_weight(metadata)
                analysis["benchmark_weights"][benchmark_name] = weight

        return analysis

    def _detect_suspicious_patterns(
        self,
        scores: Dict[str, float],
        catalog: BenchmarkCatalog
    ) -> List[str]:
        """Detect patterns suggesting benchmark gaming."""
        patterns = []

        # Check for unrealistic scores on contaminated benchmarks
        for benchmark, score in scores.items():
            metadata = catalog.benchmarks.get(benchmark)
            if metadata:
                if metadata.contamination_risk == "critical" and score > 0.95:
                    patterns.append(
                        f"Suspiciously high score ({score:.2f}) on "
                        f"heavily contaminated benchmark {benchmark}"
                    )

                # Check if score exceeds human baseline significantly
                if metadata.human_baseline and score > metadata.human_baseline * 1.1:
                    patterns.append(
                        f"Score on {benchmark} ({score:.2f}) exceeds human "
                        f"baseline ({metadata.human_baseline:.2f}) by >10%"
                    )

        return patterns

    def _compute_benchmark_weight(
        self,
        metadata: BenchmarkMetadata
    ) -> float:
        """Compute weight for benchmark in aggregate scoring."""
        base_weight = 1.0

        # Reduce weight for contaminated benchmarks
        contamination_multipliers = {
            "low": 1.0,
            "medium": 0.8,
            "high": 0.5,
            "critical": 0.2
        }
        base_weight *= contamination_multipliers.get(metadata.contamination_risk, 0.5)

        # Reduce weight for saturated benchmarks
        if metadata.difficulty == DifficultyLevel.EASY:
            base_weight *= 0.5

        # Increase weight for reliable metrics
        if metadata.metric in ["accuracy", "exact_match", "pass@k"]:
            base_weight *= 1.2

        return base_weight
```

---

## 5.2.4 Gaming and Limitations

### Benchmark Gaming Detection

```python
"""
ABOUTME: Detection and prevention of benchmark gaming strategies.
ABOUTME: Identifies overfitting, format hacking, and other gaming patterns.
"""

from typing import List, Dict, Optional
from dataclasses import dataclass


@dataclass
class GamingIndicator:
    """Indicator of potential benchmark gaming."""
    indicator_type: str
    severity: str  # low, medium, high
    description: str
    evidence: Dict[str, any]
    mitigation: str


class GamingDetector:
    """
    Detects potential benchmark gaming in model evaluations.

    Identifies patterns suggesting models may be optimized for
    specific benchmarks rather than general capability.
    """

    KNOWN_GAMING_STRATEGIES = [
        "format_overfitting",      # Learning benchmark-specific formats
        "answer_memorization",     # Memorizing test set answers
        "prompt_template_leak",    # Matching benchmark prompts in training
        "length_gaming",           # Exploiting length biases in metrics
        "style_matching",          # Matching grader preferences
    ]

    def detect_gaming(
        self,
        model_results: Dict[str, float],
        model_outputs: Dict[str, List[str]],
        reference_models: Dict[str, Dict[str, float]]
    ) -> List[GamingIndicator]:
        """
        Detect potential gaming across benchmarks.

        Args:
            model_results: benchmark -> score for target model
            model_outputs: benchmark -> list of model outputs
            reference_models: reference model results for comparison
        """
        indicators = []

        # Check for format overfitting
        format_indicators = self._check_format_overfitting(model_outputs)
        indicators.extend(format_indicators)

        # Check for outlier performance
        outlier_indicators = self._check_outlier_scores(model_results, reference_models)
        indicators.extend(outlier_indicators)

        # Check for length gaming
        length_indicators = self._check_length_gaming(model_outputs)
        indicators.extend(length_indicators)

        return indicators

    def _check_format_overfitting(
        self,
        outputs: Dict[str, List[str]]
    ) -> List[GamingIndicator]:
        """Check if outputs are suspiciously format-matched."""
        indicators = []

        # Check for rigid formatting patterns
        for benchmark, benchmark_outputs in outputs.items():
            # Analyze output format consistency
            format_patterns = self._extract_format_patterns(benchmark_outputs)

            # Very high format consistency is suspicious
            if format_patterns["consistency"] > 0.95:
                indicators.append(GamingIndicator(
                    indicator_type="format_overfitting",
                    severity="medium",
                    description=f"Outputs for {benchmark} show >95% format consistency",
                    evidence={"consistency": format_patterns["consistency"]},
                    mitigation="Test with varied prompt formats"
                ))

        return indicators

    def _check_outlier_scores(
        self,
        scores: Dict[str, float],
        references: Dict[str, Dict[str, float]]
    ) -> List[GamingIndicator]:
        """Check for suspiciously high scores on specific benchmarks."""
        indicators = []

        for benchmark, score in scores.items():
            # Get reference scores
            ref_scores = [
                ref[benchmark]
                for ref in references.values()
                if benchmark in ref
            ]

            if len(ref_scores) < 3:
                continue

            import numpy as np
            ref_mean = np.mean(ref_scores)
            ref_std = np.std(ref_scores)

            # Check for extreme outliers (>3 std above mean)
            if ref_std > 0 and (score - ref_mean) / ref_std > 3:
                indicators.append(GamingIndicator(
                    indicator_type="outlier_score",
                    severity="high",
                    description=f"Score on {benchmark} is >3 std above reference models",
                    evidence={
                        "score": score,
                        "ref_mean": ref_mean,
                        "ref_std": ref_std,
                        "z_score": (score - ref_mean) / ref_std
                    },
                    mitigation="Check for data contamination or benchmark-specific tuning"
                ))

        return indicators

    def _check_length_gaming(
        self,
        outputs: Dict[str, List[str]]
    ) -> List[GamingIndicator]:
        """Check for length-based gaming strategies."""
        indicators = []

        for benchmark, benchmark_outputs in outputs.items():
            lengths = [len(o.split()) for o in benchmark_outputs]
            import numpy as np

            avg_length = np.mean(lengths)
            length_std = np.std(lengths)

            # Check for suspiciously uniform length
            if length_std < 5 and avg_length > 100:
                indicators.append(GamingIndicator(
                    indicator_type="length_gaming",
                    severity="medium",
                    description=f"Outputs for {benchmark} have suspiciously uniform length",
                    evidence={
                        "avg_length": avg_length,
                        "std": length_std
                    },
                    mitigation="Use length-controlled evaluation (e.g., LC-AlpacaEval)"
                ))

            # Check for excessive verbosity
            if avg_length > 500:  # Threshold depends on task
                indicators.append(GamingIndicator(
                    indicator_type="verbosity_gaming",
                    severity="low",
                    description=f"Average output length ({avg_length:.0f} words) suggests verbosity gaming",
                    evidence={"avg_length": avg_length},
                    mitigation="Evaluate with length penalty or length-controlled metrics"
                ))

        return indicators

    def _extract_format_patterns(
        self,
        outputs: List[str]
    ) -> Dict[str, float]:
        """Analyze formatting patterns in outputs."""
        if not outputs:
            return {"consistency": 0.0}

        # Check common format patterns
        starts_with_number = sum(1 for o in outputs if o.strip()[0:1].isdigit())
        starts_with_bullet = sum(1 for o in outputs if o.strip().startswith(("-", "*", "•")))
        has_numbered_list = sum(1 for o in outputs if "\n1." in o or "\n2." in o)

        n = len(outputs)
        patterns = [
            starts_with_number / n,
            starts_with_bullet / n,
            has_numbered_list / n
        ]

        # Consistency is max pattern frequency
        consistency = max(patterns) if patterns else 0

        return {"consistency": consistency}
```

---

## Appendix A: Benchmark Quick Reference

| Benchmark | Category | Samples | Metric | Contamination | Difficulty |
|-----------|----------|---------|--------|---------------|------------|
| MMLU | Knowledge | 14,042 | accuracy | critical | medium |
| MMLU-Pro | Knowledge | 12,032 | accuracy | low | hard |
| GPQA | Knowledge | 448 | accuracy | low | expert |
| ARC-Challenge | Reasoning | 1,172 | accuracy | high | medium |
| HellaSwag | Reasoning | 10,042 | accuracy | high | easy |
| WinoGrande | Reasoning | 1,767 | accuracy | high | medium |
| GSM8K | Math | 1,319 | accuracy | critical | medium |
| MATH | Math | 5,000 | accuracy | high | hard |
| HumanEval | Coding | 164 | pass@k | critical | medium |
| LiveCodeBench | Coding | ~500 | pass@k | low | hard |
| SWE-bench | Coding | 2,294 | resolve% | medium | expert |
| RULER | Long Context | 4,000+ | accuracy | low | medium |
| IFEval | Instruction | 541 | accuracy | medium | medium |
| MT-Bench | Instruction | 80 | judge_score | medium | medium |
| AlpacaEval | Instruction | 805 | win_rate | medium | medium |

---

## Appendix B: Contamination-Free Alternatives

| Contaminated Benchmark | Alternative | Notes |
|------------------------|-------------|-------|
| MMLU | MMLU-Pro, MMLU-CF | Harder, less contaminated |
| GSM8K | MATH, MathVista | Competition-level math |
| HumanEval | LiveCodeBench | Continuously updated |
| HellaSwag | PIQA, SIQA | Different commonsense tasks |
| TruthfulQA | FreshQA | Time-sensitive questions |

---

## Troubleshooting

**Issue: Model scores much higher on old benchmarks than new ones**
```
Likely cause: Data contamination from training on benchmark data

Diagnosis:
1. Check if model was trained after benchmark release
2. Run n-gram contamination check
3. Compare performance on contamination-free alternatives

Mitigation:
- Use newer benchmarks (MMLU-Pro, LiveCodeBench)
- Apply contamination detection
- Discount contaminated benchmark scores
```

**Issue: Large variance in scores across evaluation runs**
```
Likely cause: Small test set or high-variance metric

Diagnosis:
1. Calculate confidence intervals
2. Check test set size
3. Review sampling strategy

Mitigation:
- Use bootstrap confidence intervals
- Run multiple trials with different seeds
- Consider larger benchmark subsets
```

**Issue: Model beats human baseline but fails simple real-world tasks**
```
Likely cause: Benchmark overfitting or task mismatch

Diagnosis:
1. Test on held-out real-world examples
2. Check for format sensitivity
3. Compare with diverse prompt templates

Mitigation:
- Include human evaluation
- Use multiple benchmark families
- Test robustness to prompt variations
```

---

## Glossary

| Term | Definition |
|------|------------|
| **Contamination** | When benchmark data appears in training data |
| **Saturation** | When benchmark scores approach maximum, limiting discrimination |
| **Gaming** | Optimizing for benchmark scores rather than general capability |
| **Pass@k** | Probability at least 1 of k samples is correct |
| **Few-shot** | Providing examples in the prompt |
| **Chain-of-thought** | Prompting for step-by-step reasoning |
| **Confidence interval** | Range likely containing true parameter |
| **Effect size** | Magnitude of difference (Cohen's d) |

---

## References

1. Hendrycks, D., et al. (2021). "Measuring Massive Multitask Language Understanding." [MMLU]
2. Cobbe, K., et al. (2021). "Training Verifiers to Solve Math Word Problems." [GSM8K]
3. Chen, M., et al. (2021). "Evaluating Large Language Models Trained on Code." [HumanEval]
4. Jain, N., et al. (2024). "LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code."
5. Wang, Y., et al. (2024). "MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark."
6. Yang, S., et al. (2023). "Rethinking Benchmark and Contamination for Language Models with Rephrased Samples."
7. Zheng, L., et al. (2023). "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena."

---

> **Navigation**
> [← 5.1 Evaluation Framework](5.1_llm_evaluation_framework.md) | **[Index](../README.md#15-repository-structure)** | [5.3 LLM-as-Judge →](5.3_llm_as_judge_evaluation.md)
