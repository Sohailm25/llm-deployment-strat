> **Navigation** | [← 11.3 Compliance](11.3_compliance_framework_guide.md) | [11.5 Access Control →](11.5_access_control_authentication_guide.md)
>
> | | |
> |---|---|
> | **Prerequisites** | [11.3 Compliance](11.3_compliance_framework_guide.md) &#124; [11.1 Security](11.1_llm_security_guide.md) |
> | **Related** | [8.1 Model Registry](../08_mlops_lifecycle/8.1_model_registry_guide.md) &#124; [10.2 Logging](../10_monitoring_observability/10.2_llm_logging_tracing_guide.md) |
> | **Next** | [11.5 Access Control & Authentication](11.5_access_control_authentication_guide.md) |

# Document 11.4: Model Governance Guide

## Purpose

Establishing governance frameworks for responsible AI development and deployment, ensuring accountability, transparency, and compliance throughout the model lifecycle.

## Prerequisites

- Understanding of ML model lifecycle
- Familiarity with compliance requirements (11.3)
- Experience with LLM security concepts (11.1)
- Knowledge of risk management principles

## Table of Contents

1. [Governance Framework](#1-governance-framework)
2. [Model Documentation](#2-model-documentation)
3. [Risk Assessment](#3-risk-assessment)
4. [Change Management](#4-change-management)
5. [Audit & Accountability](#5-audit--accountability)
6. [Implementation](#6-implementation)

---

## 1. Governance Framework

### 1.1 Framework Architecture

```python
"""
Model governance framework with roles, policies, and workflows.
"""
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Set
from enum import Enum
from datetime import datetime
import json
import hashlib


class GovernanceRole(Enum):
    """Governance roles in the organization."""
    MODEL_OWNER = "model_owner"
    DATA_STEWARD = "data_steward"
    ETHICS_REVIEWER = "ethics_reviewer"
    SECURITY_REVIEWER = "security_reviewer"
    BUSINESS_OWNER = "business_owner"
    COMPLIANCE_OFFICER = "compliance_officer"
    RISK_MANAGER = "risk_manager"
    DEPLOYER = "deployer"


class ModelRiskTier(Enum):
    """Model risk classification tiers."""
    TIER_1_CRITICAL = "tier_1_critical"      # High risk, customer-facing, regulated
    TIER_2_IMPORTANT = "tier_2_important"    # Medium risk, internal high-impact
    TIER_3_STANDARD = "tier_3_standard"      # Standard risk, limited scope
    TIER_4_EXPERIMENTAL = "tier_4_experimental"  # Low risk, experimental


class ApprovalStatus(Enum):
    """Approval workflow status."""
    PENDING = "pending"
    APPROVED = "approved"
    REJECTED = "rejected"
    CONDITIONAL = "conditional"
    EXPIRED = "expired"


@dataclass
class GovernancePolicy:
    """Policy definition for model governance."""
    policy_id: str
    name: str
    description: str
    applicable_tiers: List[ModelRiskTier]
    required_roles: List[GovernanceRole]
    requirements: List[str]
    enforcement: str  # "blocking" or "advisory"
    effective_date: datetime
    expiration_date: Optional[datetime] = None


@dataclass
class Stakeholder:
    """Stakeholder in the governance process."""
    user_id: str
    name: str
    email: str
    role: GovernanceRole
    department: str
    authorized_tiers: List[ModelRiskTier]


@dataclass
class ApprovalDecision:
    """Record of an approval decision."""
    decision_id: str
    reviewer: Stakeholder
    status: ApprovalStatus
    timestamp: datetime
    comments: str
    conditions: Optional[List[str]] = None
    expiration: Optional[datetime] = None


@dataclass
class ModelGovernanceRecord:
    """Complete governance record for a model."""
    model_id: str
    model_name: str
    version: str
    risk_tier: ModelRiskTier
    owner: Stakeholder
    data_steward: Stakeholder
    purpose: str
    use_cases: List[str]
    restrictions: List[str]
    approvals: List[ApprovalDecision] = field(default_factory=list)
    policies_applied: List[str] = field(default_factory=list)
    created_at: datetime = field(default_factory=datetime.utcnow)
    last_review: Optional[datetime] = None
    next_review: Optional[datetime] = None
    status: str = "draft"


class GovernanceFramework:
    """
    Comprehensive model governance framework.
    """

    def __init__(self, config: Dict):
        self.config = config
        self.policies: Dict[str, GovernancePolicy] = {}
        self.stakeholders: Dict[str, Stakeholder] = {}
        self.records: Dict[str, ModelGovernanceRecord] = {}
        self._initialize_default_policies()

    def _initialize_default_policies(self):
        """Initialize default governance policies."""
        default_policies = [
            GovernancePolicy(
                policy_id="POL-001",
                name="Model Documentation",
                description="All models must have complete documentation including model card and datasheet",
                applicable_tiers=[ModelRiskTier.TIER_1_CRITICAL, ModelRiskTier.TIER_2_IMPORTANT,
                                ModelRiskTier.TIER_3_STANDARD],
                required_roles=[GovernanceRole.MODEL_OWNER],
                requirements=[
                    "Model card with intended use",
                    "Training data documentation",
                    "Evaluation results",
                    "Limitations and risks"
                ],
                enforcement="blocking",
                effective_date=datetime.utcnow()
            ),
            GovernancePolicy(
                policy_id="POL-002",
                name="Ethics Review",
                description="Tier 1 and Tier 2 models require ethics review before deployment",
                applicable_tiers=[ModelRiskTier.TIER_1_CRITICAL, ModelRiskTier.TIER_2_IMPORTANT],
                required_roles=[GovernanceRole.ETHICS_REVIEWER],
                requirements=[
                    "Bias assessment completed",
                    "Fairness metrics documented",
                    "Potential harms identified",
                    "Mitigation strategies defined"
                ],
                enforcement="blocking",
                effective_date=datetime.utcnow()
            ),
            GovernancePolicy(
                policy_id="POL-003",
                name="Security Review",
                description="All production models require security review",
                applicable_tiers=[ModelRiskTier.TIER_1_CRITICAL, ModelRiskTier.TIER_2_IMPORTANT,
                                ModelRiskTier.TIER_3_STANDARD],
                required_roles=[GovernanceRole.SECURITY_REVIEWER],
                requirements=[
                    "Prompt injection testing",
                    "Data leakage assessment",
                    "Access control verification",
                    "Audit logging enabled"
                ],
                enforcement="blocking",
                effective_date=datetime.utcnow()
            ),
            GovernancePolicy(
                policy_id="POL-004",
                name="Periodic Review",
                description="Models must be reviewed periodically based on risk tier",
                applicable_tiers=[ModelRiskTier.TIER_1_CRITICAL, ModelRiskTier.TIER_2_IMPORTANT,
                                ModelRiskTier.TIER_3_STANDARD],
                required_roles=[GovernanceRole.MODEL_OWNER, GovernanceRole.RISK_MANAGER],
                requirements=[
                    "Performance metrics review",
                    "Drift analysis",
                    "Incident review",
                    "Compliance check"
                ],
                enforcement="advisory",
                effective_date=datetime.utcnow()
            )
        ]

        for policy in default_policies:
            self.policies[policy.policy_id] = policy

    def register_model(
        self,
        model_id: str,
        model_name: str,
        version: str,
        owner_id: str,
        data_steward_id: str,
        purpose: str,
        use_cases: List[str],
        risk_tier: Optional[ModelRiskTier] = None
    ) -> ModelGovernanceRecord:
        """Register a model for governance tracking."""
        owner = self.stakeholders.get(owner_id)
        data_steward = self.stakeholders.get(data_steward_id)

        if not owner or not data_steward:
            raise ValueError("Owner and data steward must be registered stakeholders")

        # Auto-classify risk tier if not provided
        if risk_tier is None:
            risk_tier = self._classify_risk_tier(purpose, use_cases)

        # Get applicable policies
        applicable_policies = self._get_applicable_policies(risk_tier)

        record = ModelGovernanceRecord(
            model_id=model_id,
            model_name=model_name,
            version=version,
            risk_tier=risk_tier,
            owner=owner,
            data_steward=data_steward,
            purpose=purpose,
            use_cases=use_cases,
            restrictions=[],
            policies_applied=[p.policy_id for p in applicable_policies]
        )

        self.records[model_id] = record
        return record

    def _classify_risk_tier(self, purpose: str, use_cases: List[str]) -> ModelRiskTier:
        """Auto-classify model risk tier based on purpose and use cases."""
        high_risk_keywords = [
            "healthcare", "medical", "financial", "legal", "hiring",
            "credit", "insurance", "criminal", "safety-critical"
        ]

        customer_facing_keywords = [
            "customer", "user-facing", "public", "external", "production"
        ]

        text = f"{purpose} {' '.join(use_cases)}".lower()

        # Check for high-risk indicators
        if any(kw in text for kw in high_risk_keywords):
            return ModelRiskTier.TIER_1_CRITICAL

        if any(kw in text for kw in customer_facing_keywords):
            return ModelRiskTier.TIER_2_IMPORTANT

        if "experimental" in text or "prototype" in text:
            return ModelRiskTier.TIER_4_EXPERIMENTAL

        return ModelRiskTier.TIER_3_STANDARD

    def _get_applicable_policies(self, risk_tier: ModelRiskTier) -> List[GovernancePolicy]:
        """Get policies applicable to a risk tier."""
        return [
            p for p in self.policies.values()
            if risk_tier in p.applicable_tiers
        ]

    def get_required_approvals(self, model_id: str) -> Dict[GovernanceRole, GovernancePolicy]:
        """Get required approvals for a model."""
        record = self.records.get(model_id)
        if not record:
            raise ValueError(f"Model {model_id} not found")

        required = {}
        for policy_id in record.policies_applied:
            policy = self.policies.get(policy_id)
            if policy and policy.enforcement == "blocking":
                for role in policy.required_roles:
                    required[role] = policy

        return required

    def submit_approval(
        self,
        model_id: str,
        reviewer: Stakeholder,
        status: ApprovalStatus,
        comments: str,
        conditions: Optional[List[str]] = None
    ) -> ApprovalDecision:
        """Submit an approval decision."""
        record = self.records.get(model_id)
        if not record:
            raise ValueError(f"Model {model_id} not found")

        # Verify reviewer authorization
        required_approvals = self.get_required_approvals(model_id)
        if reviewer.role not in required_approvals:
            raise PermissionError(f"Reviewer {reviewer.user_id} not authorized for this approval")

        if record.risk_tier not in reviewer.authorized_tiers:
            raise PermissionError(f"Reviewer not authorized for {record.risk_tier}")

        decision = ApprovalDecision(
            decision_id=f"APR-{model_id}-{len(record.approvals) + 1}",
            reviewer=reviewer,
            status=status,
            timestamp=datetime.utcnow(),
            comments=comments,
            conditions=conditions
        )

        record.approvals.append(decision)

        # Update model status based on approvals
        self._update_model_status(record)

        return decision

    def _update_model_status(self, record: ModelGovernanceRecord):
        """Update model status based on approval state."""
        required = self.get_required_approvals(record.model_id)
        approved_roles = set()

        for approval in record.approvals:
            if approval.status == ApprovalStatus.APPROVED:
                approved_roles.add(approval.reviewer.role)
            elif approval.status == ApprovalStatus.REJECTED:
                record.status = "rejected"
                return

        if all(role in approved_roles for role in required.keys()):
            record.status = "approved"
        else:
            record.status = "pending_approval"

    def check_deployment_eligibility(self, model_id: str) -> Dict:
        """Check if a model is eligible for deployment."""
        record = self.records.get(model_id)
        if not record:
            raise ValueError(f"Model {model_id} not found")

        required = self.get_required_approvals(model_id)
        missing_approvals = []

        approved_roles = {
            a.reviewer.role for a in record.approvals
            if a.status == ApprovalStatus.APPROVED
        }

        for role in required.keys():
            if role not in approved_roles:
                missing_approvals.append(role.value)

        # Check for policy compliance
        policy_violations = []
        for policy_id in record.policies_applied:
            policy = self.policies.get(policy_id)
            if policy and policy.enforcement == "blocking":
                # Check requirements (simplified - in production, verify each requirement)
                if not self._verify_policy_requirements(record, policy):
                    policy_violations.append(policy.name)

        eligible = len(missing_approvals) == 0 and len(policy_violations) == 0

        return {
            "eligible": eligible,
            "model_id": model_id,
            "risk_tier": record.risk_tier.value,
            "status": record.status,
            "missing_approvals": missing_approvals,
            "policy_violations": policy_violations,
            "conditions": [
                c for a in record.approvals
                if a.conditions
                for c in a.conditions
            ]
        }

    def _verify_policy_requirements(
        self,
        record: ModelGovernanceRecord,
        policy: GovernancePolicy
    ) -> bool:
        """Verify policy requirements are met."""
        # In production, this would check actual documentation and artifacts
        # Here we just check if model is in approved status
        return record.status == "approved"

    def generate_governance_report(self, model_id: str) -> Dict:
        """Generate a comprehensive governance report."""
        record = self.records.get(model_id)
        if not record:
            raise ValueError(f"Model {model_id} not found")

        eligibility = self.check_deployment_eligibility(model_id)

        return {
            "report_generated": datetime.utcnow().isoformat(),
            "model_info": {
                "id": record.model_id,
                "name": record.model_name,
                "version": record.version,
                "risk_tier": record.risk_tier.value,
                "status": record.status
            },
            "ownership": {
                "owner": {
                    "id": record.owner.user_id,
                    "name": record.owner.name,
                    "department": record.owner.department
                },
                "data_steward": {
                    "id": record.data_steward.user_id,
                    "name": record.data_steward.name,
                    "department": record.data_steward.department
                }
            },
            "purpose_and_scope": {
                "purpose": record.purpose,
                "use_cases": record.use_cases,
                "restrictions": record.restrictions
            },
            "governance": {
                "policies_applied": [
                    {
                        "id": p_id,
                        "name": self.policies[p_id].name if p_id in self.policies else "Unknown"
                    }
                    for p_id in record.policies_applied
                ],
                "approvals": [
                    {
                        "decision_id": a.decision_id,
                        "reviewer": a.reviewer.name,
                        "role": a.reviewer.role.value,
                        "status": a.status.value,
                        "timestamp": a.timestamp.isoformat(),
                        "comments": a.comments
                    }
                    for a in record.approvals
                ]
            },
            "deployment_eligibility": eligibility,
            "review_schedule": {
                "last_review": record.last_review.isoformat() if record.last_review else None,
                "next_review": record.next_review.isoformat() if record.next_review else None
            }
        }
```

### 1.2 Roles and Responsibilities

```python
class RolesAndResponsibilities:
    """
    Define roles and responsibilities for model governance.
    """

    ROLE_DEFINITIONS = {
        GovernanceRole.MODEL_OWNER: {
            "title": "Model Owner",
            "description": "Accountable for the model throughout its lifecycle",
            "responsibilities": [
                "Ensure model documentation is complete and accurate",
                "Initiate and coordinate approval workflows",
                "Monitor model performance and quality",
                "Respond to incidents involving the model",
                "Ensure compliance with governance policies",
                "Schedule and conduct periodic reviews",
                "Manage model versioning and updates"
            ],
            "authority": [
                "Approve minor model updates",
                "Request model retirement",
                "Grant access to model artifacts"
            ]
        },
        GovernanceRole.DATA_STEWARD: {
            "title": "Data Steward",
            "description": "Responsible for data quality and compliance",
            "responsibilities": [
                "Ensure training data quality and integrity",
                "Maintain data lineage documentation",
                "Verify data usage compliance",
                "Monitor for data-related issues",
                "Handle data subject requests affecting model",
                "Coordinate data updates and refreshes"
            ],
            "authority": [
                "Approve data sources for training",
                "Mandate data quality requirements",
                "Restrict data usage"
            ]
        },
        GovernanceRole.ETHICS_REVIEWER: {
            "title": "Ethics Reviewer",
            "description": "Evaluate ethical implications and fairness",
            "responsibilities": [
                "Review bias and fairness assessments",
                "Evaluate potential harms and benefits",
                "Assess alignment with ethical principles",
                "Recommend mitigations for ethical concerns",
                "Monitor for emerging ethical issues"
            ],
            "authority": [
                "Block deployment for ethical concerns",
                "Require additional bias testing",
                "Mandate monitoring for specific populations"
            ]
        },
        GovernanceRole.SECURITY_REVIEWER: {
            "title": "Security Reviewer",
            "description": "Ensure security requirements are met",
            "responsibilities": [
                "Review security assessment results",
                "Verify access controls and encryption",
                "Assess vulnerability to attacks",
                "Review audit logging configuration",
                "Validate incident response procedures"
            ],
            "authority": [
                "Block deployment for security issues",
                "Require additional security testing",
                "Mandate security controls"
            ]
        },
        GovernanceRole.BUSINESS_OWNER: {
            "title": "Business Owner",
            "description": "Accountable for business value and alignment",
            "responsibilities": [
                "Define business requirements and success criteria",
                "Approve use cases and restrictions",
                "Monitor business impact and ROI",
                "Communicate with stakeholders",
                "Prioritize model investments"
            ],
            "authority": [
                "Approve new use cases",
                "Request model enhancements",
                "Retire underperforming models"
            ]
        },
        GovernanceRole.COMPLIANCE_OFFICER: {
            "title": "Compliance Officer",
            "description": "Ensure regulatory compliance",
            "responsibilities": [
                "Map regulatory requirements to model",
                "Review compliance documentation",
                "Coordinate with legal team",
                "Monitor regulatory changes",
                "Manage audit requests"
            ],
            "authority": [
                "Block deployment for compliance issues",
                "Require additional compliance controls",
                "Mandate specific documentation"
            ]
        },
        GovernanceRole.RISK_MANAGER: {
            "title": "Risk Manager",
            "description": "Assess and manage model risks",
            "responsibilities": [
                "Conduct risk assessments",
                "Monitor risk indicators",
                "Review incident trends",
                "Recommend risk mitigations",
                "Report on risk posture"
            ],
            "authority": [
                "Assign risk tier to models",
                "Require additional risk controls",
                "Escalate critical risks"
            ]
        },
        GovernanceRole.DEPLOYER: {
            "title": "Deployer",
            "description": "Execute approved deployments",
            "responsibilities": [
                "Verify deployment prerequisites",
                "Execute deployment procedures",
                "Monitor deployment health",
                "Execute rollback if needed",
                "Document deployment outcomes"
            ],
            "authority": [
                "Execute approved deployments",
                "Initiate emergency rollback",
                "Request deployment approval"
            ]
        }
    }

    @classmethod
    def get_role_matrix(cls) -> Dict:
        """Generate a role-responsibility matrix."""
        return cls.ROLE_DEFINITIONS

    @classmethod
    def get_escalation_path(cls, role: GovernanceRole) -> List[GovernanceRole]:
        """Get escalation path for a role."""
        escalation_paths = {
            GovernanceRole.DEPLOYER: [
                GovernanceRole.MODEL_OWNER,
                GovernanceRole.RISK_MANAGER
            ],
            GovernanceRole.MODEL_OWNER: [
                GovernanceRole.BUSINESS_OWNER,
                GovernanceRole.RISK_MANAGER
            ],
            GovernanceRole.DATA_STEWARD: [
                GovernanceRole.COMPLIANCE_OFFICER,
                GovernanceRole.MODEL_OWNER
            ],
            GovernanceRole.ETHICS_REVIEWER: [
                GovernanceRole.RISK_MANAGER,
                GovernanceRole.COMPLIANCE_OFFICER
            ],
            GovernanceRole.SECURITY_REVIEWER: [
                GovernanceRole.RISK_MANAGER,
                GovernanceRole.COMPLIANCE_OFFICER
            ],
            GovernanceRole.RISK_MANAGER: [
                GovernanceRole.COMPLIANCE_OFFICER,
                GovernanceRole.BUSINESS_OWNER
            ]
        }
        return escalation_paths.get(role, [])
```

---

## 2. Model Documentation

### 2.1 Model Cards

```python
"""
Model card implementation following industry best practices.
"""
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any
from datetime import datetime
import json


@dataclass
class ModelCardMetrics:
    """Evaluation metrics for model card."""
    metric_name: str
    value: float
    dataset: str
    date_measured: datetime
    conditions: Optional[str] = None
    disaggregated: Optional[Dict[str, float]] = None  # By subgroup


@dataclass
class ModelCardRisk:
    """Risk documentation for model card."""
    risk_type: str
    description: str
    likelihood: str  # low, medium, high
    impact: str  # low, medium, high
    mitigation: str
    residual_risk: str


@dataclass
class ModelCard:
    """
    Comprehensive model card following ML community standards.
    Based on Model Cards for Model Reporting (Mitchell et al., 2019)
    """
    # Model Details
    model_id: str
    model_name: str
    version: str
    model_type: str  # e.g., "Large Language Model", "Embedding Model"
    architecture: str  # e.g., "Transformer decoder-only, 7B parameters"
    developers: List[str]
    release_date: datetime
    license: str
    contact: str

    # Intended Use
    primary_intended_uses: List[str]
    primary_intended_users: List[str]
    out_of_scope_uses: List[str]

    # Training Data
    training_data_description: str
    training_data_sources: List[str]
    training_data_size: str
    training_data_preprocessing: str
    training_date_range: Optional[str] = None

    # Evaluation Data
    evaluation_datasets: List[str] = field(default_factory=list)
    evaluation_methodology: str = ""

    # Metrics
    metrics: List[ModelCardMetrics] = field(default_factory=list)

    # Ethical Considerations
    ethical_considerations: List[str] = field(default_factory=list)

    # Caveats and Recommendations
    known_limitations: List[str] = field(default_factory=list)
    risks: List[ModelCardRisk] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)

    # Additional Information
    additional_info: Dict[str, Any] = field(default_factory=dict)

    def to_markdown(self) -> str:
        """Generate markdown representation of model card."""
        md = f"""# Model Card: {self.model_name}

## Model Details

| Property | Value |
|----------|-------|
| Model ID | {self.model_id} |
| Version | {self.version} |
| Type | {self.model_type} |
| Architecture | {self.architecture} |
| Developers | {', '.join(self.developers)} |
| Release Date | {self.release_date.strftime('%Y-%m-%d')} |
| License | {self.license} |
| Contact | {self.contact} |

## Intended Use

### Primary Intended Uses
{self._list_to_markdown(self.primary_intended_uses)}

### Primary Intended Users
{self._list_to_markdown(self.primary_intended_users)}

### Out-of-Scope Uses
{self._list_to_markdown(self.out_of_scope_uses)}

## Training Data

{self.training_data_description}

**Sources:** {', '.join(self.training_data_sources)}

**Size:** {self.training_data_size}

**Preprocessing:** {self.training_data_preprocessing}

## Evaluation

### Datasets
{self._list_to_markdown(self.evaluation_datasets)}

### Methodology
{self.evaluation_methodology}

### Metrics

| Metric | Value | Dataset | Conditions |
|--------|-------|---------|------------|
"""
        for m in self.metrics:
            conditions = m.conditions or "N/A"
            md += f"| {m.metric_name} | {m.value:.4f} | {m.dataset} | {conditions} |\n"

        md += f"""
## Ethical Considerations
{self._list_to_markdown(self.ethical_considerations)}

## Known Limitations
{self._list_to_markdown(self.known_limitations)}

## Risks and Mitigations

| Risk Type | Description | Likelihood | Impact | Mitigation |
|-----------|-------------|------------|--------|------------|
"""
        for r in self.risks:
            md += f"| {r.risk_type} | {r.description} | {r.likelihood} | {r.impact} | {r.mitigation} |\n"

        md += f"""
## Recommendations
{self._list_to_markdown(self.recommendations)}

---
*Model card generated: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')}*
"""
        return md

    def _list_to_markdown(self, items: List[str]) -> str:
        """Convert list to markdown bullet points."""
        if not items:
            return "- None specified\n"
        return '\n'.join(f"- {item}" for item in items)

    def to_json(self) -> str:
        """Generate JSON representation of model card."""
        data = {
            "model_details": {
                "model_id": self.model_id,
                "model_name": self.model_name,
                "version": self.version,
                "model_type": self.model_type,
                "architecture": self.architecture,
                "developers": self.developers,
                "release_date": self.release_date.isoformat(),
                "license": self.license,
                "contact": self.contact
            },
            "intended_use": {
                "primary_intended_uses": self.primary_intended_uses,
                "primary_intended_users": self.primary_intended_users,
                "out_of_scope_uses": self.out_of_scope_uses
            },
            "training_data": {
                "description": self.training_data_description,
                "sources": self.training_data_sources,
                "size": self.training_data_size,
                "preprocessing": self.training_data_preprocessing
            },
            "evaluation": {
                "datasets": self.evaluation_datasets,
                "methodology": self.evaluation_methodology,
                "metrics": [
                    {
                        "name": m.metric_name,
                        "value": m.value,
                        "dataset": m.dataset,
                        "date": m.date_measured.isoformat(),
                        "disaggregated": m.disaggregated
                    }
                    for m in self.metrics
                ]
            },
            "ethical_considerations": self.ethical_considerations,
            "limitations": self.known_limitations,
            "risks": [
                {
                    "type": r.risk_type,
                    "description": r.description,
                    "likelihood": r.likelihood,
                    "impact": r.impact,
                    "mitigation": r.mitigation,
                    "residual_risk": r.residual_risk
                }
                for r in self.risks
            ],
            "recommendations": self.recommendations
        }
        return json.dumps(data, indent=2)


class ModelCardGenerator:
    """Generate model cards from model metadata."""

    @staticmethod
    def from_training_run(
        model_id: str,
        training_config: Dict,
        evaluation_results: Dict,
        metadata: Dict
    ) -> ModelCard:
        """Generate model card from training run data."""
        # Extract metrics
        metrics = []
        for dataset, results in evaluation_results.items():
            for metric_name, value in results.items():
                if isinstance(value, (int, float)):
                    metrics.append(ModelCardMetrics(
                        metric_name=metric_name,
                        value=float(value),
                        dataset=dataset,
                        date_measured=datetime.utcnow()
                    ))

        return ModelCard(
            model_id=model_id,
            model_name=metadata.get("name", model_id),
            version=metadata.get("version", "1.0.0"),
            model_type=metadata.get("model_type", "Large Language Model"),
            architecture=training_config.get("architecture", "Unknown"),
            developers=metadata.get("developers", ["Unknown"]),
            release_date=datetime.utcnow(),
            license=metadata.get("license", "Proprietary"),
            contact=metadata.get("contact", "ml-team@company.com"),
            primary_intended_uses=metadata.get("intended_uses", []),
            primary_intended_users=metadata.get("intended_users", []),
            out_of_scope_uses=metadata.get("out_of_scope", []),
            training_data_description=training_config.get("data_description", ""),
            training_data_sources=training_config.get("data_sources", []),
            training_data_size=training_config.get("data_size", "Unknown"),
            training_data_preprocessing=training_config.get("preprocessing", ""),
            evaluation_datasets=list(evaluation_results.keys()),
            metrics=metrics
        )
```

### 2.2 Datasheets for Datasets

```python
@dataclass
class DataSheet:
    """
    Datasheet for documenting training datasets.
    Based on Datasheets for Datasets (Gebru et al., 2018)
    """
    # Motivation
    dataset_name: str
    dataset_id: str
    creators: List[str]
    creation_date: datetime
    purpose: str
    funding: Optional[str] = None

    # Composition
    instance_description: str
    instance_count: int
    sampling_strategy: str
    data_types: List[str]
    labels_or_targets: Optional[str] = None
    missing_data: Optional[str] = None
    confidentiality: str  # "public", "internal", "confidential"
    sensitive_data: List[str] = field(default_factory=list)

    # Collection Process
    collection_mechanism: str
    collection_timeframe: str
    data_sources: List[str] = field(default_factory=list)
    consent_collected: bool = False
    consent_mechanism: Optional[str] = None

    # Preprocessing
    preprocessing_steps: List[str] = field(default_factory=list)
    raw_data_saved: bool = True
    raw_data_location: Optional[str] = None

    # Uses
    intended_uses: List[str] = field(default_factory=list)
    out_of_scope_uses: List[str] = field(default_factory=list)
    prior_uses: List[str] = field(default_factory=list)

    # Distribution
    distribution_method: str = ""
    distribution_license: str = ""
    access_restrictions: List[str] = field(default_factory=list)

    # Maintenance
    maintainer: str = ""
    update_frequency: str = ""
    version_history: List[Dict] = field(default_factory=list)
    retention_policy: str = ""

    def to_markdown(self) -> str:
        """Generate markdown representation of datasheet."""
        md = f"""# Datasheet: {self.dataset_name}

## Motivation

**Dataset ID:** {self.dataset_id}

**Purpose:** {self.purpose}

**Creators:** {', '.join(self.creators)}

**Creation Date:** {self.creation_date.strftime('%Y-%m-%d')}

**Funding:** {self.funding or 'Not specified'}

## Composition

**What do the instances represent?**
{self.instance_description}

**Total Instances:** {self.instance_count:,}

**Sampling Strategy:** {self.sampling_strategy}

**Data Types:** {', '.join(self.data_types)}

**Labels/Targets:** {self.labels_or_targets or 'None'}

**Missing Data:** {self.missing_data or 'None identified'}

**Confidentiality Level:** {self.confidentiality}

**Sensitive Data Categories:**
{self._list_to_markdown(self.sensitive_data) if self.sensitive_data else '- None identified'}

## Collection Process

**Collection Mechanism:** {self.collection_mechanism}

**Timeframe:** {self.collection_timeframe}

**Data Sources:**
{self._list_to_markdown(self.data_sources)}

**Consent:** {'Yes' if self.consent_collected else 'No'} {f'({self.consent_mechanism})' if self.consent_mechanism else ''}

## Preprocessing

**Steps Applied:**
{self._list_to_markdown(self.preprocessing_steps) if self.preprocessing_steps else '- None'}

**Raw Data Preserved:** {'Yes' if self.raw_data_saved else 'No'}

## Uses

### Intended Uses
{self._list_to_markdown(self.intended_uses)}

### Out-of-Scope Uses
{self._list_to_markdown(self.out_of_scope_uses)}

## Distribution

**Method:** {self.distribution_method}

**License:** {self.distribution_license}

**Access Restrictions:**
{self._list_to_markdown(self.access_restrictions) if self.access_restrictions else '- None'}

## Maintenance

**Maintainer:** {self.maintainer}

**Update Frequency:** {self.update_frequency}

**Retention Policy:** {self.retention_policy}

---
*Datasheet generated: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')}*
"""
        return md

    def _list_to_markdown(self, items: List[str]) -> str:
        if not items:
            return "- None specified\n"
        return '\n'.join(f"- {item}" for item in items)
```

---

## 3. Risk Assessment

### 3.1 Model Risk Assessment Framework

```python
"""
Comprehensive risk assessment framework for ML models.
"""
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Tuple
from enum import Enum
from datetime import datetime
import json


class RiskCategory(Enum):
    """Categories of model risk."""
    MODEL_QUALITY = "model_quality"
    DATA_QUALITY = "data_quality"
    SECURITY = "security"
    PRIVACY = "privacy"
    FAIRNESS = "fairness"
    OPERATIONAL = "operational"
    REGULATORY = "regulatory"
    REPUTATIONAL = "reputational"
    FINANCIAL = "financial"


class RiskLikelihood(Enum):
    """Likelihood of risk occurrence."""
    RARE = 1          # < 10% chance
    UNLIKELY = 2      # 10-25% chance
    POSSIBLE = 3      # 25-50% chance
    LIKELY = 4        # 50-75% chance
    ALMOST_CERTAIN = 5  # > 75% chance


class RiskImpact(Enum):
    """Impact severity of risk."""
    NEGLIGIBLE = 1    # Minor inconvenience
    MINOR = 2         # Limited impact, easily recoverable
    MODERATE = 3      # Significant impact, recoverable with effort
    MAJOR = 4         # Severe impact, difficult to recover
    CATASTROPHIC = 5  # Critical impact, potentially unrecoverable


@dataclass
class RiskFactor:
    """Individual risk factor."""
    risk_id: str
    category: RiskCategory
    description: str
    likelihood: RiskLikelihood
    impact: RiskImpact
    inherent_score: float = 0.0  # Calculated

    controls: List[str] = field(default_factory=list)
    control_effectiveness: float = 0.0  # 0-1
    residual_score: float = 0.0  # Calculated

    owner: Optional[str] = None
    identified_date: datetime = field(default_factory=datetime.utcnow)
    review_date: Optional[datetime] = None

    def __post_init__(self):
        self.inherent_score = self.likelihood.value * self.impact.value
        self.residual_score = self.inherent_score * (1 - self.control_effectiveness)


@dataclass
class RiskAssessment:
    """Complete risk assessment for a model."""
    assessment_id: str
    model_id: str
    assessor: str
    assessment_date: datetime

    risks: List[RiskFactor] = field(default_factory=list)
    overall_risk_score: float = 0.0
    risk_tier: str = ""

    recommendations: List[str] = field(default_factory=list)
    accepted_risks: List[str] = field(default_factory=list)

    approved_by: Optional[str] = None
    approval_date: Optional[datetime] = None

    def calculate_overall_risk(self) -> float:
        """Calculate overall risk score."""
        if not self.risks:
            return 0.0

        # Weighted average with higher weight for high-impact risks
        total_weight = 0.0
        weighted_score = 0.0

        for risk in self.risks:
            weight = risk.impact.value
            weighted_score += risk.residual_score * weight
            total_weight += weight

        self.overall_risk_score = weighted_score / total_weight if total_weight > 0 else 0.0

        # Determine risk tier
        if self.overall_risk_score >= 15:
            self.risk_tier = "CRITICAL"
        elif self.overall_risk_score >= 10:
            self.risk_tier = "HIGH"
        elif self.overall_risk_score >= 5:
            self.risk_tier = "MEDIUM"
        else:
            self.risk_tier = "LOW"

        return self.overall_risk_score


class ModelRiskAssessor:
    """
    Automated model risk assessment.
    """

    # Risk templates by category
    RISK_TEMPLATES = {
        RiskCategory.MODEL_QUALITY: [
            {
                "id": "MQ-001",
                "description": "Model produces incorrect or hallucinated outputs",
                "assessment_questions": [
                    "Has the model been evaluated on domain-specific benchmarks?",
                    "What is the hallucination rate in testing?",
                    "Are there automated quality checks in production?"
                ]
            },
            {
                "id": "MQ-002",
                "description": "Model performance degrades over time (drift)",
                "assessment_questions": [
                    "Is input/output distribution monitored?",
                    "Are there alerts for performance degradation?",
                    "How often is the model re-evaluated?"
                ]
            }
        ],
        RiskCategory.SECURITY: [
            {
                "id": "SEC-001",
                "description": "Model vulnerable to prompt injection attacks",
                "assessment_questions": [
                    "Has the model been tested for prompt injection?",
                    "Are input sanitization controls in place?",
                    "Is system prompt protected from extraction?"
                ]
            },
            {
                "id": "SEC-002",
                "description": "Model leaks training data or sensitive information",
                "assessment_questions": [
                    "Has training data extraction been tested?",
                    "Are there output filtering controls?",
                    "Is PII detection in place?"
                ]
            }
        ],
        RiskCategory.FAIRNESS: [
            {
                "id": "FAIR-001",
                "description": "Model exhibits bias against protected groups",
                "assessment_questions": [
                    "Has bias testing been conducted?",
                    "Are performance metrics disaggregated by group?",
                    "What mitigation strategies are in place?"
                ]
            }
        ],
        RiskCategory.PRIVACY: [
            {
                "id": "PRIV-001",
                "description": "Model processes or generates PII inappropriately",
                "assessment_questions": [
                    "Is PII detection enabled?",
                    "What handling strategies are in place?",
                    "Are data retention policies defined?"
                ]
            }
        ],
        RiskCategory.OPERATIONAL: [
            {
                "id": "OPS-001",
                "description": "Model unavailability impacts business operations",
                "assessment_questions": [
                    "What is the SLA for model availability?",
                    "Are fallback mechanisms in place?",
                    "Is there capacity for demand spikes?"
                ]
            }
        ],
        RiskCategory.REGULATORY: [
            {
                "id": "REG-001",
                "description": "Model use violates applicable regulations",
                "assessment_questions": [
                    "What regulations apply to this use case?",
                    "Has legal review been completed?",
                    "Are compliance controls documented?"
                ]
            }
        ]
    }

    def __init__(self):
        self.assessments: Dict[str, RiskAssessment] = {}

    def create_assessment(
        self,
        model_id: str,
        assessor: str,
        model_info: Dict,
        questionnaire_responses: Dict[str, Dict]
    ) -> RiskAssessment:
        """Create a risk assessment from questionnaire responses."""
        assessment_id = f"RA-{model_id}-{datetime.utcnow().strftime('%Y%m%d')}"

        risks = []

        for category, templates in self.RISK_TEMPLATES.items():
            for template in templates:
                risk_id = template["id"]
                responses = questionnaire_responses.get(risk_id, {})

                # Score risk based on responses
                likelihood, impact, effectiveness = self._score_risk(
                    template, responses, model_info
                )

                controls = responses.get("controls", [])

                risk = RiskFactor(
                    risk_id=risk_id,
                    category=category,
                    description=template["description"],
                    likelihood=likelihood,
                    impact=impact,
                    controls=controls,
                    control_effectiveness=effectiveness,
                    owner=responses.get("owner")
                )
                risks.append(risk)

        assessment = RiskAssessment(
            assessment_id=assessment_id,
            model_id=model_id,
            assessor=assessor,
            assessment_date=datetime.utcnow(),
            risks=risks
        )

        assessment.calculate_overall_risk()
        assessment.recommendations = self._generate_recommendations(assessment)

        self.assessments[assessment_id] = assessment
        return assessment

    def _score_risk(
        self,
        template: Dict,
        responses: Dict,
        model_info: Dict
    ) -> Tuple[RiskLikelihood, RiskImpact, float]:
        """Score a risk based on responses."""
        # Default values
        likelihood = RiskLikelihood.POSSIBLE
        impact = RiskImpact.MODERATE
        effectiveness = 0.0

        # Use response-based scoring if available
        if "likelihood" in responses:
            likelihood = RiskLikelihood(responses["likelihood"])
        if "impact" in responses:
            impact = RiskImpact(responses["impact"])
        if "control_effectiveness" in responses:
            effectiveness = responses["control_effectiveness"]

        # Adjust based on model risk tier
        if model_info.get("risk_tier") == "TIER_1_CRITICAL":
            impact = RiskImpact(min(impact.value + 1, 5))

        return likelihood, impact, effectiveness

    def _generate_recommendations(self, assessment: RiskAssessment) -> List[str]:
        """Generate recommendations based on assessment."""
        recommendations = []

        # High residual risks
        high_risks = [r for r in assessment.risks if r.residual_score >= 10]
        for risk in high_risks:
            recommendations.append(
                f"[HIGH PRIORITY] Address {risk.risk_id}: {risk.description}. "
                f"Current controls insufficient (effectiveness: {risk.control_effectiveness:.0%})"
            )

        # Risks without controls
        no_controls = [r for r in assessment.risks if not r.controls]
        for risk in no_controls:
            recommendations.append(
                f"[MEDIUM PRIORITY] Implement controls for {risk.risk_id}: {risk.description}"
            )

        # General recommendations based on tier
        if assessment.risk_tier == "CRITICAL":
            recommendations.append(
                "CRITICAL risk tier: Require executive approval before deployment"
            )
            recommendations.append(
                "CRITICAL risk tier: Implement continuous monitoring with automated alerts"
            )
        elif assessment.risk_tier == "HIGH":
            recommendations.append(
                "HIGH risk tier: Conduct additional security and bias testing"
            )

        return recommendations

    def generate_risk_report(self, assessment_id: str) -> Dict:
        """Generate a risk assessment report."""
        assessment = self.assessments.get(assessment_id)
        if not assessment:
            raise ValueError(f"Assessment {assessment_id} not found")

        # Create risk matrix
        risk_matrix = {}
        for likelihood in RiskLikelihood:
            for impact in RiskImpact:
                key = f"{likelihood.name}_{impact.name}"
                risk_matrix[key] = []

        for risk in assessment.risks:
            key = f"{risk.likelihood.name}_{risk.impact.name}"
            risk_matrix[key].append(risk.risk_id)

        return {
            "report_date": datetime.utcnow().isoformat(),
            "assessment_id": assessment.assessment_id,
            "model_id": assessment.model_id,
            "assessor": assessment.assessor,
            "assessment_date": assessment.assessment_date.isoformat(),
            "summary": {
                "overall_risk_score": assessment.overall_risk_score,
                "risk_tier": assessment.risk_tier,
                "total_risks": len(assessment.risks),
                "high_risks": len([r for r in assessment.risks if r.residual_score >= 10]),
                "medium_risks": len([r for r in assessment.risks if 5 <= r.residual_score < 10]),
                "low_risks": len([r for r in assessment.risks if r.residual_score < 5])
            },
            "risk_details": [
                {
                    "risk_id": r.risk_id,
                    "category": r.category.value,
                    "description": r.description,
                    "inherent_score": r.inherent_score,
                    "controls": r.controls,
                    "control_effectiveness": r.control_effectiveness,
                    "residual_score": r.residual_score,
                    "owner": r.owner
                }
                for r in sorted(assessment.risks, key=lambda x: -x.residual_score)
            ],
            "risk_matrix": risk_matrix,
            "recommendations": assessment.recommendations,
            "accepted_risks": assessment.accepted_risks,
            "approval": {
                "approved_by": assessment.approved_by,
                "approval_date": assessment.approval_date.isoformat() if assessment.approval_date else None
            }
        }
```

---

## 4. Change Management

### 4.1 Model Change Management System

```python
"""
Change management for model updates and deployments.
"""
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Set
from enum import Enum
from datetime import datetime
import hashlib


class ChangeType(Enum):
    """Types of model changes."""
    MODEL_UPDATE = "model_update"           # New model version
    CONFIG_CHANGE = "config_change"         # Configuration change
    PROMPT_UPDATE = "prompt_update"         # System prompt change
    INFRASTRUCTURE = "infrastructure"       # Infrastructure change
    ROLLBACK = "rollback"                   # Rollback to previous version
    EMERGENCY = "emergency"                 # Emergency change


class ChangeStatus(Enum):
    """Change request status."""
    DRAFT = "draft"
    SUBMITTED = "submitted"
    UNDER_REVIEW = "under_review"
    APPROVED = "approved"
    REJECTED = "rejected"
    SCHEDULED = "scheduled"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"
    ROLLED_BACK = "rolled_back"


class ChangeRisk(Enum):
    """Risk level of change."""
    LOW = "low"           # Minimal impact, easily reversible
    MEDIUM = "medium"     # Moderate impact, reversible
    HIGH = "high"         # Significant impact, complex rollback
    CRITICAL = "critical" # Major impact, requires extensive planning


@dataclass
class ChangeImpact:
    """Impact assessment for a change."""
    affected_systems: List[str]
    affected_users: str  # e.g., "All users", "Internal only", "Specific teams"
    estimated_downtime: str  # e.g., "None", "< 5 minutes", "30 minutes"
    rollback_time: str
    testing_required: List[str]
    dependencies: List[str]


@dataclass
class ChangeRequest:
    """Model change request."""
    change_id: str
    change_type: ChangeType
    title: str
    description: str
    justification: str

    model_id: str
    current_version: str
    target_version: str

    requester: str
    created_at: datetime

    risk_level: ChangeRisk = ChangeRisk.MEDIUM
    impact: Optional[ChangeImpact] = None

    status: ChangeStatus = ChangeStatus.DRAFT

    reviewers: List[str] = field(default_factory=list)
    approvals: Dict[str, Dict] = field(default_factory=dict)

    scheduled_time: Optional[datetime] = None
    completed_time: Optional[datetime] = None

    test_results: Dict[str, Any] = field(default_factory=dict)
    deployment_notes: str = ""
    rollback_plan: str = ""

    comments: List[Dict] = field(default_factory=list)


class ModelChangeManager:
    """
    Manages model change requests and approvals.
    """

    def __init__(self, config: Dict):
        self.config = config
        self.changes: Dict[str, ChangeRequest] = {}

        # Approval requirements by risk level
        self.approval_requirements = {
            ChangeRisk.LOW: ["model_owner"],
            ChangeRisk.MEDIUM: ["model_owner", "tech_lead"],
            ChangeRisk.HIGH: ["model_owner", "tech_lead", "security_reviewer"],
            ChangeRisk.CRITICAL: ["model_owner", "tech_lead", "security_reviewer",
                                 "business_owner", "risk_manager"]
        }

    def create_change_request(
        self,
        change_type: ChangeType,
        title: str,
        description: str,
        justification: str,
        model_id: str,
        current_version: str,
        target_version: str,
        requester: str,
        impact: Optional[ChangeImpact] = None
    ) -> ChangeRequest:
        """Create a new change request."""
        change_id = f"CR-{datetime.utcnow().strftime('%Y%m%d%H%M%S')}-{model_id[:8]}"

        # Auto-assess risk level
        risk_level = self._assess_risk(change_type, impact)

        # Get required reviewers
        reviewers = self.approval_requirements[risk_level]

        change = ChangeRequest(
            change_id=change_id,
            change_type=change_type,
            title=title,
            description=description,
            justification=justification,
            model_id=model_id,
            current_version=current_version,
            target_version=target_version,
            requester=requester,
            created_at=datetime.utcnow(),
            risk_level=risk_level,
            impact=impact,
            reviewers=reviewers
        )

        self.changes[change_id] = change
        return change

    def _assess_risk(
        self,
        change_type: ChangeType,
        impact: Optional[ChangeImpact]
    ) -> ChangeRisk:
        """Assess risk level of change."""
        # Emergency changes are always critical
        if change_type == ChangeType.EMERGENCY:
            return ChangeRisk.CRITICAL

        # Rollbacks are typically medium risk
        if change_type == ChangeType.ROLLBACK:
            return ChangeRisk.MEDIUM

        if impact:
            # Assess based on impact
            if impact.estimated_downtime != "None":
                return ChangeRisk.HIGH
            if "production" in str(impact.affected_systems).lower():
                return ChangeRisk.HIGH
            if impact.affected_users == "All users":
                return ChangeRisk.MEDIUM

        # Default risk levels by change type
        type_risks = {
            ChangeType.MODEL_UPDATE: ChangeRisk.HIGH,
            ChangeType.CONFIG_CHANGE: ChangeRisk.MEDIUM,
            ChangeType.PROMPT_UPDATE: ChangeRisk.MEDIUM,
            ChangeType.INFRASTRUCTURE: ChangeRisk.HIGH
        }

        return type_risks.get(change_type, ChangeRisk.MEDIUM)

    def submit_for_review(self, change_id: str) -> bool:
        """Submit change request for review."""
        change = self.changes.get(change_id)
        if not change:
            raise ValueError(f"Change {change_id} not found")

        if change.status != ChangeStatus.DRAFT:
            raise ValueError(f"Change must be in DRAFT status to submit")

        # Validate required fields
        if not change.impact:
            raise ValueError("Impact assessment required before submission")
        if not change.rollback_plan:
            raise ValueError("Rollback plan required before submission")

        change.status = ChangeStatus.SUBMITTED
        return True

    def add_approval(
        self,
        change_id: str,
        reviewer: str,
        approved: bool,
        comments: str
    ) -> Dict:
        """Add approval to change request."""
        change = self.changes.get(change_id)
        if not change:
            raise ValueError(f"Change {change_id} not found")

        if reviewer not in change.reviewers:
            raise PermissionError(f"{reviewer} is not a required reviewer")

        change.approvals[reviewer] = {
            "approved": approved,
            "comments": comments,
            "timestamp": datetime.utcnow().isoformat()
        }

        # Check if all approvals received
        if self._check_all_approved(change):
            change.status = ChangeStatus.APPROVED
        elif any(not a["approved"] for a in change.approvals.values()):
            change.status = ChangeStatus.REJECTED
        else:
            change.status = ChangeStatus.UNDER_REVIEW

        return {
            "change_id": change_id,
            "status": change.status.value,
            "approvals_received": len(change.approvals),
            "approvals_required": len(change.reviewers)
        }

    def _check_all_approved(self, change: ChangeRequest) -> bool:
        """Check if all required approvals are received."""
        for reviewer in change.reviewers:
            if reviewer not in change.approvals:
                return False
            if not change.approvals[reviewer]["approved"]:
                return False
        return True

    def schedule_deployment(
        self,
        change_id: str,
        scheduled_time: datetime
    ) -> bool:
        """Schedule approved change for deployment."""
        change = self.changes.get(change_id)
        if not change:
            raise ValueError(f"Change {change_id} not found")

        if change.status != ChangeStatus.APPROVED:
            raise ValueError("Only approved changes can be scheduled")

        # Validate scheduling window
        if scheduled_time < datetime.utcnow():
            raise ValueError("Scheduled time must be in the future")

        change.scheduled_time = scheduled_time
        change.status = ChangeStatus.SCHEDULED

        return True

    def start_deployment(self, change_id: str) -> Dict:
        """Start deployment of scheduled change."""
        change = self.changes.get(change_id)
        if not change:
            raise ValueError(f"Change {change_id} not found")

        if change.status not in [ChangeStatus.SCHEDULED, ChangeStatus.APPROVED]:
            raise ValueError("Change must be approved or scheduled to deploy")

        change.status = ChangeStatus.IN_PROGRESS

        return {
            "change_id": change_id,
            "status": "in_progress",
            "model_id": change.model_id,
            "target_version": change.target_version,
            "rollback_plan": change.rollback_plan
        }

    def complete_deployment(
        self,
        change_id: str,
        success: bool,
        notes: str
    ) -> Dict:
        """Complete or fail deployment."""
        change = self.changes.get(change_id)
        if not change:
            raise ValueError(f"Change {change_id} not found")

        if change.status != ChangeStatus.IN_PROGRESS:
            raise ValueError("Change must be in progress to complete")

        change.completed_time = datetime.utcnow()
        change.deployment_notes = notes

        if success:
            change.status = ChangeStatus.COMPLETED
        else:
            change.status = ChangeStatus.FAILED

        return {
            "change_id": change_id,
            "status": change.status.value,
            "completed_at": change.completed_time.isoformat(),
            "notes": notes
        }

    def rollback(
        self,
        change_id: str,
        reason: str
    ) -> ChangeRequest:
        """Create rollback for a completed change."""
        original = self.changes.get(change_id)
        if not original:
            raise ValueError(f"Change {change_id} not found")

        # Create rollback change request
        rollback = self.create_change_request(
            change_type=ChangeType.ROLLBACK,
            title=f"Rollback: {original.title}",
            description=f"Rolling back change {change_id}: {reason}",
            justification=reason,
            model_id=original.model_id,
            current_version=original.target_version,
            target_version=original.current_version,
            requester=original.requester,
            impact=original.impact
        )

        # Rollbacks get expedited review
        rollback.risk_level = ChangeRisk.MEDIUM

        return rollback

    def get_change_history(
        self,
        model_id: Optional[str] = None,
        status: Optional[ChangeStatus] = None,
        since: Optional[datetime] = None
    ) -> List[Dict]:
        """Get change history with optional filters."""
        results = []

        for change in self.changes.values():
            if model_id and change.model_id != model_id:
                continue
            if status and change.status != status:
                continue
            if since and change.created_at < since:
                continue

            results.append({
                "change_id": change.change_id,
                "change_type": change.change_type.value,
                "title": change.title,
                "model_id": change.model_id,
                "current_version": change.current_version,
                "target_version": change.target_version,
                "risk_level": change.risk_level.value,
                "status": change.status.value,
                "requester": change.requester,
                "created_at": change.created_at.isoformat(),
                "completed_at": change.completed_time.isoformat() if change.completed_time else None
            })

        return sorted(results, key=lambda x: x["created_at"], reverse=True)
```

---

## 5. Audit & Accountability

### 5.1 Audit Trail System

```python
"""
Comprehensive audit trail for model governance.
"""
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any
from datetime import datetime
from enum import Enum
import hashlib
import json


class AuditEventType(Enum):
    """Types of auditable events."""
    # Model lifecycle
    MODEL_REGISTERED = "model_registered"
    MODEL_UPDATED = "model_updated"
    MODEL_DEPLOYED = "model_deployed"
    MODEL_RETIRED = "model_retired"

    # Governance
    APPROVAL_REQUESTED = "approval_requested"
    APPROVAL_GRANTED = "approval_granted"
    APPROVAL_DENIED = "approval_denied"

    # Access
    ACCESS_GRANTED = "access_granted"
    ACCESS_REVOKED = "access_revoked"
    ACCESS_ATTEMPT = "access_attempt"

    # Changes
    CHANGE_REQUESTED = "change_requested"
    CHANGE_APPROVED = "change_approved"
    CHANGE_DEPLOYED = "change_deployed"
    CHANGE_ROLLED_BACK = "change_rolled_back"

    # Risk
    RISK_ASSESSED = "risk_assessed"
    RISK_ACCEPTED = "risk_accepted"
    RISK_MITIGATED = "risk_mitigated"

    # Compliance
    COMPLIANCE_CHECK = "compliance_check"
    POLICY_VIOLATION = "policy_violation"

    # Incidents
    INCIDENT_CREATED = "incident_created"
    INCIDENT_RESOLVED = "incident_resolved"


@dataclass
class AuditEvent:
    """Single audit event."""
    event_id: str
    event_type: AuditEventType
    timestamp: datetime

    actor: str  # User or system that triggered event
    actor_type: str  # "user", "system", "automated"

    resource_type: str  # "model", "change_request", "policy", etc.
    resource_id: str

    action: str  # Human-readable action description
    details: Dict[str, Any]

    outcome: str  # "success", "failure", "partial"

    ip_address: Optional[str] = None
    session_id: Optional[str] = None

    # For integrity verification
    previous_hash: Optional[str] = None
    event_hash: Optional[str] = None

    def __post_init__(self):
        if not self.event_hash:
            self.event_hash = self._compute_hash()

    def _compute_hash(self) -> str:
        """Compute hash for integrity verification."""
        data = {
            "event_id": self.event_id,
            "event_type": self.event_type.value,
            "timestamp": self.timestamp.isoformat(),
            "actor": self.actor,
            "resource_id": self.resource_id,
            "action": self.action,
            "previous_hash": self.previous_hash
        }
        return hashlib.sha256(json.dumps(data, sort_keys=True).encode()).hexdigest()


class AuditTrailManager:
    """
    Manages audit trail with integrity verification.
    """

    def __init__(self, storage_backend: Any = None):
        self.storage = storage_backend or {}
        self.events: List[AuditEvent] = []
        self.last_hash: Optional[str] = None

    def log_event(
        self,
        event_type: AuditEventType,
        actor: str,
        actor_type: str,
        resource_type: str,
        resource_id: str,
        action: str,
        details: Dict[str, Any],
        outcome: str = "success",
        ip_address: Optional[str] = None,
        session_id: Optional[str] = None
    ) -> AuditEvent:
        """Log an audit event."""
        event_id = f"AUD-{datetime.utcnow().strftime('%Y%m%d%H%M%S%f')}"

        event = AuditEvent(
            event_id=event_id,
            event_type=event_type,
            timestamp=datetime.utcnow(),
            actor=actor,
            actor_type=actor_type,
            resource_type=resource_type,
            resource_id=resource_id,
            action=action,
            details=details,
            outcome=outcome,
            ip_address=ip_address,
            session_id=session_id,
            previous_hash=self.last_hash
        )

        self.events.append(event)
        self.last_hash = event.event_hash

        # Persist to storage
        self._persist_event(event)

        return event

    def _persist_event(self, event: AuditEvent):
        """Persist event to storage backend."""
        # In production, this would write to a secure, immutable log
        event_data = {
            "event_id": event.event_id,
            "event_type": event.event_type.value,
            "timestamp": event.timestamp.isoformat(),
            "actor": event.actor,
            "actor_type": event.actor_type,
            "resource_type": event.resource_type,
            "resource_id": event.resource_id,
            "action": event.action,
            "details": event.details,
            "outcome": event.outcome,
            "ip_address": event.ip_address,
            "session_id": event.session_id,
            "previous_hash": event.previous_hash,
            "event_hash": event.event_hash
        }

        self.storage[event.event_id] = event_data

    def verify_chain_integrity(self) -> Dict:
        """Verify audit trail integrity."""
        if not self.events:
            return {"valid": True, "events_verified": 0}

        broken_links = []
        previous_hash = None

        for i, event in enumerate(self.events):
            # Verify hash chain
            if event.previous_hash != previous_hash:
                broken_links.append({
                    "index": i,
                    "event_id": event.event_id,
                    "expected_previous": previous_hash,
                    "actual_previous": event.previous_hash
                })

            # Verify event hash
            computed_hash = event._compute_hash()
            if computed_hash != event.event_hash:
                broken_links.append({
                    "index": i,
                    "event_id": event.event_id,
                    "issue": "hash_mismatch"
                })

            previous_hash = event.event_hash

        return {
            "valid": len(broken_links) == 0,
            "events_verified": len(self.events),
            "broken_links": broken_links
        }

    def query_events(
        self,
        event_type: Optional[AuditEventType] = None,
        actor: Optional[str] = None,
        resource_id: Optional[str] = None,
        start_time: Optional[datetime] = None,
        end_time: Optional[datetime] = None,
        limit: int = 100
    ) -> List[Dict]:
        """Query audit events with filters."""
        results = []

        for event in reversed(self.events):
            if event_type and event.event_type != event_type:
                continue
            if actor and event.actor != actor:
                continue
            if resource_id and event.resource_id != resource_id:
                continue
            if start_time and event.timestamp < start_time:
                continue
            if end_time and event.timestamp > end_time:
                continue

            results.append({
                "event_id": event.event_id,
                "event_type": event.event_type.value,
                "timestamp": event.timestamp.isoformat(),
                "actor": event.actor,
                "resource_type": event.resource_type,
                "resource_id": event.resource_id,
                "action": event.action,
                "outcome": event.outcome
            })

            if len(results) >= limit:
                break

        return results

    def generate_audit_report(
        self,
        resource_id: str,
        start_time: Optional[datetime] = None,
        end_time: Optional[datetime] = None
    ) -> Dict:
        """Generate comprehensive audit report for a resource."""
        events = self.query_events(
            resource_id=resource_id,
            start_time=start_time,
            end_time=end_time,
            limit=1000
        )

        # Analyze events
        event_counts = {}
        actors = set()
        outcomes = {"success": 0, "failure": 0, "partial": 0}

        for event in events:
            event_type = event["event_type"]
            event_counts[event_type] = event_counts.get(event_type, 0) + 1
            actors.add(event["actor"])
            outcome = event.get("outcome", "success")
            outcomes[outcome] = outcomes.get(outcome, 0) + 1

        return {
            "report_generated": datetime.utcnow().isoformat(),
            "resource_id": resource_id,
            "period": {
                "start": start_time.isoformat() if start_time else "all time",
                "end": end_time.isoformat() if end_time else "present"
            },
            "summary": {
                "total_events": len(events),
                "unique_actors": len(actors),
                "outcomes": outcomes
            },
            "event_breakdown": event_counts,
            "actors": list(actors),
            "events": events[:50],  # Last 50 events
            "integrity": self.verify_chain_integrity()
        }
```

### 5.2 Accountability Assignment

```python
class AccountabilityManager:
    """
    Manages accountability assignments for models.
    """

    def __init__(self, governance_framework: GovernanceFramework):
        self.framework = governance_framework
        self.assignments: Dict[str, Dict[str, str]] = {}  # model_id -> {role -> user_id}

    def assign_accountability(
        self,
        model_id: str,
        role: GovernanceRole,
        user_id: str,
        assigned_by: str
    ) -> Dict:
        """Assign accountability for a role on a model."""
        if model_id not in self.assignments:
            self.assignments[model_id] = {}

        previous = self.assignments[model_id].get(role.value)
        self.assignments[model_id][role.value] = user_id

        return {
            "model_id": model_id,
            "role": role.value,
            "user_id": user_id,
            "previous_user": previous,
            "assigned_by": assigned_by,
            "timestamp": datetime.utcnow().isoformat()
        }

    def get_accountable_party(self, model_id: str, role: GovernanceRole) -> Optional[str]:
        """Get accountable party for a role."""
        return self.assignments.get(model_id, {}).get(role.value)

    def get_all_accountable_parties(self, model_id: str) -> Dict[str, str]:
        """Get all accountable parties for a model."""
        return self.assignments.get(model_id, {})

    def verify_accountability_coverage(self, model_id: str) -> Dict:
        """Verify all required roles have assignments."""
        record = self.framework.records.get(model_id)
        if not record:
            raise ValueError(f"Model {model_id} not found")

        required_roles = self._get_required_roles(record.risk_tier)
        assignments = self.assignments.get(model_id, {})

        missing = []
        assigned = []

        for role in required_roles:
            if role.value in assignments:
                assigned.append({
                    "role": role.value,
                    "user_id": assignments[role.value]
                })
            else:
                missing.append(role.value)

        return {
            "model_id": model_id,
            "risk_tier": record.risk_tier.value,
            "complete": len(missing) == 0,
            "assigned_roles": assigned,
            "missing_roles": missing
        }

    def _get_required_roles(self, risk_tier: ModelRiskTier) -> List[GovernanceRole]:
        """Get required roles based on risk tier."""
        base_roles = [
            GovernanceRole.MODEL_OWNER,
            GovernanceRole.DATA_STEWARD
        ]

        if risk_tier in [ModelRiskTier.TIER_1_CRITICAL, ModelRiskTier.TIER_2_IMPORTANT]:
            base_roles.extend([
                GovernanceRole.ETHICS_REVIEWER,
                GovernanceRole.SECURITY_REVIEWER,
                GovernanceRole.BUSINESS_OWNER
            ])

        if risk_tier == ModelRiskTier.TIER_1_CRITICAL:
            base_roles.extend([
                GovernanceRole.COMPLIANCE_OFFICER,
                GovernanceRole.RISK_MANAGER
            ])

        return base_roles
```

---

## 6. Implementation

### 6.1 Complete Governance Implementation

```python
"""
Complete model governance implementation example.
"""

def setup_governance_system() -> Dict:
    """Set up complete governance system."""
    # Initialize framework
    framework = GovernanceFramework(config={})

    # Register stakeholders
    stakeholders = [
        Stakeholder(
            user_id="user-001",
            name="Alice Smith",
            email="alice@company.com",
            role=GovernanceRole.MODEL_OWNER,
            department="ML Engineering",
            authorized_tiers=[ModelRiskTier.TIER_1_CRITICAL,
                            ModelRiskTier.TIER_2_IMPORTANT,
                            ModelRiskTier.TIER_3_STANDARD]
        ),
        Stakeholder(
            user_id="user-002",
            name="Bob Johnson",
            email="bob@company.com",
            role=GovernanceRole.DATA_STEWARD,
            department="Data Engineering",
            authorized_tiers=[ModelRiskTier.TIER_1_CRITICAL,
                            ModelRiskTier.TIER_2_IMPORTANT,
                            ModelRiskTier.TIER_3_STANDARD]
        ),
        Stakeholder(
            user_id="user-003",
            name="Carol Williams",
            email="carol@company.com",
            role=GovernanceRole.ETHICS_REVIEWER,
            department="AI Ethics",
            authorized_tiers=[ModelRiskTier.TIER_1_CRITICAL,
                            ModelRiskTier.TIER_2_IMPORTANT]
        ),
        Stakeholder(
            user_id="user-004",
            name="David Brown",
            email="david@company.com",
            role=GovernanceRole.SECURITY_REVIEWER,
            department="Security",
            authorized_tiers=[ModelRiskTier.TIER_1_CRITICAL,
                            ModelRiskTier.TIER_2_IMPORTANT,
                            ModelRiskTier.TIER_3_STANDARD]
        )
    ]

    for s in stakeholders:
        framework.stakeholders[s.user_id] = s

    # Initialize audit trail
    audit_trail = AuditTrailManager()

    # Initialize risk assessor
    risk_assessor = ModelRiskAssessor()

    # Initialize change manager
    change_manager = ModelChangeManager(config={})

    return {
        "framework": framework,
        "audit_trail": audit_trail,
        "risk_assessor": risk_assessor,
        "change_manager": change_manager
    }


def example_model_governance_workflow():
    """Example workflow for governing a new model."""
    # Set up system
    system = setup_governance_system()
    framework = system["framework"]
    audit_trail = system["audit_trail"]
    risk_assessor = system["risk_assessor"]
    change_manager = system["change_manager"]

    # Step 1: Register model
    record = framework.register_model(
        model_id="model-customer-support-v1",
        model_name="Customer Support Assistant",
        version="1.0.0",
        owner_id="user-001",
        data_steward_id="user-002",
        purpose="Automated customer support for common queries",
        use_cases=[
            "Answer product FAQs",
            "Handle order status inquiries",
            "Process refund requests"
        ]
    )

    # Log registration
    audit_trail.log_event(
        event_type=AuditEventType.MODEL_REGISTERED,
        actor="user-001",
        actor_type="user",
        resource_type="model",
        resource_id=record.model_id,
        action="Registered new model for governance",
        details={"version": record.version, "risk_tier": record.risk_tier.value}
    )

    print(f"Model registered: {record.model_id}, Risk Tier: {record.risk_tier.value}")

    # Step 2: Conduct risk assessment
    questionnaire_responses = {
        "MQ-001": {
            "likelihood": 3,
            "impact": 4,
            "control_effectiveness": 0.6,
            "controls": ["Hallucination detection", "Confidence scoring"],
            "owner": "user-001"
        },
        "SEC-001": {
            "likelihood": 3,
            "impact": 4,
            "control_effectiveness": 0.7,
            "controls": ["Input sanitization", "System prompt protection"],
            "owner": "user-004"
        },
        "FAIR-001": {
            "likelihood": 2,
            "impact": 4,
            "control_effectiveness": 0.5,
            "controls": ["Bias testing completed"],
            "owner": "user-003"
        }
    }

    assessment = risk_assessor.create_assessment(
        model_id=record.model_id,
        assessor="user-001",
        model_info={"risk_tier": record.risk_tier.value},
        questionnaire_responses=questionnaire_responses
    )

    print(f"Risk Assessment: Score={assessment.overall_risk_score:.2f}, "
          f"Tier={assessment.risk_tier}")

    # Step 3: Get required approvals
    required = framework.get_required_approvals(record.model_id)
    print(f"Required approvals: {[r.value for r in required.keys()]}")

    # Step 4: Submit approvals
    for reviewer_id in ["user-003", "user-004"]:  # Ethics and Security
        reviewer = framework.stakeholders[reviewer_id]
        decision = framework.submit_approval(
            model_id=record.model_id,
            reviewer=reviewer,
            status=ApprovalStatus.APPROVED,
            comments="Reviewed and approved with conditions",
            conditions=["Monitor for 30 days post-deployment"]
        )

        audit_trail.log_event(
            event_type=AuditEventType.APPROVAL_GRANTED,
            actor=reviewer_id,
            actor_type="user",
            resource_type="model",
            resource_id=record.model_id,
            action=f"{reviewer.role.value} approval granted",
            details={"conditions": decision.conditions}
        )

    # Step 5: Check deployment eligibility
    eligibility = framework.check_deployment_eligibility(record.model_id)
    print(f"Deployment eligible: {eligibility['eligible']}")

    # Step 6: Create change request for deployment
    if eligibility['eligible']:
        change = change_manager.create_change_request(
            change_type=ChangeType.MODEL_UPDATE,
            title="Deploy Customer Support Assistant v1.0.0",
            description="Initial deployment of customer support model",
            justification="Enable automated customer support",
            model_id=record.model_id,
            current_version="none",
            target_version="1.0.0",
            requester="user-001",
            impact=ChangeImpact(
                affected_systems=["customer-portal", "support-api"],
                affected_users="All customer support users",
                estimated_downtime="None",
                rollback_time="5 minutes",
                testing_required=["Integration tests", "Load tests"],
                dependencies=["Vector DB", "LLM API"]
            )
        )

        change.rollback_plan = "Revert to previous version, switch traffic back"

        print(f"Change request created: {change.change_id}")

    # Generate governance report
    report = framework.generate_governance_report(record.model_id)
    print(f"\nGovernance Report Generated:")
    print(f"  Status: {report['model_info']['status']}")
    print(f"  Approvals: {len(report['governance']['approvals'])}")
    print(f"  Deployment Eligible: {report['deployment_eligibility']['eligible']}")

    return report


if __name__ == "__main__":
    report = example_model_governance_workflow()
```

---

## Appendices

### Appendix A: Model Card Template

```markdown
# Model Card: [Model Name]

## Model Details
- **Model ID:** [unique identifier]
- **Version:** [semantic version]
- **Type:** [model type]
- **Architecture:** [architecture details]
- **Developers:** [team/individuals]
- **Release Date:** [date]
- **License:** [license type]
- **Contact:** [email]

## Intended Use
### Primary Uses
- [Use case 1]
- [Use case 2]

### Users
- [User group 1]
- [User group 2]

### Out of Scope
- [Out of scope use 1]
- [Out of scope use 2]

## Training Data
[Description of training data]

## Evaluation
### Metrics
| Metric | Value | Dataset |
|--------|-------|---------|
| [Metric 1] | [Value] | [Dataset] |

## Ethical Considerations
- [Consideration 1]

## Limitations
- [Limitation 1]

## Recommendations
- [Recommendation 1]
```

### Appendix B: Risk Assessment Template

```yaml
risk_assessment:
  model_id: ""
  assessment_date: ""
  assessor: ""

  risks:
    - risk_id: ""
      category: ""  # model_quality, security, privacy, fairness, etc.
      description: ""
      likelihood: 1-5
      impact: 1-5
      inherent_score: calculated
      controls:
        - ""
      control_effectiveness: 0.0-1.0
      residual_score: calculated
      owner: ""

  overall:
    risk_score: calculated
    risk_tier: ""  # LOW, MEDIUM, HIGH, CRITICAL

  recommendations:
    - ""

  accepted_risks:
    - risk_id: ""
      rationale: ""
      accepted_by: ""
      expiration: ""
```

### Appendix C: Change Request Form

```yaml
change_request:
  # Basic Information
  change_type: ""  # model_update, config_change, prompt_update, etc.
  title: ""
  description: ""
  justification: ""

  # Model Details
  model_id: ""
  current_version: ""
  target_version: ""

  # Impact Assessment
  impact:
    affected_systems: []
    affected_users: ""
    estimated_downtime: ""
    rollback_time: ""
    testing_required: []
    dependencies: []

  # Risk
  risk_level: ""  # low, medium, high, critical

  # Rollback Plan
  rollback_plan: ""
  rollback_trigger: ""

  # Schedule
  preferred_window: ""
  blackout_dates: []

  # Requester
  requester: ""
  department: ""
  date: ""
```

### Appendix D: Governance Dashboard Metrics

```yaml
governance_dashboard:
  model_inventory:
    - total_models: count
    - by_risk_tier:
        tier_1_critical: count
        tier_2_important: count
        tier_3_standard: count
        tier_4_experimental: count
    - pending_approval: count
    - overdue_review: count

  compliance:
    - models_with_model_cards: percentage
    - models_with_risk_assessments: percentage
    - models_with_all_approvals: percentage
    - policy_violations: count

  change_management:
    - open_changes: count
    - pending_approval: count
    - deployed_this_month: count
    - rollbacks_this_month: count

  risk:
    - high_risk_models: count
    - unmitigated_risks: count
    - accepted_risks: count
    - risks_trending_up: count

  audit:
    - audit_events_today: count
    - policy_violations: count
    - access_anomalies: count
    - chain_integrity: boolean
```

---

## References

1. Mitchell, M., et al. (2019). Model Cards for Model Reporting
2. Gebru, T., et al. (2018). Datasheets for Datasets
3. NIST AI Risk Management Framework
4. EU AI Act Requirements
5. ISO/IEC 42001 AI Management System Standard
