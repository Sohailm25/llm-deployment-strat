> **Navigation** | [← 11.2 PII & Privacy](11.2_pii_data_privacy_guide.md) | [11.4 Model Governance →](11.4_model_governance_guide.md)
>
> | | |
> |---|---|
> | **Prerequisites** | [11.1-11.2 Security & Privacy](11.1_llm_security_guide.md) &#124; Audit experience |
> | **Related** | [11.4 Model Governance](11.4_model_governance_guide.md) &#124; [4.4 Bias & Fairness](../04_alignment_safety/4.4_bias_fairness_evaluation.md) |
> | **Next** | [11.4 Model Governance](11.4_model_governance_guide.md) |

# Document 11.3: Compliance Framework Guide

## Purpose

This guide provides comprehensive strategies for achieving and maintaining compliance certifications for LLM systems, including SOC 2, ISO 27001, GDPR, and industry-specific requirements.

## Prerequisites

- Understanding of LLM system architecture
- Familiarity with compliance frameworks and audits
- Knowledge of security controls and risk management
- Experience with documentation and evidence collection

## 11.3.1 Compliance Framework Overview

### Framework Comparison

```python
"""
Compliance framework definitions and mappings.
"""

from dataclasses import dataclass, field
from typing import Dict, Any, List, Optional, Set
from enum import Enum


class ComplianceFramework(Enum):
    """Supported compliance frameworks."""
    SOC2_TYPE1 = "soc2_type1"
    SOC2_TYPE2 = "soc2_type2"
    ISO_27001 = "iso_27001"
    ISO_27701 = "iso_27701"
    GDPR = "gdpr"
    HIPAA = "hipaa"
    PCI_DSS = "pci_dss"
    NIST_AI_RMF = "nist_ai_rmf"
    EU_AI_ACT = "eu_ai_act"


@dataclass
class ControlDomain:
    """A domain of controls within a framework."""
    id: str
    name: str
    description: str
    controls: List[str]


@dataclass
class ComplianceControl:
    """Individual compliance control."""
    id: str
    framework: ComplianceFramework
    domain: str
    title: str
    description: str
    implementation_guidance: str
    evidence_required: List[str]
    testing_procedures: List[str]
    llm_specific_considerations: Optional[str] = None


class FrameworkRegistry:
    """
    Registry of compliance frameworks and controls.
    """

    # SOC 2 Trust Service Criteria
    SOC2_DOMAINS = [
        ControlDomain(
            id="CC",
            name="Common Criteria",
            description="Organization and management controls",
            controls=["CC1", "CC2", "CC3", "CC4", "CC5", "CC6", "CC7", "CC8", "CC9"]
        ),
        ControlDomain(
            id="A",
            name="Availability",
            description="System availability controls",
            controls=["A1"]
        ),
        ControlDomain(
            id="PI",
            name="Processing Integrity",
            description="Data processing accuracy controls",
            controls=["PI1"]
        ),
        ControlDomain(
            id="C",
            name="Confidentiality",
            description="Data confidentiality controls",
            controls=["C1"]
        ),
        ControlDomain(
            id="P",
            name="Privacy",
            description="Privacy controls",
            controls=["P1", "P2", "P3", "P4", "P5", "P6", "P7", "P8"]
        ),
    ]

    # ISO 27001 Control Domains
    ISO27001_DOMAINS = [
        ControlDomain(
            id="A.5",
            name="Information Security Policies",
            description="Management direction for information security",
            controls=["A.5.1"]
        ),
        ControlDomain(
            id="A.6",
            name="Organization of Information Security",
            description="Internal organization and mobile devices",
            controls=["A.6.1", "A.6.2"]
        ),
        ControlDomain(
            id="A.7",
            name="Human Resource Security",
            description="Prior to, during, and termination of employment",
            controls=["A.7.1", "A.7.2", "A.7.3"]
        ),
        ControlDomain(
            id="A.8",
            name="Asset Management",
            description="Responsibility for assets and classification",
            controls=["A.8.1", "A.8.2", "A.8.3"]
        ),
        ControlDomain(
            id="A.9",
            name="Access Control",
            description="Business requirements and user access management",
            controls=["A.9.1", "A.9.2", "A.9.3", "A.9.4"]
        ),
        ControlDomain(
            id="A.10",
            name="Cryptography",
            description="Cryptographic controls",
            controls=["A.10.1"]
        ),
        ControlDomain(
            id="A.12",
            name="Operations Security",
            description="Operational procedures and responsibilities",
            controls=["A.12.1", "A.12.2", "A.12.3", "A.12.4", "A.12.5", "A.12.6", "A.12.7"]
        ),
        ControlDomain(
            id="A.14",
            name="System Development",
            description="Security requirements and secure development",
            controls=["A.14.1", "A.14.2", "A.14.3"]
        ),
        ControlDomain(
            id="A.16",
            name="Incident Management",
            description="Management of security incidents",
            controls=["A.16.1"]
        ),
        ControlDomain(
            id="A.18",
            name="Compliance",
            description="Compliance with legal and contractual requirements",
            controls=["A.18.1", "A.18.2"]
        ),
    ]

    # LLM-specific controls for SOC 2
    LLM_SPECIFIC_CONTROLS = [
        ComplianceControl(
            id="LLM-CC6.1",
            framework=ComplianceFramework.SOC2_TYPE2,
            domain="CC6",
            title="Logical Access Security for AI Systems",
            description="Logical access to AI/ML systems and training data is restricted",
            implementation_guidance="""
            1. Implement RBAC for model access
            2. Separate model development from production
            3. Control access to training data
            4. Log all model API access
            """,
            evidence_required=[
                "Access control matrix for AI systems",
                "Model access logs",
                "Training data access logs",
                "RBAC documentation"
            ],
            testing_procedures=[
                "Review access control configurations",
                "Test role separation",
                "Verify logging completeness"
            ],
            llm_specific_considerations="""
            For LLM systems:
            - Control access to system prompts
            - Protect model weights and configurations
            - Separate fine-tuning environments
            - Monitor for unauthorized model access
            """
        ),
        ComplianceControl(
            id="LLM-PI1.1",
            framework=ComplianceFramework.SOC2_TYPE2,
            domain="PI1",
            title="AI Output Validation",
            description="AI system outputs are validated for accuracy and completeness",
            implementation_guidance="""
            1. Implement output validation checks
            2. Monitor model quality metrics
            3. Establish human review processes
            4. Track model drift
            """,
            evidence_required=[
                "Output validation procedures",
                "Quality monitoring dashboards",
                "Human review records",
                "Drift detection reports"
            ],
            testing_procedures=[
                "Review validation logic",
                "Test quality monitoring",
                "Verify human review process"
            ],
            llm_specific_considerations="""
            For LLM systems:
            - Validate response format compliance
            - Check for hallucination indicators
            - Monitor factual accuracy
            - Implement safety filters on outputs
            """
        ),
        ComplianceControl(
            id="LLM-C1.1",
            framework=ComplianceFramework.SOC2_TYPE2,
            domain="C1",
            title="AI Data Confidentiality",
            description="Confidential information processed by AI is protected",
            implementation_guidance="""
            1. Classify data processed by AI
            2. Implement PII detection and handling
            3. Control prompt/response logging
            4. Encrypt data at rest and in transit
            """,
            evidence_required=[
                "Data classification policy",
                "PII handling procedures",
                "Encryption configurations",
                "Logging policies"
            ],
            testing_procedures=[
                "Test PII detection accuracy",
                "Verify encryption implementation",
                "Review logging practices"
            ],
            llm_specific_considerations="""
            For LLM systems:
            - Prevent training data memorization
            - Filter PII from prompts and responses
            - Protect system prompt confidentiality
            - Implement data retention limits
            """
        ),
    ]

    def __init__(self):
        self.frameworks: Dict[ComplianceFramework, Dict] = {}
        self._initialize_frameworks()

    def _initialize_frameworks(self):
        """Initialize framework definitions."""
        self.frameworks[ComplianceFramework.SOC2_TYPE2] = {
            "name": "SOC 2 Type II",
            "description": "Service Organization Control 2 Type II",
            "domains": self.SOC2_DOMAINS,
            "audit_period": "12 months",
            "certification_body": "AICPA-licensed CPA firms"
        }

        self.frameworks[ComplianceFramework.ISO_27001] = {
            "name": "ISO/IEC 27001:2022",
            "description": "Information Security Management System",
            "domains": self.ISO27001_DOMAINS,
            "audit_period": "3 years with annual surveillance",
            "certification_body": "Accredited certification bodies"
        }

    def get_framework_controls(
        self,
        framework: ComplianceFramework
    ) -> List[ControlDomain]:
        """Get all control domains for a framework."""
        if framework in self.frameworks:
            return self.frameworks[framework].get("domains", [])
        return []

    def get_llm_controls(self) -> List[ComplianceControl]:
        """Get LLM-specific compliance controls."""
        return self.LLM_SPECIFIC_CONTROLS

    def get_control_mappings(self) -> Dict[str, List[str]]:
        """Get mappings between frameworks."""
        return {
            "SOC2-CC6 <-> ISO27001-A.9": ["Access Control"],
            "SOC2-CC7 <-> ISO27001-A.12": ["Operations Security"],
            "SOC2-C1 <-> ISO27001-A.8": ["Data Classification"],
            "SOC2-PI1 <-> ISO27001-A.14": ["System Development"],
        }
```

## 11.3.2 SOC 2 Compliance

### SOC 2 Implementation for LLM Systems

```python
"""
SOC 2 compliance implementation for LLM systems.
"""

from dataclasses import dataclass, field
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
from enum import Enum


class SOC2TrustPrinciple(Enum):
    """SOC 2 Trust Service Principles."""
    SECURITY = "security"           # Common Criteria
    AVAILABILITY = "availability"
    PROCESSING_INTEGRITY = "processing_integrity"
    CONFIDENTIALITY = "confidentiality"
    PRIVACY = "privacy"


@dataclass
class SOC2Control:
    """SOC 2 control with LLM context."""
    control_id: str
    principle: SOC2TrustPrinciple
    description: str
    llm_implementation: str
    evidence_artifacts: List[str]
    test_procedures: List[str]
    control_owner: str = ""
    status: str = "not_implemented"


class SOC2ControlLibrary:
    """
    Library of SOC 2 controls adapted for LLM systems.
    """

    SECURITY_CONTROLS = [
        # CC1: Control Environment
        SOC2Control(
            control_id="CC1.1",
            principle=SOC2TrustPrinciple.SECURITY,
            description="Management establishes accountability for AI/ML systems",
            llm_implementation="""
            - Define AI governance committee
            - Establish model owner accountability
            - Create AI ethics board
            - Document decision-making authority for AI systems
            """,
            evidence_artifacts=[
                "AI governance charter",
                "Model ownership registry",
                "Ethics committee meeting minutes",
                "RACI matrix for AI systems"
            ],
            test_procedures=[
                "Review governance documentation",
                "Interview AI system owners",
                "Verify accountability structures"
            ]
        ),
        # CC6: Logical and Physical Access
        SOC2Control(
            control_id="CC6.1",
            principle=SOC2TrustPrinciple.SECURITY,
            description="Logical access to AI systems is restricted to authorized users",
            llm_implementation="""
            - Implement RBAC for LLM API access
            - Use API key management with rotation
            - Separate dev/prod model environments
            - Audit all model access attempts
            - Protect model weights and configurations
            """,
            evidence_artifacts=[
                "Access control policy for AI systems",
                "API key management procedures",
                "Environment separation documentation",
                "Access audit logs"
            ],
            test_procedures=[
                "Test unauthorized access attempts",
                "Review access control configurations",
                "Verify key rotation schedules",
                "Audit log completeness testing"
            ]
        ),
        SOC2Control(
            control_id="CC6.6",
            principle=SOC2TrustPrinciple.SECURITY,
            description="Measures exist to protect against security threats",
            llm_implementation="""
            - Implement prompt injection detection
            - Deploy jailbreak prevention
            - Monitor for adversarial attacks
            - Rate limiting and abuse prevention
            - Input/output validation
            """,
            evidence_artifacts=[
                "Threat model for LLM systems",
                "Security monitoring dashboards",
                "Attack detection procedures",
                "Incident response records"
            ],
            test_procedures=[
                "Penetration testing of LLM APIs",
                "Prompt injection testing",
                "Rate limit testing",
                "Security monitoring review"
            ]
        ),
        # CC7: System Operations
        SOC2Control(
            control_id="CC7.1",
            principle=SOC2TrustPrinciple.SECURITY,
            description="Detection of security events",
            llm_implementation="""
            - Monitor for unusual query patterns
            - Detect potential data extraction attempts
            - Alert on model behavior anomalies
            - Track prompt injection attempts
            - Monitor for jailbreak patterns
            """,
            evidence_artifacts=[
                "Security monitoring configuration",
                "Alert definitions",
                "Security event logs",
                "Anomaly detection reports"
            ],
            test_procedures=[
                "Review monitoring coverage",
                "Test alert triggering",
                "Verify event correlation"
            ]
        ),
        # CC8: Change Management
        SOC2Control(
            control_id="CC8.1",
            principle=SOC2TrustPrinciple.SECURITY,
            description="Changes to AI systems are authorized and tested",
            llm_implementation="""
            - Formal model deployment approval process
            - Model versioning and rollback capability
            - Pre-deployment testing requirements
            - System prompt change control
            - Fine-tuning approval workflow
            """,
            evidence_artifacts=[
                "Model deployment procedures",
                "Change approval records",
                "Testing documentation",
                "Rollback procedures"
            ],
            test_procedures=[
                "Review change records",
                "Test rollback procedures",
                "Verify approval workflows"
            ]
        ),
    ]

    AVAILABILITY_CONTROLS = [
        SOC2Control(
            control_id="A1.1",
            principle=SOC2TrustPrinciple.AVAILABILITY,
            description="Current processing capacity and usage are maintained",
            llm_implementation="""
            - Monitor GPU utilization and capacity
            - Track inference queue depths
            - Implement auto-scaling policies
            - Capacity planning for model serving
            - SLA monitoring and reporting
            """,
            evidence_artifacts=[
                "Capacity monitoring dashboards",
                "Auto-scaling configuration",
                "Capacity planning documents",
                "SLA reports"
            ],
            test_procedures=[
                "Review capacity metrics",
                "Test auto-scaling triggers",
                "Verify SLA measurements"
            ]
        ),
        SOC2Control(
            control_id="A1.2",
            principle=SOC2TrustPrinciple.AVAILABILITY,
            description="Environmental protections and backups",
            llm_implementation="""
            - Multi-region model deployment
            - Model weight backups
            - Disaster recovery procedures
            - Failover testing
            """,
            evidence_artifacts=[
                "Deployment architecture documentation",
                "Backup procedures",
                "DR runbooks",
                "Failover test results"
            ],
            test_procedures=[
                "Review DR architecture",
                "Test backup restoration",
                "Conduct failover drills"
            ]
        ),
    ]

    PROCESSING_INTEGRITY_CONTROLS = [
        SOC2Control(
            control_id="PI1.1",
            principle=SOC2TrustPrinciple.PROCESSING_INTEGRITY,
            description="Procedures to ensure completeness and accuracy",
            llm_implementation="""
            - Input validation for prompts
            - Output format validation
            - Hallucination detection
            - Quality scoring on responses
            - Human review sampling
            """,
            evidence_artifacts=[
                "Input validation procedures",
                "Output validation logic",
                "Quality monitoring reports",
                "Human review records"
            ],
            test_procedures=[
                "Test input validation",
                "Review quality metrics",
                "Verify human review process"
            ]
        ),
        SOC2Control(
            control_id="PI1.4",
            principle=SOC2TrustPrinciple.PROCESSING_INTEGRITY,
            description="Processing errors are detected and corrected",
            llm_implementation="""
            - Error logging and monitoring
            - Automated error detection
            - Error correction procedures
            - User feedback integration
            - Model retraining triggers
            """,
            evidence_artifacts=[
                "Error monitoring configuration",
                "Error correction procedures",
                "Feedback processing records",
                "Retraining documentation"
            ],
            test_procedures=[
                "Review error handling",
                "Test error detection",
                "Verify correction procedures"
            ]
        ),
    ]

    CONFIDENTIALITY_CONTROLS = [
        SOC2Control(
            control_id="C1.1",
            principle=SOC2TrustPrinciple.CONFIDENTIALITY,
            description="Confidential information is protected",
            llm_implementation="""
            - PII detection in prompts/responses
            - Confidential data classification
            - Prompt logging restrictions
            - System prompt protection
            - Training data protection
            """,
            evidence_artifacts=[
                "Data classification policy",
                "PII detection procedures",
                "Logging configuration",
                "Access restrictions"
            ],
            test_procedures=[
                "Test PII detection",
                "Review logging practices",
                "Verify data protection"
            ]
        ),
        SOC2Control(
            control_id="C1.2",
            principle=SOC2TrustPrinciple.CONFIDENTIALITY,
            description="Confidential information is disposed of properly",
            llm_implementation="""
            - Conversation history deletion
            - Training data retention limits
            - Model retirement procedures
            - Secure deletion verification
            """,
            evidence_artifacts=[
                "Data retention policy",
                "Deletion procedures",
                "Deletion verification logs",
                "Model retirement records"
            ],
            test_procedures=[
                "Review retention policies",
                "Test deletion procedures",
                "Verify data destruction"
            ]
        ),
    ]

    def __init__(self):
        self.all_controls = (
            self.SECURITY_CONTROLS +
            self.AVAILABILITY_CONTROLS +
            self.PROCESSING_INTEGRITY_CONTROLS +
            self.CONFIDENTIALITY_CONTROLS
        )

    def get_controls_by_principle(
        self,
        principle: SOC2TrustPrinciple
    ) -> List[SOC2Control]:
        """Get all controls for a trust principle."""
        return [c for c in self.all_controls if c.principle == principle]

    def get_all_evidence_artifacts(self) -> List[str]:
        """Get all required evidence artifacts."""
        artifacts = set()
        for control in self.all_controls:
            artifacts.update(control.evidence_artifacts)
        return sorted(list(artifacts))


class SOC2AuditPreparation:
    """
    Prepare for SOC 2 audit of LLM systems.
    """

    def __init__(self, control_library: SOC2ControlLibrary):
        self.library = control_library
        self.evidence_status: Dict[str, Dict] = {}
        self.control_status: Dict[str, str] = {}

    def assess_readiness(self) -> Dict[str, Any]:
        """Assess SOC 2 audit readiness."""
        total_controls = len(self.library.all_controls)
        implemented = sum(
            1 for c in self.library.all_controls
            if self.control_status.get(c.control_id) == "implemented"
        )

        evidence_collected = sum(
            1 for e in self.evidence_status.values()
            if e.get("collected", False)
        )
        total_evidence = len(self.library.get_all_evidence_artifacts())

        return {
            "control_readiness": {
                "total": total_controls,
                "implemented": implemented,
                "percentage": (implemented / total_controls * 100) if total_controls > 0 else 0
            },
            "evidence_readiness": {
                "total": total_evidence,
                "collected": evidence_collected,
                "percentage": (evidence_collected / total_evidence * 100) if total_evidence > 0 else 0
            },
            "gaps": self._identify_gaps(),
            "recommendations": self._generate_recommendations()
        }

    def _identify_gaps(self) -> List[Dict[str, Any]]:
        """Identify compliance gaps."""
        gaps = []

        for control in self.library.all_controls:
            if self.control_status.get(control.control_id) != "implemented":
                gaps.append({
                    "control_id": control.control_id,
                    "principle": control.principle.value,
                    "description": control.description,
                    "priority": "high" if control.principle == SOC2TrustPrinciple.SECURITY else "medium"
                })

        return gaps

    def _generate_recommendations(self) -> List[str]:
        """Generate recommendations for audit preparation."""
        recommendations = []

        gaps = self._identify_gaps()
        if gaps:
            high_priority = [g for g in gaps if g["priority"] == "high"]
            if high_priority:
                recommendations.append(
                    f"Address {len(high_priority)} high-priority security control gaps"
                )

        # Evidence recommendations
        missing_evidence = [
            a for a in self.library.get_all_evidence_artifacts()
            if not self.evidence_status.get(a, {}).get("collected", False)
        ]
        if missing_evidence:
            recommendations.append(
                f"Collect {len(missing_evidence)} missing evidence artifacts"
            )

        return recommendations

    def generate_audit_package(self) -> Dict[str, Any]:
        """Generate audit documentation package."""
        return {
            "control_matrix": [
                {
                    "control_id": c.control_id,
                    "principle": c.principle.value,
                    "description": c.description,
                    "implementation": c.llm_implementation,
                    "evidence": c.evidence_artifacts,
                    "status": self.control_status.get(c.control_id, "not_assessed")
                }
                for c in self.library.all_controls
            ],
            "evidence_index": self.evidence_status,
            "readiness_summary": self.assess_readiness()
        }
```

## 11.3.3 ISO 27001 Implementation

### Information Security Management System

```python
"""
ISO 27001 implementation for LLM systems.
"""

from dataclasses import dataclass, field
from typing import Dict, Any, List, Optional
from datetime import datetime
from enum import Enum


class ISO27001Annex(Enum):
    """ISO 27001:2022 Annex A Control Categories."""
    ORGANIZATIONAL = "5"
    PEOPLE = "6"
    PHYSICAL = "7"
    TECHNOLOGICAL = "8"


@dataclass
class ISO27001Control:
    """ISO 27001 control with LLM context."""
    control_id: str
    category: ISO27001Annex
    title: str
    objective: str
    llm_implementation: str
    implementation_evidence: List[str]
    applicable: bool = True
    justification: str = ""


class ISO27001ControlLibrary:
    """
    ISO 27001:2022 controls for LLM systems.
    """

    ORGANIZATIONAL_CONTROLS = [
        ISO27001Control(
            control_id="5.1",
            category=ISO27001Annex.ORGANIZATIONAL,
            title="Policies for information security",
            objective="Provide management direction for information security",
            llm_implementation="""
            - AI/ML security policy
            - LLM acceptable use policy
            - Model governance policy
            - Data classification policy for AI training data
            """,
            implementation_evidence=[
                "AI security policy document",
                "Policy approval records",
                "Policy communication evidence",
                "Policy review records"
            ]
        ),
        ISO27001Control(
            control_id="5.3",
            category=ISO27001Annex.ORGANIZATIONAL,
            title="Segregation of duties",
            objective="Reduce opportunities for unauthorized modification or misuse",
            llm_implementation="""
            - Separate model development and deployment roles
            - Segregate training data management
            - Independent model review process
            - Separation of API key management
            """,
            implementation_evidence=[
                "Role definitions",
                "Access control matrix",
                "Approval workflows",
                "Segregation of duties matrix"
            ]
        ),
        ISO27001Control(
            control_id="5.8",
            category=ISO27001Annex.ORGANIZATIONAL,
            title="Information security in project management",
            objective="Integrate information security into project management",
            llm_implementation="""
            - Security requirements for ML projects
            - Model security review checkpoints
            - Security testing in model deployment
            - Risk assessment for new models
            """,
            implementation_evidence=[
                "Project security requirements",
                "Security review records",
                "Model security assessments",
                "Risk assessment documents"
            ]
        ),
        ISO27001Control(
            control_id="5.12",
            category=ISO27001Annex.ORGANIZATIONAL,
            title="Classification of information",
            objective="Ensure information receives appropriate level of protection",
            llm_implementation="""
            - Classify training data sensitivity
            - Label prompt/response data
            - Categorize model configurations
            - Classify system prompts
            """,
            implementation_evidence=[
                "Data classification policy",
                "Classification labels in systems",
                "Training data inventory",
                "Handling procedures by classification"
            ]
        ),
        ISO27001Control(
            control_id="5.23",
            category=ISO27001Annex.ORGANIZATIONAL,
            title="Information security for use of cloud services",
            objective="Security when using cloud services",
            llm_implementation="""
            - Cloud LLM provider security assessment
            - API security requirements
            - Data residency requirements
            - Cloud service level agreements
            """,
            implementation_evidence=[
                "Cloud provider assessments",
                "Service agreements",
                "Security configurations",
                "Monitoring evidence"
            ]
        ),
    ]

    TECHNOLOGICAL_CONTROLS = [
        ISO27001Control(
            control_id="8.2",
            category=ISO27001Annex.TECHNOLOGICAL,
            title="Privileged access rights",
            objective="Restrict and manage privileged access",
            llm_implementation="""
            - Privileged access to model weights
            - Admin access to LLM infrastructure
            - Training pipeline access control
            - System prompt modification rights
            """,
            implementation_evidence=[
                "Privileged access inventory",
                "Access approval records",
                "Access review evidence",
                "Privileged access logs"
            ]
        ),
        ISO27001Control(
            control_id="8.9",
            category=ISO27001Annex.TECHNOLOGICAL,
            title="Configuration management",
            objective="Establish and maintain configurations",
            llm_implementation="""
            - Model configuration management
            - Inference server configurations
            - System prompt version control
            - Hyperparameter tracking
            """,
            implementation_evidence=[
                "Configuration management policy",
                "Configuration baselines",
                "Change records",
                "Version control evidence"
            ]
        ),
        ISO27001Control(
            control_id="8.12",
            category=ISO27001Annex.TECHNOLOGICAL,
            title="Data leakage prevention",
            objective="Prevent unauthorized disclosure of information",
            llm_implementation="""
            - PII detection in outputs
            - Training data leakage prevention
            - System prompt protection
            - Output filtering
            """,
            implementation_evidence=[
                "DLP configurations",
                "Detection logs",
                "Filter configurations",
                "Testing evidence"
            ]
        ),
        ISO27001Control(
            control_id="8.16",
            category=ISO27001Annex.TECHNOLOGICAL,
            title="Monitoring activities",
            objective="Detect anomalous behavior",
            llm_implementation="""
            - Monitor LLM usage patterns
            - Detect prompt injection attempts
            - Track model performance drift
            - Alert on security events
            """,
            implementation_evidence=[
                "Monitoring configurations",
                "Alert definitions",
                "Monitoring logs",
                "Incident records"
            ]
        ),
        ISO27001Control(
            control_id="8.23",
            category=ISO27001Annex.TECHNOLOGICAL,
            title="Web filtering",
            objective="Protect against malicious websites",
            llm_implementation="""
            - Filter URLs in LLM inputs
            - Validate external data sources
            - Block malicious content in RAG
            - URL reputation checking
            """,
            implementation_evidence=[
                "Filtering configurations",
                "Blocked URL logs",
                "Filter effectiveness reports"
            ]
        ),
        ISO27001Control(
            control_id="8.28",
            category=ISO27001Annex.TECHNOLOGICAL,
            title="Secure coding",
            objective="Apply secure coding principles",
            llm_implementation="""
            - Secure API development
            - Input validation implementation
            - Output encoding
            - Dependency security
            """,
            implementation_evidence=[
                "Coding standards",
                "Code review records",
                "Security testing results",
                "Dependency scan results"
            ]
        ),
    ]

    def __init__(self):
        self.controls = self.ORGANIZATIONAL_CONTROLS + self.TECHNOLOGICAL_CONTROLS

    def get_controls_by_category(
        self,
        category: ISO27001Annex
    ) -> List[ISO27001Control]:
        """Get controls by Annex category."""
        return [c for c in self.controls if c.category == category]

    def generate_soa(self) -> Dict[str, Any]:
        """Generate Statement of Applicability."""
        return {
            "document_title": "Statement of Applicability - LLM System",
            "version": "1.0",
            "date": datetime.utcnow().isoformat(),
            "controls": [
                {
                    "control_id": c.control_id,
                    "title": c.title,
                    "applicable": c.applicable,
                    "justification": c.justification or "Required for LLM system security",
                    "implementation_status": "implemented",
                    "evidence_references": c.implementation_evidence
                }
                for c in self.controls
            ]
        }


class ISMSDocumentation:
    """
    ISMS documentation for ISO 27001 certification.
    """

    REQUIRED_DOCUMENTS = {
        "mandatory": [
            "Information security policy",
            "Scope of the ISMS",
            "Risk assessment methodology",
            "Risk assessment report",
            "Risk treatment plan",
            "Statement of Applicability",
            "Security objectives",
            "Roles and responsibilities",
            "Inventory of assets",
            "Acceptable use policy",
            "Access control policy",
            "Operating procedures for IT",
            "Supplier security policy",
            "Incident response procedure",
            "Business continuity procedures",
            "Compliance requirements"
        ],
        "llm_specific": [
            "AI/ML governance policy",
            "Model development lifecycle",
            "Training data management procedure",
            "Model deployment procedure",
            "LLM monitoring and alerting",
            "Prompt injection prevention",
            "Output validation procedures",
            "Model retirement procedure"
        ]
    }

    def __init__(self):
        self.documents: Dict[str, Dict] = {}

    def check_documentation_completeness(self) -> Dict[str, Any]:
        """Check if all required documentation exists."""
        missing_mandatory = [
            doc for doc in self.REQUIRED_DOCUMENTS["mandatory"]
            if doc not in self.documents
        ]
        missing_llm = [
            doc for doc in self.REQUIRED_DOCUMENTS["llm_specific"]
            if doc not in self.documents
        ]

        return {
            "mandatory_complete": len(missing_mandatory) == 0,
            "llm_specific_complete": len(missing_llm) == 0,
            "missing_mandatory": missing_mandatory,
            "missing_llm_specific": missing_llm,
            "total_required": len(self.REQUIRED_DOCUMENTS["mandatory"]) + len(self.REQUIRED_DOCUMENTS["llm_specific"]),
            "documented": len(self.documents)
        }

    def generate_document_register(self) -> Dict[str, Any]:
        """Generate document register."""
        return {
            "register_date": datetime.utcnow().isoformat(),
            "documents": [
                {
                    "title": title,
                    "version": info.get("version", "1.0"),
                    "owner": info.get("owner", ""),
                    "review_date": info.get("review_date", ""),
                    "classification": info.get("classification", "internal")
                }
                for title, info in self.documents.items()
            ]
        }
```

## 11.3.4 AI-Specific Compliance

### NIST AI Risk Management Framework

```python
"""
NIST AI Risk Management Framework implementation.
"""

from dataclasses import dataclass, field
from typing import Dict, Any, List, Optional
from enum import Enum


class NISTAIFunction(Enum):
    """NIST AI RMF Core Functions."""
    GOVERN = "govern"
    MAP = "map"
    MEASURE = "measure"
    MANAGE = "manage"


@dataclass
class NISTAIControl:
    """NIST AI RMF control."""
    function: NISTAIFunction
    category_id: str
    subcategory_id: str
    description: str
    llm_implementation: str
    metrics: List[str]


class NISTAIRMFLibrary:
    """
    NIST AI Risk Management Framework controls.
    """

    GOVERN_CONTROLS = [
        NISTAIControl(
            function=NISTAIFunction.GOVERN,
            category_id="GOVERN 1",
            subcategory_id="1.1",
            description="Legal and regulatory requirements are identified",
            llm_implementation="""
            - Identify applicable AI regulations
            - Track EU AI Act requirements
            - Monitor state AI legislation
            - Document compliance obligations
            """,
            metrics=["regulatory_coverage", "compliance_gap_count"]
        ),
        NISTAIControl(
            function=NISTAIFunction.GOVERN,
            category_id="GOVERN 1",
            subcategory_id="1.3",
            description="AI risk management processes are established",
            llm_implementation="""
            - Define LLM risk taxonomy
            - Establish risk assessment procedures
            - Create risk appetite statements
            - Implement risk monitoring
            """,
            metrics=["risk_register_completeness", "risk_review_frequency"]
        ),
        NISTAIControl(
            function=NISTAIFunction.GOVERN,
            category_id="GOVERN 2",
            subcategory_id="2.1",
            description="Roles and responsibilities are defined",
            llm_implementation="""
            - Define AI governance committee
            - Assign model owner responsibilities
            - Establish ethics review board
            - Document accountability matrix
            """,
            metrics=["role_coverage", "accountability_clarity"]
        ),
        NISTAIControl(
            function=NISTAIFunction.GOVERN,
            category_id="GOVERN 4",
            subcategory_id="4.1",
            description="Organizational culture embraces AI risk management",
            llm_implementation="""
            - AI ethics training programs
            - Responsible AI awareness
            - Risk reporting culture
            - Incident reporting mechanisms
            """,
            metrics=["training_completion_rate", "incident_report_count"]
        ),
    ]

    MAP_CONTROLS = [
        NISTAIControl(
            function=NISTAIFunction.MAP,
            category_id="MAP 1",
            subcategory_id="1.1",
            description="Context is established and understood",
            llm_implementation="""
            - Document LLM use cases
            - Define intended purpose
            - Identify stakeholders
            - Understand deployment context
            """,
            metrics=["use_case_documentation", "stakeholder_identification"]
        ),
        NISTAIControl(
            function=NISTAIFunction.MAP,
            category_id="MAP 2",
            subcategory_id="2.1",
            description="AI system categorization",
            llm_implementation="""
            - Classify LLM risk level
            - Categorize by impact
            - Identify high-risk applications
            - Document categorization rationale
            """,
            metrics=["categorization_completeness", "high_risk_system_count"]
        ),
        NISTAIControl(
            function=NISTAIFunction.MAP,
            category_id="MAP 3",
            subcategory_id="3.1",
            description="Potential negative impacts are identified",
            llm_implementation="""
            - Assess harm potential
            - Identify bias risks
            - Evaluate safety concerns
            - Document failure modes
            """,
            metrics=["impact_assessment_coverage", "identified_risk_count"]
        ),
    ]

    MEASURE_CONTROLS = [
        NISTAIControl(
            function=NISTAIFunction.MEASURE,
            category_id="MEASURE 1",
            subcategory_id="1.1",
            description="Appropriate methods are used to measure AI performance",
            llm_implementation="""
            - Define LLM evaluation metrics
            - Implement quality benchmarks
            - Measure fairness metrics
            - Track safety indicators
            """,
            metrics=["metric_coverage", "benchmark_scores"]
        ),
        NISTAIControl(
            function=NISTAIFunction.MEASURE,
            category_id="MEASURE 2",
            subcategory_id="2.1",
            description="AI systems are evaluated for trustworthiness",
            llm_implementation="""
            - Evaluate accuracy/reliability
            - Test for hallucinations
            - Assess consistency
            - Measure robustness
            """,
            metrics=["trustworthiness_score", "hallucination_rate"]
        ),
        NISTAIControl(
            function=NISTAIFunction.MEASURE,
            category_id="MEASURE 2",
            subcategory_id="2.5",
            description="AI systems are evaluated for fairness",
            llm_implementation="""
            - Bias testing across demographics
            - Fairness metric calculation
            - Disparate impact analysis
            - Representation testing
            """,
            metrics=["fairness_metrics", "bias_detection_count"]
        ),
    ]

    MANAGE_CONTROLS = [
        NISTAIControl(
            function=NISTAIFunction.MANAGE,
            category_id="MANAGE 1",
            subcategory_id="1.1",
            description="AI risks are prioritized and acted upon",
            llm_implementation="""
            - Risk prioritization matrix
            - Treatment plan development
            - Mitigation implementation
            - Residual risk acceptance
            """,
            metrics=["risk_treatment_rate", "residual_risk_level"]
        ),
        NISTAIControl(
            function=NISTAIFunction.MANAGE,
            category_id="MANAGE 2",
            subcategory_id="2.1",
            description="Strategies to maximize benefits and minimize negative impacts",
            llm_implementation="""
            - Benefit-risk analysis
            - Safeguard implementation
            - Human oversight mechanisms
            - Fallback procedures
            """,
            metrics=["safeguard_coverage", "human_oversight_rate"]
        ),
        NISTAIControl(
            function=NISTAIFunction.MANAGE,
            category_id="MANAGE 4",
            subcategory_id="4.1",
            description="Post-deployment monitoring is in place",
            llm_implementation="""
            - Continuous model monitoring
            - Drift detection
            - Performance tracking
            - Incident management
            """,
            metrics=["monitoring_coverage", "drift_detection_rate"]
        ),
    ]

    def __init__(self):
        self.controls = (
            self.GOVERN_CONTROLS +
            self.MAP_CONTROLS +
            self.MEASURE_CONTROLS +
            self.MANAGE_CONTROLS
        )

    def get_controls_by_function(
        self,
        function: NISTAIFunction
    ) -> List[NISTAIControl]:
        """Get controls by function."""
        return [c for c in self.controls if c.function == function]

    def generate_playbook(self) -> Dict[str, Any]:
        """Generate NIST AI RMF playbook."""
        playbook = {}

        for function in NISTAIFunction:
            function_controls = self.get_controls_by_function(function)
            playbook[function.value] = {
                "description": self._get_function_description(function),
                "controls": [
                    {
                        "category": c.category_id,
                        "subcategory": c.subcategory_id,
                        "description": c.description,
                        "implementation": c.llm_implementation,
                        "metrics": c.metrics
                    }
                    for c in function_controls
                ]
            }

        return playbook

    def _get_function_description(self, function: NISTAIFunction) -> str:
        """Get function description."""
        descriptions = {
            NISTAIFunction.GOVERN: "Cultivate and implement a culture of risk management",
            NISTAIFunction.MAP: "Categorize and understand AI risks in context",
            NISTAIFunction.MEASURE: "Analyze and track AI risks and impacts",
            NISTAIFunction.MANAGE: "Prioritize and act on AI risks"
        }
        return descriptions.get(function, "")


class EUAIActCompliance:
    """
    EU AI Act compliance for LLM systems.
    """

    RISK_CATEGORIES = {
        "unacceptable": {
            "description": "Prohibited AI practices",
            "examples": [
                "Social scoring by governments",
                "Manipulative subliminal techniques",
                "Exploitation of vulnerabilities"
            ],
            "llm_relevance": "Generally not applicable to standard LLM deployments"
        },
        "high_risk": {
            "description": "AI systems with significant impact",
            "examples": [
                "Employment and worker management",
                "Educational/vocational training",
                "Essential services access",
                "Law enforcement",
                "Migration/border control"
            ],
            "llm_relevance": "LLMs used in these domains are high-risk",
            "requirements": [
                "Risk management system",
                "Data governance",
                "Technical documentation",
                "Record-keeping",
                "Transparency",
                "Human oversight",
                "Accuracy and robustness",
                "Cybersecurity"
            ]
        },
        "limited_risk": {
            "description": "Transparency obligations",
            "examples": [
                "Chatbots",
                "Emotion recognition",
                "Deep fake generation"
            ],
            "llm_relevance": "Most customer-facing LLM applications",
            "requirements": [
                "User notification of AI interaction",
                "Labeling of AI-generated content"
            ]
        },
        "minimal_risk": {
            "description": "No additional requirements",
            "examples": [
                "AI-enabled video games",
                "Spam filters"
            ],
            "llm_relevance": "Internal tools, development assistants"
        }
    }

    def classify_llm_system(
        self,
        use_case: str,
        deployment_context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Classify LLM system under EU AI Act."""
        # Simple classification logic
        high_risk_domains = [
            "employment", "education", "healthcare",
            "legal", "financial", "government"
        ]

        is_high_risk = any(
            domain in use_case.lower()
            for domain in high_risk_domains
        )

        if is_high_risk:
            risk_category = "high_risk"
        elif deployment_context.get("customer_facing", False):
            risk_category = "limited_risk"
        else:
            risk_category = "minimal_risk"

        return {
            "use_case": use_case,
            "risk_category": risk_category,
            "requirements": self.RISK_CATEGORIES[risk_category].get("requirements", []),
            "description": self.RISK_CATEGORIES[risk_category]["description"]
        }

    def get_high_risk_requirements(self) -> List[Dict[str, Any]]:
        """Get detailed high-risk AI system requirements."""
        return [
            {
                "requirement": "Risk management system",
                "llm_implementation": [
                    "Continuous risk identification",
                    "Risk mitigation measures",
                    "Testing and validation",
                    "Post-market monitoring"
                ]
            },
            {
                "requirement": "Data governance",
                "llm_implementation": [
                    "Training data documentation",
                    "Data quality assessment",
                    "Bias detection and mitigation",
                    "Data provenance tracking"
                ]
            },
            {
                "requirement": "Technical documentation",
                "llm_implementation": [
                    "Model architecture documentation",
                    "Training methodology",
                    "Evaluation results",
                    "Deployment specifications"
                ]
            },
            {
                "requirement": "Transparency",
                "llm_implementation": [
                    "User notification of AI use",
                    "Capability disclosure",
                    "Limitation documentation",
                    "Interpretability measures"
                ]
            },
            {
                "requirement": "Human oversight",
                "llm_implementation": [
                    "Human-in-the-loop mechanisms",
                    "Override capabilities",
                    "Monitoring dashboards",
                    "Escalation procedures"
                ]
            },
            {
                "requirement": "Accuracy and robustness",
                "llm_implementation": [
                    "Performance benchmarking",
                    "Error rate monitoring",
                    "Adversarial testing",
                    "Reliability testing"
                ]
            }
        ]
```

## 11.3.5 Audit Preparation

### Evidence Collection and Management

```python
"""
Audit evidence collection and management.
"""

from dataclasses import dataclass, field
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
from enum import Enum
import hashlib
import json


class EvidenceType(Enum):
    """Types of audit evidence."""
    DOCUMENT = "document"
    LOG = "log"
    SCREENSHOT = "screenshot"
    CONFIGURATION = "configuration"
    REPORT = "report"
    INTERVIEW = "interview"
    OBSERVATION = "observation"


@dataclass
class AuditEvidence:
    """Individual piece of audit evidence."""
    evidence_id: str
    title: str
    evidence_type: EvidenceType
    control_ids: List[str]
    description: str
    file_path: Optional[str] = None
    collected_date: datetime = None
    collected_by: str = ""
    hash_value: str = ""
    retention_date: datetime = None


class EvidenceCollector:
    """
    Collect and manage audit evidence.
    """

    def __init__(self):
        self.evidence: Dict[str, AuditEvidence] = {}
        self.evidence_mapping: Dict[str, List[str]] = {}  # control_id -> evidence_ids

    def collect_evidence(
        self,
        title: str,
        evidence_type: EvidenceType,
        control_ids: List[str],
        description: str,
        file_path: str = None,
        content: bytes = None
    ) -> AuditEvidence:
        """Collect a piece of evidence."""
        evidence_id = f"EV-{datetime.utcnow().strftime('%Y%m%d%H%M%S')}-{len(self.evidence)}"

        # Calculate hash
        hash_value = ""
        if content:
            hash_value = hashlib.sha256(content).hexdigest()
        elif file_path:
            with open(file_path, "rb") as f:
                hash_value = hashlib.sha256(f.read()).hexdigest()

        evidence = AuditEvidence(
            evidence_id=evidence_id,
            title=title,
            evidence_type=evidence_type,
            control_ids=control_ids,
            description=description,
            file_path=file_path,
            collected_date=datetime.utcnow(),
            hash_value=hash_value,
            retention_date=datetime.utcnow() + timedelta(days=365 * 7)  # 7 year retention
        )

        self.evidence[evidence_id] = evidence

        # Update mapping
        for control_id in control_ids:
            if control_id not in self.evidence_mapping:
                self.evidence_mapping[control_id] = []
            self.evidence_mapping[control_id].append(evidence_id)

        return evidence

    def get_evidence_for_control(self, control_id: str) -> List[AuditEvidence]:
        """Get all evidence for a control."""
        evidence_ids = self.evidence_mapping.get(control_id, [])
        return [self.evidence[eid] for eid in evidence_ids if eid in self.evidence]

    def verify_evidence_integrity(self, evidence_id: str) -> Dict[str, Any]:
        """Verify evidence has not been tampered with."""
        if evidence_id not in self.evidence:
            return {"valid": False, "error": "Evidence not found"}

        evidence = self.evidence[evidence_id]
        if not evidence.file_path:
            return {"valid": True, "note": "No file to verify"}

        try:
            with open(evidence.file_path, "rb") as f:
                current_hash = hashlib.sha256(f.read()).hexdigest()

            return {
                "valid": current_hash == evidence.hash_value,
                "stored_hash": evidence.hash_value,
                "current_hash": current_hash
            }
        except FileNotFoundError:
            return {"valid": False, "error": "File not found"}

    def generate_evidence_register(self) -> Dict[str, Any]:
        """Generate evidence register for auditors."""
        return {
            "generated_date": datetime.utcnow().isoformat(),
            "total_evidence": len(self.evidence),
            "by_type": self._group_by_type(),
            "evidence_list": [
                {
                    "id": e.evidence_id,
                    "title": e.title,
                    "type": e.evidence_type.value,
                    "controls": e.control_ids,
                    "collected_date": e.collected_date.isoformat() if e.collected_date else None,
                    "hash": e.hash_value[:16] + "..." if e.hash_value else None
                }
                for e in self.evidence.values()
            ]
        }

    def _group_by_type(self) -> Dict[str, int]:
        """Group evidence count by type."""
        counts = {}
        for e in self.evidence.values():
            type_name = e.evidence_type.value
            counts[type_name] = counts.get(type_name, 0) + 1
        return counts


class AuditReadinessChecker:
    """
    Check audit readiness across frameworks.
    """

    def __init__(
        self,
        evidence_collector: EvidenceCollector,
        control_status: Dict[str, str]
    ):
        self.evidence = evidence_collector
        self.control_status = control_status

    def check_readiness(
        self,
        framework: str,
        required_controls: List[str]
    ) -> Dict[str, Any]:
        """Check readiness for specific framework audit."""
        results = {
            "framework": framework,
            "check_date": datetime.utcnow().isoformat(),
            "overall_ready": True,
            "control_readiness": [],
            "gaps": []
        }

        for control_id in required_controls:
            status = self.control_status.get(control_id, "not_implemented")
            evidence = self.evidence.get_evidence_for_control(control_id)

            is_ready = status == "implemented" and len(evidence) > 0

            results["control_readiness"].append({
                "control_id": control_id,
                "status": status,
                "evidence_count": len(evidence),
                "ready": is_ready
            })

            if not is_ready:
                results["overall_ready"] = False
                results["gaps"].append({
                    "control_id": control_id,
                    "issue": "Not implemented" if status != "implemented" else "Missing evidence"
                })

        results["readiness_score"] = sum(
            1 for c in results["control_readiness"] if c["ready"]
        ) / len(required_controls) * 100 if required_controls else 0

        return results

    def generate_remediation_plan(
        self,
        gaps: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """Generate remediation plan for identified gaps."""
        plan = []

        for gap in gaps:
            remediation = {
                "control_id": gap["control_id"],
                "issue": gap["issue"],
                "actions": [],
                "priority": "high"
            }

            if gap["issue"] == "Not implemented":
                remediation["actions"] = [
                    "Design control implementation",
                    "Implement control",
                    "Test control effectiveness",
                    "Document implementation"
                ]
            else:  # Missing evidence
                remediation["actions"] = [
                    "Identify required evidence",
                    "Collect evidence",
                    "Verify evidence completeness",
                    "Store in evidence repository"
                ]

            plan.append(remediation)

        return plan
```

## 11.3.6 Continuous Compliance

### Compliance Monitoring Dashboard

```python
"""
Continuous compliance monitoring.
"""

from dataclasses import dataclass
from typing import Dict, Any, List
from datetime import datetime, timedelta


@dataclass
class ComplianceMetric:
    """A compliance metric to track."""
    name: str
    framework: str
    target: float
    current_value: float
    measurement_date: datetime
    trend: str  # improving, stable, declining


class ComplianceMonitor:
    """
    Continuous compliance monitoring system.
    """

    def __init__(self):
        self.metrics: Dict[str, List[ComplianceMetric]] = {}
        self.alerts: List[Dict[str, Any]] = []

    def record_metric(
        self,
        name: str,
        framework: str,
        value: float,
        target: float
    ):
        """Record a compliance metric."""
        if name not in self.metrics:
            self.metrics[name] = []

        # Calculate trend
        historical = self.metrics[name][-10:] if self.metrics[name] else []
        trend = self._calculate_trend(historical, value)

        metric = ComplianceMetric(
            name=name,
            framework=framework,
            target=target,
            current_value=value,
            measurement_date=datetime.utcnow(),
            trend=trend
        )

        self.metrics[name].append(metric)

        # Check for alerts
        self._check_alerts(metric)

    def _calculate_trend(
        self,
        historical: List[ComplianceMetric],
        current: float
    ) -> str:
        """Calculate metric trend."""
        if len(historical) < 3:
            return "stable"

        recent_avg = sum(m.current_value for m in historical[-3:]) / 3

        if current > recent_avg * 1.05:
            return "improving"
        elif current < recent_avg * 0.95:
            return "declining"
        return "stable"

    def _check_alerts(self, metric: ComplianceMetric):
        """Check if metric triggers an alert."""
        if metric.current_value < metric.target * 0.9:
            self.alerts.append({
                "metric": metric.name,
                "framework": metric.framework,
                "severity": "warning",
                "message": f"{metric.name} is below target ({metric.current_value:.1%} vs {metric.target:.1%})",
                "timestamp": datetime.utcnow().isoformat()
            })

        if metric.trend == "declining":
            self.alerts.append({
                "metric": metric.name,
                "framework": metric.framework,
                "severity": "info",
                "message": f"{metric.name} is showing declining trend",
                "timestamp": datetime.utcnow().isoformat()
            })

    def get_compliance_dashboard(self) -> Dict[str, Any]:
        """Generate compliance dashboard data."""
        dashboard = {
            "generated_at": datetime.utcnow().isoformat(),
            "frameworks": {},
            "overall_health": "healthy",
            "recent_alerts": self.alerts[-10:]
        }

        # Group metrics by framework
        framework_metrics = {}
        for name, history in self.metrics.items():
            if not history:
                continue
            latest = history[-1]
            framework = latest.framework

            if framework not in framework_metrics:
                framework_metrics[framework] = []
            framework_metrics[framework].append(latest)

        # Calculate framework health
        for framework, metrics in framework_metrics.items():
            at_target = sum(1 for m in metrics if m.current_value >= m.target)
            total = len(metrics)
            health_score = at_target / total if total > 0 else 0

            dashboard["frameworks"][framework] = {
                "metrics_count": total,
                "at_target": at_target,
                "health_score": health_score,
                "status": "healthy" if health_score >= 0.9 else "warning" if health_score >= 0.7 else "critical",
                "metrics": [
                    {
                        "name": m.name,
                        "current": m.current_value,
                        "target": m.target,
                        "trend": m.trend
                    }
                    for m in metrics
                ]
            }

            if health_score < 0.7:
                dashboard["overall_health"] = "critical"
            elif health_score < 0.9 and dashboard["overall_health"] != "critical":
                dashboard["overall_health"] = "warning"

        return dashboard

    def generate_compliance_report(
        self,
        framework: str,
        period_days: int = 30
    ) -> Dict[str, Any]:
        """Generate compliance report for a framework."""
        cutoff = datetime.utcnow() - timedelta(days=period_days)

        report = {
            "framework": framework,
            "period_start": cutoff.isoformat(),
            "period_end": datetime.utcnow().isoformat(),
            "metrics": [],
            "summary": {}
        }

        for name, history in self.metrics.items():
            framework_history = [
                m for m in history
                if m.framework == framework and m.measurement_date >= cutoff
            ]

            if not framework_history:
                continue

            values = [m.current_value for m in framework_history]
            report["metrics"].append({
                "name": name,
                "min": min(values),
                "max": max(values),
                "avg": sum(values) / len(values),
                "latest": values[-1],
                "measurements": len(values)
            })

        # Summary statistics
        if report["metrics"]:
            report["summary"] = {
                "total_metrics": len(report["metrics"]),
                "avg_compliance": sum(m["avg"] for m in report["metrics"]) / len(report["metrics"]),
                "below_target_count": sum(
                    1 for m in report["metrics"]
                    if m["latest"] < 0.9  # Assuming 90% target
                )
            }

        return report
```

## Summary

This guide covered comprehensive compliance frameworks for LLM systems:

1. **Framework Overview**: SOC 2, ISO 27001, GDPR, NIST AI RMF, EU AI Act
2. **SOC 2 Compliance**: Trust principles adapted for LLM systems
3. **ISO 27001**: ISMS controls for AI/ML environments
4. **AI-Specific Compliance**: NIST AI RMF and EU AI Act requirements
5. **Audit Preparation**: Evidence collection and readiness assessment
6. **Continuous Compliance**: Monitoring and reporting

Key takeaways:
- Map standard compliance frameworks to LLM-specific controls
- Document AI governance and risk management
- Collect and preserve audit evidence systematically
- Implement continuous compliance monitoring
- Prepare for AI-specific regulations (EU AI Act)
- Maintain comprehensive documentation and evidence trails

---

> **Navigation**
> [← 11.2 PII & Privacy](11.2_pii_data_privacy_guide.md) | **[Index](../README.md#15-repository-structure)** | [11.4 Model Governance →](11.4_model_governance_guide.md)
