> **Navigation** | [← 8.1 Model Registry](8.1_model_registry_guide.md) | [8.3 Model Versioning →](8.3_model_versioning_artifacts_guide.md)
>
> | | |
> |---|---|
> | **Prerequisites** | [8.1 Model Registry](8.1_model_registry_guide.md) &#124; ML training workflows |
> | **Related** | [2.5 Training Monitoring](../02_model_training/2.5_training_monitoring_debugging.md) &#124; [5.1 Evaluation Framework](../05_evaluation_testing/5.1_llm_evaluation_framework.md) |
> | **Next** | [8.3 Model Versioning & Artifacts](8.3_model_versioning_artifacts_guide.md) |

# 8.2 Experiment Tracking Guide

## Document Purpose

This guide provides comprehensive guidance for implementing experiment tracking systems to ensure reproducibility, enable systematic optimization, and facilitate team collaboration during LLM development and fine-tuning.

## Prerequisites

- Understanding of ML training workflows
- Familiarity with hyperparameter tuning
- Basic knowledge of ML metrics and evaluation
- Experience with Python and ML frameworks

## Target Audience

- ML Engineers running training experiments
- Researchers iterating on model architectures
- MLOps Engineers setting up tracking infrastructure
- Data Scientists analyzing experiment results

---

## 1. Experiment Tracking Fundamentals

### 1.1 Core Concepts

```python
"""
Core experiment tracking concepts and data structures
"""
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional, Union
from datetime import datetime
from enum import Enum
import hashlib
import json


class ExperimentStatus(Enum):
    """Experiment run status"""
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    KILLED = "killed"


@dataclass
class Parameter:
    """Tracked parameter"""
    name: str
    value: Any
    category: str = "hyperparameter"  # hyperparameter, config, data


@dataclass
class Metric:
    """Tracked metric with history"""
    name: str
    values: List[float] = field(default_factory=list)
    steps: List[int] = field(default_factory=list)
    timestamps: List[datetime] = field(default_factory=list)

    def log(self, value: float, step: Optional[int] = None):
        """Log a new metric value"""
        self.values.append(value)
        self.steps.append(step if step is not None else len(self.values))
        self.timestamps.append(datetime.now())

    @property
    def latest(self) -> Optional[float]:
        return self.values[-1] if self.values else None

    @property
    def best(self) -> Optional[float]:
        return min(self.values) if self.values else None  # Assuming lower is better


@dataclass
class Artifact:
    """Tracked artifact (files, models, outputs)"""
    name: str
    path: str
    artifact_type: str  # model, dataset, config, output, visualization
    size_bytes: int
    checksum: str
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class SystemMetrics:
    """System resource metrics"""
    gpu_utilization: List[float] = field(default_factory=list)
    gpu_memory_used: List[float] = field(default_factory=list)
    gpu_temperature: List[float] = field(default_factory=list)
    cpu_utilization: List[float] = field(default_factory=list)
    memory_used: List[float] = field(default_factory=list)
    timestamps: List[datetime] = field(default_factory=list)


@dataclass
class ExperimentRun:
    """Complete experiment run record"""
    run_id: str
    experiment_id: str
    name: str
    status: ExperimentStatus

    # Timing
    start_time: datetime
    end_time: Optional[datetime] = None

    # Tracked data
    parameters: Dict[str, Parameter] = field(default_factory=dict)
    metrics: Dict[str, Metric] = field(default_factory=dict)
    artifacts: List[Artifact] = field(default_factory=list)
    system_metrics: SystemMetrics = field(default_factory=SystemMetrics)

    # Metadata
    tags: Dict[str, str] = field(default_factory=dict)
    notes: str = ""
    git_commit: Optional[str] = None
    git_branch: Optional[str] = None
    code_path: Optional[str] = None

    # Environment
    environment: Dict[str, Any] = field(default_factory=dict)
    dependencies: List[str] = field(default_factory=list)


@dataclass
class Experiment:
    """Collection of related runs"""
    experiment_id: str
    name: str
    description: str
    runs: List[ExperimentRun] = field(default_factory=list)
    tags: Dict[str, str] = field(default_factory=dict)
    created_at: datetime = field(default_factory=datetime.now)


class ExperimentTracker:
    """
    Abstract experiment tracking interface
    """

    def create_experiment(
        self,
        name: str,
        description: str = ""
    ) -> Experiment:
        """Create new experiment"""
        raise NotImplementedError

    def start_run(
        self,
        experiment_id: str,
        run_name: Optional[str] = None,
        tags: Optional[Dict[str, str]] = None
    ) -> ExperimentRun:
        """Start new run in experiment"""
        raise NotImplementedError

    def log_param(self, name: str, value: Any):
        """Log parameter"""
        raise NotImplementedError

    def log_params(self, params: Dict[str, Any]):
        """Log multiple parameters"""
        raise NotImplementedError

    def log_metric(
        self,
        name: str,
        value: float,
        step: Optional[int] = None
    ):
        """Log metric value"""
        raise NotImplementedError

    def log_metrics(
        self,
        metrics: Dict[str, float],
        step: Optional[int] = None
    ):
        """Log multiple metrics"""
        raise NotImplementedError

    def log_artifact(
        self,
        local_path: str,
        artifact_path: Optional[str] = None
    ):
        """Log artifact file"""
        raise NotImplementedError

    def set_tag(self, key: str, value: str):
        """Set run tag"""
        raise NotImplementedError

    def end_run(self, status: ExperimentStatus = ExperimentStatus.COMPLETED):
        """End current run"""
        raise NotImplementedError
```

### 1.2 What to Track for LLM Experiments

```python
"""
LLM-specific tracking configuration
"""
from dataclasses import dataclass, field
from typing import Dict, Any, List, Optional


@dataclass
class LLMExperimentConfig:
    """
    Comprehensive configuration tracking for LLM experiments.
    Tracks everything needed for full reproducibility.
    """

    # Model configuration
    model_name_or_path: str
    model_type: str  # causal_lm, seq2seq, encoder
    model_architecture: Dict[str, Any] = field(default_factory=dict)

    # Tokenizer
    tokenizer_name: str = ""
    vocab_size: int = 0
    max_length: int = 2048

    # Training hyperparameters
    learning_rate: float = 2e-5
    lr_scheduler_type: str = "cosine"
    warmup_ratio: float = 0.1
    weight_decay: float = 0.01

    batch_size: int = 8
    gradient_accumulation_steps: int = 4
    effective_batch_size: int = 0  # Computed

    num_epochs: float = 3.0
    max_steps: int = -1

    # Optimizer
    optimizer: str = "adamw"
    adam_beta1: float = 0.9
    adam_beta2: float = 0.999
    adam_epsilon: float = 1e-8

    # Precision and efficiency
    precision: str = "bf16"  # fp32, fp16, bf16
    gradient_checkpointing: bool = True
    use_flash_attention: bool = True

    # PEFT configuration
    use_peft: bool = False
    peft_method: str = ""  # lora, qlora, prefix
    lora_r: int = 0
    lora_alpha: int = 0
    lora_target_modules: List[str] = field(default_factory=list)

    # Data configuration
    dataset_name: str = ""
    dataset_version: str = ""
    train_samples: int = 0
    val_samples: int = 0
    data_preprocessing: Dict[str, Any] = field(default_factory=dict)

    # Distributed training
    num_gpus: int = 1
    distributed_strategy: str = "ddp"  # ddp, fsdp, deepspeed
    deepspeed_config: Optional[str] = None

    # Regularization
    dropout: float = 0.0
    label_smoothing: float = 0.0
    max_grad_norm: float = 1.0

    # Evaluation
    eval_strategy: str = "steps"
    eval_steps: int = 500
    save_steps: int = 500

    # Random seeds
    seed: int = 42

    def __post_init__(self):
        """Compute derived values"""
        self.effective_batch_size = (
            self.batch_size *
            self.gradient_accumulation_steps *
            self.num_gpus
        )

    def to_dict(self) -> Dict[str, Any]:
        """Convert to flat dictionary for logging"""
        import dataclasses
        result = {}
        for f in dataclasses.fields(self):
            value = getattr(self, f.name)
            if isinstance(value, dict):
                for k, v in value.items():
                    result[f"{f.name}.{k}"] = v
            elif isinstance(value, list):
                result[f.name] = str(value)
            else:
                result[f.name] = value
        return result


@dataclass
class LLMMetricsConfig:
    """
    Metrics to track during LLM training
    """

    # Training metrics
    training_loss: bool = True
    learning_rate: bool = True
    gradient_norm: bool = True
    tokens_per_second: bool = True

    # Validation metrics
    validation_loss: bool = True
    perplexity: bool = True

    # Generation metrics (if applicable)
    generation_samples: bool = True
    sample_frequency: int = 500

    # Evaluation benchmarks
    eval_benchmarks: List[str] = field(default_factory=list)
    benchmark_frequency: int = 1000

    # Resource metrics
    gpu_memory: bool = True
    gpu_utilization: bool = True
    throughput: bool = True


class LLMExperimentLogger:
    """
    Specialized logger for LLM experiments
    """

    def __init__(
        self,
        tracker: ExperimentTracker,
        config: LLMExperimentConfig,
        metrics_config: LLMMetricsConfig
    ):
        self.tracker = tracker
        self.config = config
        self.metrics_config = metrics_config
        self.step = 0

    def log_config(self):
        """Log full experiment configuration"""
        self.tracker.log_params(self.config.to_dict())

        # Log environment info
        self._log_environment()

    def _log_environment(self):
        """Log environment and dependencies"""
        import sys
        import torch
        import transformers

        env_info = {
            "python_version": sys.version,
            "pytorch_version": torch.__version__,
            "transformers_version": transformers.__version__,
            "cuda_version": torch.version.cuda if torch.cuda.is_available() else "N/A",
            "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0
        }

        if torch.cuda.is_available():
            env_info["gpu_name"] = torch.cuda.get_device_name(0)
            env_info["gpu_memory_gb"] = torch.cuda.get_device_properties(0).total_memory / 1e9

        for key, value in env_info.items():
            self.tracker.set_tag(f"env.{key}", str(value))

    def log_training_step(
        self,
        loss: float,
        learning_rate: float,
        step: int,
        **kwargs
    ):
        """Log training step metrics"""
        metrics = {
            "train/loss": loss,
            "train/learning_rate": learning_rate
        }

        if "gradient_norm" in kwargs and self.metrics_config.gradient_norm:
            metrics["train/gradient_norm"] = kwargs["gradient_norm"]

        if "tokens_per_second" in kwargs and self.metrics_config.tokens_per_second:
            metrics["train/tokens_per_second"] = kwargs["tokens_per_second"]

        self.tracker.log_metrics(metrics, step=step)
        self.step = step

    def log_validation(
        self,
        val_loss: float,
        step: int,
        **kwargs
    ):
        """Log validation metrics"""
        import math

        metrics = {
            "val/loss": val_loss,
            "val/perplexity": math.exp(val_loss) if val_loss < 100 else float('inf')
        }

        # Additional validation metrics
        for key, value in kwargs.items():
            metrics[f"val/{key}"] = value

        self.tracker.log_metrics(metrics, step=step)

    def log_benchmark_results(
        self,
        benchmark_name: str,
        results: Dict[str, float],
        step: int
    ):
        """Log evaluation benchmark results"""
        metrics = {
            f"benchmark/{benchmark_name}/{key}": value
            for key, value in results.items()
        }
        self.tracker.log_metrics(metrics, step=step)

    def log_generation_sample(
        self,
        prompt: str,
        generation: str,
        step: int
    ):
        """Log generation sample for qualitative inspection"""
        # Log as artifact or use platform-specific table logging
        sample = {
            "step": step,
            "prompt": prompt,
            "generation": generation
        }

        # Save to temp file and log
        import tempfile
        import json

        with tempfile.NamedTemporaryFile(
            mode='w', suffix='.json', delete=False
        ) as f:
            json.dump(sample, f, indent=2)
            self.tracker.log_artifact(f.name, f"generations/step_{step}.json")

    def log_resource_usage(self, step: int):
        """Log GPU/CPU resource usage"""
        import torch

        if torch.cuda.is_available():
            metrics = {}
            for i in range(torch.cuda.device_count()):
                metrics[f"resources/gpu_{i}_memory_used_gb"] = (
                    torch.cuda.memory_allocated(i) / 1e9
                )
                metrics[f"resources/gpu_{i}_memory_reserved_gb"] = (
                    torch.cuda.memory_reserved(i) / 1e9
                )

            self.tracker.log_metrics(metrics, step=step)

    def log_checkpoint(
        self,
        checkpoint_path: str,
        step: int,
        metrics: Dict[str, float]
    ):
        """Log model checkpoint"""
        self.tracker.log_artifact(
            checkpoint_path,
            f"checkpoints/step_{step}"
        )

        # Log associated metrics as tags
        for key, value in metrics.items():
            self.tracker.set_tag(f"checkpoint_{step}_{key}", str(value))
```

---

## 2. Weights & Biases Implementation

### 2.1 W&B Setup and Configuration

```python
"""
Weights & Biases experiment tracking implementation
"""
import wandb
from typing import Dict, Any, Optional, List
from pathlib import Path
import os


class WandBConfig:
    """Configuration for W&B tracking"""

    def __init__(
        self,
        project: str,
        entity: Optional[str] = None,
        group: Optional[str] = None,
        job_type: str = "training",
        tags: Optional[List[str]] = None,
        notes: Optional[str] = None,
        save_code: bool = True,
        mode: str = "online"  # online, offline, disabled
    ):
        self.project = project
        self.entity = entity
        self.group = group
        self.job_type = job_type
        self.tags = tags or []
        self.notes = notes
        self.save_code = save_code
        self.mode = mode


class WandBTracker(ExperimentTracker):
    """
    Weights & Biases experiment tracking implementation
    """

    def __init__(self, config: WandBConfig):
        self.config = config
        self.run = None

    def create_experiment(
        self,
        name: str,
        description: str = ""
    ) -> Experiment:
        """W&B experiments are implicitly created via project"""
        return Experiment(
            experiment_id=self.config.project,
            name=name,
            description=description
        )

    def start_run(
        self,
        experiment_id: str,
        run_name: Optional[str] = None,
        tags: Optional[Dict[str, str]] = None,
        config: Optional[Dict[str, Any]] = None
    ) -> ExperimentRun:
        """Start new W&B run"""

        all_tags = self.config.tags.copy()
        if tags:
            all_tags.extend([f"{k}:{v}" for k, v in tags.items()])

        self.run = wandb.init(
            project=self.config.project,
            entity=self.config.entity,
            group=self.config.group,
            job_type=self.config.job_type,
            name=run_name,
            tags=all_tags,
            notes=self.config.notes,
            save_code=self.config.save_code,
            mode=self.config.mode,
            config=config
        )

        return ExperimentRun(
            run_id=self.run.id,
            experiment_id=experiment_id,
            name=run_name or self.run.name,
            status=ExperimentStatus.RUNNING,
            start_time=datetime.now()
        )

    def log_param(self, name: str, value: Any):
        """Log single parameter"""
        if self.run:
            self.run.config[name] = value

    def log_params(self, params: Dict[str, Any]):
        """Log multiple parameters"""
        if self.run:
            self.run.config.update(params)

    def log_metric(
        self,
        name: str,
        value: float,
        step: Optional[int] = None
    ):
        """Log single metric"""
        if self.run:
            self.run.log({name: value}, step=step)

    def log_metrics(
        self,
        metrics: Dict[str, float],
        step: Optional[int] = None
    ):
        """Log multiple metrics"""
        if self.run:
            self.run.log(metrics, step=step)

    def log_artifact(
        self,
        local_path: str,
        artifact_path: Optional[str] = None,
        artifact_type: str = "output"
    ):
        """Log artifact to W&B"""
        if self.run:
            artifact_name = artifact_path or Path(local_path).name
            artifact = wandb.Artifact(
                name=artifact_name.replace("/", "_"),
                type=artifact_type
            )

            if os.path.isdir(local_path):
                artifact.add_dir(local_path)
            else:
                artifact.add_file(local_path)

            self.run.log_artifact(artifact)

    def log_model(
        self,
        model_path: str,
        model_name: str,
        metadata: Optional[Dict[str, Any]] = None
    ):
        """Log model artifact with metadata"""
        if self.run:
            artifact = wandb.Artifact(
                name=model_name,
                type="model",
                metadata=metadata or {}
            )
            artifact.add_dir(model_path)
            self.run.log_artifact(artifact)

    def set_tag(self, key: str, value: str):
        """Set run tag"""
        if self.run:
            self.run.tags = self.run.tags + (f"{key}:{value}",)

    def log_table(
        self,
        table_name: str,
        columns: List[str],
        data: List[List[Any]]
    ):
        """Log tabular data"""
        if self.run:
            table = wandb.Table(columns=columns, data=data)
            self.run.log({table_name: table})

    def log_image(
        self,
        name: str,
        image_path: str,
        caption: Optional[str] = None
    ):
        """Log image"""
        if self.run:
            self.run.log({
                name: wandb.Image(image_path, caption=caption)
            })

    def log_histogram(
        self,
        name: str,
        values: List[float],
        step: Optional[int] = None
    ):
        """Log histogram"""
        if self.run:
            self.run.log({
                name: wandb.Histogram(values)
            }, step=step)

    def watch_model(
        self,
        model,
        log_freq: int = 100,
        log_gradients: bool = True
    ):
        """Watch model for gradient and parameter logging"""
        if self.run:
            wandb.watch(
                model,
                log="all" if log_gradients else "parameters",
                log_freq=log_freq
            )

    def end_run(self, status: ExperimentStatus = ExperimentStatus.COMPLETED):
        """End current run"""
        if self.run:
            # Mark run status
            if status == ExperimentStatus.FAILED:
                self.run.finish(exit_code=1)
            else:
                self.run.finish()
            self.run = None

    def alert(
        self,
        title: str,
        text: str,
        level: str = "INFO"  # INFO, WARN, ERROR
    ):
        """Send alert notification"""
        if self.run:
            wandb.alert(
                title=title,
                text=text,
                level=getattr(wandb.AlertLevel, level)
            )


class WandBCallbackHandler:
    """
    Callback handler for HuggingFace Trainer integration
    """

    def __init__(self, tracker: WandBTracker):
        self.tracker = tracker

    def on_train_begin(self, args, state, control, **kwargs):
        """Called at the beginning of training"""
        self.tracker.log_params({
            "training_args": vars(args)
        })

    def on_log(self, args, state, control, logs=None, **kwargs):
        """Called when logging"""
        if logs:
            self.tracker.log_metrics(logs, step=state.global_step)

    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        """Called after evaluation"""
        if metrics:
            eval_metrics = {
                f"eval/{k}": v for k, v in metrics.items()
            }
            self.tracker.log_metrics(eval_metrics, step=state.global_step)

    def on_save(self, args, state, control, **kwargs):
        """Called when saving checkpoint"""
        self.tracker.set_tag(
            "latest_checkpoint_step",
            str(state.global_step)
        )

    def on_train_end(self, args, state, control, **kwargs):
        """Called at the end of training"""
        self.tracker.end_run()
```

### 2.2 W&B Sweeps for Hyperparameter Optimization

```python
"""
W&B Sweeps for hyperparameter search
"""
import wandb
from typing import Dict, Any, Optional, Callable
from dataclasses import dataclass


@dataclass
class SweepConfig:
    """Configuration for W&B sweep"""
    method: str = "bayes"  # grid, random, bayes
    metric_name: str = "val/loss"
    metric_goal: str = "minimize"  # minimize, maximize
    parameters: Dict[str, Any] = None
    early_terminate: Optional[Dict[str, Any]] = None


class WandBSweepManager:
    """
    Manage hyperparameter sweeps with W&B
    """

    def __init__(
        self,
        project: str,
        entity: Optional[str] = None
    ):
        self.project = project
        self.entity = entity

    def create_sweep(self, config: SweepConfig) -> str:
        """Create a new sweep and return sweep ID"""

        sweep_config = {
            "method": config.method,
            "metric": {
                "name": config.metric_name,
                "goal": config.metric_goal
            },
            "parameters": config.parameters
        }

        if config.early_terminate:
            sweep_config["early_terminate"] = config.early_terminate

        sweep_id = wandb.sweep(
            sweep_config,
            project=self.project,
            entity=self.entity
        )

        return sweep_id

    def run_sweep(
        self,
        sweep_id: str,
        train_function: Callable,
        count: int = 10
    ):
        """Run sweep agents"""
        wandb.agent(
            sweep_id,
            function=train_function,
            count=count,
            project=self.project,
            entity=self.entity
        )


def create_llm_sweep_config() -> SweepConfig:
    """
    Example sweep configuration for LLM fine-tuning
    """
    return SweepConfig(
        method="bayes",
        metric_name="val/loss",
        metric_goal="minimize",
        parameters={
            "learning_rate": {
                "distribution": "log_uniform_values",
                "min": 1e-6,
                "max": 1e-4
            },
            "batch_size": {
                "values": [2, 4, 8, 16]
            },
            "warmup_ratio": {
                "distribution": "uniform",
                "min": 0.0,
                "max": 0.2
            },
            "weight_decay": {
                "distribution": "log_uniform_values",
                "min": 0.001,
                "max": 0.1
            },
            "lora_r": {
                "values": [8, 16, 32, 64]
            },
            "lora_alpha": {
                "values": [16, 32, 64, 128]
            },
            "num_epochs": {
                "values": [1, 2, 3]
            }
        },
        early_terminate={
            "type": "hyperband",
            "min_iter": 100,
            "s": 2
        }
    )


def sweep_train_function():
    """
    Example training function for sweep
    """
    # Initialize run - W&B passes config via wandb.config
    run = wandb.init()

    # Get hyperparameters from sweep
    config = wandb.config

    # Create training config from sweep params
    train_config = LLMExperimentConfig(
        model_name_or_path="meta-llama/Llama-2-7b-hf",
        learning_rate=config.learning_rate,
        batch_size=config.batch_size,
        warmup_ratio=config.warmup_ratio,
        weight_decay=config.weight_decay,
        use_peft=True,
        peft_method="lora",
        lora_r=config.lora_r,
        lora_alpha=config.lora_alpha,
        num_epochs=config.num_epochs
    )

    # Train model
    # trainer = create_trainer(train_config)
    # trainer.train()

    # Log final metrics
    # wandb.log({"val/loss": final_loss})

    run.finish()
```

---

## 3. MLflow Tracking Implementation

### 3.1 MLflow Setup

```python
"""
MLflow experiment tracking implementation
"""
import mlflow
from mlflow.tracking import MlflowClient
from typing import Dict, Any, Optional, List
from pathlib import Path
import os


class MLflowConfig:
    """Configuration for MLflow tracking"""

    def __init__(
        self,
        tracking_uri: str = "http://localhost:5000",
        artifact_location: Optional[str] = None,
        experiment_name: str = "default"
    ):
        self.tracking_uri = tracking_uri
        self.artifact_location = artifact_location
        self.experiment_name = experiment_name

    def configure(self):
        """Apply MLflow configuration"""
        mlflow.set_tracking_uri(self.tracking_uri)


class MLflowTracker(ExperimentTracker):
    """
    MLflow experiment tracking implementation
    """

    def __init__(self, config: MLflowConfig):
        self.config = config
        config.configure()
        self.client = MlflowClient()
        self.current_run = None
        self.experiment_id = None

    def create_experiment(
        self,
        name: str,
        description: str = ""
    ) -> Experiment:
        """Create or get MLflow experiment"""

        # Check if experiment exists
        experiment = mlflow.get_experiment_by_name(name)

        if experiment is None:
            experiment_id = mlflow.create_experiment(
                name=name,
                artifact_location=self.config.artifact_location,
                tags={"description": description}
            )
            experiment = mlflow.get_experiment(experiment_id)

        self.experiment_id = experiment.experiment_id

        return Experiment(
            experiment_id=experiment.experiment_id,
            name=name,
            description=description
        )

    def start_run(
        self,
        experiment_id: str,
        run_name: Optional[str] = None,
        tags: Optional[Dict[str, str]] = None,
        **kwargs
    ) -> ExperimentRun:
        """Start new MLflow run"""

        self.current_run = mlflow.start_run(
            experiment_id=experiment_id,
            run_name=run_name,
            tags=tags
        )

        # Log git info automatically
        self._log_git_info()

        return ExperimentRun(
            run_id=self.current_run.info.run_id,
            experiment_id=experiment_id,
            name=run_name or self.current_run.info.run_name,
            status=ExperimentStatus.RUNNING,
            start_time=datetime.now()
        )

    def _log_git_info(self):
        """Log git commit and branch info"""
        try:
            import subprocess
            git_commit = subprocess.check_output(
                ["git", "rev-parse", "HEAD"]
            ).decode().strip()
            git_branch = subprocess.check_output(
                ["git", "rev-parse", "--abbrev-ref", "HEAD"]
            ).decode().strip()

            mlflow.set_tag("git.commit", git_commit)
            mlflow.set_tag("git.branch", git_branch)
        except:
            pass

    def log_param(self, name: str, value: Any):
        """Log single parameter"""
        mlflow.log_param(name, value)

    def log_params(self, params: Dict[str, Any]):
        """Log multiple parameters"""
        # MLflow has a limit on param value length, handle long values
        processed_params = {}
        for k, v in params.items():
            str_v = str(v)
            if len(str_v) > 500:
                # Store as artifact instead
                self._log_long_param(k, v)
            else:
                processed_params[k] = v

        if processed_params:
            mlflow.log_params(processed_params)

    def _log_long_param(self, name: str, value: Any):
        """Log long parameter as artifact"""
        import tempfile
        import json

        with tempfile.NamedTemporaryFile(
            mode='w', suffix='.json', delete=False
        ) as f:
            json.dump({name: value}, f, indent=2)
            mlflow.log_artifact(f.name, "params")

    def log_metric(
        self,
        name: str,
        value: float,
        step: Optional[int] = None
    ):
        """Log single metric"""
        mlflow.log_metric(name, value, step=step)

    def log_metrics(
        self,
        metrics: Dict[str, float],
        step: Optional[int] = None
    ):
        """Log multiple metrics"""
        mlflow.log_metrics(metrics, step=step)

    def log_artifact(
        self,
        local_path: str,
        artifact_path: Optional[str] = None
    ):
        """Log artifact file or directory"""
        if os.path.isdir(local_path):
            mlflow.log_artifacts(local_path, artifact_path)
        else:
            mlflow.log_artifact(local_path, artifact_path)

    def log_model(
        self,
        model,
        artifact_path: str = "model",
        registered_model_name: Optional[str] = None,
        **kwargs
    ):
        """Log model using appropriate MLflow flavor"""

        # Detect model type and use appropriate flavor
        model_type = type(model).__module__

        if "transformers" in model_type:
            mlflow.transformers.log_model(
                transformers_model=model,
                artifact_path=artifact_path,
                registered_model_name=registered_model_name,
                **kwargs
            )
        elif "torch" in model_type:
            mlflow.pytorch.log_model(
                pytorch_model=model,
                artifact_path=artifact_path,
                registered_model_name=registered_model_name,
                **kwargs
            )
        else:
            # Generic pyfunc model
            mlflow.pyfunc.log_model(
                artifact_path=artifact_path,
                python_model=model,
                registered_model_name=registered_model_name,
                **kwargs
            )

    def set_tag(self, key: str, value: str):
        """Set run tag"""
        mlflow.set_tag(key, value)

    def end_run(self, status: ExperimentStatus = ExperimentStatus.COMPLETED):
        """End current run"""
        status_map = {
            ExperimentStatus.COMPLETED: "FINISHED",
            ExperimentStatus.FAILED: "FAILED",
            ExperimentStatus.KILLED: "KILLED"
        }
        mlflow.end_run(status=status_map.get(status, "FINISHED"))
        self.current_run = None

    def search_runs(
        self,
        experiment_ids: Optional[List[str]] = None,
        filter_string: str = "",
        order_by: Optional[List[str]] = None,
        max_results: int = 100
    ) -> List[Dict[str, Any]]:
        """Search runs with filter"""
        runs = mlflow.search_runs(
            experiment_ids=experiment_ids or [self.experiment_id],
            filter_string=filter_string,
            order_by=order_by,
            max_results=max_results
        )
        return runs.to_dict(orient="records")


class MLflowAutolog:
    """
    Configure MLflow autologging for different frameworks
    """

    @staticmethod
    def enable_transformers(
        log_models: bool = True,
        log_input_examples: bool = False,
        log_model_signatures: bool = True
    ):
        """Enable autologging for HuggingFace Transformers"""
        mlflow.transformers.autolog(
            log_models=log_models,
            log_input_examples=log_input_examples,
            log_model_signatures=log_model_signatures
        )

    @staticmethod
    def enable_pytorch(
        log_models: bool = True,
        log_every_n_step: int = 100
    ):
        """Enable autologging for PyTorch"""
        mlflow.pytorch.autolog(
            log_models=log_models,
            log_every_n_step=log_every_n_step
        )

    @staticmethod
    def disable_all():
        """Disable all autologging"""
        mlflow.autolog(disable=True)
```

---

## 4. Custom Tracking Implementation

### 4.1 Filesystem-Based Tracker

```python
"""
Simple filesystem-based experiment tracker
"""
import json
import os
import shutil
from pathlib import Path
from typing import Dict, Any, Optional, List
from datetime import datetime
import uuid


class FilesystemTracker(ExperimentTracker):
    """
    Filesystem-based experiment tracking for offline/simple use cases
    """

    def __init__(self, base_dir: str = "./experiments"):
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(parents=True, exist_ok=True)
        self.current_run_dir: Optional[Path] = None
        self.current_run_id: Optional[str] = None
        self.metrics_buffer: Dict[str, List[Dict]] = {}

    def create_experiment(
        self,
        name: str,
        description: str = ""
    ) -> Experiment:
        """Create experiment directory"""
        exp_dir = self.base_dir / name
        exp_dir.mkdir(exist_ok=True)

        # Save experiment metadata
        metadata = {
            "name": name,
            "description": description,
            "created_at": datetime.now().isoformat()
        }

        with open(exp_dir / "experiment.json", "w") as f:
            json.dump(metadata, f, indent=2)

        return Experiment(
            experiment_id=name,
            name=name,
            description=description
        )

    def start_run(
        self,
        experiment_id: str,
        run_name: Optional[str] = None,
        tags: Optional[Dict[str, str]] = None,
        **kwargs
    ) -> ExperimentRun:
        """Start new run"""
        self.current_run_id = run_name or str(uuid.uuid4())[:8]
        self.current_run_dir = self.base_dir / experiment_id / self.current_run_id

        self.current_run_dir.mkdir(parents=True, exist_ok=True)
        (self.current_run_dir / "artifacts").mkdir(exist_ok=True)
        (self.current_run_dir / "metrics").mkdir(exist_ok=True)

        # Initialize run metadata
        metadata = {
            "run_id": self.current_run_id,
            "experiment_id": experiment_id,
            "name": run_name,
            "status": "running",
            "start_time": datetime.now().isoformat(),
            "tags": tags or {},
            "params": {}
        }

        self._save_metadata(metadata)
        self.metrics_buffer = {}

        return ExperimentRun(
            run_id=self.current_run_id,
            experiment_id=experiment_id,
            name=run_name or self.current_run_id,
            status=ExperimentStatus.RUNNING,
            start_time=datetime.now()
        )

    def log_param(self, name: str, value: Any):
        """Log parameter"""
        metadata = self._load_metadata()
        metadata["params"][name] = value
        self._save_metadata(metadata)

    def log_params(self, params: Dict[str, Any]):
        """Log multiple parameters"""
        metadata = self._load_metadata()
        metadata["params"].update(params)
        self._save_metadata(metadata)

    def log_metric(
        self,
        name: str,
        value: float,
        step: Optional[int] = None
    ):
        """Log metric value"""
        if name not in self.metrics_buffer:
            self.metrics_buffer[name] = []

        self.metrics_buffer[name].append({
            "value": value,
            "step": step,
            "timestamp": datetime.now().isoformat()
        })

        # Periodically flush to disk
        if len(self.metrics_buffer[name]) >= 100:
            self._flush_metrics(name)

    def log_metrics(
        self,
        metrics: Dict[str, float],
        step: Optional[int] = None
    ):
        """Log multiple metrics"""
        for name, value in metrics.items():
            self.log_metric(name, value, step)

    def log_artifact(
        self,
        local_path: str,
        artifact_path: Optional[str] = None
    ):
        """Copy artifact to run directory"""
        if not self.current_run_dir:
            raise ValueError("No active run")

        src = Path(local_path)
        dest_name = artifact_path or src.name
        dest = self.current_run_dir / "artifacts" / dest_name

        dest.parent.mkdir(parents=True, exist_ok=True)

        if src.is_dir():
            shutil.copytree(src, dest, dirs_exist_ok=True)
        else:
            shutil.copy2(src, dest)

    def set_tag(self, key: str, value: str):
        """Set run tag"""
        metadata = self._load_metadata()
        metadata["tags"][key] = value
        self._save_metadata(metadata)

    def end_run(self, status: ExperimentStatus = ExperimentStatus.COMPLETED):
        """End current run"""
        # Flush all metrics
        for name in list(self.metrics_buffer.keys()):
            self._flush_metrics(name)

        # Update metadata
        metadata = self._load_metadata()
        metadata["status"] = status.value
        metadata["end_time"] = datetime.now().isoformat()
        self._save_metadata(metadata)

        self.current_run_dir = None
        self.current_run_id = None

    def _load_metadata(self) -> Dict:
        """Load run metadata"""
        with open(self.current_run_dir / "run.json") as f:
            return json.load(f)

    def _save_metadata(self, metadata: Dict):
        """Save run metadata"""
        with open(self.current_run_dir / "run.json", "w") as f:
            json.dump(metadata, f, indent=2)

    def _flush_metrics(self, name: str):
        """Flush metric buffer to disk"""
        if not self.metrics_buffer.get(name):
            return

        metric_file = self.current_run_dir / "metrics" / f"{name.replace('/', '_')}.jsonl"

        with open(metric_file, "a") as f:
            for entry in self.metrics_buffer[name]:
                f.write(json.dumps(entry) + "\n")

        self.metrics_buffer[name] = []

    def load_run(
        self,
        experiment_id: str,
        run_id: str
    ) -> Dict[str, Any]:
        """Load run data for analysis"""
        run_dir = self.base_dir / experiment_id / run_id

        # Load metadata
        with open(run_dir / "run.json") as f:
            metadata = json.load(f)

        # Load metrics
        metrics = {}
        metrics_dir = run_dir / "metrics"
        if metrics_dir.exists():
            for metric_file in metrics_dir.glob("*.jsonl"):
                metric_name = metric_file.stem.replace("_", "/")
                metrics[metric_name] = []
                with open(metric_file) as f:
                    for line in f:
                        metrics[metric_name].append(json.loads(line))

        return {
            "metadata": metadata,
            "metrics": metrics
        }

    def compare_runs(
        self,
        experiment_id: str,
        run_ids: List[str],
        metric_names: List[str]
    ) -> Dict[str, Any]:
        """Compare multiple runs"""
        comparison = {
            "runs": {},
            "best": {}
        }

        for run_id in run_ids:
            run_data = self.load_run(experiment_id, run_id)
            comparison["runs"][run_id] = {
                "params": run_data["metadata"]["params"],
                "metrics": {}
            }

            for metric_name in metric_names:
                if metric_name in run_data["metrics"]:
                    values = [
                        m["value"] for m in run_data["metrics"][metric_name]
                    ]
                    comparison["runs"][run_id]["metrics"][metric_name] = {
                        "min": min(values),
                        "max": max(values),
                        "final": values[-1] if values else None
                    }

        # Find best runs for each metric
        for metric_name in metric_names:
            best_run = None
            best_value = float("inf")
            for run_id, data in comparison["runs"].items():
                if metric_name in data["metrics"]:
                    value = data["metrics"][metric_name]["final"]
                    if value and value < best_value:
                        best_value = value
                        best_run = run_id
            comparison["best"][metric_name] = best_run

        return comparison
```

---

## 5. Framework Integrations

### 5.1 HuggingFace Trainer Integration

```python
"""
HuggingFace Trainer callback for experiment tracking
"""
from transformers import TrainerCallback, TrainerState, TrainerControl
from transformers.trainer_callback import TrainerCallback
from typing import Dict, Any, Optional
import math


class ExperimentTrackingCallback(TrainerCallback):
    """
    Generic experiment tracking callback for HuggingFace Trainer
    """

    def __init__(
        self,
        tracker: ExperimentTracker,
        log_model: bool = True,
        log_generations: bool = True,
        generation_frequency: int = 500
    ):
        self.tracker = tracker
        self.log_model = log_model
        self.log_generations = log_generations
        self.generation_frequency = generation_frequency
        self.best_metric = None

    def on_train_begin(
        self,
        args,
        state: TrainerState,
        control: TrainerControl,
        **kwargs
    ):
        """Log training configuration at start"""
        # Log training arguments
        train_args = {
            "learning_rate": args.learning_rate,
            "per_device_train_batch_size": args.per_device_train_batch_size,
            "per_device_eval_batch_size": args.per_device_eval_batch_size,
            "gradient_accumulation_steps": args.gradient_accumulation_steps,
            "num_train_epochs": args.num_train_epochs,
            "max_steps": args.max_steps,
            "warmup_ratio": args.warmup_ratio,
            "warmup_steps": args.warmup_steps,
            "weight_decay": args.weight_decay,
            "adam_beta1": args.adam_beta1,
            "adam_beta2": args.adam_beta2,
            "adam_epsilon": args.adam_epsilon,
            "max_grad_norm": args.max_grad_norm,
            "lr_scheduler_type": str(args.lr_scheduler_type),
            "seed": args.seed,
            "fp16": args.fp16,
            "bf16": args.bf16,
            "gradient_checkpointing": args.gradient_checkpointing
        }

        self.tracker.log_params(train_args)

        # Log model info if available
        model = kwargs.get("model")
        if model:
            model_params = sum(p.numel() for p in model.parameters())
            trainable_params = sum(
                p.numel() for p in model.parameters() if p.requires_grad
            )
            self.tracker.log_params({
                "model_params": model_params,
                "trainable_params": trainable_params,
                "trainable_ratio": trainable_params / model_params
            })

    def on_log(
        self,
        args,
        state: TrainerState,
        control: TrainerControl,
        logs: Optional[Dict[str, float]] = None,
        **kwargs
    ):
        """Log metrics during training"""
        if logs is None:
            return

        # Process logs
        metrics = {}
        for key, value in logs.items():
            if isinstance(value, (int, float)):
                # Categorize metric
                if key.startswith("eval_"):
                    metrics[f"eval/{key[5:]}"] = value
                elif key == "loss":
                    metrics["train/loss"] = value
                elif key == "learning_rate":
                    metrics["train/learning_rate"] = value
                else:
                    metrics[f"train/{key}"] = value

        # Add perplexity if loss is available
        if "train/loss" in metrics:
            loss = metrics["train/loss"]
            if loss < 100:  # Prevent overflow
                metrics["train/perplexity"] = math.exp(loss)

        if "eval/loss" in metrics:
            loss = metrics["eval/loss"]
            if loss < 100:
                metrics["eval/perplexity"] = math.exp(loss)

        self.tracker.log_metrics(metrics, step=state.global_step)

    def on_evaluate(
        self,
        args,
        state: TrainerState,
        control: TrainerControl,
        metrics: Optional[Dict[str, float]] = None,
        **kwargs
    ):
        """Log evaluation metrics"""
        if metrics is None:
            return

        eval_metrics = {
            f"eval/{k.replace('eval_', '')}": v
            for k, v in metrics.items()
            if isinstance(v, (int, float))
        }

        self.tracker.log_metrics(eval_metrics, step=state.global_step)

        # Track best metric
        if "eval_loss" in metrics:
            current = metrics["eval_loss"]
            if self.best_metric is None or current < self.best_metric:
                self.best_metric = current
                self.tracker.set_tag("best_eval_loss", str(current))
                self.tracker.set_tag("best_step", str(state.global_step))

    def on_save(
        self,
        args,
        state: TrainerState,
        control: TrainerControl,
        **kwargs
    ):
        """Log checkpoint"""
        checkpoint_dir = f"{args.output_dir}/checkpoint-{state.global_step}"
        self.tracker.set_tag(
            "latest_checkpoint",
            str(state.global_step)
        )

        if self.log_model:
            self.tracker.log_artifact(
                checkpoint_dir,
                f"checkpoints/step-{state.global_step}"
            )

    def on_train_end(
        self,
        args,
        state: TrainerState,
        control: TrainerControl,
        **kwargs
    ):
        """Finalize tracking at end of training"""
        # Log final model
        if self.log_model:
            self.tracker.log_artifact(
                args.output_dir,
                "final_model"
            )

        # Log training summary
        self.tracker.log_metrics({
            "summary/total_steps": state.global_step,
            "summary/total_epochs": state.epoch,
            "summary/best_eval_loss": self.best_metric or 0
        })


class GenerationLoggingCallback(TrainerCallback):
    """
    Log generation samples during training
    """

    def __init__(
        self,
        tracker: ExperimentTracker,
        tokenizer,
        eval_prompts: list,
        frequency: int = 500,
        max_new_tokens: int = 100
    ):
        self.tracker = tracker
        self.tokenizer = tokenizer
        self.eval_prompts = eval_prompts
        self.frequency = frequency
        self.max_new_tokens = max_new_tokens

    def on_step_end(
        self,
        args,
        state: TrainerState,
        control: TrainerControl,
        **kwargs
    ):
        """Log generation samples at specified frequency"""
        if state.global_step % self.frequency != 0:
            return

        model = kwargs.get("model")
        if model is None:
            return

        model.eval()
        samples = []

        for prompt in self.eval_prompts[:3]:  # Limit to 3 samples
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt"
            ).to(model.device)

            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=self.max_new_tokens,
                    do_sample=True,
                    temperature=0.7,
                    pad_token_id=self.tokenizer.pad_token_id
                )

            generation = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True
            )

            samples.append({
                "prompt": prompt,
                "generation": generation
            })

        # Log as table (W&B specific) or artifact
        if hasattr(self.tracker, 'log_table'):
            self.tracker.log_table(
                f"generations/step_{state.global_step}",
                columns=["prompt", "generation"],
                data=[[s["prompt"], s["generation"]] for s in samples]
            )
        else:
            import tempfile
            import json
            with tempfile.NamedTemporaryFile(
                mode='w', suffix='.json', delete=False
            ) as f:
                json.dump(samples, f, indent=2)
                self.tracker.log_artifact(
                    f.name,
                    f"generations/step_{state.global_step}.json"
                )

        model.train()
```

### 5.2 PyTorch Lightning Integration

```python
"""
PyTorch Lightning integration for experiment tracking
"""
from pytorch_lightning.callbacks import Callback
from pytorch_lightning.loggers import LightningLoggerBase
from typing import Dict, Any, Optional, List
import torch


class ExperimentTrackerLogger(LightningLoggerBase):
    """
    Lightning logger that wraps our ExperimentTracker
    """

    def __init__(
        self,
        tracker: ExperimentTracker,
        experiment_name: str
    ):
        super().__init__()
        self.tracker = tracker
        self._experiment_name = experiment_name
        self._experiment = None
        self._run = None

    @property
    def name(self) -> str:
        return self._experiment_name

    @property
    def version(self) -> str:
        return self._run.run_id if self._run else "0"

    @property
    def experiment(self):
        if self._experiment is None:
            self._experiment = self.tracker.create_experiment(
                self._experiment_name
            )
        return self._experiment

    def log_hyperparams(self, params: Dict[str, Any]):
        """Log hyperparameters"""
        # Flatten nested dicts
        flat_params = self._flatten_dict(params)
        self.tracker.log_params(flat_params)

    def log_metrics(
        self,
        metrics: Dict[str, float],
        step: Optional[int] = None
    ):
        """Log metrics"""
        self.tracker.log_metrics(metrics, step=step)

    def save(self):
        """Save tracked data"""
        pass  # Handled by tracker

    def finalize(self, status: str):
        """Finalize logging"""
        status_map = {
            "success": ExperimentStatus.COMPLETED,
            "failed": ExperimentStatus.FAILED,
            "interrupted": ExperimentStatus.KILLED
        }
        self.tracker.end_run(status_map.get(status, ExperimentStatus.COMPLETED))

    def _flatten_dict(
        self,
        d: Dict,
        parent_key: str = "",
        sep: str = "."
    ) -> Dict:
        """Flatten nested dictionary"""
        items = []
        for k, v in d.items():
            new_key = f"{parent_key}{sep}{k}" if parent_key else k
            if isinstance(v, dict):
                items.extend(self._flatten_dict(v, new_key, sep).items())
            else:
                items.append((new_key, v))
        return dict(items)


class ResourceMonitorCallback(Callback):
    """
    Monitor and log resource usage during training
    """

    def __init__(
        self,
        tracker: ExperimentTracker,
        log_frequency: int = 100
    ):
        self.tracker = tracker
        self.log_frequency = log_frequency

    def on_train_batch_end(
        self,
        trainer,
        pl_module,
        outputs,
        batch,
        batch_idx
    ):
        """Log resources periodically"""
        if batch_idx % self.log_frequency != 0:
            return

        metrics = {}

        # GPU metrics
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                metrics[f"resources/gpu_{i}_memory_allocated_gb"] = (
                    torch.cuda.memory_allocated(i) / 1e9
                )
                metrics[f"resources/gpu_{i}_memory_reserved_gb"] = (
                    torch.cuda.memory_reserved(i) / 1e9
                )

                # Get utilization if nvidia-smi available
                try:
                    import subprocess
                    result = subprocess.check_output([
                        'nvidia-smi',
                        f'--id={i}',
                        '--query-gpu=utilization.gpu',
                        '--format=csv,noheader,nounits'
                    ]).decode().strip()
                    metrics[f"resources/gpu_{i}_utilization"] = float(result)
                except:
                    pass

        # CPU and memory metrics
        try:
            import psutil
            metrics["resources/cpu_percent"] = psutil.cpu_percent()
            metrics["resources/memory_percent"] = psutil.virtual_memory().percent
        except:
            pass

        step = trainer.global_step
        self.tracker.log_metrics(metrics, step=step)
```

---

## 6. Analysis and Visualization

### 6.1 Experiment Comparison

```python
"""
Experiment analysis and comparison utilities
"""
from typing import List, Dict, Any, Optional
import pandas as pd
from dataclasses import dataclass


@dataclass
class RunSummary:
    """Summary of a single run"""
    run_id: str
    experiment_id: str
    name: str
    status: str
    params: Dict[str, Any]
    final_metrics: Dict[str, float]
    best_metrics: Dict[str, float]
    duration_seconds: float


class ExperimentAnalyzer:
    """
    Analyze and compare experiment runs
    """

    def __init__(self, tracker: ExperimentTracker):
        self.tracker = tracker

    def get_run_summaries(
        self,
        experiment_id: str,
        filter_status: Optional[str] = None
    ) -> List[RunSummary]:
        """Get summaries of all runs in experiment"""
        # Implementation depends on tracker type
        pass

    def runs_to_dataframe(
        self,
        experiment_id: str,
        include_metrics: List[str] = None
    ) -> pd.DataFrame:
        """Convert runs to pandas DataFrame for analysis"""
        summaries = self.get_run_summaries(experiment_id)

        rows = []
        for summary in summaries:
            row = {
                "run_id": summary.run_id,
                "name": summary.name,
                "status": summary.status,
                "duration_s": summary.duration_seconds,
                **{f"param.{k}": v for k, v in summary.params.items()},
                **{f"metric.{k}": v for k, v in summary.final_metrics.items()}
            }
            rows.append(row)

        return pd.DataFrame(rows)

    def find_best_runs(
        self,
        experiment_id: str,
        metric: str,
        n: int = 5,
        minimize: bool = True
    ) -> List[RunSummary]:
        """Find top N runs by metric"""
        summaries = self.get_run_summaries(
            experiment_id,
            filter_status="completed"
        )

        # Sort by metric
        sorted_runs = sorted(
            summaries,
            key=lambda x: x.final_metrics.get(metric, float('inf')),
            reverse=not minimize
        )

        return sorted_runs[:n]

    def parameter_importance(
        self,
        experiment_id: str,
        metric: str
    ) -> Dict[str, float]:
        """
        Estimate parameter importance using correlation analysis.
        Returns dict of param_name -> correlation with metric.
        """
        df = self.runs_to_dataframe(experiment_id)

        metric_col = f"metric.{metric}"
        if metric_col not in df.columns:
            return {}

        importances = {}
        param_cols = [c for c in df.columns if c.startswith("param.")]

        for col in param_cols:
            try:
                # Convert to numeric if possible
                values = pd.to_numeric(df[col], errors='coerce')
                if values.notna().sum() > 1:
                    corr = values.corr(df[metric_col])
                    if not pd.isna(corr):
                        importances[col.replace("param.", "")] = abs(corr)
            except:
                pass

        return dict(sorted(
            importances.items(),
            key=lambda x: x[1],
            reverse=True
        ))

    def generate_comparison_report(
        self,
        experiment_id: str,
        run_ids: List[str],
        metrics: List[str]
    ) -> Dict[str, Any]:
        """Generate comparison report for selected runs"""
        report = {
            "runs": {},
            "comparison": {},
            "recommendation": None
        }

        for run_id in run_ids:
            summary = self._get_run_summary(experiment_id, run_id)
            report["runs"][run_id] = {
                "params": summary.params,
                "metrics": {
                    m: summary.final_metrics.get(m)
                    for m in metrics
                }
            }

        # Compare metrics
        for metric in metrics:
            values = {
                run_id: data["metrics"][metric]
                for run_id, data in report["runs"].items()
                if data["metrics"][metric] is not None
            }

            if values:
                best_run = min(values.items(), key=lambda x: x[1])
                report["comparison"][metric] = {
                    "best_run": best_run[0],
                    "best_value": best_run[1],
                    "all_values": values
                }

        return report


class VisualizationGenerator:
    """
    Generate visualizations for experiment analysis
    """

    def __init__(self, analyzer: ExperimentAnalyzer):
        self.analyzer = analyzer

    def plot_learning_curves(
        self,
        experiment_id: str,
        run_ids: List[str],
        metric: str = "train/loss",
        save_path: Optional[str] = None
    ):
        """Plot learning curves for multiple runs"""
        import matplotlib.pyplot as plt

        fig, ax = plt.subplots(figsize=(10, 6))

        for run_id in run_ids:
            # Get metric history
            # Implementation depends on tracker
            steps = []
            values = []

            ax.plot(steps, values, label=run_id)

        ax.set_xlabel("Step")
        ax.set_ylabel(metric)
        ax.set_title(f"Learning Curves - {metric}")
        ax.legend()
        ax.grid(True, alpha=0.3)

        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
        else:
            plt.show()

    def plot_hyperparameter_importance(
        self,
        experiment_id: str,
        metric: str,
        save_path: Optional[str] = None
    ):
        """Plot hyperparameter importance"""
        import matplotlib.pyplot as plt

        importances = self.analyzer.parameter_importance(
            experiment_id, metric
        )

        if not importances:
            return

        params = list(importances.keys())[:10]  # Top 10
        values = [importances[p] for p in params]

        fig, ax = plt.subplots(figsize=(10, 6))
        ax.barh(params, values)
        ax.set_xlabel("Correlation with metric")
        ax.set_title(f"Hyperparameter Importance for {metric}")

        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
        else:
            plt.show()

    def plot_parallel_coordinates(
        self,
        experiment_id: str,
        params: List[str],
        metric: str,
        save_path: Optional[str] = None
    ):
        """Plot parallel coordinates for hyperparameter exploration"""
        import matplotlib.pyplot as plt
        from pandas.plotting import parallel_coordinates

        df = self.analyzer.runs_to_dataframe(experiment_id)

        # Select columns
        cols = [f"param.{p}" for p in params] + [f"metric.{metric}"]
        cols = [c for c in cols if c in df.columns]

        if len(cols) < 2:
            return

        # Normalize values for visualization
        plot_df = df[cols].copy()
        for col in cols:
            plot_df[col] = (plot_df[col] - plot_df[col].min()) / (
                plot_df[col].max() - plot_df[col].min() + 1e-8
            )

        # Create buckets for coloring
        metric_col = f"metric.{metric}"
        plot_df["quality"] = pd.cut(
            df[metric_col],
            bins=3,
            labels=["poor", "medium", "good"]
        )

        fig, ax = plt.subplots(figsize=(12, 6))
        parallel_coordinates(plot_df, "quality", ax=ax, colormap="viridis")
        ax.set_title("Hyperparameter Parallel Coordinates")

        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
        else:
            plt.show()
```

---

## 7. Best Practices

### 7.1 Tracking Checklist

```python
"""
Best practices checklist for experiment tracking
"""
from dataclasses import dataclass
from typing import List, Dict


@dataclass
class TrackingChecklist:
    """Checklist of what to track for LLM experiments"""

    # Configuration - ALWAYS track
    model_config: bool = True          # Architecture, size, type
    training_config: bool = True       # All hyperparameters
    data_config: bool = True           # Dataset versions, preprocessing
    environment: bool = True           # Python, CUDA, library versions
    git_info: bool = True              # Commit hash, branch, diff
    random_seeds: bool = True          # All random seeds

    # Metrics - ALWAYS track
    training_loss: bool = True
    validation_loss: bool = True
    learning_rate: bool = True
    gradient_norms: bool = True

    # Metrics - RECOMMEND track
    throughput: bool = True            # Tokens/second
    gpu_utilization: bool = True
    memory_usage: bool = True
    benchmark_scores: bool = True

    # Artifacts - ALWAYS track
    final_model: bool = True
    best_checkpoint: bool = True
    training_config_file: bool = True

    # Artifacts - RECOMMEND track
    intermediate_checkpoints: bool = True
    generation_samples: bool = True
    evaluation_outputs: bool = True


# Naming conventions
NAMING_CONVENTIONS = """
## Experiment Naming Conventions

### Experiments
- Use descriptive names: `llama2-7b-finetuning-medical`
- Include model family: `gpt-neo-2.7b-code-generation`
- Avoid timestamps in names (use tags instead)

### Runs
- Include key differentiator: `lr-1e-5-batch-16`
- Use semantic names for sweeps: `sweep-learning-rate-v1`
- Keep under 50 characters

### Metrics
- Use hierarchical naming: `train/loss`, `eval/accuracy`
- Be consistent: always `loss` not sometimes `loss` and `training_loss`
- Include units in name if relevant: `latency_ms`

### Tags
- Use for filtering: `model_size:7b`, `dataset:alpaca`
- Version tracking: `experiment_version:2`
- Purpose: `purpose:baseline`, `purpose:ablation`
"""


class TrackingValidator:
    """
    Validate tracking completeness
    """

    def __init__(self, checklist: TrackingChecklist):
        self.checklist = checklist

    def validate_run(
        self,
        run_params: Dict,
        run_metrics: Dict,
        run_artifacts: List[str]
    ) -> Dict[str, List[str]]:
        """Validate a run has complete tracking"""

        issues = {
            "missing_required": [],
            "missing_recommended": [],
            "warnings": []
        }

        # Check required params
        required_params = [
            "learning_rate", "batch_size", "model_name",
            "num_epochs", "seed"
        ]
        for param in required_params:
            if param not in run_params:
                issues["missing_required"].append(f"param:{param}")

        # Check required metrics
        required_metrics = ["train/loss", "eval/loss"]
        for metric in required_metrics:
            if metric not in run_metrics:
                issues["missing_required"].append(f"metric:{metric}")

        # Check recommended
        if self.checklist.throughput:
            if "train/tokens_per_second" not in run_metrics:
                issues["missing_recommended"].append("metric:throughput")

        if self.checklist.gpu_utilization:
            if not any("gpu" in m for m in run_metrics):
                issues["missing_recommended"].append("metric:gpu_metrics")

        return issues
```

### 7.2 Organization Patterns

```python
"""
Experiment organization patterns
"""
from typing import Dict, Any


class ExperimentOrganization:
    """
    Patterns for organizing experiments
    """

    @staticmethod
    def project_structure() -> Dict[str, str]:
        """Recommended project structure"""
        return {
            "base_models/": "Track base model evaluations",
            "fine_tuning/": "Fine-tuning experiments",
            "ablations/": "Ablation studies",
            "hyperparameter_search/": "HP sweep experiments",
            "production_candidates/": "Models being considered for prod",
            "archived/": "Old experiments for reference"
        }

    @staticmethod
    def tagging_schema() -> Dict[str, str]:
        """Recommended tagging schema"""
        return {
            "model_family": "llama|mistral|gpt|...",
            "model_size": "7b|13b|70b|...",
            "task": "instruction|chat|code|...",
            "dataset": "alpaca|dolly|...",
            "method": "sft|rlhf|dpo|...",
            "stage": "dev|eval|prod_candidate|...",
            "owner": "team or person name"
        }

    @staticmethod
    def run_lifecycle_stages() -> Dict[str, str]:
        """Run lifecycle for tracking"""
        return {
            "queued": "Experiment planned but not started",
            "running": "Currently executing",
            "completed": "Finished successfully",
            "failed": "Crashed or errored",
            "killed": "Manually terminated",
            "analyzing": "Completed, under analysis",
            "promoted": "Selected for next stage"
        }
```

---

## 8. Troubleshooting

### Common Issues

| Issue | Cause | Solution |
|-------|-------|----------|
| Missing metrics | Logging frequency too low | Log more frequently, use summary metrics |
| Large artifacts | Checkpoints too big | Use efficient formats, compress |
| Slow UI | Too many metrics | Downsample, use summary stats |
| Lost runs | Process killed | Enable auto-save, use robust backend |
| Inconsistent params | Different logging code | Create standard logging utilities |

### Debugging Tips

```python
"""
Debugging utilities for experiment tracking
"""

def diagnose_tracking_issues(tracker):
    """Diagnose common tracking issues"""

    issues = []

    # Check if run is active
    if not hasattr(tracker, 'current_run') or tracker.current_run is None:
        issues.append("No active run - metrics won't be logged")

    # Check connectivity
    try:
        # Try a test log
        tracker.set_tag("_diagnostic_test", "test")
    except Exception as e:
        issues.append(f"Connectivity issue: {str(e)}")

    return issues


def verify_experiment_reproducibility(
    experiment_id: str,
    run_id: str,
    tracker: ExperimentTracker
) -> Dict[str, bool]:
    """Verify a run has sufficient info for reproduction"""

    checks = {
        "has_seed": False,
        "has_model_config": False,
        "has_data_version": False,
        "has_git_commit": False,
        "has_dependencies": False,
        "has_full_config": False
    }

    # Get run info
    run = tracker.get_version(experiment_id, run_id)

    if "seed" in run.parameters:
        checks["has_seed"] = True

    if "model_name" in run.parameters or "model_config" in run.parameters:
        checks["has_model_config"] = True

    if "data_version" in run.parameters or "dataset" in run.parameters:
        checks["has_data_version"] = True

    if run.git_commit:
        checks["has_git_commit"] = True

    if run.dependencies:
        checks["has_dependencies"] = True

    # Check for config artifact
    config_artifacts = [
        a for a in run.artifacts
        if "config" in a.name.lower()
    ]
    if config_artifacts:
        checks["has_full_config"] = True

    return checks
```

---

## 9. References

### Documentation
- Weights & Biases: https://docs.wandb.ai/
- MLflow Tracking: https://mlflow.org/docs/latest/tracking.html
- TensorBoard: https://www.tensorflow.org/tensorboard

### Resources
- W&B LLM Guide: https://wandb.ai/site/solutions/llm-fine-tuning/
- MLflow for LLMs: https://mlflow.org/docs/latest/llms/
- HuggingFace Integration Guide

---

## Document Metadata

- **Version**: 1.0
- **Last Updated**: 2024
- **Prerequisites**: ML training basics, Python
- **Estimated Reading Time**: 45 minutes
- **Hands-on Lab Time**: 3-4 hours

---

> **Navigation**
> [← 8.1 Model Registry](8.1_model_registry_guide.md) | **[Index](../README.md#15-repository-structure)** | [8.3 Model Versioning →](8.3_model_versioning_artifacts_guide.md)
