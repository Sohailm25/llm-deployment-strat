> **Navigation** | [← 8.3 Model Versioning](8.3_model_versioning_artifacts_guide.md) | [8.5 LLM CI/CD →](8.5_llm_cicd_pipeline_guide.md)
>
> | | |
> |---|---|
> | **Prerequisites** | [7.1 Vector Database](../07_rag_pipeline/7.1_vector_database_guide.md) &#124; [7.2 Embeddings](../07_rag_pipeline/7.2_embedding_model_guide.md) |
> | **Related** | [7.6 Advanced RAG](../07_rag_pipeline/7.6_advanced_rag_patterns_guide.md) &#124; [12.1 Prompt Engineering](../12_user_developer_experience/12.1_prompt_engineering_guide.md) |
> | **Next** | [8.5 LLM CI/CD Pipeline](8.5_llm_cicd_pipeline_guide.md) |

# 8.4 Feature Store for LLMs Guide

## Document Purpose

This guide covers using feature stores for LLM applications, including embedding management, prompt template versioning, user context features, and integration with RAG systems.

## Prerequisites

- Understanding of ML feature engineering
- Familiarity with vector databases (Document 7.1)
- Basic knowledge of embeddings (Document 7.2)
- Experience with RAG pipelines

## Target Audience

- ML Engineers building LLM applications
- Data Engineers managing feature pipelines
- MLOps Engineers setting up feature infrastructure

---

## 1. Feature Store Concepts for LLMs

### 1.1 LLM Feature Types

```python
"""
Feature types and concepts for LLM applications
"""
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional, Union
from enum import Enum
from datetime import datetime
import numpy as np


class FeatureType(Enum):
    """Types of features for LLM applications"""
    EMBEDDING = "embedding"           # Vector embeddings
    TEXT = "text"                     # Raw text features
    NUMERIC = "numeric"               # Numerical features
    CATEGORICAL = "categorical"       # Categorical features
    TIMESTAMP = "timestamp"           # Time-based features
    STRUCTURED = "structured"         # JSON/nested structures
    PROMPT_TEMPLATE = "prompt"        # Prompt templates


class FeatureSource(Enum):
    """Where features come from"""
    BATCH = "batch"                   # Computed in batch pipelines
    STREAMING = "streaming"           # Real-time computation
    ON_DEMAND = "on_demand"           # Computed at request time
    EXTERNAL = "external"             # From external APIs


@dataclass
class Feature:
    """Individual feature definition"""
    name: str
    feature_type: FeatureType
    source: FeatureSource
    description: str

    # Type-specific properties
    embedding_dim: Optional[int] = None  # For embeddings
    categories: Optional[List[str]] = None  # For categorical

    # Metadata
    ttl_seconds: Optional[int] = None  # Cache TTL
    version: str = "1.0"


@dataclass
class FeatureView:
    """Collection of related features"""
    name: str
    features: List[Feature]
    entity_key: str  # Primary key (user_id, document_id, etc.)
    description: str
    tags: Dict[str, str] = field(default_factory=dict)


# LLM-specific feature views
class LLMFeatureViews:
    """Standard feature views for LLM applications"""

    @staticmethod
    def user_context_view() -> FeatureView:
        """User context features for personalization"""
        return FeatureView(
            name="user_context",
            entity_key="user_id",
            description="User context for personalized responses",
            features=[
                Feature(
                    name="user_preferences",
                    feature_type=FeatureType.STRUCTURED,
                    source=FeatureSource.BATCH,
                    description="User preference settings"
                ),
                Feature(
                    name="interaction_history_embedding",
                    feature_type=FeatureType.EMBEDDING,
                    source=FeatureSource.STREAMING,
                    description="Embedding of recent interactions",
                    embedding_dim=768
                ),
                Feature(
                    name="expertise_level",
                    feature_type=FeatureType.CATEGORICAL,
                    source=FeatureSource.BATCH,
                    description="User's expertise level",
                    categories=["beginner", "intermediate", "expert"]
                ),
                Feature(
                    name="last_active",
                    feature_type=FeatureType.TIMESTAMP,
                    source=FeatureSource.STREAMING,
                    description="Last activity timestamp"
                )
            ]
        )

    @staticmethod
    def document_view() -> FeatureView:
        """Document features for RAG"""
        return FeatureView(
            name="document_features",
            entity_key="document_id",
            description="Document features for retrieval",
            features=[
                Feature(
                    name="content_embedding",
                    feature_type=FeatureType.EMBEDDING,
                    source=FeatureSource.BATCH,
                    description="Document content embedding",
                    embedding_dim=1024
                ),
                Feature(
                    name="title_embedding",
                    feature_type=FeatureType.EMBEDDING,
                    source=FeatureSource.BATCH,
                    description="Document title embedding",
                    embedding_dim=768
                ),
                Feature(
                    name="metadata",
                    feature_type=FeatureType.STRUCTURED,
                    source=FeatureSource.BATCH,
                    description="Document metadata (author, date, etc.)"
                ),
                Feature(
                    name="summary",
                    feature_type=FeatureType.TEXT,
                    source=FeatureSource.BATCH,
                    description="Document summary"
                ),
                Feature(
                    name="access_count",
                    feature_type=FeatureType.NUMERIC,
                    source=FeatureSource.STREAMING,
                    description="Number of times accessed"
                )
            ]
        )

    @staticmethod
    def prompt_template_view() -> FeatureView:
        """Prompt template management"""
        return FeatureView(
            name="prompt_templates",
            entity_key="template_id",
            description="Versioned prompt templates",
            features=[
                Feature(
                    name="template_content",
                    feature_type=FeatureType.PROMPT_TEMPLATE,
                    source=FeatureSource.BATCH,
                    description="The prompt template string"
                ),
                Feature(
                    name="variables",
                    feature_type=FeatureType.STRUCTURED,
                    source=FeatureSource.BATCH,
                    description="Template variable definitions"
                ),
                Feature(
                    name="model_family",
                    feature_type=FeatureType.CATEGORICAL,
                    source=FeatureSource.BATCH,
                    description="Target model family",
                    categories=["llama", "gpt", "claude", "mistral"]
                ),
                Feature(
                    name="performance_metrics",
                    feature_type=FeatureType.STRUCTURED,
                    source=FeatureSource.STREAMING,
                    description="Template performance stats"
                )
            ]
        )
```

### 1.2 Embedding Feature Management

```python
"""
Embedding feature management for LLMs
"""
from typing import List, Dict, Any, Optional, Union
from dataclasses import dataclass
import numpy as np
from datetime import datetime


@dataclass
class EmbeddingFeature:
    """Embedding with metadata"""
    embedding: np.ndarray
    model_name: str
    model_version: str
    created_at: datetime
    metadata: Dict[str, Any] = None

    def to_dict(self) -> Dict:
        return {
            "embedding": self.embedding.tolist(),
            "model_name": self.model_name,
            "model_version": self.model_version,
            "created_at": self.created_at.isoformat(),
            "metadata": self.metadata
        }

    @classmethod
    def from_dict(cls, data: Dict) -> "EmbeddingFeature":
        return cls(
            embedding=np.array(data["embedding"]),
            model_name=data["model_name"],
            model_version=data["model_version"],
            created_at=datetime.fromisoformat(data["created_at"]),
            metadata=data.get("metadata")
        )


class EmbeddingVersionManager:
    """
    Manage embedding versions for consistency
    """

    def __init__(self):
        self.active_models: Dict[str, str] = {}  # feature_name -> model_version
        self.model_registry: Dict[str, Dict] = {}

    def register_model(
        self,
        feature_name: str,
        model_name: str,
        model_version: str,
        embedding_dim: int
    ):
        """Register an embedding model for a feature"""
        key = f"{feature_name}:{model_name}:{model_version}"
        self.model_registry[key] = {
            "feature_name": feature_name,
            "model_name": model_name,
            "model_version": model_version,
            "embedding_dim": embedding_dim,
            "registered_at": datetime.now()
        }

    def set_active_model(
        self,
        feature_name: str,
        model_name: str,
        model_version: str
    ):
        """Set the active model for a feature"""
        self.active_models[feature_name] = f"{model_name}:{model_version}"

    def get_active_model(self, feature_name: str) -> Optional[Dict]:
        """Get the active model for a feature"""
        if feature_name not in self.active_models:
            return None

        model_key = self.active_models[feature_name]
        full_key = f"{feature_name}:{model_key}"
        return self.model_registry.get(full_key)

    def check_compatibility(
        self,
        feature_name: str,
        embedding: EmbeddingFeature
    ) -> bool:
        """Check if embedding is compatible with active model"""
        active = self.get_active_model(feature_name)
        if not active:
            return True  # No active model set

        return (
            embedding.model_name == active["model_name"] and
            embedding.model_version == active["model_version"]
        )


class EmbeddingStore:
    """
    Store and retrieve embeddings with versioning
    """

    def __init__(
        self,
        vector_db_client,
        version_manager: EmbeddingVersionManager
    ):
        self.db = vector_db_client
        self.version_manager = version_manager

    def store_embedding(
        self,
        entity_id: str,
        feature_name: str,
        embedding: EmbeddingFeature,
        replace_existing: bool = True
    ):
        """Store embedding with version tracking"""
        # Check compatibility
        if not self.version_manager.check_compatibility(feature_name, embedding):
            active = self.version_manager.get_active_model(feature_name)
            raise ValueError(
                f"Embedding model mismatch. Active: {active['model_name']}:"
                f"{active['model_version']}, Got: {embedding.model_name}:"
                f"{embedding.model_version}"
            )

        # Store in vector DB
        self.db.upsert(
            collection=feature_name,
            id=entity_id,
            vector=embedding.embedding.tolist(),
            metadata={
                "model_name": embedding.model_name,
                "model_version": embedding.model_version,
                "created_at": embedding.created_at.isoformat(),
                **(embedding.metadata or {})
            }
        )

    def get_embedding(
        self,
        entity_id: str,
        feature_name: str
    ) -> Optional[EmbeddingFeature]:
        """Retrieve embedding by entity ID"""
        result = self.db.get(
            collection=feature_name,
            id=entity_id
        )

        if not result:
            return None

        return EmbeddingFeature(
            embedding=np.array(result["vector"]),
            model_name=result["metadata"]["model_name"],
            model_version=result["metadata"]["model_version"],
            created_at=datetime.fromisoformat(result["metadata"]["created_at"]),
            metadata={
                k: v for k, v in result["metadata"].items()
                if k not in ["model_name", "model_version", "created_at"]
            }
        )

    def search_similar(
        self,
        feature_name: str,
        query_embedding: np.ndarray,
        top_k: int = 10,
        filter: Optional[Dict] = None
    ) -> List[Dict]:
        """Search for similar embeddings"""
        return self.db.search(
            collection=feature_name,
            query_vector=query_embedding.tolist(),
            top_k=top_k,
            filter=filter
        )

    def recompute_embeddings(
        self,
        feature_name: str,
        new_model_name: str,
        new_model_version: str,
        embed_function,
        batch_size: int = 100
    ):
        """Recompute all embeddings with new model"""
        # Get all entities
        all_entities = self.db.list_all(collection=feature_name)

        for i in range(0, len(all_entities), batch_size):
            batch = all_entities[i:i + batch_size]

            # Recompute embeddings
            for entity in batch:
                # Get original content (from metadata)
                content = entity["metadata"].get("original_content")
                if not content:
                    continue

                # Compute new embedding
                new_embedding = embed_function(content)

                # Store
                self.store_embedding(
                    entity_id=entity["id"],
                    feature_name=feature_name,
                    embedding=EmbeddingFeature(
                        embedding=new_embedding,
                        model_name=new_model_name,
                        model_version=new_model_version,
                        created_at=datetime.now(),
                        metadata=entity["metadata"]
                    )
                )
```

---

## 2. Feast Feature Store Implementation

### 2.1 Feast Setup for LLMs

```python
"""
Feast feature store setup for LLM applications
"""
from feast import (
    FeatureStore, Entity, FeatureView, Field, FileSource,
    PushSource, OnDemandFeatureView, RequestSource
)
from feast.types import Float32, String, Int64, Array
from datetime import timedelta


# Define entities
user_entity = Entity(
    name="user",
    join_keys=["user_id"],
    description="User entity for personalization"
)

document_entity = Entity(
    name="document",
    join_keys=["document_id"],
    description="Document entity for RAG"
)

# Define data sources
user_features_source = FileSource(
    name="user_features_source",
    path="data/user_features.parquet",
    timestamp_field="event_timestamp"
)

document_embeddings_source = FileSource(
    name="document_embeddings_source",
    path="data/document_embeddings.parquet",
    timestamp_field="event_timestamp"
)

# Real-time push source for streaming features
user_activity_push = PushSource(
    name="user_activity_push",
    batch_source=FileSource(
        name="user_activity_batch",
        path="data/user_activity.parquet",
        timestamp_field="event_timestamp"
    )
)


# Define feature views
user_profile_fv = FeatureView(
    name="user_profile",
    entities=[user_entity],
    schema=[
        Field(name="user_id", dtype=String),
        Field(name="expertise_level", dtype=String),
        Field(name="preferred_language", dtype=String),
        Field(name="account_age_days", dtype=Int64),
    ],
    source=user_features_source,
    ttl=timedelta(days=1),
    online=True,
    tags={"team": "ml", "use_case": "personalization"}
)

# Embedding feature view
document_embedding_fv = FeatureView(
    name="document_embeddings",
    entities=[document_entity],
    schema=[
        Field(name="document_id", dtype=String),
        Field(name="content_embedding", dtype=Array(Float32)),
        Field(name="title", dtype=String),
        Field(name="category", dtype=String),
        Field(name="word_count", dtype=Int64),
    ],
    source=document_embeddings_source,
    ttl=timedelta(days=7),
    online=True,
    tags={"team": "ml", "use_case": "rag"}
)

# Streaming feature view
user_activity_fv = FeatureView(
    name="user_recent_activity",
    entities=[user_entity],
    schema=[
        Field(name="user_id", dtype=String),
        Field(name="last_query_embedding", dtype=Array(Float32)),
        Field(name="session_queries_count", dtype=Int64),
        Field(name="last_active_timestamp", dtype=Int64),
    ],
    source=user_activity_push,
    ttl=timedelta(hours=1),
    online=True,
    tags={"team": "ml", "use_case": "real_time"}
)


# On-demand feature computation
from feast import on_demand_feature_view
from feast.transformation_status import TransformationStatus

request_source = RequestSource(
    name="query_request",
    schema=[
        Field(name="query_text", dtype=String),
    ]
)


@on_demand_feature_view(
    sources=[user_profile_fv, request_source],
    schema=[
        Field(name="personalized_context", dtype=String),
        Field(name="expertise_modifier", dtype=Float32),
    ]
)
def personalized_features(inputs: dict) -> dict:
    """Compute personalized features on-demand"""
    import pandas as pd

    df = pd.DataFrame()

    # Build personalized context based on user expertise
    expertise = inputs["expertise_level"]
    query = inputs["query_text"]

    context_templates = {
        "beginner": f"Explain in simple terms: {query}",
        "intermediate": f"Provide detailed explanation: {query}",
        "expert": f"Technical deep dive: {query}"
    }

    df["personalized_context"] = expertise.map(
        lambda x: context_templates.get(x, query)
    )

    # Expertise modifier for response length
    modifier_map = {"beginner": 1.5, "intermediate": 1.0, "expert": 0.8}
    df["expertise_modifier"] = expertise.map(
        lambda x: modifier_map.get(x, 1.0)
    )

    return df


class FeastLLMFeatureStore:
    """
    Feast-based feature store for LLM applications
    """

    def __init__(self, repo_path: str):
        self.store = FeatureStore(repo_path=repo_path)

    def get_user_features(
        self,
        user_ids: list,
        include_activity: bool = True
    ) -> dict:
        """Get user features for personalization"""
        feature_refs = [
            "user_profile:expertise_level",
            "user_profile:preferred_language",
            "user_profile:account_age_days",
        ]

        if include_activity:
            feature_refs.extend([
                "user_recent_activity:last_query_embedding",
                "user_recent_activity:session_queries_count",
            ])

        entity_rows = [{"user_id": uid} for uid in user_ids]

        return self.store.get_online_features(
            features=feature_refs,
            entity_rows=entity_rows
        ).to_dict()

    def get_document_embeddings(
        self,
        document_ids: list
    ) -> dict:
        """Get document embeddings for RAG"""
        entity_rows = [{"document_id": did} for did in document_ids]

        return self.store.get_online_features(
            features=[
                "document_embeddings:content_embedding",
                "document_embeddings:title",
                "document_embeddings:category",
            ],
            entity_rows=entity_rows
        ).to_dict()

    def get_personalized_context(
        self,
        user_id: str,
        query_text: str
    ) -> dict:
        """Get personalized context for query"""
        return self.store.get_online_features(
            features=[
                "personalized_features:personalized_context",
                "personalized_features:expertise_modifier",
            ],
            entity_rows=[{
                "user_id": user_id,
                "query_text": query_text
            }]
        ).to_dict()

    def push_user_activity(
        self,
        user_id: str,
        query_embedding: list,
        timestamp: int
    ):
        """Push real-time user activity"""
        import pandas as pd

        df = pd.DataFrame({
            "user_id": [user_id],
            "last_query_embedding": [query_embedding],
            "session_queries_count": [1],
            "last_active_timestamp": [timestamp],
            "event_timestamp": [pd.Timestamp.now()]
        })

        self.store.push("user_activity_push", df)

    def materialize_features(
        self,
        start_date: str,
        end_date: str
    ):
        """Materialize features to online store"""
        from datetime import datetime

        self.store.materialize(
            start_date=datetime.fromisoformat(start_date),
            end_date=datetime.fromisoformat(end_date)
        )
```

### 2.2 Embedding Pipeline with Feast

```python
"""
Embedding computation pipeline with Feast
"""
from typing import List, Dict, Any, Optional
from datetime import datetime
import pandas as pd
import numpy as np


class FeastEmbeddingPipeline:
    """
    Pipeline for computing and storing embeddings in Feast
    """

    def __init__(
        self,
        feast_store,
        embedding_model,
        batch_size: int = 100
    ):
        self.store = feast_store
        self.model = embedding_model
        self.batch_size = batch_size

    def compute_document_embeddings(
        self,
        documents: List[Dict[str, Any]],
        output_path: str
    ):
        """Compute embeddings for documents and save for Feast"""
        all_rows = []

        for i in range(0, len(documents), self.batch_size):
            batch = documents[i:i + self.batch_size]

            # Extract content
            contents = [doc["content"] for doc in batch]

            # Compute embeddings
            embeddings = self.model.encode(contents)

            # Build rows
            for doc, embedding in zip(batch, embeddings):
                all_rows.append({
                    "document_id": doc["id"],
                    "content_embedding": embedding.tolist(),
                    "title": doc.get("title", ""),
                    "category": doc.get("category", ""),
                    "word_count": len(doc["content"].split()),
                    "event_timestamp": datetime.now()
                })

        # Save as parquet
        df = pd.DataFrame(all_rows)
        df.to_parquet(output_path, index=False)

        return len(all_rows)

    def update_embeddings_for_model_change(
        self,
        new_model,
        source_path: str,
        output_path: str
    ):
        """Recompute embeddings when model changes"""
        # Load existing data
        df = pd.read_parquet(source_path)

        # Get original content (would need to store separately)
        # This example assumes we have the content

        # Recompute embeddings
        # ...

        # Save with new model embeddings
        df.to_parquet(output_path, index=False)


class FeastRAGIntegration:
    """
    Integrate Feast features with RAG pipeline
    """

    def __init__(
        self,
        feast_store,
        vector_db,
        llm_client
    ):
        self.feast = feast_store
        self.vector_db = vector_db
        self.llm = llm_client

    def query_with_features(
        self,
        user_id: str,
        query: str,
        top_k: int = 5
    ) -> Dict[str, Any]:
        """RAG query with user personalization"""

        # Get user features
        user_features = self.feast.get_user_features([user_id])

        # Get personalized context
        personalized = self.feast.get_personalized_context(
            user_id, query
        )

        # Retrieve documents
        query_embedding = self.vector_db.embed_query(query)
        documents = self.vector_db.search(query_embedding, top_k=top_k)

        # Build prompt with features
        prompt = self._build_personalized_prompt(
            query=query,
            documents=documents,
            user_features=user_features,
            personalized_context=personalized
        )

        # Generate response
        response = self.llm.generate(prompt)

        # Update user activity
        self.feast.push_user_activity(
            user_id=user_id,
            query_embedding=query_embedding.tolist(),
            timestamp=int(datetime.now().timestamp())
        )

        return {
            "response": response,
            "documents": documents,
            "personalization_applied": True
        }

    def _build_personalized_prompt(
        self,
        query: str,
        documents: List[Dict],
        user_features: Dict,
        personalized_context: Dict
    ) -> str:
        """Build prompt with personalization"""
        expertise = user_features.get("expertise_level", ["intermediate"])[0]

        # Format documents
        doc_context = "\n\n".join([
            f"[{i+1}] {doc['content']}"
            for i, doc in enumerate(documents)
        ])

        # Personalized instructions
        expertise_instructions = {
            "beginner": "Use simple language and provide examples.",
            "intermediate": "Provide balanced explanations with some technical details.",
            "expert": "Be concise and include technical details."
        }

        instruction = expertise_instructions.get(
            expertise, "Provide a helpful response."
        )

        prompt = f"""You are a helpful assistant. {instruction}

Context documents:
{doc_context}

Question: {query}

Answer:"""

        return prompt
```

---

## 3. Prompt Template Management

### 3.1 Prompt Template Feature Store

```python
"""
Prompt template management as features
"""
from dataclasses import dataclass, field
from typing import Dict, List, Any, Optional
from datetime import datetime
import json
import hashlib


@dataclass
class PromptTemplate:
    """Versioned prompt template"""
    template_id: str
    name: str
    template: str
    variables: List[str]
    model_family: str  # Target model family
    version: str
    created_at: datetime
    metadata: Dict[str, Any] = field(default_factory=dict)

    @property
    def content_hash(self) -> str:
        """Hash of template content for change detection"""
        return hashlib.sha256(self.template.encode()).hexdigest()[:12]

    def render(self, **kwargs) -> str:
        """Render template with variables"""
        result = self.template
        for var in self.variables:
            if var in kwargs:
                result = result.replace(f"{{{var}}}", str(kwargs[var]))
        return result


class PromptTemplateStore:
    """
    Store and version prompt templates
    """

    def __init__(self, backend):
        self.backend = backend
        self.templates: Dict[str, Dict[str, PromptTemplate]] = {}

    def register_template(
        self,
        template: PromptTemplate,
        set_active: bool = True
    ):
        """Register a new template version"""
        if template.template_id not in self.templates:
            self.templates[template.template_id] = {}

        self.templates[template.template_id][template.version] = template

        if set_active:
            self.backend.set_active_version(
                template.template_id,
                template.version
            )

    def get_template(
        self,
        template_id: str,
        version: Optional[str] = None
    ) -> Optional[PromptTemplate]:
        """Get template by ID and version"""
        if template_id not in self.templates:
            return None

        if version:
            return self.templates[template_id].get(version)

        # Get active version
        active_version = self.backend.get_active_version(template_id)
        return self.templates[template_id].get(active_version)

    def list_versions(self, template_id: str) -> List[str]:
        """List all versions of a template"""
        if template_id not in self.templates:
            return []
        return list(self.templates[template_id].keys())

    def compare_templates(
        self,
        template_id: str,
        version_a: str,
        version_b: str
    ) -> Dict[str, Any]:
        """Compare two template versions"""
        template_a = self.templates[template_id].get(version_a)
        template_b = self.templates[template_id].get(version_b)

        if not template_a or not template_b:
            return {"error": "Template version not found"}

        return {
            "template_changed": template_a.template != template_b.template,
            "variables_added": set(template_b.variables) - set(template_a.variables),
            "variables_removed": set(template_a.variables) - set(template_b.variables),
            "model_family_changed": template_a.model_family != template_b.model_family
        }


class PromptTemplateFeatureView:
    """
    Feast-compatible prompt template feature view
    """

    def __init__(self, template_store: PromptTemplateStore):
        self.store = template_store

    def get_features(self, template_ids: List[str]) -> Dict[str, Any]:
        """Get template features for given IDs"""
        features = {
            "template_id": [],
            "template_content": [],
            "variables": [],
            "model_family": [],
            "version": [],
            "content_hash": []
        }

        for tid in template_ids:
            template = self.store.get_template(tid)
            if template:
                features["template_id"].append(template.template_id)
                features["template_content"].append(template.template)
                features["variables"].append(json.dumps(template.variables))
                features["model_family"].append(template.model_family)
                features["version"].append(template.version)
                features["content_hash"].append(template.content_hash)
            else:
                # Fill with None for missing
                for key in features:
                    features[key].append(None)

        return features


# Example templates
EXAMPLE_TEMPLATES = {
    "qa_basic": PromptTemplate(
        template_id="qa_basic",
        name="Basic Q&A",
        template="""Answer the question based on the context.

Context: {context}

Question: {question}

Answer:""",
        variables=["context", "question"],
        model_family="general",
        version="1.0.0",
        created_at=datetime.now()
    ),

    "qa_with_citations": PromptTemplate(
        template_id="qa_with_citations",
        name="Q&A with Citations",
        template="""Answer the question based on the provided context.
Include citations in the format [N] referring to the context number.

Context:
{context}

Question: {question}

Provide a detailed answer with citations:""",
        variables=["context", "question"],
        model_family="general",
        version="1.0.0",
        created_at=datetime.now()
    ),

    "code_generation": PromptTemplate(
        template_id="code_generation",
        name="Code Generation",
        template="""You are an expert programmer.
Generate {language} code for the following task.

Task: {task}

Requirements:
{requirements}

```{language}""",
        variables=["language", "task", "requirements"],
        model_family="code",
        version="1.0.0",
        created_at=datetime.now()
    )
}
```

---

## 4. Real-Time Feature Serving

### 4.1 Low-Latency Feature Server

```python
"""
Low-latency feature serving for LLM applications
"""
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
import asyncio
from collections import OrderedDict
import time


@dataclass
class FeatureRequest:
    """Request for features"""
    entity_type: str
    entity_ids: List[str]
    feature_names: List[str]
    timestamp: Optional[float] = None


@dataclass
class FeatureResponse:
    """Response with features"""
    features: Dict[str, List[Any]]
    latency_ms: float
    cache_hit: bool


class LRUCache:
    """Simple LRU cache for features"""

    def __init__(self, capacity: int = 10000):
        self.cache = OrderedDict()
        self.capacity = capacity

    def get(self, key: str) -> Optional[Any]:
        if key not in self.cache:
            return None
        self.cache.move_to_end(key)
        return self.cache[key]

    def put(self, key: str, value: Any):
        if key in self.cache:
            self.cache.move_to_end(key)
        self.cache[key] = value
        if len(self.cache) > self.capacity:
            self.cache.popitem(last=False)


class FeatureServer:
    """
    Low-latency feature server with caching
    """

    def __init__(
        self,
        online_store,
        cache_capacity: int = 10000,
        cache_ttl_seconds: int = 300
    ):
        self.store = online_store
        self.cache = LRUCache(cache_capacity)
        self.cache_ttl = cache_ttl_seconds
        self.metrics = {
            "requests": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "avg_latency_ms": 0
        }

    def get_features(
        self,
        request: FeatureRequest
    ) -> FeatureResponse:
        """Get features with caching"""
        start_time = time.time()

        # Check cache
        cache_key = self._make_cache_key(request)
        cached = self.cache.get(cache_key)

        if cached and self._is_cache_valid(cached):
            self.metrics["cache_hits"] += 1
            return FeatureResponse(
                features=cached["features"],
                latency_ms=(time.time() - start_time) * 1000,
                cache_hit=True
            )

        self.metrics["cache_misses"] += 1

        # Fetch from store
        features = self._fetch_from_store(request)

        # Update cache
        self.cache.put(cache_key, {
            "features": features,
            "timestamp": time.time()
        })

        latency = (time.time() - start_time) * 1000
        self._update_metrics(latency)

        return FeatureResponse(
            features=features,
            latency_ms=latency,
            cache_hit=False
        )

    async def get_features_async(
        self,
        request: FeatureRequest
    ) -> FeatureResponse:
        """Async feature fetching"""
        start_time = time.time()

        # Check cache
        cache_key = self._make_cache_key(request)
        cached = self.cache.get(cache_key)

        if cached and self._is_cache_valid(cached):
            self.metrics["cache_hits"] += 1
            return FeatureResponse(
                features=cached["features"],
                latency_ms=(time.time() - start_time) * 1000,
                cache_hit=True
            )

        # Fetch async
        features = await self._fetch_from_store_async(request)

        # Update cache
        self.cache.put(cache_key, {
            "features": features,
            "timestamp": time.time()
        })

        return FeatureResponse(
            features=features,
            latency_ms=(time.time() - start_time) * 1000,
            cache_hit=False
        )

    def _make_cache_key(self, request: FeatureRequest) -> str:
        """Create cache key from request"""
        return f"{request.entity_type}:{','.join(sorted(request.entity_ids))}:{','.join(sorted(request.feature_names))}"

    def _is_cache_valid(self, cached: Dict) -> bool:
        """Check if cached value is still valid"""
        age = time.time() - cached["timestamp"]
        return age < self.cache_ttl

    def _fetch_from_store(
        self,
        request: FeatureRequest
    ) -> Dict[str, List]:
        """Fetch features from online store"""
        return self.store.get_features(
            entity_type=request.entity_type,
            entity_ids=request.entity_ids,
            feature_names=request.feature_names
        )

    async def _fetch_from_store_async(
        self,
        request: FeatureRequest
    ) -> Dict[str, List]:
        """Async fetch from store"""
        # Run in executor for non-async stores
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            None,
            lambda: self._fetch_from_store(request)
        )

    def _update_metrics(self, latency_ms: float):
        """Update server metrics"""
        self.metrics["requests"] += 1
        n = self.metrics["requests"]
        self.metrics["avg_latency_ms"] = (
            (self.metrics["avg_latency_ms"] * (n - 1) + latency_ms) / n
        )


class BatchFeatureServer:
    """
    Batch feature requests for efficiency
    """

    def __init__(
        self,
        feature_server: FeatureServer,
        batch_size: int = 100,
        batch_timeout_ms: int = 10
    ):
        self.server = feature_server
        self.batch_size = batch_size
        self.batch_timeout_ms = batch_timeout_ms
        self.pending_requests: List[FeatureRequest] = []

    async def get_features_batched(
        self,
        request: FeatureRequest
    ) -> FeatureResponse:
        """Add request to batch and wait for result"""
        # For simplicity, this is synchronous
        # In production, use async queuing
        return await self.server.get_features_async(request)

    async def process_batch(self):
        """Process accumulated batch"""
        if not self.pending_requests:
            return

        # Group by entity type and features
        # Execute batch queries
        # Distribute results
        pass
```

---

## 5. Feature Monitoring

### 5.1 Feature Quality Monitoring

```python
"""
Feature monitoring for LLM applications
"""
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime
import numpy as np


@dataclass
class FeatureStats:
    """Statistics for a feature"""
    feature_name: str
    mean: float
    std: float
    min_val: float
    max_val: float
    null_count: int
    total_count: int
    timestamp: datetime


@dataclass
class FeatureDrift:
    """Feature drift detection result"""
    feature_name: str
    drift_score: float
    is_drifted: bool
    drift_type: str  # distribution, magnitude, null_rate
    baseline_stats: FeatureStats
    current_stats: FeatureStats


class FeatureMonitor:
    """
    Monitor feature quality and drift
    """

    def __init__(
        self,
        drift_threshold: float = 0.1,
        null_rate_threshold: float = 0.05
    ):
        self.drift_threshold = drift_threshold
        self.null_rate_threshold = null_rate_threshold
        self.baselines: Dict[str, FeatureStats] = {}

    def set_baseline(
        self,
        feature_name: str,
        values: List[Any]
    ):
        """Set baseline statistics for a feature"""
        numeric_values = [v for v in values if v is not None]

        self.baselines[feature_name] = FeatureStats(
            feature_name=feature_name,
            mean=np.mean(numeric_values) if numeric_values else 0,
            std=np.std(numeric_values) if numeric_values else 0,
            min_val=min(numeric_values) if numeric_values else 0,
            max_val=max(numeric_values) if numeric_values else 0,
            null_count=len(values) - len(numeric_values),
            total_count=len(values),
            timestamp=datetime.now()
        )

    def check_drift(
        self,
        feature_name: str,
        current_values: List[Any]
    ) -> Optional[FeatureDrift]:
        """Check for feature drift"""
        if feature_name not in self.baselines:
            return None

        baseline = self.baselines[feature_name]

        # Compute current stats
        numeric_values = [v for v in current_values if v is not None]
        current_stats = FeatureStats(
            feature_name=feature_name,
            mean=np.mean(numeric_values) if numeric_values else 0,
            std=np.std(numeric_values) if numeric_values else 0,
            min_val=min(numeric_values) if numeric_values else 0,
            max_val=max(numeric_values) if numeric_values else 0,
            null_count=len(current_values) - len(numeric_values),
            total_count=len(current_values),
            timestamp=datetime.now()
        )

        # Check null rate drift
        baseline_null_rate = baseline.null_count / baseline.total_count
        current_null_rate = current_stats.null_count / current_stats.total_count

        if abs(current_null_rate - baseline_null_rate) > self.null_rate_threshold:
            return FeatureDrift(
                feature_name=feature_name,
                drift_score=abs(current_null_rate - baseline_null_rate),
                is_drifted=True,
                drift_type="null_rate",
                baseline_stats=baseline,
                current_stats=current_stats
            )

        # Check distribution drift (simplified - use KS test in production)
        if baseline.std > 0:
            mean_drift = abs(current_stats.mean - baseline.mean) / baseline.std
            if mean_drift > self.drift_threshold:
                return FeatureDrift(
                    feature_name=feature_name,
                    drift_score=mean_drift,
                    is_drifted=True,
                    drift_type="distribution",
                    baseline_stats=baseline,
                    current_stats=current_stats
                )

        return FeatureDrift(
            feature_name=feature_name,
            drift_score=0,
            is_drifted=False,
            drift_type="none",
            baseline_stats=baseline,
            current_stats=current_stats
        )


class EmbeddingQualityMonitor:
    """
    Monitor embedding quality
    """

    def __init__(self):
        self.reference_embeddings: Dict[str, np.ndarray] = {}

    def set_reference(
        self,
        feature_name: str,
        embeddings: np.ndarray
    ):
        """Set reference embeddings for comparison"""
        self.reference_embeddings[feature_name] = embeddings

    def check_embedding_quality(
        self,
        feature_name: str,
        embeddings: np.ndarray
    ) -> Dict[str, Any]:
        """Check embedding quality"""
        results = {
            "feature_name": feature_name,
            "issues": []
        }

        # Check for zero vectors
        zero_count = np.sum(np.all(embeddings == 0, axis=1))
        if zero_count > 0:
            results["issues"].append({
                "type": "zero_vectors",
                "count": int(zero_count),
                "percentage": zero_count / len(embeddings)
            })

        # Check for NaN/Inf
        nan_count = np.sum(np.any(np.isnan(embeddings), axis=1))
        if nan_count > 0:
            results["issues"].append({
                "type": "nan_values",
                "count": int(nan_count)
            })

        # Check dimensionality
        if feature_name in self.reference_embeddings:
            ref = self.reference_embeddings[feature_name]
            if embeddings.shape[1] != ref.shape[1]:
                results["issues"].append({
                    "type": "dimension_mismatch",
                    "expected": ref.shape[1],
                    "actual": embeddings.shape[1]
                })

        # Check norm distribution
        norms = np.linalg.norm(embeddings, axis=1)
        results["norm_stats"] = {
            "mean": float(np.mean(norms)),
            "std": float(np.std(norms)),
            "min": float(np.min(norms)),
            "max": float(np.max(norms))
        }

        results["is_healthy"] = len(results["issues"]) == 0

        return results
```

---

## 6. Troubleshooting

### Common Issues

| Issue | Cause | Solution |
|-------|-------|----------|
| Stale features | Cache TTL too long | Reduce cache TTL |
| High latency | Cold cache | Pre-warm cache |
| Embedding mismatch | Model version change | Track model versions |
| Missing features | Pipeline failure | Monitor pipeline health |
| Drift detected | Data distribution changed | Update baselines |

---

## 7. References

### Documentation
- Feast: https://docs.feast.dev/
- Feast GenAI: https://docs.feast.dev/getting-started/genai
- Tecton: https://docs.tecton.ai/

---

## Document Metadata

- **Version**: 1.0
- **Last Updated**: 2024
- **Prerequisites**: Vector databases, embeddings
- **Estimated Reading Time**: 35 minutes
- **Hands-on Lab Time**: 2-3 hours
