> **Navigation** | [← 8.2 Experiment Tracking](8.2_experiment_tracking_guide.md) | [8.4 Feature Store →](8.4_feature_store_llms_guide.md)
>
> | | |
> |---|---|
> | **Prerequisites** | [8.1 Model Registry](8.1_model_registry_guide.md) &#124; Cloud storage |
> | **Related** | [1.4 Data Versioning](../01_data_pipeline/1.4_data_versioning_lineage.md) &#124; [8.5 CI/CD](8.5_llm_cicd_pipeline_guide.md) |
> | **Next** | [8.4 Feature Store for LLMs](8.4_feature_store_llms_guide.md) |

# 8.3 Model Versioning & Artifacts Guide

## Document Purpose

This guide covers strategies for versioning ML models and managing associated artifacts throughout the model lifecycle, with focus on LLM-specific formats, storage solutions, and integrity verification.

## Prerequisites

- Understanding of model registry concepts (Document 8.1)
- Familiarity with cloud storage services
- Basic knowledge of model formats and serialization

## Target Audience

- MLOps Engineers managing model artifacts
- ML Engineers handling model checkpoints
- Platform Engineers designing storage infrastructure

---

## 1. Model Versioning Strategies

### 1.1 Versioning Approaches

```python
"""
Model versioning strategies and implementation
"""
from dataclasses import dataclass, field
from typing import Dict, Any, Optional, List
from enum import Enum
from datetime import datetime
import re


class VersioningScheme(Enum):
    """Versioning schemes for models"""
    SEMANTIC = "semantic"      # Major.Minor.Patch
    SEQUENTIAL = "sequential"  # v1, v2, v3
    TIMESTAMP = "timestamp"    # 2024-01-15-12-30-45
    COMMIT_BASED = "commit"    # Git commit hash
    HYBRID = "hybrid"          # semantic + timestamp


@dataclass
class SemanticVersion:
    """
    Semantic versioning for models.
    Major: Breaking changes (architecture, tokenizer)
    Minor: Feature additions (new capabilities, fine-tuning)
    Patch: Bug fixes, small improvements
    """
    major: int
    minor: int
    patch: int
    prerelease: Optional[str] = None  # alpha, beta, rc1
    build: Optional[str] = None        # Build metadata

    def __str__(self) -> str:
        version = f"{self.major}.{self.minor}.{self.patch}"
        if self.prerelease:
            version += f"-{self.prerelease}"
        if self.build:
            version += f"+{self.build}"
        return version

    @classmethod
    def parse(cls, version_string: str) -> "SemanticVersion":
        """Parse semantic version string"""
        pattern = r"(\d+)\.(\d+)\.(\d+)(?:-([a-zA-Z0-9.]+))?(?:\+([a-zA-Z0-9.]+))?"
        match = re.match(pattern, version_string)
        if not match:
            raise ValueError(f"Invalid semantic version: {version_string}")

        return cls(
            major=int(match.group(1)),
            minor=int(match.group(2)),
            patch=int(match.group(3)),
            prerelease=match.group(4),
            build=match.group(5)
        )

    def bump_major(self) -> "SemanticVersion":
        return SemanticVersion(self.major + 1, 0, 0)

    def bump_minor(self) -> "SemanticVersion":
        return SemanticVersion(self.major, self.minor + 1, 0)

    def bump_patch(self) -> "SemanticVersion":
        return SemanticVersion(self.major, self.minor, self.patch + 1)

    def __lt__(self, other: "SemanticVersion") -> bool:
        return (self.major, self.minor, self.patch) < (
            other.major, other.minor, other.patch
        )


class ModelVersionManager:
    """
    Manage model versions with semantic versioning
    """

    def __init__(self, model_name: str):
        self.model_name = model_name
        self.versions: Dict[str, Dict] = {}

    def create_version(
        self,
        version: SemanticVersion,
        metadata: Dict[str, Any]
    ) -> str:
        """Create a new model version"""
        version_str = str(version)

        if version_str in self.versions:
            raise ValueError(f"Version {version_str} already exists")

        self.versions[version_str] = {
            "version": version_str,
            "created_at": datetime.now().isoformat(),
            "metadata": metadata
        }

        return version_str

    def get_next_version(
        self,
        bump_type: str = "patch"
    ) -> SemanticVersion:
        """Calculate next version based on bump type"""
        if not self.versions:
            return SemanticVersion(1, 0, 0)

        # Get latest version
        latest = max(
            self.versions.keys(),
            key=lambda v: SemanticVersion.parse(v)
        )
        current = SemanticVersion.parse(latest)

        if bump_type == "major":
            return current.bump_major()
        elif bump_type == "minor":
            return current.bump_minor()
        else:
            return current.bump_patch()


@dataclass
class VersionChangeLog:
    """Track changes between versions"""
    from_version: str
    to_version: str
    change_type: str  # major, minor, patch
    changes: List[str]
    breaking_changes: List[str] = field(default_factory=list)
    migration_notes: str = ""


class VersioningPolicy:
    """
    Define versioning policies for different scenarios
    """

    POLICIES = {
        "architecture_change": {
            "bump": "major",
            "description": "Changes to model architecture (layers, attention)",
            "examples": ["Changed attention mechanism", "Added new layers"]
        },
        "tokenizer_change": {
            "bump": "major",
            "description": "Changes to tokenizer vocabulary or encoding",
            "examples": ["New vocabulary", "Different tokenizer type"]
        },
        "fine_tuning": {
            "bump": "minor",
            "description": "Fine-tuned on new data or tasks",
            "examples": ["SFT on new dataset", "Domain adaptation"]
        },
        "capability_addition": {
            "bump": "minor",
            "description": "New capabilities added",
            "examples": ["Added code generation", "Added tool use"]
        },
        "quantization": {
            "bump": "minor",
            "description": "Quantization applied to base model",
            "examples": ["INT8 quantization", "GPTQ quantization"]
        },
        "adapter_update": {
            "bump": "patch",
            "description": "Updated LoRA/adapter weights",
            "examples": ["Retrained adapter", "Merged adapter"]
        },
        "config_fix": {
            "bump": "patch",
            "description": "Configuration fixes without weight changes",
            "examples": ["Fixed generation config", "Updated metadata"]
        },
        "safety_update": {
            "bump": "patch",
            "description": "Safety or alignment improvements",
            "examples": ["Safety fine-tuning", "Guardrail updates"]
        }
    }

    def determine_bump_type(self, changes: List[str]) -> str:
        """Determine version bump based on changes"""
        has_major = False
        has_minor = False

        for change in changes:
            change_lower = change.lower()

            # Check for major changes
            if any(kw in change_lower for kw in [
                "architecture", "tokenizer", "breaking", "incompatible"
            ]):
                has_major = True

            # Check for minor changes
            if any(kw in change_lower for kw in [
                "fine-tuned", "capability", "quantized", "new feature"
            ]):
                has_minor = True

        if has_major:
            return "major"
        elif has_minor:
            return "minor"
        return "patch"
```

### 1.2 Checkpoint vs Release Versions

```python
"""
Distinguish between checkpoints and releases
"""
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional
from enum import Enum
from datetime import datetime


class ArtifactStage(Enum):
    """Stages in model artifact lifecycle"""
    CHECKPOINT = "checkpoint"    # Training checkpoint
    CANDIDATE = "candidate"      # Release candidate
    RELEASE = "release"          # Official release
    DEPRECATED = "deprecated"    # No longer recommended
    ARCHIVED = "archived"        # Historical reference


@dataclass
class ModelCheckpoint:
    """
    Training checkpoint - intermediate save during training
    """
    checkpoint_id: str
    step: int
    epoch: float
    timestamp: datetime

    # Training state
    loss: float
    learning_rate: float
    optimizer_state_path: Optional[str] = None

    # Artifacts
    weights_path: str
    config_path: str

    # Metadata
    training_run_id: str
    metrics: Dict[str, float] = field(default_factory=dict)

    @property
    def is_best(self) -> bool:
        """Check if this is the best checkpoint"""
        return self.metrics.get("is_best", False)


@dataclass
class ModelRelease:
    """
    Official model release - production-ready artifact
    """
    release_id: str
    version: str  # Semantic version
    release_date: datetime
    stage: ArtifactStage = ArtifactStage.RELEASE

    # Source checkpoint
    source_checkpoint_id: Optional[str] = None
    source_training_run: Optional[str] = None

    # Artifacts (processed for release)
    artifacts: List[str] = field(default_factory=list)

    # Quality gates passed
    evaluation_results: Dict[str, float] = field(default_factory=dict)
    safety_results: Dict[str, Any] = field(default_factory=dict)

    # Documentation
    model_card: Optional[str] = None
    changelog: Optional[str] = None
    release_notes: str = ""

    # Approvals
    approved_by: List[str] = field(default_factory=list)


class CheckpointToReleaseConverter:
    """
    Convert training checkpoints to release artifacts
    """

    def __init__(
        self,
        model_name: str,
        version_manager: ModelVersionManager
    ):
        self.model_name = model_name
        self.version_manager = version_manager

    def prepare_release(
        self,
        checkpoint: ModelCheckpoint,
        release_type: str = "minor",
        evaluation_results: Optional[Dict] = None
    ) -> ModelRelease:
        """Prepare checkpoint for release"""

        # Get next version
        version = self.version_manager.get_next_version(release_type)

        # Process artifacts
        processed_artifacts = self._process_artifacts(checkpoint)

        # Create release
        release = ModelRelease(
            release_id=f"{self.model_name}-{version}",
            version=str(version),
            release_date=datetime.now(),
            source_checkpoint_id=checkpoint.checkpoint_id,
            artifacts=processed_artifacts,
            evaluation_results=evaluation_results or {}
        )

        return release

    def _process_artifacts(
        self,
        checkpoint: ModelCheckpoint
    ) -> List[str]:
        """Process checkpoint artifacts for release"""
        processed = []

        # Convert to safetensors if needed
        weights_path = self._convert_to_safetensors(checkpoint.weights_path)
        processed.append(weights_path)

        # Include config
        processed.append(checkpoint.config_path)

        # Exclude optimizer state (not needed for inference)
        # checkpoint.optimizer_state_path is NOT included

        return processed

    def _convert_to_safetensors(self, weights_path: str) -> str:
        """Convert weights to safetensors format"""
        from safetensors.torch import save_file, load_file
        import torch

        if weights_path.endswith(".safetensors"):
            return weights_path

        # Load and convert
        if weights_path.endswith(".bin"):
            state_dict = torch.load(weights_path, map_location="cpu")
            new_path = weights_path.replace(".bin", ".safetensors")
            save_file(state_dict, new_path)
            return new_path

        return weights_path
```

---

## 2. Model Artifact Formats

### 2.1 Weight Formats

```python
"""
LLM weight formats and conversion utilities
"""
from dataclasses import dataclass
from typing import Dict, Any, Optional, List, BinaryIO
from enum import Enum
from pathlib import Path
import struct


class WeightFormat(Enum):
    """Common weight storage formats"""
    SAFETENSORS = "safetensors"    # Recommended: safe, fast
    PYTORCH_BIN = "pytorch_bin"    # Legacy: .bin files
    GGUF = "gguf"                  # llama.cpp format
    ONNX = "onnx"                  # Cross-platform inference
    TENSORRT = "tensorrt"          # NVIDIA optimized


@dataclass
class FormatInfo:
    """Information about a weight format"""
    name: str
    extension: str
    supports_mmap: bool
    secure: bool
    use_cases: List[str]
    pros: List[str]
    cons: List[str]


FORMAT_CATALOG = {
    WeightFormat.SAFETENSORS: FormatInfo(
        name="Safetensors",
        extension=".safetensors",
        supports_mmap=True,
        secure=True,
        use_cases=[
            "Production inference",
            "Model distribution",
            "Multi-GPU loading"
        ],
        pros=[
            "No arbitrary code execution",
            "2-5x faster loading than pickle",
            "Memory-efficient mmap loading",
            "Clear tensor metadata"
        ],
        cons=[
            "Doesn't store optimizer state",
            "No Python objects"
        ]
    ),
    WeightFormat.PYTORCH_BIN: FormatInfo(
        name="PyTorch Binary",
        extension=".bin",
        supports_mmap=False,
        secure=False,
        use_cases=[
            "Legacy compatibility",
            "Training checkpoints"
        ],
        pros=[
            "Can store any Python object",
            "Includes optimizer state"
        ],
        cons=[
            "Security risk (arbitrary code execution)",
            "Slower loading",
            "Requires 2x memory to load"
        ]
    ),
    WeightFormat.GGUF: FormatInfo(
        name="GGUF",
        extension=".gguf",
        supports_mmap=True,
        secure=True,
        use_cases=[
            "llama.cpp inference",
            "CPU inference",
            "Edge deployment"
        ],
        pros=[
            "Single file includes model + metadata",
            "Optimized for CPU inference",
            "Supports various quantization"
        ],
        cons=[
            "Limited ecosystem support",
            "Conversion required"
        ]
    )
}


class SafetensorsManager:
    """
    Manage safetensors model weights
    """

    @staticmethod
    def save_model(
        model,
        output_path: str,
        metadata: Optional[Dict[str, str]] = None
    ):
        """Save model weights as safetensors"""
        from safetensors.torch import save_model

        save_model(
            model,
            output_path,
            metadata=metadata or {}
        )

    @staticmethod
    def save_state_dict(
        state_dict: Dict,
        output_path: str,
        metadata: Optional[Dict[str, str]] = None
    ):
        """Save state dict as safetensors"""
        from safetensors.torch import save_file

        save_file(
            state_dict,
            output_path,
            metadata=metadata or {}
        )

    @staticmethod
    def load_model(
        model,
        weights_path: str,
        device: str = "cpu",
        strict: bool = True
    ):
        """Load weights into model"""
        from safetensors.torch import load_model

        load_model(
            model,
            weights_path,
            device=device,
            strict=strict
        )

    @staticmethod
    def load_state_dict(
        weights_path: str,
        device: str = "cpu"
    ) -> Dict:
        """Load state dict from safetensors"""
        from safetensors import safe_open

        state_dict = {}
        with safe_open(weights_path, framework="pt", device=device) as f:
            for key in f.keys():
                state_dict[key] = f.get_tensor(key)

        return state_dict

    @staticmethod
    def get_metadata(weights_path: str) -> Dict[str, str]:
        """Get metadata from safetensors file"""
        from safetensors import safe_open

        with safe_open(weights_path, framework="pt") as f:
            return dict(f.metadata())

    @staticmethod
    def get_tensor_info(weights_path: str) -> Dict[str, Dict]:
        """Get tensor shapes and dtypes without loading"""
        from safetensors import safe_open

        info = {}
        with safe_open(weights_path, framework="pt") as f:
            for key in f.keys():
                tensor = f.get_tensor(key)
                info[key] = {
                    "shape": list(tensor.shape),
                    "dtype": str(tensor.dtype),
                    "num_params": tensor.numel()
                }
        return info


class FormatConverter:
    """
    Convert between model weight formats
    """

    def pytorch_to_safetensors(
        self,
        input_path: str,
        output_path: str,
        metadata: Optional[Dict[str, str]] = None
    ):
        """Convert PyTorch .bin to safetensors"""
        import torch
        from safetensors.torch import save_file

        # Load with weights_only for security
        state_dict = torch.load(
            input_path,
            map_location="cpu",
            weights_only=True
        )

        # Filter non-tensor items
        tensor_dict = {
            k: v for k, v in state_dict.items()
            if isinstance(v, torch.Tensor)
        }

        save_file(tensor_dict, output_path, metadata=metadata)

    def safetensors_to_gguf(
        self,
        model_path: str,
        output_path: str,
        model_type: str,
        quantization: Optional[str] = None
    ):
        """Convert safetensors to GGUF format"""
        import subprocess

        # Use llama.cpp conversion script
        cmd = [
            "python", "convert.py",
            model_path,
            "--outfile", output_path,
            "--outtype", quantization or "f16"
        ]

        subprocess.run(cmd, check=True)

    def convert_to_hf_format(
        self,
        input_dir: str,
        output_dir: str
    ):
        """Ensure model is in HuggingFace format"""
        from transformers import AutoModelForCausalLM, AutoTokenizer

        # Load and save
        model = AutoModelForCausalLM.from_pretrained(input_dir)
        tokenizer = AutoTokenizer.from_pretrained(input_dir)

        model.save_pretrained(output_dir, safe_serialization=True)
        tokenizer.save_pretrained(output_dir)
```

### 2.2 Complete Artifact Structure

```python
"""
Complete model artifact structure
"""
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional
from pathlib import Path
import json


@dataclass
class ModelArtifactStructure:
    """
    Standard structure for LLM model artifacts
    """

    # Core model files
    weights_files: List[str] = field(default_factory=list)

    # Configuration
    config_json: str = "config.json"
    generation_config: str = "generation_config.json"

    # Tokenizer
    tokenizer_json: str = "tokenizer.json"
    tokenizer_config: str = "tokenizer_config.json"
    special_tokens_map: str = "special_tokens_map.json"
    vocab_file: Optional[str] = None  # vocab.json or tokenizer.model

    # Adapters (if applicable)
    adapter_config: Optional[str] = None  # adapter_config.json
    adapter_weights: Optional[str] = None  # adapter_model.safetensors

    # Documentation
    model_card: str = "README.md"
    model_index: Optional[str] = None  # model_index.json


STANDARD_ARTIFACT_STRUCTURE = """
model_name/
├── config.json                    # Model architecture config
├── generation_config.json         # Default generation parameters
├── model.safetensors              # Weights (or model-00001-of-00002.safetensors for sharded)
├── model.safetensors.index.json   # Index for sharded weights
├── tokenizer.json                 # Fast tokenizer
├── tokenizer_config.json          # Tokenizer configuration
├── special_tokens_map.json        # Special tokens
├── vocab.json / tokenizer.model   # Vocabulary (format varies)
├── README.md                      # Model card
└── (optional)/
    ├── adapter_config.json        # LoRA config (if adapter)
    └── adapter_model.safetensors  # Adapter weights
"""


class ArtifactValidator:
    """
    Validate model artifact completeness
    """

    REQUIRED_FILES = [
        "config.json",
    ]

    REQUIRED_WEIGHTS = [
        "model.safetensors",
        "model.bin",
        "pytorch_model.safetensors",
        "pytorch_model.bin"
    ]

    TOKENIZER_FILES = [
        "tokenizer.json",
        "tokenizer.model",
        "spiece.model"
    ]

    def validate(self, model_dir: str) -> Dict[str, Any]:
        """Validate artifact structure"""
        model_path = Path(model_dir)
        results = {
            "valid": True,
            "errors": [],
            "warnings": [],
            "files_found": []
        }

        # Check required files
        for req_file in self.REQUIRED_FILES:
            if not (model_path / req_file).exists():
                results["errors"].append(f"Missing required: {req_file}")
                results["valid"] = False

        # Check weights
        weights_found = False
        for weights_file in self.REQUIRED_WEIGHTS:
            if (model_path / weights_file).exists():
                weights_found = True
                results["files_found"].append(weights_file)
                break

            # Check for sharded weights
            sharded_pattern = f"{weights_file.replace('.', '-')}*"
            sharded_files = list(model_path.glob(sharded_pattern))
            if sharded_files:
                weights_found = True
                results["files_found"].extend([f.name for f in sharded_files])
                break

        if not weights_found:
            results["errors"].append("No model weights found")
            results["valid"] = False

        # Check tokenizer
        tokenizer_found = False
        for tok_file in self.TOKENIZER_FILES:
            if (model_path / tok_file).exists():
                tokenizer_found = True
                results["files_found"].append(tok_file)
                break

        if not tokenizer_found:
            results["warnings"].append("No tokenizer found")

        # Check for recommended files
        if not (model_path / "generation_config.json").exists():
            results["warnings"].append("Missing generation_config.json")

        if not (model_path / "README.md").exists():
            results["warnings"].append("Missing model card (README.md)")

        return results

    def validate_weights_integrity(
        self,
        weights_path: str
    ) -> Dict[str, Any]:
        """Validate weight file integrity"""
        from safetensors import safe_open

        results = {
            "valid": True,
            "format": None,
            "num_tensors": 0,
            "total_params": 0,
            "errors": []
        }

        if weights_path.endswith(".safetensors"):
            results["format"] = "safetensors"
            try:
                with safe_open(weights_path, framework="pt") as f:
                    results["num_tensors"] = len(f.keys())
                    for key in f.keys():
                        tensor = f.get_tensor(key)
                        results["total_params"] += tensor.numel()
            except Exception as e:
                results["valid"] = False
                results["errors"].append(str(e))

        elif weights_path.endswith(".bin"):
            results["format"] = "pytorch_bin"
            import torch
            try:
                state_dict = torch.load(
                    weights_path,
                    map_location="cpu",
                    weights_only=True
                )
                results["num_tensors"] = len(state_dict)
                for v in state_dict.values():
                    if hasattr(v, 'numel'):
                        results["total_params"] += v.numel()
            except Exception as e:
                results["valid"] = False
                results["errors"].append(str(e))

        return results
```

---

## 3. Storage Solutions

### 3.1 Cloud Storage Integration

```python
"""
Cloud storage backends for model artifacts
"""
from abc import ABC, abstractmethod
from typing import Optional, List, Dict, Any, BinaryIO
from dataclasses import dataclass
from pathlib import Path
import hashlib
import os


class StorageBackend(ABC):
    """Abstract storage backend interface"""

    @abstractmethod
    def upload(
        self,
        local_path: str,
        remote_path: str,
        metadata: Optional[Dict] = None
    ) -> str:
        """Upload file and return URI"""
        pass

    @abstractmethod
    def download(
        self,
        remote_path: str,
        local_path: str
    ):
        """Download file"""
        pass

    @abstractmethod
    def exists(self, remote_path: str) -> bool:
        """Check if path exists"""
        pass

    @abstractmethod
    def list(self, prefix: str) -> List[str]:
        """List files with prefix"""
        pass

    @abstractmethod
    def delete(self, remote_path: str):
        """Delete file"""
        pass

    @abstractmethod
    def get_signed_url(
        self,
        remote_path: str,
        expiration_seconds: int = 3600
    ) -> str:
        """Get signed URL for temporary access"""
        pass


class S3StorageBackend(StorageBackend):
    """Amazon S3 storage backend"""

    def __init__(
        self,
        bucket: str,
        prefix: str = "models",
        region: str = "us-east-1"
    ):
        import boto3
        self.bucket = bucket
        self.prefix = prefix
        self.s3 = boto3.client('s3', region_name=region)
        self.s3_resource = boto3.resource('s3', region_name=region)

    def _full_path(self, path: str) -> str:
        """Get full S3 path with prefix"""
        return f"{self.prefix}/{path}".lstrip("/")

    def upload(
        self,
        local_path: str,
        remote_path: str,
        metadata: Optional[Dict] = None
    ) -> str:
        """Upload to S3 with multipart for large files"""
        from boto3.s3.transfer import TransferConfig

        full_path = self._full_path(remote_path)

        # Configure multipart upload for large files
        config = TransferConfig(
            multipart_threshold=100 * 1024 * 1024,  # 100MB
            multipart_chunksize=100 * 1024 * 1024,
            max_concurrency=10,
            use_threads=True
        )

        extra_args = {}
        if metadata:
            extra_args['Metadata'] = {
                k: str(v) for k, v in metadata.items()
            }

        self.s3.upload_file(
            local_path,
            self.bucket,
            full_path,
            Config=config,
            ExtraArgs=extra_args if extra_args else None
        )

        return f"s3://{self.bucket}/{full_path}"

    def download(
        self,
        remote_path: str,
        local_path: str
    ):
        """Download from S3"""
        from boto3.s3.transfer import TransferConfig

        full_path = self._full_path(remote_path)
        os.makedirs(os.path.dirname(local_path), exist_ok=True)

        config = TransferConfig(
            multipart_threshold=100 * 1024 * 1024,
            max_concurrency=10,
            use_threads=True
        )

        self.s3.download_file(
            self.bucket,
            full_path,
            local_path,
            Config=config
        )

    def exists(self, remote_path: str) -> bool:
        """Check if object exists"""
        full_path = self._full_path(remote_path)
        try:
            self.s3.head_object(Bucket=self.bucket, Key=full_path)
            return True
        except:
            return False

    def list(self, prefix: str) -> List[str]:
        """List objects with prefix"""
        full_prefix = self._full_path(prefix)
        paginator = self.s3.get_paginator('list_objects_v2')

        files = []
        for page in paginator.paginate(Bucket=self.bucket, Prefix=full_prefix):
            for obj in page.get('Contents', []):
                files.append(obj['Key'])

        return files

    def delete(self, remote_path: str):
        """Delete object"""
        full_path = self._full_path(remote_path)
        self.s3.delete_object(Bucket=self.bucket, Key=full_path)

    def get_signed_url(
        self,
        remote_path: str,
        expiration_seconds: int = 3600
    ) -> str:
        """Generate presigned URL"""
        full_path = self._full_path(remote_path)
        return self.s3.generate_presigned_url(
            'get_object',
            Params={'Bucket': self.bucket, 'Key': full_path},
            ExpiresIn=expiration_seconds
        )


class GCSStorageBackend(StorageBackend):
    """Google Cloud Storage backend"""

    def __init__(
        self,
        bucket: str,
        prefix: str = "models"
    ):
        from google.cloud import storage
        self.bucket_name = bucket
        self.prefix = prefix
        self.client = storage.Client()
        self.bucket = self.client.bucket(bucket)

    def _full_path(self, path: str) -> str:
        return f"{self.prefix}/{path}".lstrip("/")

    def upload(
        self,
        local_path: str,
        remote_path: str,
        metadata: Optional[Dict] = None
    ) -> str:
        """Upload to GCS"""
        full_path = self._full_path(remote_path)
        blob = self.bucket.blob(full_path)

        if metadata:
            blob.metadata = metadata

        # Use resumable upload for large files
        blob.upload_from_filename(
            local_path,
            timeout=3600  # 1 hour timeout for large files
        )

        return f"gs://{self.bucket_name}/{full_path}"

    def download(
        self,
        remote_path: str,
        local_path: str
    ):
        """Download from GCS"""
        full_path = self._full_path(remote_path)
        blob = self.bucket.blob(full_path)
        os.makedirs(os.path.dirname(local_path), exist_ok=True)
        blob.download_to_filename(local_path)

    def exists(self, remote_path: str) -> bool:
        full_path = self._full_path(remote_path)
        blob = self.bucket.blob(full_path)
        return blob.exists()

    def list(self, prefix: str) -> List[str]:
        full_prefix = self._full_path(prefix)
        return [blob.name for blob in self.bucket.list_blobs(prefix=full_prefix)]

    def delete(self, remote_path: str):
        full_path = self._full_path(remote_path)
        blob = self.bucket.blob(full_path)
        blob.delete()

    def get_signed_url(
        self,
        remote_path: str,
        expiration_seconds: int = 3600
    ) -> str:
        from datetime import timedelta
        full_path = self._full_path(remote_path)
        blob = self.bucket.blob(full_path)
        return blob.generate_signed_url(
            expiration=timedelta(seconds=expiration_seconds)
        )


class HuggingFaceHubBackend(StorageBackend):
    """HuggingFace Hub storage backend"""

    def __init__(
        self,
        organization: str,
        token: Optional[str] = None
    ):
        from huggingface_hub import HfApi
        self.org = organization
        self.api = HfApi(token=token)

    def upload(
        self,
        local_path: str,
        remote_path: str,
        metadata: Optional[Dict] = None
    ) -> str:
        """Upload to HuggingFace Hub"""
        # Parse remote_path: repo_name/file_path
        parts = remote_path.split("/", 1)
        repo_id = f"{self.org}/{parts[0]}"
        file_path = parts[1] if len(parts) > 1 else os.path.basename(local_path)

        # Create repo if needed
        try:
            self.api.create_repo(repo_id, exist_ok=True)
        except:
            pass

        # Upload
        self.api.upload_file(
            path_or_fileobj=local_path,
            path_in_repo=file_path,
            repo_id=repo_id,
            commit_message=f"Upload {file_path}"
        )

        return f"https://huggingface.co/{repo_id}/blob/main/{file_path}"

    def download(
        self,
        remote_path: str,
        local_path: str
    ):
        """Download from Hub"""
        from huggingface_hub import hf_hub_download

        parts = remote_path.split("/", 1)
        repo_id = f"{self.org}/{parts[0]}"
        file_path = parts[1]

        os.makedirs(os.path.dirname(local_path), exist_ok=True)
        hf_hub_download(
            repo_id=repo_id,
            filename=file_path,
            local_dir=os.path.dirname(local_path)
        )

    def exists(self, remote_path: str) -> bool:
        parts = remote_path.split("/", 1)
        repo_id = f"{self.org}/{parts[0]}"
        file_path = parts[1]

        try:
            self.api.file_info(repo_id=repo_id, path=file_path)
            return True
        except:
            return False

    def list(self, prefix: str) -> List[str]:
        parts = prefix.split("/", 1)
        repo_id = f"{self.org}/{parts[0]}"

        files = self.api.list_repo_files(repo_id)
        return files

    def delete(self, remote_path: str):
        parts = remote_path.split("/", 1)
        repo_id = f"{self.org}/{parts[0]}"
        file_path = parts[1]

        self.api.delete_file(
            path_in_repo=file_path,
            repo_id=repo_id
        )

    def get_signed_url(
        self,
        remote_path: str,
        expiration_seconds: int = 3600
    ) -> str:
        # HuggingFace Hub doesn't use signed URLs
        # Return direct URL (requires auth for private repos)
        parts = remote_path.split("/", 1)
        repo_id = f"{self.org}/{parts[0]}"
        file_path = parts[1]
        return f"https://huggingface.co/{repo_id}/resolve/main/{file_path}"
```

### 3.2 Storage Organization

```python
"""
Model storage organization patterns
"""
from dataclasses import dataclass
from typing import Optional


@dataclass
class StorageLayout:
    """Define storage layout for models"""

    # Base paths
    base_path: str = "models"

    # Organizational structure
    by_model: bool = True        # Organize by model name first
    by_version: bool = True      # Separate versions
    by_stage: bool = False       # Separate by lifecycle stage

    def get_model_path(
        self,
        model_name: str,
        version: str,
        stage: Optional[str] = None
    ) -> str:
        """Get storage path for model"""
        parts = [self.base_path]

        if self.by_stage and stage:
            parts.append(stage)

        if self.by_model:
            parts.append(model_name)

        if self.by_version:
            parts.append(f"v{version}")

        return "/".join(parts)


# Example layouts
LAYOUTS = {
    "flat": StorageLayout(
        base_path="models",
        by_model=True,
        by_version=True,
        by_stage=False
    ),
    # models/llama-7b/v1.0.0/

    "staged": StorageLayout(
        base_path="models",
        by_model=True,
        by_version=True,
        by_stage=True
    ),
    # models/production/llama-7b/v1.0.0/

    "simple": StorageLayout(
        base_path="models",
        by_model=True,
        by_version=False,
        by_stage=False
    )
    # models/llama-7b/
}


class ModelStorageManager:
    """
    High-level model storage management
    """

    def __init__(
        self,
        backend: StorageBackend,
        layout: StorageLayout
    ):
        self.backend = backend
        self.layout = layout

    def store_model(
        self,
        model_name: str,
        version: str,
        local_dir: str,
        stage: Optional[str] = None,
        metadata: Optional[Dict] = None
    ) -> Dict[str, str]:
        """Store complete model directory"""
        base_path = self.layout.get_model_path(model_name, version, stage)
        stored_files = {}

        for file_path in Path(local_dir).rglob("*"):
            if file_path.is_file():
                relative = file_path.relative_to(local_dir)
                remote_path = f"{base_path}/{relative}"

                uri = self.backend.upload(
                    str(file_path),
                    remote_path,
                    metadata=metadata
                )
                stored_files[str(relative)] = uri

        return stored_files

    def retrieve_model(
        self,
        model_name: str,
        version: str,
        local_dir: str,
        stage: Optional[str] = None
    ):
        """Download complete model to local directory"""
        base_path = self.layout.get_model_path(model_name, version, stage)
        files = self.backend.list(base_path)

        for remote_file in files:
            relative = remote_file.replace(base_path + "/", "")
            local_path = os.path.join(local_dir, relative)

            self.backend.download(remote_file, local_path)

    def delete_model(
        self,
        model_name: str,
        version: str,
        stage: Optional[str] = None
    ):
        """Delete model from storage"""
        base_path = self.layout.get_model_path(model_name, version, stage)
        files = self.backend.list(base_path)

        for remote_file in files:
            self.backend.delete(remote_file)
```

---

## 4. Artifact Integrity

### 4.1 Checksum and Verification

```python
"""
Artifact integrity verification
"""
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path
import hashlib
import json


@dataclass
class FileChecksum:
    """Checksum record for a file"""
    file_path: str
    algorithm: str
    checksum: str
    size_bytes: int


class IntegrityVerifier:
    """
    Verify artifact integrity using checksums
    """

    ALGORITHMS = {
        "sha256": hashlib.sha256,
        "sha512": hashlib.sha512,
        "md5": hashlib.md5
    }

    def __init__(self, algorithm: str = "sha256"):
        if algorithm not in self.ALGORITHMS:
            raise ValueError(f"Unsupported algorithm: {algorithm}")
        self.algorithm = algorithm

    def compute_checksum(self, file_path: str) -> FileChecksum:
        """Compute checksum for file"""
        hasher = self.ALGORITHMS[self.algorithm]()
        file_size = 0

        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(8192), b''):
                hasher.update(chunk)
                file_size += len(chunk)

        return FileChecksum(
            file_path=file_path,
            algorithm=self.algorithm,
            checksum=hasher.hexdigest(),
            size_bytes=file_size
        )

    def compute_directory_checksums(
        self,
        directory: str
    ) -> List[FileChecksum]:
        """Compute checksums for all files in directory"""
        checksums = []
        for file_path in Path(directory).rglob("*"):
            if file_path.is_file():
                checksum = self.compute_checksum(str(file_path))
                # Store relative path
                checksum.file_path = str(
                    file_path.relative_to(directory)
                )
                checksums.append(checksum)
        return checksums

    def verify_file(
        self,
        file_path: str,
        expected_checksum: str
    ) -> Tuple[bool, Optional[str]]:
        """Verify file matches expected checksum"""
        actual = self.compute_checksum(file_path)

        if actual.checksum == expected_checksum:
            return True, None
        else:
            return False, f"Checksum mismatch: expected {expected_checksum}, got {actual.checksum}"

    def verify_directory(
        self,
        directory: str,
        expected_checksums: List[FileChecksum]
    ) -> Dict[str, bool]:
        """Verify all files in directory"""
        results = {}
        expected_map = {c.file_path: c.checksum for c in expected_checksums}

        for file_path in Path(directory).rglob("*"):
            if file_path.is_file():
                relative = str(file_path.relative_to(directory))

                if relative not in expected_map:
                    results[relative] = False
                    continue

                valid, _ = self.verify_file(
                    str(file_path),
                    expected_map[relative]
                )
                results[relative] = valid

        return results

    def save_manifest(
        self,
        checksums: List[FileChecksum],
        output_path: str
    ):
        """Save checksum manifest file"""
        manifest = {
            "algorithm": self.algorithm,
            "files": [
                {
                    "path": c.file_path,
                    "checksum": c.checksum,
                    "size": c.size_bytes
                }
                for c in checksums
            ]
        }

        with open(output_path, 'w') as f:
            json.dump(manifest, f, indent=2)

    def load_manifest(self, manifest_path: str) -> List[FileChecksum]:
        """Load checksum manifest"""
        with open(manifest_path) as f:
            manifest = json.load(f)

        return [
            FileChecksum(
                file_path=f["path"],
                algorithm=manifest["algorithm"],
                checksum=f["checksum"],
                size_bytes=f["size"]
            )
            for f in manifest["files"]
        ]


class ModelSignature:
    """
    Cryptographic signing for model artifacts
    """

    def __init__(self, private_key_path: Optional[str] = None):
        self.private_key_path = private_key_path

    def sign_manifest(
        self,
        manifest_path: str,
        signature_path: str
    ):
        """Sign a manifest file"""
        from cryptography.hazmat.primitives import hashes, serialization
        from cryptography.hazmat.primitives.asymmetric import padding
        from cryptography.hazmat.backends import default_backend

        # Load private key
        with open(self.private_key_path, 'rb') as f:
            private_key = serialization.load_pem_private_key(
                f.read(),
                password=None,
                backend=default_backend()
            )

        # Read manifest
        with open(manifest_path, 'rb') as f:
            manifest_data = f.read()

        # Sign
        signature = private_key.sign(
            manifest_data,
            padding.PSS(
                mgf=padding.MGF1(hashes.SHA256()),
                salt_length=padding.PSS.MAX_LENGTH
            ),
            hashes.SHA256()
        )

        # Write signature
        with open(signature_path, 'wb') as f:
            f.write(signature)

    def verify_signature(
        self,
        manifest_path: str,
        signature_path: str,
        public_key_path: str
    ) -> bool:
        """Verify manifest signature"""
        from cryptography.hazmat.primitives import hashes, serialization
        from cryptography.hazmat.primitives.asymmetric import padding
        from cryptography.hazmat.backends import default_backend

        # Load public key
        with open(public_key_path, 'rb') as f:
            public_key = serialization.load_pem_public_key(
                f.read(),
                backend=default_backend()
            )

        # Read manifest and signature
        with open(manifest_path, 'rb') as f:
            manifest_data = f.read()
        with open(signature_path, 'rb') as f:
            signature = f.read()

        # Verify
        try:
            public_key.verify(
                signature,
                manifest_data,
                padding.PSS(
                    mgf=padding.MGF1(hashes.SHA256()),
                    salt_length=padding.PSS.MAX_LENGTH
                ),
                hashes.SHA256()
            )
            return True
        except:
            return False
```

---

## 5. Distribution and Access Control

### 5.1 Model Distribution

```python
"""
Model distribution patterns
"""
from dataclasses import dataclass
from typing import Dict, List, Optional
from enum import Enum


class AccessLevel(Enum):
    """Model access levels"""
    PUBLIC = "public"           # Anyone can access
    AUTHENTICATED = "auth"      # Requires authentication
    TEAM = "team"              # Team members only
    RESTRICTED = "restricted"   # Specific users only


@dataclass
class DistributionConfig:
    """Configuration for model distribution"""
    model_name: str
    version: str
    access_level: AccessLevel
    allowed_users: Optional[List[str]] = None
    rate_limit_requests: int = 100  # Per minute
    rate_limit_bytes: int = 10_000_000_000  # 10GB per day


class ModelDistributor:
    """
    Handle model distribution with access control
    """

    def __init__(
        self,
        storage: StorageBackend,
        access_control: "AccessControlManager"
    ):
        self.storage = storage
        self.access_control = access_control
        self.distribution_configs: Dict[str, DistributionConfig] = {}

    def configure_distribution(
        self,
        config: DistributionConfig
    ):
        """Set up distribution for a model"""
        key = f"{config.model_name}:{config.version}"
        self.distribution_configs[key] = config

    def get_download_url(
        self,
        model_name: str,
        version: str,
        user_id: str,
        file_path: str
    ) -> Optional[str]:
        """Get download URL with access check"""
        key = f"{model_name}:{version}"
        config = self.distribution_configs.get(key)

        if not config:
            return None

        # Check access
        if not self._check_access(config, user_id):
            return None

        # Check rate limits
        if not self._check_rate_limit(config, user_id):
            return None

        # Generate signed URL
        remote_path = f"{model_name}/v{version}/{file_path}"
        return self.storage.get_signed_url(remote_path, expiration_seconds=3600)

    def _check_access(
        self,
        config: DistributionConfig,
        user_id: str
    ) -> bool:
        """Check user access"""
        if config.access_level == AccessLevel.PUBLIC:
            return True

        if config.access_level == AccessLevel.AUTHENTICATED:
            return self.access_control.is_authenticated(user_id)

        if config.access_level == AccessLevel.TEAM:
            return self.access_control.is_team_member(user_id)

        if config.access_level == AccessLevel.RESTRICTED:
            return user_id in (config.allowed_users or [])

        return False

    def _check_rate_limit(
        self,
        config: DistributionConfig,
        user_id: str
    ) -> bool:
        """Check rate limits"""
        # Implementation would track usage
        return True


class AccessControlManager:
    """
    Manage access control for models
    """

    def __init__(self):
        self.authenticated_users = set()
        self.team_members = set()

    def is_authenticated(self, user_id: str) -> bool:
        return user_id in self.authenticated_users

    def is_team_member(self, user_id: str) -> bool:
        return user_id in self.team_members

    def add_team_member(self, user_id: str):
        self.team_members.add(user_id)
        self.authenticated_users.add(user_id)
```

---

## 6. Troubleshooting

### Common Issues

| Issue | Cause | Solution |
|-------|-------|----------|
| Checksum mismatch | Corrupted download | Re-download with verification |
| Slow uploads | Large files | Use multipart upload |
| Missing files | Incomplete artifact | Validate before storage |
| Loading errors | Format incompatibility | Convert to safetensors |
| Out of memory | Loading full model | Use mmap/lazy loading |

---

## 7. References

### Documentation
- Safetensors: https://github.com/huggingface/safetensors
- HuggingFace Hub: https://huggingface.co/docs/huggingface_hub/
- DVC: https://dvc.org/doc

---

## Document Metadata

- **Version**: 1.0
- **Last Updated**: 2024
- **Prerequisites**: Model registry, cloud storage
- **Estimated Reading Time**: 40 minutes
- **Hands-on Lab Time**: 2-3 hours

---

> **Navigation**
> [← 8.2 Experiment Tracking](8.2_experiment_tracking_guide.md) | **[Index](../README.md#15-repository-structure)** | [8.4 Feature Store →](8.4_feature_store_llms_guide.md)
