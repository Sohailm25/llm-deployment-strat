> **Navigation** | [← 3.2 PEFT](3.2_parameter_efficient_fine_tuning.md) | [3.4 Continued Pre-training →](3.4_continued_pretraining.md)
>
> | | |
> |---|---|
> | **Prerequisites** | [3.1 SFT](3.1_supervised_fine_tuning.md) &#124; [3.2 PEFT](3.2_parameter_efficient_fine_tuning.md) &#124; [Category 2 Training](../02_model_training/) |
> | **Related** | [3.4 Continued Pre-training](3.4_continued_pretraining.md) &#124; [7.1 Vector Database](../07_rag_pipeline/7.1_vector_database_guide.md) &#124; [1.1 Data Collection](../01_data_pipeline/1.1_data_collection_sourcing.md) |
> | **Next** | [3.4 Continued Pre-training](3.4_continued_pretraining.md) |

# Domain Adaptation Guide

## Executive Summary

Domain adaptation transforms general-purpose LLMs into specialized models for specific industries like legal, medical, financial, or scientific domains. This guide covers continued pre-training strategies, domain-specific considerations, data collection approaches, and evaluation methodologies while addressing catastrophic forgetting.

## Prerequisites

- Understanding of pre-training (Category 2)
- SFT and PEFT fundamentals (Documents 3.1, 3.2)
- Domain expertise or access to domain experts
- Domain-specific data sources

---

## 3.3.1 Domain Adaptation Strategies

### Strategy Comparison

| Strategy | Data Required | Compute Cost | Quality | Forgetting Risk |
|----------|---------------|--------------|---------|-----------------|
| Continued Pre-training (CPT) | 1B-100B+ tokens | Very High | Best for deep adaptation | High |
| Domain SFT | 10K-100K examples | Medium | Good for task focus | Medium |
| Vocabulary Extension | Domain corpus | Medium | Better tokenization | Low |
| RAG Augmentation | Knowledge base | Low | Good for factual tasks | None |
| Hybrid (CPT + SFT) | Both | Very High | Best overall | Medium |

### Strategy Selection Framework

```python
"""
Domain adaptation strategy selection
"""
from dataclasses import dataclass
from typing import List, Optional, Dict
from enum import Enum

class AdaptationStrategy(Enum):
    CONTINUED_PRETRAINING = "continued_pretraining"
    DOMAIN_SFT = "domain_sft"
    VOCABULARY_EXTENSION = "vocabulary_extension"
    RAG_AUGMENTATION = "rag_augmentation"
    HYBRID = "hybrid"

@dataclass
class DomainRequirements:
    """Requirements for domain adaptation"""
    domain_name: str
    specialized_vocabulary: bool  # Domain has unique terminology
    requires_reasoning: bool      # Tasks need domain reasoning
    factual_accuracy_critical: bool
    data_availability_tokens: int
    labeled_examples: int
    compute_budget_gpu_hours: int
    update_frequency: str  # "daily", "weekly", "monthly", "rare"
    compliance_requirements: List[str]  # ["HIPAA", "GDPR", etc.]

def recommend_strategy(requirements: DomainRequirements) -> Dict:
    """
    Recommend domain adaptation strategy based on requirements.

    Returns:
        Strategy recommendation with reasoning
    """
    strategies = []

    # Continued pre-training for deep domain knowledge
    if (requirements.data_availability_tokens >= 1_000_000_000 and
        requirements.compute_budget_gpu_hours >= 1000 and
        requirements.specialized_vocabulary):
        strategies.append({
            "strategy": AdaptationStrategy.CONTINUED_PRETRAINING,
            "priority": 1,
            "reason": "Large domain corpus enables deep knowledge integration"
        })

    # Domain SFT for task-specific behavior
    if requirements.labeled_examples >= 1000:
        strategies.append({
            "strategy": AdaptationStrategy.DOMAIN_SFT,
            "priority": 2,
            "reason": "Sufficient labeled data for supervised fine-tuning"
        })

    # RAG for frequently updated or factual tasks
    if (requirements.update_frequency in ["daily", "weekly"] or
        requirements.factual_accuracy_critical):
        strategies.append({
            "strategy": AdaptationStrategy.RAG_AUGMENTATION,
            "priority": 1 if requirements.update_frequency == "daily" else 2,
            "reason": "Enables real-time knowledge updates and factual grounding"
        })

    # Vocabulary extension for heavy terminology
    if requirements.specialized_vocabulary:
        strategies.append({
            "strategy": AdaptationStrategy.VOCABULARY_EXTENSION,
            "priority": 3,
            "reason": "Domain terminology benefits from specialized tokenization"
        })

    # Hybrid for complex requirements
    if (len(strategies) >= 2 and
        requirements.requires_reasoning and
        requirements.factual_accuracy_critical):
        return {
            "recommended": AdaptationStrategy.HYBRID,
            "components": [s["strategy"] for s in strategies[:2]],
            "reasoning": "Complex requirements benefit from combined approaches"
        }

    if strategies:
        return {
            "recommended": strategies[0]["strategy"],
            "alternatives": [s["strategy"] for s in strategies[1:]],
            "reasoning": strategies[0]["reason"]
        }

    return {
        "recommended": AdaptationStrategy.RAG_AUGMENTATION,
        "reasoning": "Default for limited resources - minimal risk"
    }

# Example
legal_requirements = DomainRequirements(
    domain_name="legal",
    specialized_vocabulary=True,
    requires_reasoning=True,
    factual_accuracy_critical=True,
    data_availability_tokens=10_000_000_000,
    labeled_examples=50000,
    compute_budget_gpu_hours=5000,
    update_frequency="monthly",
    compliance_requirements=["confidentiality"]
)

recommendation = recommend_strategy(legal_requirements)
print(f"Recommended: {recommendation}")
```

### Continued Pre-training Implementation

```python
"""
Continued pre-training (CPT) for domain adaptation
"""
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from typing import Dict, List, Optional
from dataclasses import dataclass

@dataclass
class CPTConfig:
    """Configuration for continued pre-training"""
    # Model
    base_model: str
    output_dir: str

    # Data
    domain_data_path: str
    general_replay_path: Optional[str] = None
    replay_ratio: float = 0.1  # Fraction of general data to mix

    # Training
    total_tokens: int = 10_000_000_000
    batch_size: int = 32
    gradient_accumulation_steps: int = 8
    learning_rate: float = 1e-5  # Lower than pre-training
    warmup_ratio: float = 0.05
    weight_decay: float = 0.1
    max_seq_length: int = 4096

    # Regularization
    use_replay: bool = True
    freeze_layers: int = 0  # Number of early layers to freeze

class ContinuedPretrainer:
    """
    Continued pre-training pipeline with forgetting mitigation.
    """

    def __init__(self, config: CPTConfig):
        self.config = config
        self.model = None
        self.tokenizer = None

    def load_model(self):
        """Load base model for continued pre-training"""
        self.model = AutoModelForCausalLM.from_pretrained(
            self.config.base_model,
            torch_dtype=torch.bfloat16,
            attn_implementation="flash_attention_2"
        )

        self.tokenizer = AutoTokenizer.from_pretrained(self.config.base_model)

        # Freeze early layers if configured
        if self.config.freeze_layers > 0:
            self._freeze_early_layers()

        return self.model, self.tokenizer

    def _freeze_early_layers(self):
        """Freeze early transformer layers"""
        layers_frozen = 0

        for name, param in self.model.named_parameters():
            # Freeze embedding layer
            if "embed" in name:
                param.requires_grad = False
                continue

            # Freeze early transformer blocks
            if "layers" in name:
                layer_num = int(name.split("layers.")[1].split(".")[0])
                if layer_num < self.config.freeze_layers:
                    param.requires_grad = False
                    layers_frozen += 1

        print(f"Frozen {layers_frozen} parameters in first {self.config.freeze_layers} layers")

    def create_mixed_dataset(
        self,
        domain_dataset,
        general_dataset
    ):
        """
        Create dataset mixing domain and general data.

        Helps prevent catastrophic forgetting by replaying general data.
        """
        from torch.utils.data import ConcatDataset, WeightedRandomSampler

        # Calculate weights
        domain_weight = 1 - self.config.replay_ratio
        general_weight = self.config.replay_ratio

        # Adjust sampling
        domain_samples = int(len(domain_dataset) * domain_weight)
        general_samples = int(len(general_dataset) * general_weight)

        # Create weighted sampler
        weights = (
            [domain_weight] * len(domain_dataset) +
            [general_weight] * len(general_dataset)
        )

        combined = ConcatDataset([domain_dataset, general_dataset])
        sampler = WeightedRandomSampler(
            weights=weights,
            num_samples=domain_samples + general_samples,
            replacement=True
        )

        return combined, sampler

    def train(self, train_dataset, eval_dataset=None):
        """Run continued pre-training"""
        training_args = TrainingArguments(
            output_dir=self.config.output_dir,
            max_steps=self.config.total_tokens // (self.config.batch_size * self.config.max_seq_length),
            per_device_train_batch_size=self.config.batch_size,
            gradient_accumulation_steps=self.config.gradient_accumulation_steps,
            learning_rate=self.config.learning_rate,
            warmup_ratio=self.config.warmup_ratio,
            weight_decay=self.config.weight_decay,
            lr_scheduler_type="cosine",
            bf16=True,
            logging_steps=100,
            save_strategy="steps",
            save_steps=1000,
            evaluation_strategy="steps" if eval_dataset else "no",
            eval_steps=1000 if eval_dataset else None,
        )

        from transformers import Trainer, DataCollatorForLanguageModeling

        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            data_collator=DataCollatorForLanguageModeling(
                tokenizer=self.tokenizer,
                mlm=False
            ),
        )

        trainer.train()
        return trainer

# Multi-phase CPT schedule
CPT_PHASE_SCHEDULE = {
    "phase1_general_warmup": {
        "data_mix": {"domain": 0.3, "general": 0.7},
        "tokens": 0.2,  # 20% of total
        "learning_rate_multiplier": 1.0,
        "purpose": "Warm up on mixed data to prevent early divergence"
    },
    "phase2_domain_focus": {
        "data_mix": {"domain": 0.8, "general": 0.2},
        "tokens": 0.6,  # 60% of total
        "learning_rate_multiplier": 1.0,
        "purpose": "Main domain knowledge injection"
    },
    "phase3_consolidation": {
        "data_mix": {"domain": 0.5, "general": 0.5},
        "tokens": 0.2,  # 20% of total
        "learning_rate_multiplier": 0.5,
        "purpose": "Consolidate knowledge, reduce forgetting"
    }
}
```

---

## 3.3.2 Domain-Specific Considerations

### Legal Domain

```python
"""
Legal domain adaptation considerations
"""
from dataclasses import dataclass
from typing import List, Dict, Optional
from enum import Enum

class LegalJurisdiction(Enum):
    US_FEDERAL = "us_federal"
    US_STATE = "us_state"
    UK = "uk"
    EU = "eu"
    INTERNATIONAL = "international"

@dataclass
class LegalDomainConfig:
    """Configuration for legal domain adaptation"""
    jurisdictions: List[LegalJurisdiction]
    practice_areas: List[str]  # ["contract", "litigation", "ip", etc.]
    include_case_law: bool = True
    include_statutes: bool = True
    include_regulations: bool = True
    confidentiality_level: str = "strict"

    # Legal-specific requirements
    citation_format: str = "bluebook"  # "bluebook", "oscola", "aglc"
    redaction_patterns: List[str] = None  # Patterns to redact

class LegalDataProcessor:
    """
    Process legal documents for domain adaptation.
    """

    # Legal citation patterns
    CITATION_PATTERNS = {
        "case": r"\d+\s+\w+\.?\s*\d*d?\s+\d+",  # 123 F.2d 456
        "statute": r"\d+\s+U\.S\.C\.?\s*§?\s*\d+",  # 42 U.S.C. § 1983
        "regulation": r"\d+\s+C\.F\.R\.?\s*§?\s*\d+",  # 17 CFR § 240
    }

    # Sections to extract
    DOCUMENT_SECTIONS = [
        "parties", "facts", "procedural_history", "issues",
        "holding", "reasoning", "disposition", "dissent"
    ]

    def process_case_law(self, document: str) -> Dict:
        """Process case law document"""
        processed = {
            "text": document,
            "citations": self._extract_citations(document),
            "sections": self._extract_sections(document),
            "entities": self._extract_legal_entities(document)
        }
        return processed

    def _extract_citations(self, text: str) -> List[Dict]:
        """Extract legal citations"""
        import re
        citations = []

        for cite_type, pattern in self.CITATION_PATTERNS.items():
            matches = re.findall(pattern, text)
            for match in matches:
                citations.append({
                    "type": cite_type,
                    "citation": match,
                    "normalized": self._normalize_citation(match, cite_type)
                })

        return citations

    def _extract_sections(self, text: str) -> Dict[str, str]:
        """Extract document sections"""
        sections = {}
        # Implementation would use section headers and patterns
        return sections

    def _extract_legal_entities(self, text: str) -> List[Dict]:
        """Extract legal entities (parties, courts, judges)"""
        # Implementation would use NER or patterns
        return []

    def _normalize_citation(self, citation: str, cite_type: str) -> str:
        """Normalize citation format"""
        # Implementation would standardize citation format
        return citation

# Legal compliance considerations
LEGAL_COMPLIANCE = {
    "ethics_rules": {
        "unauthorized_practice": "Model should not provide legal advice as if attorney",
        "confidentiality": "Training data must not include privileged communications",
        "competence": "Model should acknowledge limitations"
    },
    "disclaimer_requirements": [
        "This is not legal advice",
        "Consult a licensed attorney",
        "Laws vary by jurisdiction"
    ],
    "prohibited_outputs": [
        "Specific legal advice without disclaimer",
        "Confidential client information",
        "Advice on criminal activity"
    ]
}
```

### Medical Domain

```python
"""
Medical domain adaptation considerations
"""
from dataclasses import dataclass
from typing import List, Dict, Optional

@dataclass
class MedicalDomainConfig:
    """Configuration for medical domain adaptation"""
    specialties: List[str]  # ["cardiology", "oncology", etc.]
    include_clinical_notes: bool = False  # PHI concerns
    include_literature: bool = True
    include_guidelines: bool = True
    terminology_standard: str = "snomed_ct"  # "snomed_ct", "icd10", "mesh"

    # HIPAA compliance
    hipaa_compliant: bool = True
    de_identification_method: str = "safe_harbor"  # "safe_harbor", "expert"

class MedicalDataProcessor:
    """
    Process medical documents with HIPAA compliance.
    """

    # PHI identifiers (Safe Harbor)
    PHI_PATTERNS = {
        "names": r"\b[A-Z][a-z]+\s+[A-Z][a-z]+\b",
        "dates": r"\b\d{1,2}[/-]\d{1,2}[/-]\d{2,4}\b",
        "ages_over_89": r"\b(9\d|[1-9]\d{2})\s*(years?|y/?o)\b",
        "phone": r"\b\d{3}[-.\s]?\d{3}[-.\s]?\d{4}\b",
        "email": r"\b[\w.-]+@[\w.-]+\.\w+\b",
        "ssn": r"\b\d{3}-\d{2}-\d{4}\b",
        "mrn": r"\bMRN[:\s]*\d+\b",
        "addresses": r"\b\d+\s+\w+\s+(St|Ave|Rd|Blvd|Dr)\b",
    }

    # Medical terminology to preserve
    TERMINOLOGY_PATTERNS = {
        "medications": r"\b[A-Za-z]+(?:mab|nib|vir|pril|olol|sartan)\b",
        "procedures": r"\b(CT|MRI|EKG|ECG|PET|CBC|BMP|CMP)\b",
        "diagnoses": r"\b(hypertension|diabetes|carcinoma|syndrome)\b",
    }

    def de_identify(self, text: str, method: str = "safe_harbor") -> str:
        """
        De-identify medical text according to HIPAA.

        Args:
            text: Original text
            method: De-identification method

        Returns:
            De-identified text
        """
        import re
        de_identified = text

        if method == "safe_harbor":
            # Replace all 18 HIPAA identifiers
            for phi_type, pattern in self.PHI_PATTERNS.items():
                de_identified = re.sub(
                    pattern,
                    f"[{phi_type.upper()}_REDACTED]",
                    de_identified
                )

        return de_identified

    def extract_medical_entities(self, text: str) -> Dict[str, List[str]]:
        """Extract medical entities while preserving privacy"""
        import re
        entities = {}

        for entity_type, pattern in self.TERMINOLOGY_PATTERNS.items():
            matches = re.findall(pattern, text, re.IGNORECASE)
            entities[entity_type] = list(set(matches))

        return entities

    def validate_clinical_accuracy(
        self,
        generated_text: str,
        reference_sources: List[str]
    ) -> Dict:
        """
        Validate clinical accuracy of generated content.

        Returns validation report.
        """
        validation = {
            "medication_check": self._check_medications(generated_text),
            "dosage_check": self._check_dosages(generated_text),
            "contraindication_check": self._check_contraindications(generated_text),
            "guideline_alignment": self._check_guidelines(generated_text, reference_sources)
        }
        return validation

    def _check_medications(self, text: str) -> Dict:
        """Verify medication names are valid"""
        # Would check against drug database
        return {"valid": True, "issues": []}

    def _check_dosages(self, text: str) -> Dict:
        """Verify dosages are within safe ranges"""
        return {"valid": True, "issues": []}

    def _check_contraindications(self, text: str) -> Dict:
        """Check for potential contraindication issues"""
        return {"valid": True, "issues": []}

    def _check_guidelines(self, text: str, references: List[str]) -> Dict:
        """Check alignment with clinical guidelines"""
        return {"aligned": True, "discrepancies": []}

# HIPAA compliance checklist
HIPAA_COMPLIANCE_CHECKLIST = {
    "data_handling": [
        "All PHI de-identified using Safe Harbor or Expert method",
        "No direct identifiers in training data",
        "Quasi-identifiers (age, zip, dates) generalized",
        "Data access logged and auditable"
    ],
    "model_behavior": [
        "Model refuses to generate specific patient information",
        "Disclaimers for medical advice",
        "Recommends professional consultation",
        "Does not store conversation history with PHI"
    ],
    "infrastructure": [
        "Training on HIPAA-compliant infrastructure",
        "Encryption at rest and in transit",
        "Access controls and authentication",
        "Business Associate Agreements in place"
    ]
}
```

### Financial Domain

```python
"""
Financial domain adaptation considerations
"""
from dataclasses import dataclass
from typing import List, Dict

@dataclass
class FinancialDomainConfig:
    """Configuration for financial domain adaptation"""
    sectors: List[str]  # ["banking", "investment", "insurance", etc.]
    include_filings: bool = True  # SEC filings, annual reports
    include_news: bool = True
    include_research: bool = True
    regulatory_frameworks: List[str]  # ["SEC", "FINRA", "Basel"]

    # Compliance
    trading_advice_prohibited: bool = True
    pii_handling: str = "strict"

class FinancialDataProcessor:
    """
    Process financial documents for domain adaptation.
    """

    # Financial entities to extract
    ENTITY_PATTERNS = {
        "ticker": r"\b[A-Z]{1,5}\b(?=\s+(?:stock|shares|Inc|Corp))",
        "currency": r"\$[\d,]+(?:\.\d{2})?|\d+(?:\.\d{2})?\s*(?:USD|EUR|GBP)",
        "percentage": r"\d+(?:\.\d+)?%",
        "date": r"\b(?:Q[1-4]|FY)\s*\d{4}\b",
    }

    # Financial document sections
    DOCUMENT_SECTIONS = {
        "10-K": ["business", "risk_factors", "financial_data", "mda", "financials"],
        "10-Q": ["financial_statements", "mda", "controls"],
        "8-K": ["item_number", "description", "exhibits"],
        "earnings": ["guidance", "results", "outlook"]
    }

    def extract_financial_metrics(self, text: str) -> Dict:
        """Extract key financial metrics from text"""
        import re
        metrics = {
            "revenue": [],
            "earnings": [],
            "growth_rates": [],
            "ratios": []
        }

        # Revenue patterns
        revenue_pattern = r"revenue\s+(?:of\s+)?\$?([\d,.]+)\s*(million|billion)?"
        for match in re.finditer(revenue_pattern, text, re.IGNORECASE):
            value = float(match.group(1).replace(",", ""))
            unit = match.group(2) or ""
            metrics["revenue"].append({"value": value, "unit": unit})

        return metrics

    def normalize_financial_numbers(self, text: str) -> str:
        """Normalize financial number formats"""
        import re

        # Normalize currency
        text = re.sub(r"\$\s*(\d)", r"$\1", text)

        # Normalize millions/billions
        text = re.sub(r"(\d+)\s*M\b", r"\1 million", text)
        text = re.sub(r"(\d+)\s*B\b", r"\1 billion", text)

        return text

# Financial compliance
FINANCIAL_COMPLIANCE = {
    "prohibited_content": [
        "Specific investment recommendations",
        "Guaranteed returns predictions",
        "Insider information",
        "Market manipulation advice"
    ],
    "required_disclaimers": [
        "Not financial advice",
        "Past performance not indicative of future results",
        "Consult a licensed financial advisor"
    ],
    "regulatory_considerations": {
        "SEC": "No forward-looking statements without safe harbor",
        "FINRA": "Fair and balanced presentation",
        "MiFID": "Suitability requirements for EU"
    }
}
```

### Code Domain

```python
"""
Code domain adaptation considerations
"""
from dataclasses import dataclass
from typing import List, Dict

@dataclass
class CodeDomainConfig:
    """Configuration for code domain adaptation"""
    languages: List[str]  # ["python", "javascript", etc.]
    include_documentation: bool = True
    include_tests: bool = True
    include_comments: bool = True
    license_filter: List[str]  # Allowed licenses

    # Quality filters
    min_stars: int = 10
    min_file_length: int = 100
    max_file_length: int = 10000

class CodeDataProcessor:
    """
    Process code for domain adaptation.
    """

    # Language-specific patterns
    LANGUAGE_PATTERNS = {
        "python": {
            "docstring": r'"""[\s\S]*?"""',
            "function": r"def\s+(\w+)\s*\([^)]*\):",
            "class": r"class\s+(\w+)\s*(?:\([^)]*\))?:",
            "import": r"^(?:from\s+\S+\s+)?import\s+.+$"
        },
        "javascript": {
            "function": r"(?:function\s+(\w+)|(\w+)\s*=\s*(?:async\s+)?function|\b(\w+)\s*:\s*function)",
            "class": r"class\s+(\w+)",
            "import": r"^(?:import|require)\s*\(.+\)|^import\s+.+"
        }
    }

    def extract_code_structure(self, code: str, language: str) -> Dict:
        """Extract structural information from code"""
        import re
        structure = {
            "functions": [],
            "classes": [],
            "imports": [],
            "docstrings": []
        }

        patterns = self.LANGUAGE_PATTERNS.get(language, {})

        for struct_type, pattern in patterns.items():
            matches = re.findall(pattern, code, re.MULTILINE)
            structure[struct_type + "s" if not struct_type.endswith("s") else struct_type] = matches

        return structure

    def validate_syntax(self, code: str, language: str) -> bool:
        """Validate code syntax"""
        if language == "python":
            try:
                compile(code, "<string>", "exec")
                return True
            except SyntaxError:
                return False
        # Add other language validators
        return True

    def detect_security_issues(self, code: str) -> List[Dict]:
        """Detect potential security issues in code"""
        import re
        issues = []

        # Common security patterns
        patterns = {
            "hardcoded_secret": r"(?:password|secret|api_key|token)\s*=\s*['\"][^'\"]+['\"]",
            "sql_injection": r"(?:execute|query)\s*\([^)]*%s|f['\"].*\{.*\}.*(?:SELECT|INSERT|UPDATE|DELETE)",
            "command_injection": r"os\.system\s*\(|subprocess\.call\s*\(",
            "eval_usage": r"\beval\s*\(",
        }

        for issue_type, pattern in patterns.items():
            if re.search(pattern, code, re.IGNORECASE):
                issues.append({
                    "type": issue_type,
                    "severity": "high" if "injection" in issue_type else "medium"
                })

        return issues

# Code training considerations
CODE_TRAINING_CONSIDERATIONS = {
    "data_quality": [
        "Filter for syntactically valid code",
        "Remove auto-generated code",
        "Filter by repository quality (stars, forks)",
        "Include tests for better understanding"
    ],
    "tokenization": [
        "Consider code-specific tokenizer",
        "Preserve indentation",
        "Handle long files appropriately"
    ],
    "evaluation": [
        "HumanEval / MBPP for generation",
        "Pass@k metrics",
        "Security scanning",
        "Style consistency"
    ]
}
```

---

## 3.3.3 Domain Data Collection

### Data Collection Strategies

```python
"""
Domain data collection strategies
"""
from dataclasses import dataclass
from typing import List, Dict, Optional
from abc import ABC, abstractmethod

@dataclass
class DataSource:
    """Represents a data source for domain adaptation"""
    name: str
    type: str  # "public", "licensed", "internal", "synthetic"
    size_tokens: int
    quality_score: float  # 0-1
    licensing: str
    update_frequency: str

class DomainDataCollector(ABC):
    """Base class for domain data collection"""

    @abstractmethod
    def collect(self) -> List[Dict]:
        """Collect data from source"""
        pass

    @abstractmethod
    def filter_quality(self, data: List[Dict]) -> List[Dict]:
        """Apply quality filters"""
        pass

class ExpertCurationStrategy:
    """
    Expert curation for high-quality domain data.
    """

    def __init__(
        self,
        domain: str,
        expert_count: int = 3,
        agreement_threshold: float = 0.8
    ):
        self.domain = domain
        self.expert_count = expert_count
        self.agreement_threshold = agreement_threshold

    def create_curation_guidelines(self) -> Dict:
        """Create guidelines for expert curators"""
        return {
            "selection_criteria": [
                "Technical accuracy verified",
                "Representative of domain best practices",
                "Clear and well-structured",
                "Up-to-date information"
            ],
            "rejection_criteria": [
                "Factual errors",
                "Outdated information",
                "Poor quality writing",
                "Potential bias or misinformation"
            ],
            "annotation_schema": {
                "quality": ["excellent", "good", "acceptable", "reject"],
                "topic_relevance": ["core", "related", "peripheral"],
                "difficulty": ["basic", "intermediate", "advanced"]
            }
        }

    def aggregate_expert_ratings(
        self,
        ratings: List[Dict]
    ) -> Dict:
        """Aggregate ratings from multiple experts"""
        import numpy as np

        # Check agreement
        quality_scores = [r["quality_score"] for r in ratings]
        agreement = 1 - np.std(quality_scores) / np.mean(quality_scores)

        if agreement >= self.agreement_threshold:
            return {
                "accepted": np.mean(quality_scores) >= 0.7,
                "final_score": np.mean(quality_scores),
                "agreement": agreement
            }
        else:
            return {
                "accepted": False,
                "needs_adjudication": True,
                "agreement": agreement
            }

class SyntheticDomainDataGenerator:
    """
    Generate synthetic domain-specific data.
    """

    def __init__(
        self,
        domain: str,
        teacher_model: str,
        seed_data: List[Dict]
    ):
        self.domain = domain
        self.teacher_model = teacher_model
        self.seed_data = seed_data

    def generate_variations(
        self,
        example: Dict,
        num_variations: int = 5
    ) -> List[Dict]:
        """Generate variations of a domain example"""
        variations = []

        prompts = [
            f"Rephrase this {self.domain} content while maintaining accuracy: {example['text']}",
            f"Create a similar {self.domain} example covering the same topic: {example['text'][:200]}",
            f"Expand on this {self.domain} concept with additional details: {example['text']}"
        ]

        # Would call teacher model to generate variations
        # Then filter for quality

        return variations

    def generate_qa_pairs(
        self,
        documents: List[str],
        qa_per_doc: int = 3
    ) -> List[Dict]:
        """Generate domain-specific Q&A pairs from documents"""
        qa_pairs = []

        for doc in documents:
            # Generate questions from document
            questions_prompt = f"""
            Based on this {self.domain} document, generate {qa_per_doc} questions
            that a professional would ask. Include questions of varying difficulty.

            Document: {doc[:2000]}

            Questions:
            """

            # Would call model and extract questions
            # Then generate answers grounded in document

        return qa_pairs

# Quality vs quantity tradeoffs
DATA_QUALITY_TRADEOFFS = {
    "quality_priority": {
        "approach": "Expert curation",
        "volume": "10K-100K examples",
        "cost": "High (expert time)",
        "use_case": "Safety-critical domains (medical, legal)"
    },
    "balanced": {
        "approach": "Automated + spot-check",
        "volume": "100K-1M examples",
        "cost": "Medium",
        "use_case": "General domain adaptation"
    },
    "volume_priority": {
        "approach": "Automated filtering",
        "volume": "1M-100M+ examples",
        "cost": "Low (compute only)",
        "use_case": "Pre-training data, general knowledge"
    }
}
```

---

## 3.3.4 Evaluation for Domain Adaptation

### Domain-Specific Evaluation

```python
"""
Domain-specific evaluation framework
"""
from dataclasses import dataclass
from typing import List, Dict, Optional, Callable
import numpy as np

@dataclass
class DomainBenchmark:
    """Domain-specific evaluation benchmark"""
    name: str
    domain: str
    task_type: str  # "qa", "generation", "classification"
    metrics: List[str]
    examples: List[Dict]
    expert_baseline: Optional[float] = None

class DomainEvaluator:
    """
    Comprehensive domain adaptation evaluator.
    """

    def __init__(
        self,
        model,
        tokenizer,
        domain: str,
        benchmarks: List[DomainBenchmark]
    ):
        self.model = model
        self.tokenizer = tokenizer
        self.domain = domain
        self.benchmarks = benchmarks

    def evaluate_all(self) -> Dict:
        """Run all domain evaluations"""
        results = {}

        for benchmark in self.benchmarks:
            results[benchmark.name] = self._evaluate_benchmark(benchmark)

        # Aggregate
        results["domain_score"] = np.mean([
            r["primary_metric"] for r in results.values()
        ])

        return results

    def _evaluate_benchmark(self, benchmark: DomainBenchmark) -> Dict:
        """Evaluate on a single benchmark"""
        if benchmark.task_type == "qa":
            return self._evaluate_qa(benchmark)
        elif benchmark.task_type == "generation":
            return self._evaluate_generation(benchmark)
        elif benchmark.task_type == "classification":
            return self._evaluate_classification(benchmark)
        else:
            raise ValueError(f"Unknown task type: {benchmark.task_type}")

    def _evaluate_qa(self, benchmark: DomainBenchmark) -> Dict:
        """Evaluate question answering"""
        correct = 0
        total = len(benchmark.examples)

        for example in benchmark.examples:
            response = self._generate(example["question"])
            correct += self._check_answer(response, example["answer"])

        return {
            "accuracy": correct / total,
            "primary_metric": correct / total,
            "total": total
        }

    def _evaluate_generation(self, benchmark: DomainBenchmark) -> Dict:
        """Evaluate generation quality"""
        scores = []

        for example in benchmark.examples:
            response = self._generate(example["prompt"])
            score = self._score_generation(
                response,
                example.get("reference"),
                example.get("criteria")
            )
            scores.append(score)

        return {
            "mean_score": np.mean(scores),
            "primary_metric": np.mean(scores),
            "std": np.std(scores)
        }

    def _evaluate_classification(self, benchmark: DomainBenchmark) -> Dict:
        """Evaluate classification accuracy"""
        predictions = []
        labels = []

        for example in benchmark.examples:
            response = self._generate(example["text"])
            pred = self._extract_label(response, example["label_options"])
            predictions.append(pred)
            labels.append(example["label"])

        accuracy = np.mean([p == l for p, l in zip(predictions, labels)])

        return {
            "accuracy": accuracy,
            "primary_metric": accuracy
        }

    def _generate(self, prompt: str) -> str:
        """Generate response from model"""
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        outputs = self.model.generate(**inputs, max_new_tokens=256)
        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)

    def _check_answer(self, response: str, gold: str) -> int:
        """Check if response matches gold answer"""
        # Simplified - would use more sophisticated matching
        return int(gold.lower() in response.lower())

    def _score_generation(
        self,
        response: str,
        reference: Optional[str],
        criteria: Optional[List[str]]
    ) -> float:
        """Score generated response"""
        # Would implement rubric-based or LLM-as-judge scoring
        return 0.5

    def _extract_label(
        self,
        response: str,
        options: List[str]
    ) -> str:
        """Extract classification label from response"""
        for option in options:
            if option.lower() in response.lower():
                return option
        return options[0]  # Default

class ForgettingEvaluator:
    """
    Evaluate catastrophic forgetting on general capabilities.
    """

    GENERAL_BENCHMARKS = [
        "mmlu",
        "hellaswag",
        "arc_challenge",
        "winogrande"
    ]

    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer

    def evaluate_retention(
        self,
        baseline_scores: Dict[str, float]
    ) -> Dict:
        """
        Compare current performance to baseline on general tasks.

        Args:
            baseline_scores: Pre-adaptation scores

        Returns:
            Retention metrics
        """
        current_scores = self._evaluate_general()

        retention = {}
        for benchmark, baseline in baseline_scores.items():
            current = current_scores.get(benchmark, 0)
            retention[benchmark] = {
                "baseline": baseline,
                "current": current,
                "retention_rate": current / baseline if baseline > 0 else 0,
                "absolute_drop": baseline - current
            }

        # Overall retention
        retention["overall"] = np.mean([
            r["retention_rate"] for r in retention.values()
            if isinstance(r, dict)
        ])

        return retention

    def _evaluate_general(self) -> Dict[str, float]:
        """Evaluate on general benchmarks"""
        # Would run lm-evaluation-harness
        return {}

# Expert evaluation protocol
EXPERT_EVALUATION_PROTOCOL = {
    "setup": {
        "evaluators": "3-5 domain experts",
        "examples": "50-100 representative outputs",
        "blind": True,
        "randomized": True
    },
    "criteria": {
        "accuracy": "Factual correctness (1-5)",
        "relevance": "Addresses the query (1-5)",
        "completeness": "Covers necessary aspects (1-5)",
        "professionalism": "Appropriate tone/format (1-5)",
        "safety": "No harmful content (binary)"
    },
    "aggregation": {
        "method": "Average with outlier removal",
        "agreement_metric": "Krippendorff's alpha",
        "minimum_agreement": 0.7
    }
}
```

---

## Troubleshooting

### Common Domain Adaptation Issues

| Issue | Symptoms | Solution |
|-------|----------|----------|
| Catastrophic forgetting | General task performance drops | Add data replay, reduce LR |
| Insufficient adaptation | Domain performance doesn't improve | Increase data, consider CPT |
| Terminology errors | Misuses domain terms | Add more terminology examples |
| Format issues | Wrong citation/formatting style | Add explicit format examples |
| Hallucination | Fabricates domain facts | Reduce temperature, add RAG |

### Debugging Checklist

```markdown
## Domain Adaptation Debugging

### Data Quality
- [ ] Domain data is representative
- [ ] Terminology coverage is adequate
- [ ] Data is up-to-date
- [ ] Quality filtering applied

### Training Configuration
- [ ] Learning rate appropriate for CPT (lower than pre-training)
- [ ] Replay data mixed correctly
- [ ] Early layers frozen if appropriate
- [ ] Validation includes domain and general tasks

### Evaluation
- [ ] Domain benchmarks are relevant
- [ ] General capability tracked
- [ ] Expert review conducted
- [ ] A/B testing with baseline
```

---

## Glossary

| Term | Definition |
|------|------------|
| **CPT** | Continued Pre-Training on domain data |
| **Domain Shift** | Difference between general and domain distributions |
| **Catastrophic Forgetting** | Loss of general knowledge during adaptation |
| **Data Replay** | Mixing general data during domain training |
| **HIPAA** | Health Insurance Portability and Accountability Act |
| **Safe Harbor** | HIPAA de-identification method |
| **Vocabulary Extension** | Adding domain terms to tokenizer |

---

## References

1. Gururangan, S., et al. (2020). "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks"
2. Ke, Z., et al. (2024). "Continual Pre-training of Language Models: A Comprehensive Survey"
3. Xie, S., et al. (2024). "Domain-Adaptive Pre-Training for Medical NLP"
4. Chen, M., et al. (2021). "Evaluating Large Language Models Trained on Code"
5. Singhal, K., et al. (2023). "Large Language Models Encode Clinical Knowledge"
6. Wu, S., et al. (2023). "BloombergGPT: A Large Language Model for Finance"
7. Azerbayev, Z., et al. (2023). "Llemma: An Open Language Model for Mathematics"

---

> **Navigation**
> [← 3.2 PEFT](3.2_parameter_efficient_fine_tuning.md) | **[Index](../README.md#15-repository-structure)** | [3.4 Continued Pre-training →](3.4_continued_pretraining.md)
