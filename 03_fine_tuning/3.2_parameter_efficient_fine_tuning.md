> **Navigation** | [← 3.1 SFT](3.1_supervised_fine_tuning.md) | [3.3 Domain Adaptation →](3.3_domain_adaptation.md)
>
> | | |
> |---|---|
> | **Prerequisites** | [3.1 Supervised Fine-Tuning](3.1_supervised_fine_tuning.md) |
> | **Related** | [3.3 Domain Adaptation](3.3_domain_adaptation.md) &#124; [6.1 Quantization](../06_model_optimization/6.1_quantization_guide.md) &#124; [9.1 Inference Engines](../09_inference_serving/9.1_inference_engine_selection_guide.md) |
> | **Next** | [3.3 Domain Adaptation](3.3_domain_adaptation.md) |

# Parameter-Efficient Fine-Tuning (PEFT) Guide

## Executive Summary

Parameter-Efficient Fine-Tuning (PEFT) methods enable adapting large language models with a fraction of the parameters and memory required for full fine-tuning. This guide covers LoRA, QLoRA, and other PEFT techniques, with detailed implementation guidance and best practices.

## Prerequisites

- Understanding of SFT fundamentals (Document 3.1)
- PyTorch and HuggingFace Transformers experience
- Basic linear algebra knowledge
- Access to GPU infrastructure

---

## 3.2.1 PEFT Method Overview

### Method Comparison

| Method | Trainable Params | Memory Savings | Quality | Speed |
|--------|------------------|----------------|---------|-------|
| Full Fine-Tuning | 100% | None | Best | Slowest |
| LoRA | 0.1-1% | ~70% | Near full FT | Fast |
| QLoRA | 0.1-1% | ~90% | Near full FT | Medium |
| DoRA | 0.1-1% | ~70% | Improved over LoRA | Fast |
| AdaLoRA | Variable | ~70% | Adaptive | Medium |
| Prefix Tuning | <0.1% | ~95% | Good for specific tasks | Fastest |
| Prompt Tuning | <0.01% | ~99% | Limited | Fastest |
| IA³ | <0.01% | ~99% | Good for few-shot | Fastest |

### PEFT Architecture Overview

```python
"""
PEFT methods overview and implementation
"""
import torch
import torch.nn as nn
from typing import Optional, List, Dict, Tuple
from dataclasses import dataclass
from enum import Enum

class PEFTMethod(Enum):
    """Available PEFT methods"""
    LORA = "lora"
    QLORA = "qlora"
    DORA = "dora"
    ADALORA = "adalora"
    PREFIX_TUNING = "prefix_tuning"
    PROMPT_TUNING = "prompt_tuning"
    IA3 = "ia3"

@dataclass
class PEFTConfig:
    """Base configuration for PEFT methods"""
    method: PEFTMethod
    target_modules: List[str]
    modules_to_save: Optional[List[str]] = None
    inference_mode: bool = False

def count_trainable_params(model: nn.Module) -> Tuple[int, int, float]:
    """
    Count trainable and total parameters.

    Returns:
        Tuple of (trainable_params, total_params, percentage)
    """
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total = sum(p.numel() for p in model.parameters())
    percentage = 100 * trainable / total

    return trainable, total, percentage

# Module selection helpers
COMMON_TARGET_MODULES = {
    "llama": ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    "llama_attention_only": ["q_proj", "v_proj", "k_proj", "o_proj"],
    "llama_qv_only": ["q_proj", "v_proj"],
    "mistral": ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    "gpt2": ["c_attn", "c_proj", "c_fc"],
    "falcon": ["query_key_value", "dense", "dense_h_to_4h", "dense_4h_to_h"],
    "bloom": ["query_key_value", "dense", "dense_h_to_4h", "dense_4h_to_h"],
}

def get_target_modules(model_type: str, all_linear: bool = False) -> List[str]:
    """
    Get target modules for a model type.

    Args:
        model_type: Type of model (llama, mistral, etc.)
        all_linear: Whether to target all linear layers

    Returns:
        List of module names to target
    """
    if all_linear:
        # Return all linear module patterns
        return COMMON_TARGET_MODULES.get(model_type, ["q_proj", "v_proj"])

    # Default to attention-only for efficiency
    return COMMON_TARGET_MODULES.get(f"{model_type}_attention_only",
                                      COMMON_TARGET_MODULES.get(model_type, ["q_proj", "v_proj"]))
```

---

## 3.2.2 LoRA Deep Dive

### LoRA Theory and Implementation

```python
"""
LoRA (Low-Rank Adaptation) implementation
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional, Dict
from dataclasses import dataclass

@dataclass
class LoRAConfig:
    """Configuration for LoRA"""
    r: int = 8                      # Rank of decomposition
    lora_alpha: int = 16            # Scaling factor
    lora_dropout: float = 0.05      # Dropout probability
    target_modules: List[str] = None
    bias: str = "none"              # "none", "all", "lora_only"
    task_type: str = "CAUSAL_LM"
    fan_in_fan_out: bool = False    # For Conv1D layers
    init_lora_weights: bool = True

class LoRALinear(nn.Module):
    """
    LoRA layer implementation.

    Adds low-rank decomposition BA to pre-trained weight W:
    W' = W + BA where B ∈ R^(d×r), A ∈ R^(r×k)

    This keeps the original weights frozen and only trains A and B.
    """

    def __init__(
        self,
        in_features: int,
        out_features: int,
        r: int = 8,
        lora_alpha: int = 16,
        lora_dropout: float = 0.0,
        fan_in_fan_out: bool = False,
        merge_weights: bool = False
    ):
        super().__init__()

        self.in_features = in_features
        self.out_features = out_features
        self.r = r
        self.lora_alpha = lora_alpha
        self.fan_in_fan_out = fan_in_fan_out
        self.merge_weights = merge_weights

        # Original frozen weights
        self.weight = nn.Parameter(torch.empty(out_features, in_features))
        self.bias = nn.Parameter(torch.zeros(out_features))

        # LoRA parameters
        if r > 0:
            # A is initialized with Kaiming uniform
            self.lora_A = nn.Parameter(torch.empty(r, in_features))
            # B is initialized to zero (so initial output = original)
            self.lora_B = nn.Parameter(torch.empty(out_features, r))
            # Scaling factor
            self.scaling = lora_alpha / r
            # Dropout
            self.lora_dropout = nn.Dropout(lora_dropout) if lora_dropout > 0 else nn.Identity()
        else:
            self.lora_A = None
            self.lora_B = None

        self.merged = False
        self.reset_parameters()

    def reset_parameters(self):
        """Initialize parameters"""
        # Initialize original weights (would be loaded from pretrained)
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))

        if self.r > 0:
            # Initialize A with Kaiming
            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))
            # Initialize B with zeros (crucial for starting from pretrained)
            nn.init.zeros_(self.lora_B)

    def merge_lora_weights(self):
        """Merge LoRA weights into main weights for efficient inference"""
        if not self.merged and self.r > 0:
            # W' = W + scaling * B @ A
            self.weight.data += self.scaling * (self.lora_B @ self.lora_A)
            self.merged = True

    def unmerge_lora_weights(self):
        """Unmerge LoRA weights for continued training"""
        if self.merged and self.r > 0:
            self.weight.data -= self.scaling * (self.lora_B @ self.lora_A)
            self.merged = False

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass with LoRA adaptation.

        Args:
            x: Input tensor [batch, seq_len, in_features]

        Returns:
            Output tensor [batch, seq_len, out_features]
        """
        if self.r > 0 and not self.merged:
            # Original linear transformation
            result = F.linear(x, self.weight, self.bias)

            # Add LoRA adaptation
            # x @ A^T @ B^T * scaling
            lora_out = self.lora_dropout(x)
            lora_out = F.linear(lora_out, self.lora_A)  # [batch, seq, r]
            lora_out = F.linear(lora_out, self.lora_B)  # [batch, seq, out]
            result = result + lora_out * self.scaling

            return result
        else:
            # Use merged weights
            return F.linear(x, self.weight, self.bias)

class LoRAModel(nn.Module):
    """
    Wrapper to add LoRA to an existing model.
    """

    def __init__(
        self,
        base_model: nn.Module,
        config: LoRAConfig
    ):
        super().__init__()
        self.base_model = base_model
        self.config = config
        self.lora_layers: Dict[str, LoRALinear] = {}

        # Freeze base model
        for param in self.base_model.parameters():
            param.requires_grad = False

        # Add LoRA to target modules
        self._add_lora_layers()

    def _add_lora_layers(self):
        """Add LoRA layers to target modules"""
        for name, module in self.base_model.named_modules():
            if self._should_add_lora(name, module):
                # Replace linear layer with LoRA version
                lora_layer = self._create_lora_layer(module)
                self._replace_module(name, lora_layer)
                self.lora_layers[name] = lora_layer

    def _should_add_lora(self, name: str, module: nn.Module) -> bool:
        """Check if module should have LoRA added"""
        if not isinstance(module, nn.Linear):
            return False

        for target in self.config.target_modules:
            if target in name:
                return True
        return False

    def _create_lora_layer(self, linear: nn.Linear) -> LoRALinear:
        """Create LoRA layer from existing linear layer"""
        lora_layer = LoRALinear(
            in_features=linear.in_features,
            out_features=linear.out_features,
            r=self.config.r,
            lora_alpha=self.config.lora_alpha,
            lora_dropout=self.config.lora_dropout
        )

        # Copy original weights
        lora_layer.weight.data = linear.weight.data.clone()
        if linear.bias is not None:
            lora_layer.bias.data = linear.bias.data.clone()

        # Freeze original weights
        lora_layer.weight.requires_grad = False
        lora_layer.bias.requires_grad = False

        return lora_layer

    def _replace_module(self, name: str, new_module: nn.Module):
        """Replace a module in the model hierarchy"""
        parts = name.split('.')
        parent = self.base_model

        for part in parts[:-1]:
            parent = getattr(parent, part)

        setattr(parent, parts[-1], new_module)

    def merge_and_unload(self) -> nn.Module:
        """Merge LoRA weights and return base model"""
        for layer in self.lora_layers.values():
            layer.merge_lora_weights()

        return self.base_model

    def get_trainable_parameters(self) -> List[nn.Parameter]:
        """Get only the LoRA parameters for training"""
        params = []
        for layer in self.lora_layers.values():
            params.extend([layer.lora_A, layer.lora_B])

        if self.config.bias == "all":
            for name, param in self.base_model.named_parameters():
                if "bias" in name:
                    param.requires_grad = True
                    params.append(param)

        return params

# Rank selection guidelines
RANK_GUIDELINES = {
    "minimal_adaptation": {
        "r": 4,
        "description": "Very light adaptation, <0.05% params",
        "use_case": "Style transfer, minor behavior changes"
    },
    "light_adaptation": {
        "r": 8,
        "description": "Light adaptation, ~0.1% params",
        "use_case": "Task-specific fine-tuning"
    },
    "moderate_adaptation": {
        "r": 16,
        "description": "Moderate adaptation, ~0.2% params",
        "use_case": "General instruction tuning"
    },
    "substantial_adaptation": {
        "r": 32,
        "description": "Substantial adaptation, ~0.4% params",
        "use_case": "Domain adaptation, significant behavior changes"
    },
    "heavy_adaptation": {
        "r": 64,
        "description": "Heavy adaptation, ~0.8% params",
        "use_case": "Complex domain shift, approaching full FT quality"
    }
}
```

### Using PEFT Library

```python
"""
Using HuggingFace PEFT library
"""
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    PeftModel,
    TaskType
)
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

def create_lora_model(
    model_name: str,
    r: int = 8,
    lora_alpha: int = 16,
    lora_dropout: float = 0.05,
    target_modules: List[str] = None
) -> Tuple[PeftModel, AutoTokenizer]:
    """
    Create a LoRA model using PEFT library.

    Args:
        model_name: HuggingFace model name
        r: LoRA rank
        lora_alpha: LoRA alpha (scaling)
        lora_dropout: Dropout rate
        target_modules: Modules to apply LoRA to

    Returns:
        Tuple of (PEFT model, tokenizer)
    """
    # Load base model
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # Default target modules for Llama-style models
    if target_modules is None:
        target_modules = ["q_proj", "v_proj", "k_proj", "o_proj"]

    # Create LoRA config
    lora_config = LoraConfig(
        r=r,
        lora_alpha=lora_alpha,
        lora_dropout=lora_dropout,
        target_modules=target_modules,
        bias="none",
        task_type=TaskType.CAUSAL_LM,
    )

    # Apply LoRA
    model = get_peft_model(model, lora_config)

    # Print trainable parameters
    trainable, total, percentage = count_trainable_params(model)
    print(f"Trainable: {trainable:,} / {total:,} ({percentage:.2f}%)")

    return model, tokenizer

def save_lora_adapter(model: PeftModel, output_dir: str):
    """Save only the LoRA adapter weights"""
    model.save_pretrained(output_dir)

def load_lora_adapter(
    base_model_name: str,
    adapter_path: str
) -> PeftModel:
    """Load LoRA adapter onto base model"""
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )

    model = PeftModel.from_pretrained(base_model, adapter_path)
    return model

def merge_lora_weights(model: PeftModel) -> AutoModelForCausalLM:
    """Merge LoRA weights into base model"""
    merged_model = model.merge_and_unload()
    return merged_model
```

---

## 3.2.3 QLoRA Implementation

### QLoRA Configuration

```python
"""
QLoRA (Quantized LoRA) implementation
"""
from transformers import BitsAndBytesConfig, AutoModelForCausalLM
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
import torch
from dataclasses import dataclass
from typing import Optional

@dataclass
class QLoRAConfig:
    """Configuration for QLoRA training"""
    # Quantization
    load_in_4bit: bool = True
    bnb_4bit_quant_type: str = "nf4"  # "nf4" or "fp4"
    bnb_4bit_compute_dtype: torch.dtype = torch.bfloat16
    bnb_4bit_use_double_quant: bool = True  # Double quantization

    # LoRA
    r: int = 16
    lora_alpha: int = 32
    lora_dropout: float = 0.05
    target_modules: List[str] = None

    # Training
    gradient_checkpointing: bool = True
    use_paged_adamw: bool = True  # Memory-efficient optimizer

def create_qlora_model(
    model_name: str,
    config: QLoRAConfig
) -> Tuple[PeftModel, AutoTokenizer]:
    """
    Create a QLoRA model for memory-efficient fine-tuning.

    QLoRA enables fine-tuning 70B models on a single 48GB GPU.
    """
    # Quantization config
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=config.load_in_4bit,
        bnb_4bit_quant_type=config.bnb_4bit_quant_type,
        bnb_4bit_compute_dtype=config.bnb_4bit_compute_dtype,
        bnb_4bit_use_double_quant=config.bnb_4bit_use_double_quant
    )

    # Load quantized model
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True
    )

    # Prepare model for k-bit training
    model = prepare_model_for_kbit_training(
        model,
        use_gradient_checkpointing=config.gradient_checkpointing
    )

    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # Default target modules
    target_modules = config.target_modules or [
        "q_proj", "v_proj", "k_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ]

    # LoRA config
    lora_config = LoraConfig(
        r=config.r,
        lora_alpha=config.lora_alpha,
        lora_dropout=config.lora_dropout,
        target_modules=target_modules,
        bias="none",
        task_type="CAUSAL_LM"
    )

    # Apply LoRA
    model = get_peft_model(model, lora_config)

    # Print memory usage
    print(f"GPU Memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB")

    return model, tokenizer

# Memory requirements for QLoRA
QLORA_MEMORY_REQUIREMENTS = {
    "7B": {
        "model_4bit": "~4GB",
        "with_lora_training": "~6GB",
        "recommended_gpu": "RTX 3090/4090 (24GB)",
        "max_batch_size": 4
    },
    "13B": {
        "model_4bit": "~7GB",
        "with_lora_training": "~10GB",
        "recommended_gpu": "RTX 3090/4090 (24GB)",
        "max_batch_size": 2
    },
    "70B": {
        "model_4bit": "~35GB",
        "with_lora_training": "~42GB",
        "recommended_gpu": "A100 80GB / 2x RTX 4090",
        "max_batch_size": 1
    }
}

def estimate_qlora_memory(
    model_params_billions: float,
    r: int = 16,
    batch_size: int = 1,
    seq_length: int = 2048
) -> Dict[str, float]:
    """
    Estimate memory requirements for QLoRA training.

    Args:
        model_params_billions: Model size in billions
        r: LoRA rank
        batch_size: Training batch size
        seq_length: Sequence length

    Returns:
        Memory breakdown in GB
    """
    params = model_params_billions * 1e9

    # 4-bit model weights
    model_memory = params * 0.5 / 1e9  # 4 bits = 0.5 bytes

    # LoRA parameters (bf16)
    # Approximate: 2 * num_layers * hidden_size * r * 2 bytes * 2 matrices
    hidden_size = int((params / 12) ** 0.5)  # Rough estimate
    num_layers = int(params / (12 * hidden_size ** 2))
    lora_params = 2 * num_layers * hidden_size * r * 4  # A and B for q and v
    lora_memory = lora_params * 2 / 1e9  # bf16

    # Optimizer states (for LoRA params only)
    optimizer_memory = lora_params * 4 * 2 / 1e9  # fp32 m and v

    # Activations (with gradient checkpointing)
    activation_memory = batch_size * seq_length * hidden_size * 2 * 2 / 1e9

    total = model_memory + lora_memory + optimizer_memory + activation_memory

    return {
        "model_4bit": model_memory,
        "lora_params": lora_memory,
        "optimizer": optimizer_memory,
        "activations": activation_memory,
        "total": total * 1.2  # 20% buffer
    }
```

### QLoRA Training Script

```python
"""
Complete QLoRA training script
"""
from transformers import TrainingArguments, Trainer
from trl import SFTTrainer
import torch
from typing import Dict, List

def train_qlora(
    model_name: str,
    train_dataset,
    eval_dataset=None,
    output_dir: str = "./qlora_output",
    config: Optional[QLoRAConfig] = None
):
    """
    Train a model using QLoRA.

    Args:
        model_name: Base model name
        train_dataset: Training dataset
        eval_dataset: Optional evaluation dataset
        output_dir: Output directory
        config: QLoRA configuration

    Returns:
        Trained model
    """
    if config is None:
        config = QLoRAConfig()

    # Create model
    model, tokenizer = create_qlora_model(model_name, config)

    # Training arguments
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=3,
        per_device_train_batch_size=4,
        per_device_eval_batch_size=4,
        gradient_accumulation_steps=4,
        learning_rate=2e-4,  # Higher LR for LoRA
        warmup_ratio=0.1,
        lr_scheduler_type="cosine",
        bf16=True,
        logging_steps=10,
        save_strategy="steps",
        save_steps=500,
        eval_strategy="steps" if eval_dataset else "no",
        eval_steps=500 if eval_dataset else None,
        optim="paged_adamw_32bit" if config.use_paged_adamw else "adamw_torch",
        max_grad_norm=0.3,
        group_by_length=True,
        report_to=["wandb"],
    )

    # Create trainer
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        dataset_text_field="text",
        max_seq_length=2048,
        packing=True,  # Pack multiple examples
    )

    # Train
    trainer.train()

    # Save adapter
    model.save_pretrained(f"{output_dir}/adapter")
    tokenizer.save_pretrained(f"{output_dir}/adapter")

    return model, tokenizer
```

---

## 3.2.4 When to Use Each Method

### Decision Matrix

```python
"""
PEFT method selection decision tree
"""
from dataclasses import dataclass
from typing import Optional
from enum import Enum

@dataclass
class ProjectConstraints:
    """Constraints for PEFT method selection"""
    gpu_memory_gb: float
    model_size_billions: float
    training_examples: int
    quality_requirement: str  # "high", "medium", "low"
    inference_latency_ms: Optional[float] = None
    budget_dollars: Optional[float] = None
    domain_shift: str = "moderate"  # "minimal", "moderate", "significant"

def recommend_peft_method(constraints: ProjectConstraints) -> str:
    """
    Recommend PEFT method based on constraints.

    Returns:
        Recommended method with reasoning
    """
    # Full fine-tuning check
    full_ft_memory = constraints.model_size_billions * 16  # Rough estimate for bf16

    if (constraints.gpu_memory_gb > full_ft_memory and
        constraints.quality_requirement == "high" and
        constraints.domain_shift == "significant"):
        return {
            "method": "full_fine_tuning",
            "reason": "Abundant compute and significant domain shift warrant full FT"
        }

    # QLoRA for memory-constrained scenarios
    qlora_memory = constraints.model_size_billions * 1  # 4-bit + LoRA

    if constraints.gpu_memory_gb < qlora_memory * 2:
        return {
            "method": "qlora",
            "reason": "Memory constraints require 4-bit quantization"
        }

    # Prefix/Prompt tuning for very limited scenarios
    if constraints.training_examples < 100:
        return {
            "method": "prompt_tuning",
            "reason": "Very few examples - prompt tuning is most data-efficient"
        }

    # LoRA for general cases
    if constraints.quality_requirement in ["high", "medium"]:
        # Determine rank based on domain shift
        rank_recommendation = {
            "minimal": 8,
            "moderate": 16,
            "significant": 32
        }

        return {
            "method": "lora",
            "recommended_rank": rank_recommendation[constraints.domain_shift],
            "reason": "LoRA provides good quality/efficiency tradeoff"
        }

    # IA³ for few-shot scenarios
    if constraints.training_examples < 500:
        return {
            "method": "ia3",
            "reason": "IA³ works well with limited data"
        }

    return {
        "method": "lora",
        "recommended_rank": 16,
        "reason": "Default recommendation for general use cases"
    }

# Decision flowchart as code
PEFT_DECISION_TREE = """
PEFT Method Selection:

1. Is GPU memory > 4x model size in bf16?
   Yes -> Consider full fine-tuning for best quality
   No  -> Continue to 2

2. Is GPU memory < 2x model size in 4-bit?
   Yes -> Use QLoRA (only option)
   No  -> Continue to 3

3. Is training data < 100 examples?
   Yes -> Use Prompt Tuning or few-shot
   No  -> Continue to 4

4. Is inference latency critical?
   Yes -> Use LoRA (can merge weights)
   No  -> Continue to 5

5. Is the domain shift significant?
   Yes -> Use LoRA r=32-64 or consider full FT
   No  -> Use LoRA r=8-16

6. Need to serve multiple adaptations?
   Yes -> Use LoRA with adapter switching
   No  -> Consider merging weights
"""
```

---

## 3.2.5 Multi-LoRA Strategies

### LoRA Composition and Merging

```python
"""
Multi-LoRA strategies: composition, merging, and serving
"""
import torch
import torch.nn as nn
from typing import Dict, List, Optional
from peft import PeftModel
import numpy as np

class LoRAMerger:
    """
    Merge multiple LoRA adapters using various strategies.
    """

    @staticmethod
    def linear_merge(
        adapters: Dict[str, Dict[str, torch.Tensor]],
        weights: Dict[str, float]
    ) -> Dict[str, torch.Tensor]:
        """
        Linearly combine LoRA adapters.

        merged = sum(weight_i * adapter_i)
        """
        merged = {}

        # Get all parameter names from first adapter
        first_adapter = list(adapters.values())[0]

        for param_name in first_adapter.keys():
            merged_param = None

            for adapter_name, adapter_params in adapters.items():
                weight = weights[adapter_name]
                param = adapter_params[param_name]

                if merged_param is None:
                    merged_param = weight * param.clone()
                else:
                    merged_param += weight * param

            merged[param_name] = merged_param

        return merged

    @staticmethod
    def ties_merge(
        adapters: Dict[str, Dict[str, torch.Tensor]],
        weights: Dict[str, float],
        density: float = 0.5
    ) -> Dict[str, torch.Tensor]:
        """
        TIES (TrIm, Elect Sign & merge) merging strategy.

        Better preserves distinct capabilities from each adapter.
        """
        merged = {}
        first_adapter = list(adapters.values())[0]

        for param_name in first_adapter.keys():
            # Stack all adapter parameters
            params = torch.stack([
                adapters[name][param_name] for name in adapters.keys()
            ])
            adapter_weights = torch.tensor([weights[name] for name in adapters.keys()])

            # Step 1: Trim - keep only top-k% by magnitude
            flat_params = params.view(params.shape[0], -1)
            k = int(density * flat_params.shape[1])

            mask = torch.zeros_like(flat_params)
            for i in range(flat_params.shape[0]):
                topk_indices = flat_params[i].abs().topk(k).indices
                mask[i, topk_indices] = 1

            trimmed = flat_params * mask

            # Step 2: Elect sign - majority vote on sign
            signs = torch.sign(trimmed)
            weighted_signs = (signs * adapter_weights.view(-1, 1)).sum(dim=0)
            elected_sign = torch.sign(weighted_signs)

            # Step 3: Merge - average values with matching sign
            final = torch.zeros(flat_params.shape[1])
            for i in range(flat_params.shape[0]):
                matching = (signs[i] == elected_sign) | (signs[i] == 0)
                final += adapter_weights[i] * trimmed[i] * matching

            merged[param_name] = final.view(first_adapter[param_name].shape)

        return merged

    @staticmethod
    def dare_merge(
        adapters: Dict[str, Dict[str, torch.Tensor]],
        weights: Dict[str, float],
        drop_rate: float = 0.5
    ) -> Dict[str, torch.Tensor]:
        """
        DARE (Drop And REscale) merging for better interference handling.
        """
        merged = {}
        first_adapter = list(adapters.values())[0]

        for param_name in first_adapter.keys():
            merged_param = None

            for adapter_name, adapter_params in adapters.items():
                weight = weights[adapter_name]
                param = adapter_params[param_name].clone()

                # Randomly drop elements
                mask = torch.rand_like(param) > drop_rate
                dropped = param * mask

                # Rescale to maintain expected value
                rescaled = dropped / (1 - drop_rate)

                if merged_param is None:
                    merged_param = weight * rescaled
                else:
                    merged_param += weight * rescaled

            merged[param_name] = merged_param

        return merged

class MultiLoRAServer:
    """
    Serve multiple LoRA adapters efficiently.
    """

    def __init__(
        self,
        base_model: nn.Module,
        adapters: Dict[str, str]  # name -> path
    ):
        self.base_model = base_model
        self.adapters = {}
        self.current_adapter: Optional[str] = None

        # Load all adapters
        for name, path in adapters.items():
            self.adapters[name] = self._load_adapter(path)

    def _load_adapter(self, path: str) -> Dict[str, torch.Tensor]:
        """Load adapter weights from path"""
        return torch.load(f"{path}/adapter_model.bin")

    def switch_adapter(self, adapter_name: str):
        """Switch to a different adapter"""
        if adapter_name not in self.adapters:
            raise ValueError(f"Unknown adapter: {adapter_name}")

        if self.current_adapter:
            # Unload current adapter
            self._unload_adapter(self.current_adapter)

        # Load new adapter
        self._load_adapter_to_model(adapter_name)
        self.current_adapter = adapter_name

    def _load_adapter_to_model(self, adapter_name: str):
        """Load adapter weights into model"""
        adapter_weights = self.adapters[adapter_name]

        for name, param in self.base_model.named_parameters():
            if name in adapter_weights:
                # Add LoRA delta to base weights
                param.data += adapter_weights[name]

    def _unload_adapter(self, adapter_name: str):
        """Remove adapter weights from model"""
        adapter_weights = self.adapters[adapter_name]

        for name, param in self.base_model.named_parameters():
            if name in adapter_weights:
                param.data -= adapter_weights[name]

# Batch inference with multiple LoRAs
class BatchedLoRAInference:
    """
    Efficient batched inference with different LoRA adapters per request.

    Based on S-LoRA / Punica techniques.
    """

    def __init__(
        self,
        base_model: nn.Module,
        adapter_bank: Dict[str, Dict[str, torch.Tensor]]
    ):
        self.base_model = base_model
        self.adapter_bank = adapter_bank

    def batched_forward(
        self,
        input_ids: torch.Tensor,
        adapter_indices: List[str]
    ) -> torch.Tensor:
        """
        Forward pass with different adapters per batch element.

        This is a simplified version - production would use
        custom CUDA kernels for efficiency.
        """
        batch_size = input_ids.shape[0]
        outputs = []

        # Group by adapter for efficiency
        adapter_groups: Dict[str, List[int]] = {}
        for i, adapter in enumerate(adapter_indices):
            if adapter not in adapter_groups:
                adapter_groups[adapter] = []
            adapter_groups[adapter].append(i)

        # Process each adapter group
        for adapter_name, indices in adapter_groups.items():
            # Get inputs for this adapter
            group_inputs = input_ids[indices]

            # Apply adapter (simplified)
            with self._apply_adapter(adapter_name):
                group_outputs = self.base_model(group_inputs)

            # Store outputs with original indices
            for i, idx in enumerate(indices):
                outputs.append((idx, group_outputs[i]))

        # Reorder outputs
        outputs.sort(key=lambda x: x[0])
        return torch.stack([o[1] for o in outputs])

    def _apply_adapter(self, adapter_name: str):
        """Context manager to temporarily apply adapter"""
        # Implementation would modify model weights
        pass
```

---

## 3.2.6 Hyperparameter Tuning

### Optimal Configuration Search

```python
"""
Hyperparameter tuning for PEFT methods
"""
from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple
import optuna
import torch

@dataclass
class PEFTHyperparams:
    """Hyperparameters to tune for PEFT"""
    # LoRA-specific
    r: int
    lora_alpha: int
    lora_dropout: float
    target_modules: List[str]

    # Training
    learning_rate: float
    num_epochs: int
    batch_size: int
    gradient_accumulation_steps: int
    warmup_ratio: float
    weight_decay: float

class PEFTTuner:
    """
    Hyperparameter tuning for PEFT methods using Optuna.
    """

    def __init__(
        self,
        model_name: str,
        train_dataset,
        eval_dataset,
        n_trials: int = 20
    ):
        self.model_name = model_name
        self.train_dataset = train_dataset
        self.eval_dataset = eval_dataset
        self.n_trials = n_trials

    def objective(self, trial: optuna.Trial) -> float:
        """Optuna objective function"""
        # Sample hyperparameters
        r = trial.suggest_categorical("r", [4, 8, 16, 32, 64])
        lora_alpha = trial.suggest_categorical("lora_alpha", [8, 16, 32, 64])
        lora_dropout = trial.suggest_float("lora_dropout", 0.0, 0.2)

        learning_rate = trial.suggest_float("learning_rate", 1e-5, 5e-4, log=True)
        warmup_ratio = trial.suggest_float("warmup_ratio", 0.0, 0.2)
        weight_decay = trial.suggest_float("weight_decay", 0.0, 0.1)

        # Create config
        config = LoraConfig(
            r=r,
            lora_alpha=lora_alpha,
            lora_dropout=lora_dropout,
            target_modules=["q_proj", "v_proj"]
        )

        # Train and evaluate
        try:
            eval_loss = self._train_and_evaluate(
                config, learning_rate, warmup_ratio, weight_decay
            )
            return eval_loss
        except Exception as e:
            print(f"Trial failed: {e}")
            return float('inf')

    def _train_and_evaluate(
        self,
        lora_config: LoraConfig,
        learning_rate: float,
        warmup_ratio: float,
        weight_decay: float
    ) -> float:
        """Train model and return evaluation loss"""
        # Implementation would create and train model
        # Return validation loss
        pass

    def run_tuning(self) -> Dict:
        """Run hyperparameter search"""
        study = optuna.create_study(direction="minimize")
        study.optimize(self.objective, n_trials=self.n_trials)

        return {
            "best_params": study.best_params,
            "best_value": study.best_value,
            "all_trials": study.trials
        }

# Hyperparameter guidelines
LORA_HYPERPARAMETER_GUIDELINES = {
    "rank": {
        "start": 8,
        "range": [4, 8, 16, 32, 64],
        "guidance": "Higher rank = more capacity, more compute. Start with 8-16."
    },
    "alpha": {
        "start": 16,
        "range": [8, 16, 32, 64],
        "guidance": "Often set to 2x rank. Higher = stronger adaptation."
    },
    "dropout": {
        "start": 0.05,
        "range": [0.0, 0.1, 0.2],
        "guidance": "Use 0.05-0.1 for regularization. 0 if data is limited."
    },
    "learning_rate": {
        "start": 2e-4,
        "range": [1e-5, 1e-4, 2e-4, 5e-4],
        "guidance": "LoRA typically uses higher LR than full FT. Start with 2e-4."
    },
    "target_modules": {
        "minimal": ["q_proj", "v_proj"],
        "recommended": ["q_proj", "v_proj", "k_proj", "o_proj"],
        "maximal": ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        "guidance": "More modules = more capacity. Start with attention projections."
    }
}

# Scaling learning rate with rank
def scale_learning_rate(base_lr: float, rank: int, base_rank: int = 8) -> float:
    """
    Scale learning rate based on LoRA rank.

    Higher rank may benefit from lower learning rate.
    """
    # Empirical scaling: sqrt relationship
    return base_lr * (base_rank / rank) ** 0.5

# Example tuned configurations by use case
TUNED_CONFIGURATIONS = {
    "instruction_tuning": {
        "r": 16,
        "lora_alpha": 32,
        "lora_dropout": 0.05,
        "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"],
        "learning_rate": 2e-4,
        "num_epochs": 3,
        "warmup_ratio": 0.1
    },
    "domain_adaptation": {
        "r": 32,
        "lora_alpha": 64,
        "lora_dropout": 0.1,
        "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        "learning_rate": 1e-4,
        "num_epochs": 5,
        "warmup_ratio": 0.05
    },
    "style_transfer": {
        "r": 8,
        "lora_alpha": 16,
        "lora_dropout": 0.0,
        "target_modules": ["q_proj", "v_proj"],
        "learning_rate": 3e-4,
        "num_epochs": 2,
        "warmup_ratio": 0.1
    }
}
```

---

## Troubleshooting

### Common PEFT Issues

| Issue | Symptoms | Solution |
|-------|----------|----------|
| Underfitting | High loss, poor quality | Increase rank, add more target modules |
| Overfitting | Train loss low, eval high | Add dropout, reduce epochs |
| Training instability | Loss spikes | Lower learning rate, increase warmup |
| No improvement | Loss doesn't decrease | Check target modules, verify data loading |
| OOM errors | CUDA out of memory | Use QLoRA, reduce batch size |
| Slow training | Low GPU utilization | Enable gradient checkpointing, use packing |

### Debugging Checklist

```markdown
## PEFT Debugging Checklist

### Configuration
- [ ] Target modules correctly specified
- [ ] Rank appropriate for task complexity
- [ ] Alpha scaling factor set (typically 2x rank)
- [ ] Dropout enabled for regularization

### Training
- [ ] Learning rate in appropriate range (1e-5 to 5e-4)
- [ ] Base model weights are frozen
- [ ] Only LoRA parameters are trainable
- [ ] Gradient accumulation correct

### Data
- [ ] Data format matches chat template
- [ ] Padding correctly handled
- [ ] Labels masked appropriately
- [ ] Batch size fits in memory

### Evaluation
- [ ] Evaluating on held-out data
- [ ] Generation config appropriate
- [ ] Comparing against baseline
```

---

## Glossary

| Term | Definition |
|------|------------|
| **LoRA** | Low-Rank Adaptation - adds trainable low-rank matrices |
| **QLoRA** | Quantized LoRA with 4-bit base model |
| **Rank (r)** | Dimension of the low-rank decomposition |
| **Alpha** | Scaling factor applied to LoRA output |
| **NF4** | 4-bit NormalFloat quantization format |
| **Double Quantization** | Quantizing the quantization constants |
| **Paged Optimizers** | Memory-efficient optimizers using CPU offloading |
| **TIES Merge** | Trim, Elect Sign & merge strategy for combining adapters |

---

## References

1. Hu, E., et al. (2021). "LoRA: Low-Rank Adaptation of Large Language Models"
2. Dettmers, T., et al. (2023). "QLoRA: Efficient Finetuning of Quantized LLMs"
3. Liu, S., et al. (2024). "DoRA: Weight-Decomposed Low-Rank Adaptation"
4. Zhang, Q., et al. (2023). "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning"
5. Sheng, Y., et al. (2023). "S-LoRA: Serving Thousands of Concurrent LoRA Adapters"
6. HuggingFace. (2024). "PEFT: Parameter-Efficient Fine-Tuning"
7. Yadav, P., et al. (2023). "TIES-Merging: Resolving Interference When Merging Models"

---

> **Navigation**
> [← 3.1 SFT](3.1_supervised_fine_tuning.md) | **[Index](../README.md#15-repository-structure)** | [3.3 Domain Adaptation →](3.3_domain_adaptation.md)
