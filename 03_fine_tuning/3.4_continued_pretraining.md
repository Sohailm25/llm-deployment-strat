> **Navigation** | [← 3.3 Domain Adaptation](3.3_domain_adaptation.md) | [4.1 RLHF →](../04_alignment_safety/4.1_rlhf_guide.md)
>
> | | |
> |---|---|
> | **Prerequisites** | [2.3 Distributed Training](../02_model_training/2.3_distributed_training_infrastructure.md) &#124; [2.1 Tokenizer](../02_model_training/2.1_tokenizer_training_selection.md) &#124; [2.4 Data Mix](../02_model_training/2.4_pretraining_data_mix_curriculum.md) |
> | **Related** | [3.3 Domain Adaptation](3.3_domain_adaptation.md) &#124; [2.5 Training Monitoring](../02_model_training/2.5_training_monitoring_debugging.md) |
> | **Next** | [4.1 RLHF](../04_alignment_safety/4.1_rlhf_guide.md) |

# 3.4 Continued Pre-training Guide

## Executive Summary

Continued pre-training (CPT) extends a base model's knowledge by additional training on domain-specific or temporally updated corpora. Unlike fine-tuning which adapts model behavior, CPT expands the model's fundamental knowledge base while preserving general capabilities. This guide covers when to use CPT, data preparation strategies, training configurations, and techniques to prevent catastrophic forgetting.

## Prerequisites

- Pre-trained base model (Llama, Mistral, etc.)
- Domain-specific corpus (minimum 1B+ tokens recommended)
- Distributed training infrastructure (multiple GPUs)
- Familiarity with transformer training (document 2.3)
- Understanding of tokenization (document 2.1)

## 3.4.1 When to Continue Pre-training

### Decision Framework

```python
"""
Continued Pre-training Decision Framework
"""
from dataclasses import dataclass
from enum import Enum
from typing import List, Optional

class AdaptationMethod(Enum):
    CONTINUED_PRETRAINING = "cpt"
    SUPERVISED_FINETUNING = "sft"
    RETRIEVAL_AUGMENTATION = "rag"
    PROMPT_ENGINEERING = "prompt"

@dataclass
class DomainGap:
    """Quantifies gap between base model and target domain."""
    vocabulary_overlap: float  # 0-1, higher = more overlap
    perplexity_ratio: float    # domain_ppl / general_ppl
    task_accuracy_gap: float   # gap between base and domain expert
    data_volume: int           # available domain tokens

    def recommend_method(self) -> AdaptationMethod:
        """Recommend adaptation method based on domain gap analysis."""
        # Large vocabulary gap -> CPT needed for new terminology
        if self.vocabulary_overlap < 0.7:
            return AdaptationMethod.CONTINUED_PRETRAINING

        # High perplexity ratio -> model struggles with domain text
        if self.perplexity_ratio > 2.0:
            if self.data_volume > 1_000_000_000:  # 1B+ tokens
                return AdaptationMethod.CONTINUED_PRETRAINING
            else:
                return AdaptationMethod.SUPERVISED_FINETUNING

        # Moderate gap with limited data -> SFT or RAG
        if self.task_accuracy_gap > 0.2:
            if self.data_volume > 100_000_000:  # 100M+ tokens
                return AdaptationMethod.SUPERVISED_FINETUNING
            else:
                return AdaptationMethod.RETRIEVAL_AUGMENTATION

        # Small gap -> prompt engineering may suffice
        return AdaptationMethod.PROMPT_ENGINEERING


def analyze_domain_gap(
    model,
    tokenizer,
    domain_corpus: List[str],
    general_corpus: List[str]
) -> DomainGap:
    """Analyze gap between model capabilities and domain requirements."""
    import torch
    from collections import Counter

    # Vocabulary overlap analysis
    domain_tokens = Counter()
    general_tokens = Counter()

    for text in domain_corpus[:10000]:
        tokens = tokenizer.encode(text)
        domain_tokens.update(tokens)

    for text in general_corpus[:10000]:
        tokens = tokenizer.encode(text)
        general_tokens.update(tokens)

    domain_vocab = set(domain_tokens.keys())
    general_vocab = set(general_tokens.keys())
    overlap = len(domain_vocab & general_vocab) / len(domain_vocab)

    # Perplexity comparison
    model.eval()

    def compute_perplexity(texts: List[str], max_samples: int = 1000) -> float:
        total_loss = 0
        total_tokens = 0

        for text in texts[:max_samples]:
            inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
            with torch.no_grad():
                outputs = model(**inputs, labels=inputs["input_ids"])
                total_loss += outputs.loss.item() * inputs["input_ids"].size(1)
                total_tokens += inputs["input_ids"].size(1)

        return torch.exp(torch.tensor(total_loss / total_tokens)).item()

    domain_ppl = compute_perplexity(domain_corpus)
    general_ppl = compute_perplexity(general_corpus)
    ppl_ratio = domain_ppl / general_ppl

    return DomainGap(
        vocabulary_overlap=overlap,
        perplexity_ratio=ppl_ratio,
        task_accuracy_gap=0.0,  # Requires downstream task evaluation
        data_volume=sum(len(tokenizer.encode(t)) for t in domain_corpus)
    )
```

### CPT Use Cases

| Scenario | Signal | Recommended Data Volume |
|----------|--------|------------------------|
| Significant domain shift | Perplexity ratio > 2x | 10B+ tokens |
| New knowledge integration | Knowledge cutoff issues | 1-10B tokens |
| Temporal updates | Outdated information | 1-5B tokens |
| Language extension | Low vocabulary overlap | 5-20B tokens |
| Specialized terminology | High OOV rate | 1-5B tokens |

### When NOT to Use CPT

```python
def should_skip_cpt(analysis: DomainGap, constraints: dict) -> tuple[bool, str]:
    """Determine if CPT should be skipped."""

    # Insufficient data
    if analysis.data_volume < 100_000_000:  # < 100M tokens
        return True, "Insufficient data volume. Consider SFT or RAG instead."

    # Budget constraints
    if constraints.get("max_gpu_hours", float("inf")) < 100:
        return True, "CPT requires significant compute. Consider PEFT methods."

    # Small domain gap
    if analysis.perplexity_ratio < 1.3 and analysis.vocabulary_overlap > 0.9:
        return True, "Domain gap is small. SFT or prompt engineering may suffice."

    # Real-time knowledge needs
    if constraints.get("knowledge_freshness_hours", float("inf")) < 24:
        return True, "Frequent updates needed. Use RAG for dynamic knowledge."

    return False, "CPT is recommended."
```

## 3.4.2 Data Preparation

### Domain Corpus Creation

```python
"""
Domain Corpus Preparation Pipeline
"""
import hashlib
import re
from dataclasses import dataclass
from pathlib import Path
from typing import Iterator, Optional
import json

@dataclass
class DomainDocument:
    """Represents a document in the domain corpus."""
    text: str
    source: str
    domain: str
    quality_score: float
    timestamp: Optional[str] = None
    metadata: Optional[dict] = None

    @property
    def hash(self) -> str:
        return hashlib.sha256(self.text.encode()).hexdigest()[:16]


class DomainCorpusBuilder:
    """Build and manage domain-specific corpus for CPT."""

    def __init__(
        self,
        output_dir: Path,
        min_length: int = 100,
        max_length: int = 100000,
        dedup_threshold: float = 0.8
    ):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.min_length = min_length
        self.max_length = max_length
        self.dedup_threshold = dedup_threshold
        self.seen_hashes = set()

    def process_document(self, doc: DomainDocument) -> Optional[DomainDocument]:
        """Process and validate a single document."""
        # Length filtering
        if len(doc.text) < self.min_length:
            return None
        if len(doc.text) > self.max_length:
            # Chunk long documents
            return self._chunk_document(doc)

        # Deduplication
        if doc.hash in self.seen_hashes:
            return None
        self.seen_hashes.add(doc.hash)

        # Quality filtering
        if doc.quality_score < 0.5:
            return None

        # Domain-specific cleaning
        doc.text = self._clean_text(doc.text, doc.domain)

        return doc

    def _clean_text(self, text: str, domain: str) -> str:
        """Apply domain-specific text cleaning."""
        # Remove excessive whitespace
        text = re.sub(r'\s+', ' ', text)

        # Domain-specific rules
        if domain == "legal":
            # Preserve legal citations
            text = self._normalize_legal_citations(text)
        elif domain == "medical":
            # Standardize medical abbreviations
            text = self._normalize_medical_terms(text)
        elif domain == "code":
            # Preserve code formatting
            text = self._preserve_code_blocks(text)

        return text.strip()

    def _normalize_legal_citations(self, text: str) -> str:
        """Normalize legal citation formats."""
        # Standardize case citations
        text = re.sub(r'(\d+)\s+U\.?\s*S\.?\s+(\d+)', r'\1 U.S. \2', text)
        text = re.sub(r'(\d+)\s+F\.?\s*(\d+)d?\s+(\d+)', r'\1 F.\2d \3', text)
        return text

    def _normalize_medical_terms(self, text: str) -> str:
        """Standardize medical terminology."""
        abbreviations = {
            r'\bpt\b': 'patient',
            r'\bhx\b': 'history',
            r'\bdx\b': 'diagnosis',
            r'\brx\b': 'prescription',
            r'\btx\b': 'treatment',
        }
        for pattern, replacement in abbreviations.items():
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        return text

    def _preserve_code_blocks(self, text: str) -> str:
        """Preserve code block formatting."""
        # Don't collapse whitespace in code blocks
        return text  # More sophisticated handling in production

    def _chunk_document(self, doc: DomainDocument) -> DomainDocument:
        """Chunk long documents while preserving context."""
        # Simple sentence-based chunking
        chunk_size = self.max_length
        chunks = []
        current_chunk = []
        current_length = 0

        sentences = re.split(r'(?<=[.!?])\s+', doc.text)

        for sentence in sentences:
            if current_length + len(sentence) > chunk_size:
                if current_chunk:
                    chunks.append(' '.join(current_chunk))
                current_chunk = [sentence]
                current_length = len(sentence)
            else:
                current_chunk.append(sentence)
                current_length += len(sentence)

        if current_chunk:
            chunks.append(' '.join(current_chunk))

        # Return first chunk, handle others separately
        doc.text = chunks[0]
        return doc

    def build_corpus(
        self,
        documents: Iterator[DomainDocument],
        output_format: str = "jsonl"
    ) -> dict:
        """Build corpus from document iterator."""
        stats = {
            "total_documents": 0,
            "accepted_documents": 0,
            "total_tokens_estimate": 0,
            "domains": {}
        }

        output_file = self.output_dir / f"corpus.{output_format}"

        with open(output_file, 'w') as f:
            for doc in documents:
                stats["total_documents"] += 1

                processed = self.process_document(doc)
                if processed is None:
                    continue

                stats["accepted_documents"] += 1
                stats["total_tokens_estimate"] += len(processed.text.split()) * 1.3

                domain = processed.domain
                stats["domains"][domain] = stats["domains"].get(domain, 0) + 1

                if output_format == "jsonl":
                    f.write(json.dumps({
                        "text": processed.text,
                        "source": processed.source,
                        "domain": processed.domain,
                        "quality": processed.quality_score,
                        "hash": processed.hash
                    }) + "\n")

        return stats
```

### Replay Data Mixing

```python
"""
Replay Data Mixing for Forgetting Prevention
"""
import random
from dataclasses import dataclass
from typing import List, Iterator, Tuple
import numpy as np

@dataclass
class MixingConfig:
    """Configuration for domain-general data mixing."""
    domain_ratio: float = 0.7      # Proportion of domain data
    general_ratio: float = 0.3    # Proportion of general replay data
    dynamic_mixing: bool = True   # Adjust ratios during training
    warmup_domain_ratio: float = 0.3  # Start with less domain data
    cooldown_domain_ratio: float = 0.5  # End with balanced mix


class ReplayMixer:
    """Mix domain and general data for continued pre-training."""

    def __init__(
        self,
        domain_data: List[str],
        general_data: List[str],
        config: MixingConfig
    ):
        self.domain_data = domain_data
        self.general_data = general_data
        self.config = config

    def get_mixing_ratio(self, progress: float) -> Tuple[float, float]:
        """Get domain/general ratio based on training progress."""
        if not self.config.dynamic_mixing:
            return self.config.domain_ratio, self.config.general_ratio

        # Warmup phase (0-10%): gradually increase domain data
        if progress < 0.1:
            domain_ratio = self.config.warmup_domain_ratio + \
                (self.config.domain_ratio - self.config.warmup_domain_ratio) * (progress / 0.1)
        # Main phase (10-90%): use configured ratio
        elif progress < 0.9:
            domain_ratio = self.config.domain_ratio
        # Cooldown phase (90-100%): balance for stability
        else:
            domain_ratio = self.config.domain_ratio - \
                (self.config.domain_ratio - self.config.cooldown_domain_ratio) * ((progress - 0.9) / 0.1)

        return domain_ratio, 1 - domain_ratio

    def generate_mixed_batch(
        self,
        batch_size: int,
        progress: float
    ) -> List[str]:
        """Generate a batch with mixed domain and general data."""
        domain_ratio, general_ratio = self.get_mixing_ratio(progress)

        n_domain = int(batch_size * domain_ratio)
        n_general = batch_size - n_domain

        batch = []
        batch.extend(random.sample(self.domain_data, min(n_domain, len(self.domain_data))))
        batch.extend(random.sample(self.general_data, min(n_general, len(self.general_data))))

        random.shuffle(batch)
        return batch

    def create_mixed_dataset(
        self,
        total_tokens: int,
        tokens_per_sample: int = 2048
    ) -> Iterator[str]:
        """Create iterator over mixed dataset."""
        total_samples = total_tokens // tokens_per_sample

        for i in range(total_samples):
            progress = i / total_samples
            domain_ratio, general_ratio = self.get_mixing_ratio(progress)

            # Sample from appropriate distribution
            if random.random() < domain_ratio:
                yield random.choice(self.domain_data)
            else:
                yield random.choice(self.general_data)


class CurriculumMixer(ReplayMixer):
    """Advanced mixing with curriculum learning."""

    def __init__(
        self,
        domain_data: List[Tuple[str, float]],  # (text, difficulty)
        general_data: List[str],
        config: MixingConfig
    ):
        self.domain_data_with_difficulty = domain_data
        self.general_data = general_data
        self.config = config

        # Sort by difficulty
        self.domain_data_with_difficulty.sort(key=lambda x: x[1])
        self.domain_data = [x[0] for x in self.domain_data_with_difficulty]

    def get_curriculum_range(self, progress: float) -> Tuple[int, int]:
        """Get data range based on curriculum progress."""
        n_domain = len(self.domain_data)

        # Gradually expand to harder examples
        if progress < 0.3:
            # Easy phase: bottom 50%
            end_idx = int(n_domain * (0.3 + progress))
            return 0, end_idx
        elif progress < 0.7:
            # Medium phase: middle 50-80%
            start_idx = int(n_domain * 0.2)
            end_idx = int(n_domain * (0.6 + (progress - 0.3) * 0.5))
            return start_idx, end_idx
        else:
            # Hard phase: all data with emphasis on harder
            return 0, n_domain

    def generate_curriculum_batch(
        self,
        batch_size: int,
        progress: float
    ) -> List[str]:
        """Generate curriculum-aware batch."""
        domain_ratio, general_ratio = self.get_mixing_ratio(progress)
        start_idx, end_idx = self.get_curriculum_range(progress)

        n_domain = int(batch_size * domain_ratio)
        n_general = batch_size - n_domain

        # Sample from curriculum range
        curriculum_data = self.domain_data[start_idx:end_idx]

        batch = []
        batch.extend(random.sample(curriculum_data, min(n_domain, len(curriculum_data))))
        batch.extend(random.sample(self.general_data, min(n_general, len(self.general_data))))

        random.shuffle(batch)
        return batch
```

### Quality Filtering for Domain Data

```python
"""
Quality Filtering Pipeline for Domain Data
"""
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import List, Optional
import re

@dataclass
class QualityScore:
    """Composite quality score for a document."""
    perplexity_score: float = 0.0
    format_score: float = 0.0
    content_score: float = 0.0
    domain_relevance: float = 0.0

    @property
    def overall(self) -> float:
        weights = [0.3, 0.2, 0.3, 0.2]
        scores = [
            self.perplexity_score,
            self.format_score,
            self.content_score,
            self.domain_relevance
        ]
        return sum(w * s for w, s in zip(weights, scores))


class QualityFilter(ABC):
    """Base class for quality filters."""

    @abstractmethod
    def score(self, text: str) -> float:
        """Return quality score between 0 and 1."""
        pass


class PerplexityFilter(QualityFilter):
    """Filter based on language model perplexity."""

    def __init__(self, model, tokenizer, max_ppl: float = 100.0):
        self.model = model
        self.tokenizer = tokenizer
        self.max_ppl = max_ppl

    def score(self, text: str) -> float:
        import torch

        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            max_length=512
        )

        with torch.no_grad():
            outputs = self.model(**inputs, labels=inputs["input_ids"])
            ppl = torch.exp(outputs.loss).item()

        # Normalize: lower perplexity = higher score
        if ppl > self.max_ppl:
            return 0.0
        return 1.0 - (ppl / self.max_ppl)


class FormatFilter(QualityFilter):
    """Filter based on text formatting quality."""

    def __init__(
        self,
        min_alpha_ratio: float = 0.7,
        max_repetition: float = 0.3,
        min_sentences: int = 2
    ):
        self.min_alpha_ratio = min_alpha_ratio
        self.max_repetition = max_repetition
        self.min_sentences = min_sentences

    def score(self, text: str) -> float:
        scores = []

        # Alphabetic ratio
        alpha_chars = sum(c.isalpha() for c in text)
        alpha_ratio = alpha_chars / max(len(text), 1)
        scores.append(1.0 if alpha_ratio >= self.min_alpha_ratio else alpha_ratio / self.min_alpha_ratio)

        # Repetition check
        words = text.lower().split()
        if words:
            unique_ratio = len(set(words)) / len(words)
            scores.append(unique_ratio)

        # Sentence structure
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        sentence_score = min(len(sentences) / self.min_sentences, 1.0)
        scores.append(sentence_score)

        return sum(scores) / len(scores) if scores else 0.0


class DomainRelevanceFilter(QualityFilter):
    """Filter based on domain relevance."""

    def __init__(self, domain_keywords: List[str], min_density: float = 0.01):
        self.domain_keywords = set(kw.lower() for kw in domain_keywords)
        self.min_density = min_density

    def score(self, text: str) -> float:
        words = text.lower().split()
        if not words:
            return 0.0

        keyword_count = sum(1 for w in words if w in self.domain_keywords)
        density = keyword_count / len(words)

        # Normalize based on minimum density
        return min(density / self.min_density, 1.0)


class QualityPipeline:
    """Combine multiple quality filters."""

    def __init__(self, filters: List[tuple[QualityFilter, float]]):
        """
        Args:
            filters: List of (filter, weight) tuples
        """
        self.filters = filters
        total_weight = sum(w for _, w in filters)
        self.filters = [(f, w/total_weight) for f, w in filters]

    def evaluate(self, text: str) -> QualityScore:
        """Evaluate text quality using all filters."""
        scores = QualityScore()

        for filter_obj, weight in self.filters:
            score = filter_obj.score(text)

            if isinstance(filter_obj, PerplexityFilter):
                scores.perplexity_score = score
            elif isinstance(filter_obj, FormatFilter):
                scores.format_score = score
            elif isinstance(filter_obj, DomainRelevanceFilter):
                scores.domain_relevance = score
            else:
                scores.content_score = score

        return scores

    def filter_corpus(
        self,
        texts: List[str],
        threshold: float = 0.5
    ) -> List[tuple[str, QualityScore]]:
        """Filter corpus based on quality threshold."""
        results = []

        for text in texts:
            score = self.evaluate(text)
            if score.overall >= threshold:
                results.append((text, score))

        return results
```

## 3.4.3 Training Configuration

### Learning Rate Strategy

```python
"""
Learning Rate Configuration for Continued Pre-training
"""
import math
from typing import Optional
import torch
from torch.optim.lr_scheduler import LambdaLR

class CPTLearningRateScheduler:
    """
    Learning rate scheduler optimized for continued pre-training.

    Key principles:
    1. Start with lower LR than initial pre-training (10-30% typically)
    2. Shorter warmup (model already trained)
    3. Gradual decay to very low LR for stability
    """

    def __init__(
        self,
        optimizer: torch.optim.Optimizer,
        base_lr: float,
        min_lr: float,
        warmup_steps: int,
        total_steps: int,
        schedule_type: str = "cosine_with_restarts"
    ):
        self.optimizer = optimizer
        self.base_lr = base_lr
        self.min_lr = min_lr
        self.warmup_steps = warmup_steps
        self.total_steps = total_steps
        self.schedule_type = schedule_type

    def get_lr(self, step: int) -> float:
        """Calculate learning rate for given step."""
        # Warmup phase
        if step < self.warmup_steps:
            return self.base_lr * (step / self.warmup_steps)

        # Post-warmup decay
        progress = (step - self.warmup_steps) / (self.total_steps - self.warmup_steps)

        if self.schedule_type == "cosine":
            lr = self.min_lr + 0.5 * (self.base_lr - self.min_lr) * (1 + math.cos(math.pi * progress))
        elif self.schedule_type == "cosine_with_restarts":
            # Restart every 25% of training
            restart_progress = (progress % 0.25) / 0.25
            lr = self.min_lr + 0.5 * (self.base_lr - self.min_lr) * (1 + math.cos(math.pi * restart_progress))
        elif self.schedule_type == "linear":
            lr = self.base_lr - (self.base_lr - self.min_lr) * progress
        elif self.schedule_type == "inverse_sqrt":
            lr = self.base_lr * math.sqrt(self.warmup_steps / max(step, self.warmup_steps))
            lr = max(lr, self.min_lr)
        else:
            raise ValueError(f"Unknown schedule type: {self.schedule_type}")

        return lr

    def get_scheduler(self) -> LambdaLR:
        """Return PyTorch scheduler."""
        return LambdaLR(self.optimizer, lr_lambda=lambda step: self.get_lr(step) / self.base_lr)


# Recommended configurations by model size
CPT_LR_CONFIGS = {
    "7b": {
        "base_lr": 1e-5,         # Much lower than pre-training (~3e-4)
        "min_lr": 1e-6,
        "warmup_ratio": 0.01,   # Short warmup
    },
    "13b": {
        "base_lr": 8e-6,
        "min_lr": 8e-7,
        "warmup_ratio": 0.01,
    },
    "70b": {
        "base_lr": 5e-6,
        "min_lr": 5e-7,
        "warmup_ratio": 0.01,
    },
}


def get_cpt_config(
    model_size: str,
    total_tokens: int,
    batch_size: int,
    seq_length: int
) -> dict:
    """Get recommended CPT configuration for model size."""
    config = CPT_LR_CONFIGS.get(model_size, CPT_LR_CONFIGS["7b"])

    total_steps = total_tokens // (batch_size * seq_length)
    warmup_steps = int(total_steps * config["warmup_ratio"])

    return {
        "learning_rate": config["base_lr"],
        "min_learning_rate": config["min_lr"],
        "warmup_steps": warmup_steps,
        "total_steps": total_steps,
        "lr_scheduler_type": "cosine",
        "weight_decay": 0.1,
        "adam_beta1": 0.9,
        "adam_beta2": 0.95,
        "adam_epsilon": 1e-8,
        "max_grad_norm": 1.0,
    }
```

### Batch Size and Token Budget

```python
"""
Batch Size and Token Budget Optimization
"""
from dataclasses import dataclass
from typing import Optional
import math

@dataclass
class CPTBudget:
    """Token budget configuration for CPT."""
    total_tokens: int
    domain_tokens: int
    replay_tokens: int
    effective_batch_size: int
    gradient_accumulation_steps: int
    micro_batch_size: int

    @property
    def domain_ratio(self) -> float:
        return self.domain_tokens / self.total_tokens

    @property
    def training_steps(self) -> int:
        return self.total_tokens // (self.effective_batch_size * 2048)  # Assuming 2048 seq length


def calculate_cpt_budget(
    domain_data_tokens: int,
    epochs: int = 1,
    replay_ratio: float = 0.3,
    gpu_count: int = 8,
    gpu_memory_gb: int = 80,
    model_params_b: float = 7.0,
    seq_length: int = 2048
) -> CPTBudget:
    """
    Calculate optimal training budget for CPT.

    Args:
        domain_data_tokens: Total tokens in domain corpus
        epochs: Number of passes over domain data
        replay_ratio: Proportion of general replay data
        gpu_count: Number of GPUs available
        gpu_memory_gb: Memory per GPU
        model_params_b: Model parameters in billions
        seq_length: Sequence length

    Returns:
        CPTBudget configuration
    """
    # Domain tokens for training
    domain_tokens = domain_data_tokens * epochs

    # Replay tokens to maintain general capability
    replay_tokens = int(domain_tokens * replay_ratio / (1 - replay_ratio))

    total_tokens = domain_tokens + replay_tokens

    # Estimate micro batch size based on memory
    # Rule of thumb: ~20-24 bytes per param for training with optimizer states
    bytes_per_param = 22
    model_memory_gb = (model_params_b * 1e9 * bytes_per_param) / 1e9
    available_memory = gpu_memory_gb - model_memory_gb - 5  # 5GB buffer

    # Estimate tokens per GB of activation memory
    tokens_per_gb = 500_000 / model_params_b  # Rough estimate
    max_tokens_per_gpu = int(available_memory * tokens_per_gb)
    micro_batch_size = max(1, max_tokens_per_gpu // seq_length)

    # Calculate gradient accumulation for target effective batch size
    target_tokens_per_step = 4_000_000  # 4M tokens per step is common
    target_effective_batch = target_tokens_per_step // seq_length

    tokens_per_step = micro_batch_size * gpu_count * seq_length
    gradient_accumulation = max(1, target_tokens_per_step // tokens_per_step)

    effective_batch_size = micro_batch_size * gpu_count * gradient_accumulation

    return CPTBudget(
        total_tokens=total_tokens,
        domain_tokens=domain_tokens,
        replay_tokens=replay_tokens,
        effective_batch_size=effective_batch_size,
        gradient_accumulation_steps=gradient_accumulation,
        micro_batch_size=micro_batch_size
    )


# Example usage
budget = calculate_cpt_budget(
    domain_data_tokens=5_000_000_000,  # 5B domain tokens
    epochs=1,
    replay_ratio=0.3,
    gpu_count=8,
    gpu_memory_gb=80,
    model_params_b=7.0
)
print(f"Total tokens: {budget.total_tokens:,}")
print(f"Domain ratio: {budget.domain_ratio:.1%}")
print(f"Training steps: {budget.training_steps:,}")
print(f"Effective batch size: {budget.effective_batch_size}")
```

### Full Training Configuration

```yaml
# cpt_config.yaml - Continued Pre-training Configuration

# Model settings
model:
  name_or_path: "meta-llama/Llama-2-7b-hf"
  torch_dtype: "bfloat16"
  attn_implementation: "flash_attention_2"
  use_cache: false  # Disable for training

# Data settings
data:
  domain_data_path: "/data/domain_corpus"
  replay_data_path: "/data/general_replay"
  domain_ratio: 0.7
  max_seq_length: 4096
  preprocessing_num_workers: 32

# Training settings
training:
  output_dir: "./cpt_output"
  num_train_epochs: 1
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 16

  # Learning rate (lower than pre-training)
  learning_rate: 1.0e-5
  min_learning_rate: 1.0e-6
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.01

  # Optimizer
  optim: "adamw_torch_fused"
  weight_decay: 0.1
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1.0e-8
  max_grad_norm: 1.0

  # Mixed precision
  bf16: true
  tf32: true

  # Checkpointing
  save_strategy: "steps"
  save_steps: 1000
  save_total_limit: 3

  # Logging
  logging_steps: 10
  report_to: ["wandb"]

# DeepSpeed configuration
deepspeed:
  zero_stage: 3
  offload_optimizer: false
  offload_param: false

# FSDP alternative
fsdp:
  enabled: false
  sharding_strategy: "FULL_SHARD"
  cpu_offload: false
```

## 3.4.4 Catastrophic Forgetting Mitigation

### Data Replay Strategies

```python
"""
Advanced Replay Strategies for Forgetting Prevention
"""
import random
from typing import List, Dict, Tuple
import numpy as np
from collections import defaultdict

class StratifiedReplay:
    """
    Replay data stratified by capability type.

    Ensures model retains diverse capabilities during CPT.
    """

    CAPABILITY_TYPES = [
        "reasoning",
        "knowledge",
        "language",
        "coding",
        "math",
        "instruction_following",
        "safety"
    ]

    def __init__(
        self,
        replay_data: Dict[str, List[str]],
        capability_weights: Dict[str, float] = None
    ):
        """
        Args:
            replay_data: Dict mapping capability type to examples
            capability_weights: Sampling weights per capability
        """
        self.replay_data = replay_data

        # Default equal weights
        if capability_weights is None:
            capability_weights = {cap: 1.0 for cap in replay_data.keys()}

        total = sum(capability_weights.values())
        self.weights = {k: v/total for k, v in capability_weights.items()}

    def sample(self, n: int) -> List[str]:
        """Sample n examples with stratification."""
        samples = []

        for capability, weight in self.weights.items():
            n_cap = int(n * weight)
            cap_data = self.replay_data.get(capability, [])

            if cap_data:
                samples.extend(random.choices(cap_data, k=min(n_cap, len(cap_data))))

        # Fill remaining with random
        while len(samples) < n:
            cap = random.choice(list(self.replay_data.keys()))
            if self.replay_data[cap]:
                samples.append(random.choice(self.replay_data[cap]))

        random.shuffle(samples)
        return samples[:n]


class AdaptiveReplay:
    """
    Adaptively adjust replay based on performance degradation.
    """

    def __init__(
        self,
        replay_data: List[str],
        eval_metrics: Dict[str, float],  # Baseline metrics
        degradation_threshold: float = 0.05  # 5% drop triggers more replay
    ):
        self.replay_data = replay_data
        self.baseline_metrics = eval_metrics
        self.threshold = degradation_threshold
        self.current_ratio = 0.3  # Start with 30% replay

    def update_ratio(self, current_metrics: Dict[str, float]):
        """Update replay ratio based on performance degradation."""
        max_degradation = 0.0

        for key, baseline in self.baseline_metrics.items():
            if key in current_metrics:
                degradation = (baseline - current_metrics[key]) / baseline
                max_degradation = max(max_degradation, degradation)

        # Increase replay if degradation detected
        if max_degradation > self.threshold:
            self.current_ratio = min(0.6, self.current_ratio + 0.1)
        elif max_degradation < self.threshold / 2:
            # Can reduce replay if doing well
            self.current_ratio = max(0.2, self.current_ratio - 0.05)

        return self.current_ratio

    def get_replay_samples(self, batch_size: int) -> List[str]:
        """Get replay samples based on current ratio."""
        n_replay = int(batch_size * self.current_ratio)
        return random.sample(self.replay_data, min(n_replay, len(self.replay_data)))
```

### Regularization Techniques

```python
"""
Regularization Techniques for Continued Pre-training
"""
import torch
import torch.nn as nn
from typing import Dict, Optional
import copy

class ElasticWeightConsolidation:
    """
    Elastic Weight Consolidation (EWC) for preventing catastrophic forgetting.

    Adds penalty for changing important weights identified from previous task.
    """

    def __init__(
        self,
        model: nn.Module,
        fisher_samples: int = 1000,
        lambda_ewc: float = 0.1
    ):
        self.model = model
        self.fisher_samples = fisher_samples
        self.lambda_ewc = lambda_ewc

        # Store important weights
        self.fisher_information: Dict[str, torch.Tensor] = {}
        self.optimal_params: Dict[str, torch.Tensor] = {}

    def compute_fisher_information(
        self,
        dataloader,
        device: str = "cuda"
    ):
        """Compute Fisher information matrix diagonal."""
        self.model.eval()

        fisher = {n: torch.zeros_like(p) for n, p in self.model.named_parameters() if p.requires_grad}

        for i, batch in enumerate(dataloader):
            if i >= self.fisher_samples:
                break

            batch = {k: v.to(device) for k, v in batch.items()}

            self.model.zero_grad()
            outputs = self.model(**batch)
            loss = outputs.loss
            loss.backward()

            for n, p in self.model.named_parameters():
                if p.requires_grad and p.grad is not None:
                    fisher[n] += p.grad.pow(2)

        # Normalize
        for n in fisher:
            fisher[n] /= min(i + 1, self.fisher_samples)

        self.fisher_information = fisher
        self.optimal_params = {n: p.clone().detach() for n, p in self.model.named_parameters()}

    def penalty(self) -> torch.Tensor:
        """Calculate EWC penalty loss."""
        loss = 0.0

        for n, p in self.model.named_parameters():
            if n in self.fisher_information:
                fisher = self.fisher_information[n]
                optimal = self.optimal_params[n]
                loss += (fisher * (p - optimal).pow(2)).sum()

        return self.lambda_ewc * loss


class LearningWithoutForgetting:
    """
    Learning without Forgetting (LwF) using knowledge distillation.

    Preserves model behavior by distilling from frozen original model.
    """

    def __init__(
        self,
        model: nn.Module,
        temperature: float = 2.0,
        alpha: float = 0.5
    ):
        self.student = model
        self.teacher = copy.deepcopy(model)

        # Freeze teacher
        for p in self.teacher.parameters():
            p.requires_grad = False

        self.temperature = temperature
        self.alpha = alpha

    def distillation_loss(
        self,
        student_logits: torch.Tensor,
        input_ids: torch.Tensor
    ) -> torch.Tensor:
        """Calculate distillation loss from teacher."""
        with torch.no_grad():
            teacher_logits = self.teacher(input_ids).logits

        # Soft targets
        soft_targets = torch.softmax(teacher_logits / self.temperature, dim=-1)
        soft_predictions = torch.log_softmax(student_logits / self.temperature, dim=-1)

        # KL divergence
        distill_loss = torch.nn.functional.kl_div(
            soft_predictions,
            soft_targets,
            reduction='batchmean'
        ) * (self.temperature ** 2)

        return distill_loss

    def combined_loss(
        self,
        student_logits: torch.Tensor,
        labels: torch.Tensor,
        input_ids: torch.Tensor
    ) -> torch.Tensor:
        """Combine task loss with distillation loss."""
        # Task loss (cross-entropy)
        task_loss = torch.nn.functional.cross_entropy(
            student_logits.view(-1, student_logits.size(-1)),
            labels.view(-1),
            ignore_index=-100
        )

        # Distillation loss
        distill_loss = self.distillation_loss(student_logits, input_ids)

        return (1 - self.alpha) * task_loss + self.alpha * distill_loss


class ProgressiveFreezing:
    """
    Progressive layer freezing during continued pre-training.

    Freeze lower layers first, gradually unfreeze as training progresses.
    """

    def __init__(
        self,
        model: nn.Module,
        total_steps: int,
        num_layers: int,
        freeze_embeddings: bool = True
    ):
        self.model = model
        self.total_steps = total_steps
        self.num_layers = num_layers
        self.freeze_embeddings = freeze_embeddings

        # Initially freeze all
        self._freeze_all()

    def _freeze_all(self):
        """Freeze all parameters."""
        for p in self.model.parameters():
            p.requires_grad = False

    def _unfreeze_layer(self, layer_idx: int):
        """Unfreeze specific layer."""
        layer_name = f"layers.{layer_idx}"

        for n, p in self.model.named_parameters():
            if layer_name in n:
                p.requires_grad = True

    def step(self, current_step: int):
        """Update freezing based on training progress."""
        progress = current_step / self.total_steps

        # Unfreeze from top to bottom
        layers_to_unfreeze = int(progress * self.num_layers)

        for i in range(self.num_layers - 1, self.num_layers - 1 - layers_to_unfreeze, -1):
            self._unfreeze_layer(i)

        # Optionally keep embeddings frozen
        if self.freeze_embeddings:
            for n, p in self.model.named_parameters():
                if "embed" in n.lower():
                    p.requires_grad = False
```

### Evaluation on General Capabilities

```python
"""
General Capability Retention Evaluation
"""
from dataclasses import dataclass
from typing import Dict, List, Optional
import json

@dataclass
class CapabilityMetrics:
    """Metrics for capability retention."""
    baseline: float
    current: float
    degradation: float

    @property
    def retention_rate(self) -> float:
        return self.current / self.baseline if self.baseline > 0 else 0.0


class GeneralCapabilityEvaluator:
    """Evaluate retention of general capabilities during CPT."""

    CAPABILITY_BENCHMARKS = {
        "reasoning": ["arc_challenge", "hellaswag", "winogrande"],
        "knowledge": ["mmlu", "triviaqa"],
        "math": ["gsm8k", "math"],
        "coding": ["humaneval", "mbpp"],
        "language": ["lambada", "wikitext_ppl"],
    }

    def __init__(
        self,
        model,
        tokenizer,
        baseline_results: Optional[Dict[str, float]] = None
    ):
        self.model = model
        self.tokenizer = tokenizer
        self.baseline = baseline_results or {}

    def evaluate_capability(
        self,
        capability: str,
        max_samples: int = 1000
    ) -> float:
        """Evaluate a specific capability."""
        benchmarks = self.CAPABILITY_BENCHMARKS.get(capability, [])

        if not benchmarks:
            return 0.0

        scores = []
        for benchmark in benchmarks:
            score = self._run_benchmark(benchmark, max_samples)
            if score is not None:
                scores.append(score)

        return sum(scores) / len(scores) if scores else 0.0

    def _run_benchmark(self, benchmark: str, max_samples: int) -> Optional[float]:
        """Run specific benchmark evaluation."""
        # Placeholder - integrate with lm-evaluation-harness
        return None

    def full_evaluation(self) -> Dict[str, CapabilityMetrics]:
        """Evaluate all general capabilities."""
        results = {}

        for capability in self.CAPABILITY_BENCHMARKS:
            current = self.evaluate_capability(capability)
            baseline = self.baseline.get(capability, current)

            results[capability] = CapabilityMetrics(
                baseline=baseline,
                current=current,
                degradation=(baseline - current) / baseline if baseline > 0 else 0.0
            )

        return results

    def generate_report(self, results: Dict[str, CapabilityMetrics]) -> str:
        """Generate retention report."""
        report = ["# General Capability Retention Report\n"]

        total_retention = []

        for capability, metrics in results.items():
            status = "✓" if metrics.retention_rate >= 0.95 else "⚠️" if metrics.retention_rate >= 0.9 else "❌"
            report.append(f"## {capability.title()} {status}")
            report.append(f"- Baseline: {metrics.baseline:.3f}")
            report.append(f"- Current: {metrics.current:.3f}")
            report.append(f"- Retention: {metrics.retention_rate:.1%}")
            report.append(f"- Degradation: {metrics.degradation:.1%}\n")

            total_retention.append(metrics.retention_rate)

        avg_retention = sum(total_retention) / len(total_retention) if total_retention else 0
        report.insert(1, f"**Average Retention: {avg_retention:.1%}**\n")

        return "\n".join(report)
```

## 3.4.5 Curriculum for Continued Pre-training

### Domain Introduction Strategies

```python
"""
Curriculum Strategies for Domain Introduction
"""
from enum import Enum
from typing import List, Callable
import random

class IntroductionStrategy(Enum):
    IMMEDIATE = "immediate"      # Full domain ratio from start
    GRADUAL = "gradual"          # Linear increase
    STAGED = "staged"            # Step-wise introduction
    QUALITY_FIRST = "quality"    # High quality → all quality levels


class DomainCurriculum:
    """Manage curriculum for domain data introduction."""

    def __init__(
        self,
        domain_data: List[str],
        strategy: IntroductionStrategy = IntroductionStrategy.GRADUAL,
        quality_scores: List[float] = None,
        difficulty_scores: List[float] = None
    ):
        self.domain_data = domain_data
        self.strategy = strategy
        self.quality_scores = quality_scores or [1.0] * len(domain_data)
        self.difficulty_scores = difficulty_scores or [0.5] * len(domain_data)

        # Create sorted indices
        self._sort_by_curriculum()

    def _sort_by_curriculum(self):
        """Sort data indices by curriculum criteria."""
        indices = list(range(len(self.domain_data)))

        if self.strategy == IntroductionStrategy.QUALITY_FIRST:
            # High quality first
            indices.sort(key=lambda i: self.quality_scores[i], reverse=True)
        elif self.strategy in [IntroductionStrategy.GRADUAL, IntroductionStrategy.STAGED]:
            # Easy to hard
            indices.sort(key=lambda i: self.difficulty_scores[i])

        self.curriculum_order = indices

    def get_domain_ratio(self, progress: float, target_ratio: float = 0.7) -> float:
        """Get domain data ratio based on progress and strategy."""
        if self.strategy == IntroductionStrategy.IMMEDIATE:
            return target_ratio

        elif self.strategy == IntroductionStrategy.GRADUAL:
            # Linear ramp up in first 20% of training
            if progress < 0.2:
                return target_ratio * (progress / 0.2)
            return target_ratio

        elif self.strategy == IntroductionStrategy.STAGED:
            # Three stages: 30%, 60%, 100%
            if progress < 0.1:
                return target_ratio * 0.3
            elif progress < 0.3:
                return target_ratio * 0.6
            return target_ratio

        elif self.strategy == IntroductionStrategy.QUALITY_FIRST:
            # Quality-based introduction
            return target_ratio

        return target_ratio

    def get_data_range(self, progress: float) -> tuple[int, int]:
        """Get curriculum-appropriate data range."""
        total = len(self.domain_data)

        if self.strategy == IntroductionStrategy.IMMEDIATE:
            return 0, total

        elif self.strategy in [IntroductionStrategy.GRADUAL, IntroductionStrategy.STAGED]:
            # Expand data range with progress
            end_idx = int(total * min(1.0, 0.3 + progress * 0.7))
            return 0, end_idx

        elif self.strategy == IntroductionStrategy.QUALITY_FIRST:
            # Start with top quality, expand to all
            quality_threshold = 1.0 - progress * 0.5  # 1.0 → 0.5
            end_idx = sum(1 for q in self.quality_scores if q >= quality_threshold)
            return 0, max(end_idx, int(total * 0.1))

        return 0, total

    def sample_batch(
        self,
        batch_size: int,
        progress: float
    ) -> List[str]:
        """Sample curriculum-aware batch."""
        start_idx, end_idx = self.get_data_range(progress)

        # Sample from curriculum range
        available_indices = self.curriculum_order[start_idx:end_idx]
        sampled_indices = random.choices(available_indices, k=batch_size)

        return [self.domain_data[i] for i in sampled_indices]
```

### Mixing Schedule Implementation

```python
"""
Production Mixing Schedule for CPT
"""
import torch
from torch.utils.data import Dataset, DataLoader, Sampler
from typing import List, Iterator
import numpy as np

class MixedCPTDataset(Dataset):
    """Dataset with dynamic domain/general mixing."""

    def __init__(
        self,
        domain_data: List[str],
        general_data: List[str],
        tokenizer,
        max_length: int = 4096,
        curriculum: DomainCurriculum = None
    ):
        self.domain_data = domain_data
        self.general_data = general_data
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.curriculum = curriculum

        self._current_progress = 0.0

    def set_progress(self, progress: float):
        """Update curriculum progress."""
        self._current_progress = progress

    def __len__(self):
        return len(self.domain_data) + len(self.general_data)

    def __getitem__(self, idx: int):
        # Determine if domain or general based on curriculum
        if self.curriculum:
            domain_ratio = self.curriculum.get_domain_ratio(self._current_progress)
        else:
            domain_ratio = 0.7

        # Use deterministic selection based on index
        is_domain = (idx % 100) < int(domain_ratio * 100)

        if is_domain:
            text = self.domain_data[idx % len(self.domain_data)]
        else:
            text = self.general_data[idx % len(self.general_data)]

        # Tokenize
        encoded = self.tokenizer(
            text,
            max_length=self.max_length,
            truncation=True,
            padding="max_length",
            return_tensors="pt"
        )

        return {
            "input_ids": encoded["input_ids"].squeeze(),
            "attention_mask": encoded["attention_mask"].squeeze(),
            "labels": encoded["input_ids"].squeeze()
        }


class CurriculumSampler(Sampler):
    """Sampler that respects curriculum ordering."""

    def __init__(
        self,
        dataset: MixedCPTDataset,
        batch_size: int,
        total_steps: int,
        shuffle: bool = True
    ):
        self.dataset = dataset
        self.batch_size = batch_size
        self.total_steps = total_steps
        self.shuffle = shuffle
        self.current_step = 0

    def __iter__(self) -> Iterator[int]:
        """Generate indices for training."""
        indices = list(range(len(self.dataset)))

        for step in range(self.total_steps):
            self.current_step = step
            progress = step / self.total_steps

            # Update dataset progress
            self.dataset.set_progress(progress)

            # Yield batch
            if self.shuffle:
                batch_indices = np.random.choice(indices, size=self.batch_size, replace=True)
            else:
                start = (step * self.batch_size) % len(indices)
                batch_indices = indices[start:start + self.batch_size]

            yield from batch_indices

    def __len__(self):
        return self.total_steps * self.batch_size
```

## 3.4.6 Evaluation Strategy

### Comprehensive Evaluation Suite

```python
"""
Comprehensive Evaluation for Continued Pre-training
"""
from dataclasses import dataclass, field
from typing import Dict, List, Optional
import json
from pathlib import Path

@dataclass
class CPTEvaluationResults:
    """Results from CPT evaluation."""
    domain_metrics: Dict[str, float] = field(default_factory=dict)
    general_metrics: Dict[str, float] = field(default_factory=dict)
    perplexity: Dict[str, float] = field(default_factory=dict)
    forgetting_metrics: Dict[str, float] = field(default_factory=dict)

    def overall_score(self) -> float:
        """Calculate overall CPT success score."""
        domain_score = sum(self.domain_metrics.values()) / max(len(self.domain_metrics), 1)

        # Penalize forgetting
        avg_forgetting = sum(self.forgetting_metrics.values()) / max(len(self.forgetting_metrics), 1)
        forgetting_penalty = max(0, avg_forgetting - 0.05)  # Allow 5% degradation

        return domain_score - forgetting_penalty

    def to_report(self) -> str:
        """Generate evaluation report."""
        lines = ["# CPT Evaluation Report\n"]

        lines.append("## Domain Performance")
        for k, v in self.domain_metrics.items():
            lines.append(f"- {k}: {v:.3f}")

        lines.append("\n## General Capability Retention")
        for k, v in self.general_metrics.items():
            lines.append(f"- {k}: {v:.3f}")

        lines.append("\n## Perplexity Analysis")
        for k, v in self.perplexity.items():
            lines.append(f"- {k}: {v:.2f}")

        lines.append("\n## Forgetting Analysis")
        for k, v in self.forgetting_metrics.items():
            status = "✓" if v < 0.05 else "⚠️" if v < 0.1 else "❌"
            lines.append(f"- {k}: {v:.1%} degradation {status}")

        lines.append(f"\n**Overall Score: {self.overall_score():.3f}**")

        return "\n".join(lines)


class CPTEvaluator:
    """Comprehensive evaluator for continued pre-training."""

    def __init__(
        self,
        model,
        tokenizer,
        baseline_model=None,
        device: str = "cuda"
    ):
        self.model = model
        self.tokenizer = tokenizer
        self.baseline_model = baseline_model
        self.device = device

    def evaluate_perplexity(
        self,
        texts: List[str],
        name: str = "eval"
    ) -> float:
        """Calculate perplexity on text corpus."""
        import torch

        self.model.eval()
        total_loss = 0
        total_tokens = 0

        for text in texts[:1000]:  # Limit samples
            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                max_length=2048
            ).to(self.device)

            with torch.no_grad():
                outputs = self.model(**inputs, labels=inputs["input_ids"])
                total_loss += outputs.loss.item() * inputs["input_ids"].size(1)
                total_tokens += inputs["input_ids"].size(1)

        return torch.exp(torch.tensor(total_loss / total_tokens)).item()

    def evaluate_forgetting(
        self,
        eval_data: Dict[str, List[str]],
        baseline_scores: Dict[str, float]
    ) -> Dict[str, float]:
        """Evaluate forgetting across capabilities."""
        forgetting = {}

        for capability, texts in eval_data.items():
            if capability not in baseline_scores:
                continue

            current_ppl = self.evaluate_perplexity(texts, capability)
            baseline_ppl = baseline_scores[capability]

            # Forgetting = relative perplexity increase
            forgetting[capability] = max(0, (current_ppl - baseline_ppl) / baseline_ppl)

        return forgetting

    def run_evaluation(
        self,
        domain_eval_data: List[str],
        general_eval_data: Dict[str, List[str]],
        baseline_perplexities: Dict[str, float]
    ) -> CPTEvaluationResults:
        """Run comprehensive evaluation."""
        results = CPTEvaluationResults()

        # Domain perplexity
        results.perplexity["domain"] = self.evaluate_perplexity(domain_eval_data, "domain")

        # General perplexities
        for capability, texts in general_eval_data.items():
            results.perplexity[capability] = self.evaluate_perplexity(texts, capability)

        # Forgetting metrics
        results.forgetting_metrics = self.evaluate_forgetting(
            general_eval_data,
            baseline_perplexities
        )

        return results


def create_evaluation_checkpoint(
    evaluator: CPTEvaluator,
    step: int,
    output_dir: Path,
    domain_data: List[str],
    general_data: Dict[str, List[str]],
    baseline: Dict[str, float]
) -> CPTEvaluationResults:
    """Create evaluation checkpoint during training."""
    results = evaluator.run_evaluation(domain_data, general_data, baseline)

    # Save results
    output_path = output_dir / f"eval_step_{step}.json"
    with open(output_path, 'w') as f:
        json.dump({
            "step": step,
            "domain_metrics": results.domain_metrics,
            "general_metrics": results.general_metrics,
            "perplexity": results.perplexity,
            "forgetting": results.forgetting_metrics,
            "overall_score": results.overall_score()
        }, f, indent=2)

    return results
```

## Appendix A: Continued Pre-training Configurations

### Llama 7B CPT Configuration

```python
# configs/llama_7b_cpt.py

CPT_CONFIG_7B = {
    "model": {
        "base_model": "meta-llama/Llama-2-7b-hf",
        "use_flash_attention": True,
        "dtype": "bfloat16",
    },
    "data": {
        "domain_ratio": 0.7,
        "replay_ratio": 0.3,
        "max_seq_length": 4096,
        "packing": True,
    },
    "training": {
        "learning_rate": 1e-5,
        "min_learning_rate": 1e-6,
        "warmup_steps": 100,
        "weight_decay": 0.1,
        "max_grad_norm": 1.0,
        "per_device_batch_size": 4,
        "gradient_accumulation_steps": 8,
    },
    "curriculum": {
        "strategy": "gradual",
        "domain_introduction_steps": 1000,
    },
    "forgetting_mitigation": {
        "ewc_lambda": 0.1,
        "replay_stratified": True,
    }
}
```

### DeepSpeed ZeRO-3 Configuration

```json
{
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "gradient_accumulation_steps": "auto",

    "bf16": {
        "enabled": true
    },

    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": {
            "device": "none"
        },
        "offload_param": {
            "device": "none"
        },
        "overlap_comm": true,
        "contiguous_gradients": true,
        "sub_group_size": 1e9,
        "reduce_bucket_size": "auto",
        "stage3_prefetch_bucket_size": "auto",
        "stage3_param_persistence_threshold": "auto",
        "stage3_max_live_parameters": 1e9,
        "stage3_max_reuse_distance": 1e9,
        "stage3_gather_16bit_weights_on_model_save": true
    },

    "gradient_clipping": 1.0,

    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": "auto",
            "betas": [0.9, 0.95],
            "eps": 1e-8,
            "weight_decay": "auto"
        }
    },

    "scheduler": {
        "type": "WarmupDecayLR",
        "params": {
            "warmup_min_lr": "auto",
            "warmup_max_lr": "auto",
            "warmup_num_steps": "auto",
            "total_num_steps": "auto"
        }
    }
}
```

## Appendix B: Replay Data Mixing Scripts

```python
#!/usr/bin/env python3
"""
Replay Data Mixing Script for CPT
Usage: python mix_replay_data.py --domain /path/to/domain --general /path/to/general --output /path/to/mixed
"""

import argparse
import json
import random
from pathlib import Path
from typing import Iterator

def load_jsonl(path: Path) -> list:
    """Load JSONL file."""
    data = []
    with open(path) as f:
        for line in f:
            data.append(json.loads(line))
    return data

def stream_mixed_data(
    domain_data: list,
    general_data: list,
    domain_ratio: float,
    total_samples: int
) -> Iterator[dict]:
    """Stream mixed domain and general data."""
    for i in range(total_samples):
        if random.random() < domain_ratio:
            sample = random.choice(domain_data)
            sample["source_type"] = "domain"
        else:
            sample = random.choice(general_data)
            sample["source_type"] = "general"

        yield sample

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--domain", type=Path, required=True)
    parser.add_argument("--general", type=Path, required=True)
    parser.add_argument("--output", type=Path, required=True)
    parser.add_argument("--domain-ratio", type=float, default=0.7)
    parser.add_argument("--total-samples", type=int, default=1_000_000)

    args = parser.parse_args()

    print("Loading domain data...")
    domain_data = load_jsonl(args.domain)

    print("Loading general data...")
    general_data = load_jsonl(args.general)

    print(f"Mixing with {args.domain_ratio:.0%} domain ratio...")

    with open(args.output, 'w') as f:
        for sample in stream_mixed_data(
            domain_data,
            general_data,
            args.domain_ratio,
            args.total_samples
        ):
            f.write(json.dumps(sample) + "\n")

    print(f"Mixed data saved to {args.output}")

if __name__ == "__main__":
    main()
```

## Appendix C: Forgetting Evaluation Suite

```python
#!/usr/bin/env python3
"""
Forgetting Evaluation Suite for CPT
Usage: python evaluate_forgetting.py --model /path/to/model --baseline /path/to/baseline
"""

import argparse
import json
from pathlib import Path
from typing import Dict
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset

EVAL_BENCHMARKS = {
    "reasoning": {
        "dataset": "ai2_arc",
        "subset": "ARC-Challenge",
        "split": "test"
    },
    "knowledge": {
        "dataset": "cais/mmlu",
        "subset": "all",
        "split": "test"
    },
    "language": {
        "dataset": "lambada",
        "split": "test"
    }
}

def evaluate_perplexity(model, tokenizer, texts: list, device: str = "cuda") -> float:
    """Evaluate perplexity on text corpus."""
    model.eval()
    total_loss = 0
    total_tokens = 0

    for text in texts[:500]:
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=1024).to(device)
        with torch.no_grad():
            outputs = model(**inputs, labels=inputs["input_ids"])
            total_loss += outputs.loss.item() * inputs["input_ids"].size(1)
            total_tokens += inputs["input_ids"].size(1)

    return torch.exp(torch.tensor(total_loss / total_tokens)).item()

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", type=Path, required=True)
    parser.add_argument("--baseline", type=Path)
    parser.add_argument("--output", type=Path, default=Path("forgetting_eval.json"))

    args = parser.parse_args()

    device = "cuda" if torch.cuda.is_available() else "cpu"

    print("Loading model...")
    model = AutoModelForCausalLM.from_pretrained(args.model, torch_dtype=torch.bfloat16).to(device)
    tokenizer = AutoTokenizer.from_pretrained(args.model)

    baseline_model = None
    if args.baseline:
        print("Loading baseline model...")
        baseline_model = AutoModelForCausalLM.from_pretrained(args.baseline, torch_dtype=torch.bfloat16).to(device)

    results = {}

    for capability, config in EVAL_BENCHMARKS.items():
        print(f"Evaluating {capability}...")

        try:
            ds = load_dataset(config["dataset"], config.get("subset"), split=config["split"])
            texts = [item.get("text", str(item)) for item in ds.select(range(min(500, len(ds))))]

            current_ppl = evaluate_perplexity(model, tokenizer, texts, device)

            if baseline_model:
                baseline_ppl = evaluate_perplexity(baseline_model, tokenizer, texts, device)
                forgetting = (current_ppl - baseline_ppl) / baseline_ppl
            else:
                forgetting = 0.0

            results[capability] = {
                "perplexity": current_ppl,
                "forgetting": forgetting
            }
        except Exception as e:
            print(f"Failed to evaluate {capability}: {e}")

    with open(args.output, 'w') as f:
        json.dump(results, f, indent=2)

    print(f"Results saved to {args.output}")

if __name__ == "__main__":
    main()
```

## Troubleshooting

### Common Issues

| Issue | Cause | Solution |
|-------|-------|----------|
| Severe forgetting (>10%) | Insufficient replay data | Increase replay ratio to 40-50% |
| Domain performance plateau | Learning rate too low | Try 2-3x higher LR with shorter warmup |
| Training instability | LR too high or data quality issues | Reduce LR, check data for anomalies |
| OOM during training | Batch size too large | Reduce micro batch, increase accumulation |
| Slow convergence | Data mixing imbalance | Verify domain/general ratio in batches |

### Debugging Forgetting

```python
def debug_forgetting(
    model,
    tokenizer,
    checkpoint_dir: Path,
    eval_data: dict
) -> dict:
    """Debug forgetting by evaluating across checkpoints."""
    import glob

    checkpoints = sorted(glob.glob(str(checkpoint_dir / "checkpoint-*")))

    results = {}
    for ckpt in checkpoints:
        step = int(ckpt.split("-")[-1])

        # Load checkpoint
        model = AutoModelForCausalLM.from_pretrained(ckpt)

        # Evaluate
        perplexities = {}
        for name, texts in eval_data.items():
            perplexities[name] = evaluate_perplexity(model, tokenizer, texts)

        results[step] = perplexities

    return results
```

## Glossary

- **Continued Pre-training (CPT)**: Additional pre-training on domain-specific data after initial pre-training
- **Catastrophic Forgetting**: Loss of previously learned capabilities when learning new tasks
- **Data Replay**: Including general data during domain training to preserve capabilities
- **EWC (Elastic Weight Consolidation)**: Regularization technique to protect important weights
- **Curriculum Learning**: Structured data ordering from easy to difficult
- **Domain Gap**: Difference between model's current capabilities and target domain requirements

## References

1. "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks" (Gururangan et al., 2020)
2. "Continual Pre-training of Language Models" (Ke et al., 2023)
3. "Overcoming Catastrophic Forgetting in Neural Networks" (Kirkpatrick et al., 2017)
4. "Learning without Forgetting" (Li & Hoiem, 2016)
5. "Curriculum Learning" (Bengio et al., 2009)
6. CodeLlama Technical Report - Continued Pre-training for Code (2023)

---

> **Navigation**
> [← 3.3 Domain Adaptation](3.3_domain_adaptation.md) | **[Index](../README.md#15-repository-structure)** | [4.1 RLHF →](../04_alignment_safety/4.1_rlhf_guide.md)
