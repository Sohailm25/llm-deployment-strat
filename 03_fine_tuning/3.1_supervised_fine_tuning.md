> **Navigation** | [← 2.6 Ray Train](../02_model_training/2.6_ray_train_guide.md) | [3.2 PEFT →](3.2_parameter_efficient_fine_tuning.md)
>
> | | |
> |---|---|
> | **Prerequisites** | [2.2 Model Architecture](../02_model_training/2.2_model_architecture_selection.md) &#124; [2.1 Tokenizer](../02_model_training/2.1_tokenizer_training_selection.md) &#124; [1.3 Data Labeling](../01_data_pipeline/1.3_data_labeling_annotation.md) |
> | **Related** | [3.2 PEFT](3.2_parameter_efficient_fine_tuning.md) &#124; [3.3 Domain Adaptation](3.3_domain_adaptation.md) &#124; [4.1 RLHF](../04_alignment_safety/4.1_rlhf_guide.md) |
> | **Next** | [3.2 Parameter-Efficient Fine-Tuning](3.2_parameter_efficient_fine_tuning.md) |

# Supervised Fine-Tuning (SFT) Guide

## Executive Summary

Supervised Fine-Tuning (SFT) transforms pre-trained language models into instruction-following assistants. This guide covers dataset preparation, training configuration, loss function variants, and evaluation strategies for effective instruction tuning.

## Prerequisites

- Pre-trained base model (Document 2.2)
- Understanding of tokenization (Document 2.1)
- Data labeling pipeline (Document 1.3)
- GPU infrastructure for training

---

## 3.1.1 SFT Fundamentals

### When to Fine-Tune vs Prompt Engineering

| Approach | Best For | Cost | Latency Impact |
|----------|----------|------|----------------|
| **Prompt Engineering** | Quick iteration, general tasks | Low (API costs) | Increases context length |
| **Few-Shot Prompting** | Simple patterns, limited data | Medium | Increases context significantly |
| **SFT** | Consistent behavior, specific formats | High (training) | No latency impact |
| **RAG + Prompting** | Knowledge-intensive tasks | Medium | Retrieval latency |

### Decision Framework

```python
"""
Decision framework for SFT vs alternatives
"""
from dataclasses import dataclass
from typing import List, Optional
from enum import Enum

class ApproachRecommendation(Enum):
    PROMPT_ENGINEERING = "prompt_engineering"
    FEW_SHOT = "few_shot"
    SFT = "supervised_fine_tuning"
    PEFT = "parameter_efficient_fine_tuning"
    RAG = "retrieval_augmented_generation"

@dataclass
class UseCase:
    """Describes a use case for approach selection"""
    task_complexity: str  # "simple", "moderate", "complex"
    data_availability: int  # Number of examples available
    format_consistency_required: bool
    domain_specific: bool
    latency_sensitive: bool
    compute_budget: str  # "low", "medium", "high"
    update_frequency: str  # "daily", "weekly", "monthly", "rare"

def recommend_approach(use_case: UseCase) -> ApproachRecommendation:
    """
    Recommend the best approach based on use case characteristics.

    Returns:
        Recommended approach with reasoning
    """
    # Few examples -> Prompt engineering or few-shot
    if use_case.data_availability < 50:
        if use_case.task_complexity == "simple":
            return ApproachRecommendation.PROMPT_ENGINEERING
        return ApproachRecommendation.FEW_SHOT

    # Knowledge-intensive without strict format -> RAG
    if use_case.domain_specific and not use_case.format_consistency_required:
        if use_case.update_frequency in ["daily", "weekly"]:
            return ApproachRecommendation.RAG

    # Limited compute -> PEFT
    if use_case.compute_budget == "low" and use_case.data_availability >= 100:
        return ApproachRecommendation.PEFT

    # Format consistency + enough data -> SFT
    if use_case.format_consistency_required and use_case.data_availability >= 500:
        return ApproachRecommendation.SFT

    # Complex task + high compute -> Full SFT
    if use_case.task_complexity == "complex" and use_case.compute_budget == "high":
        return ApproachRecommendation.SFT

    # Default to PEFT for moderate cases
    return ApproachRecommendation.PEFT

# Example usage
use_case = UseCase(
    task_complexity="moderate",
    data_availability=5000,
    format_consistency_required=True,
    domain_specific=True,
    latency_sensitive=True,
    compute_budget="medium",
    update_frequency="monthly"
)

recommendation = recommend_approach(use_case)
print(f"Recommended approach: {recommendation.value}")
```

### Full Fine-Tuning vs Parameter-Efficient Methods

| Method | Trainable Params | Memory Required | Best Use Case |
|--------|------------------|-----------------|---------------|
| Full Fine-Tuning | 100% | Very High | Major domain shift, abundant compute |
| LoRA | 0.1-1% | Low | Moderate adaptation, limited compute |
| QLoRA | 0.1-1% | Very Low | Single GPU, consumer hardware |
| Prefix Tuning | <0.1% | Very Low | Quick experimentation |

### Catastrophic Forgetting Mitigation

```python
"""
Strategies to mitigate catastrophic forgetting during SFT
"""
import torch
from typing import Dict, List, Optional
import random

class ForgettingMitigator:
    """
    Implements strategies to prevent catastrophic forgetting.
    """

    @staticmethod
    def create_replay_dataset(
        sft_data: List[Dict],
        general_data: List[Dict],
        replay_ratio: float = 0.1
    ) -> List[Dict]:
        """
        Mix SFT data with general capability data.

        Args:
            sft_data: Task-specific fine-tuning examples
            general_data: General instruction-following examples
            replay_ratio: Fraction of batch to be general data

        Returns:
            Mixed dataset
        """
        num_replay = int(len(sft_data) * replay_ratio / (1 - replay_ratio))
        replay_samples = random.sample(general_data, min(num_replay, len(general_data)))

        mixed_data = sft_data + replay_samples
        random.shuffle(mixed_data)

        return mixed_data

    @staticmethod
    def elastic_weight_consolidation(
        model: torch.nn.Module,
        fisher_information: Dict[str, torch.Tensor],
        lambda_ewc: float = 0.5
    ) -> torch.Tensor:
        """
        Compute EWC regularization loss.

        Args:
            model: Current model
            fisher_information: Fisher information matrix diagonal
            lambda_ewc: Regularization strength

        Returns:
            EWC loss term
        """
        ewc_loss = 0.0

        for name, param in model.named_parameters():
            if name in fisher_information:
                fisher = fisher_information[name]
                old_param = fisher_information[f"{name}_old"]

                ewc_loss += (fisher * (param - old_param) ** 2).sum()

        return lambda_ewc * ewc_loss

    @staticmethod
    def compute_fisher_information(
        model: torch.nn.Module,
        dataloader: torch.utils.data.DataLoader,
        num_samples: int = 1000
    ) -> Dict[str, torch.Tensor]:
        """
        Compute Fisher information for EWC.

        Args:
            model: Model to compute Fisher for
            dataloader: Data to compute Fisher on
            num_samples: Number of samples to use

        Returns:
            Dictionary of Fisher information per parameter
        """
        fisher_info = {name: torch.zeros_like(param)
                      for name, param in model.named_parameters()
                      if param.requires_grad}

        model.eval()
        samples_seen = 0

        for batch in dataloader:
            if samples_seen >= num_samples:
                break

            model.zero_grad()

            # Forward pass
            outputs = model(**batch)
            loss = outputs.loss

            # Backward pass
            loss.backward()

            # Accumulate squared gradients
            for name, param in model.named_parameters():
                if param.grad is not None:
                    fisher_info[name] += param.grad ** 2

            samples_seen += batch["input_ids"].shape[0]

        # Normalize
        for name in fisher_info:
            fisher_info[name] /= samples_seen

        # Save current parameters
        for name, param in model.named_parameters():
            fisher_info[f"{name}_old"] = param.clone().detach()

        return fisher_info
```

---

## 3.1.2 Dataset Preparation

### Instruction Formats

```python
"""
Instruction format templates for SFT
"""
from dataclasses import dataclass
from typing import List, Dict, Optional
import json

@dataclass
class InstructionFormat:
    """Base class for instruction formats"""
    name: str
    system_template: str
    user_template: str
    assistant_template: str
    separator: str

# Alpaca Format
ALPACA_FORMAT = InstructionFormat(
    name="alpaca",
    system_template="",
    user_template="### Instruction:\n{instruction}\n\n### Input:\n{input}\n\n### Response:\n",
    assistant_template="{response}",
    separator=""
)

# ShareGPT/Vicuna Format
SHAREGPT_FORMAT = InstructionFormat(
    name="sharegpt",
    system_template="A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n\n",
    user_template="USER: {content}\n",
    assistant_template="ASSISTANT: {content}</s>",
    separator=""
)

# ChatML Format (OpenAI style)
CHATML_FORMAT = InstructionFormat(
    name="chatml",
    system_template="<|im_start|>system\n{system}<|im_end|>\n",
    user_template="<|im_start|>user\n{content}<|im_end|>\n",
    assistant_template="<|im_start|>assistant\n{content}<|im_end|>\n",
    separator=""
)

# Llama 2/3 Chat Format
LLAMA_CHAT_FORMAT = InstructionFormat(
    name="llama_chat",
    system_template="<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{system}<|eot_id|>",
    user_template="<|start_header_id|>user<|end_header_id|>\n\n{content}<|eot_id|>",
    assistant_template="<|start_header_id|>assistant<|end_header_id|>\n\n{content}<|eot_id|>",
    separator=""
)

# Mistral Instruct Format
MISTRAL_FORMAT = InstructionFormat(
    name="mistral",
    system_template="",  # System prompt goes in first user message
    user_template="[INST] {content} [/INST]",
    assistant_template="{content}</s>",
    separator=""
)

class DatasetFormatter:
    """
    Format datasets for SFT training.
    """

    def __init__(self, format_type: InstructionFormat, tokenizer):
        self.format = format_type
        self.tokenizer = tokenizer

    def format_conversation(
        self,
        messages: List[Dict[str, str]],
        system_prompt: Optional[str] = None
    ) -> str:
        """
        Format a conversation into the target format.

        Args:
            messages: List of {"role": "user"|"assistant", "content": "..."}
            system_prompt: Optional system prompt

        Returns:
            Formatted string
        """
        formatted = ""

        # Add system prompt if provided
        if system_prompt and self.format.system_template:
            formatted += self.format.system_template.format(system=system_prompt)

        # Format each message
        for msg in messages:
            if msg["role"] == "user":
                formatted += self.format.user_template.format(content=msg["content"])
            elif msg["role"] == "assistant":
                formatted += self.format.assistant_template.format(content=msg["content"])

        return formatted

    def format_alpaca_example(
        self,
        instruction: str,
        input_text: str = "",
        response: str = ""
    ) -> str:
        """
        Format a single Alpaca-style example.
        """
        template = self.format.user_template.format(
            instruction=instruction,
            input=input_text if input_text else "N/A"
        )
        template += self.format.assistant_template.format(response=response)
        return template

    def create_training_example(
        self,
        conversation: List[Dict[str, str]],
        system_prompt: Optional[str] = None,
        mask_user: bool = True
    ) -> Dict[str, List[int]]:
        """
        Create tokenized training example with optional loss masking.

        Args:
            conversation: Conversation messages
            system_prompt: System prompt
            mask_user: Whether to mask loss on user messages

        Returns:
            Dict with input_ids, attention_mask, and optionally labels
        """
        formatted = self.format_conversation(conversation, system_prompt)

        # Tokenize
        tokenized = self.tokenizer(
            formatted,
            truncation=True,
            max_length=self.tokenizer.model_max_length,
            return_tensors="pt"
        )

        result = {
            "input_ids": tokenized["input_ids"][0].tolist(),
            "attention_mask": tokenized["attention_mask"][0].tolist()
        }

        if mask_user:
            # Create labels with -100 for user tokens (masked)
            labels = result["input_ids"].copy()

            # Find assistant response boundaries and mask everything else
            # This is a simplified version - production code needs precise boundary detection
            result["labels"] = labels

        return result

# Dataset size guidelines
DATASET_SIZE_GUIDELINES = {
    "minimum_viable": {
        "examples": 100,
        "description": "Minimum for basic pattern learning",
        "expected_quality": "Low - may overfit or miss edge cases"
    },
    "small": {
        "examples": 500,
        "description": "Small but functional dataset",
        "expected_quality": "Moderate - covers main patterns"
    },
    "medium": {
        "examples": 5000,
        "description": "Solid instruction-tuning dataset",
        "expected_quality": "Good - handles variety well"
    },
    "large": {
        "examples": 50000,
        "description": "Comprehensive coverage",
        "expected_quality": "High - robust generalization"
    },
    "diminishing_returns": {
        "examples": 100000,
        "description": "Beyond this, quality > quantity",
        "expected_quality": "Marginal improvements past this point"
    }
}
```

### Chat Templates

```python
"""
Chat template configuration for different models
"""
from transformers import AutoTokenizer
from typing import List, Dict, Optional

class ChatTemplateManager:
    """
    Manages chat templates for different model families.
    """

    TEMPLATES = {
        "llama3": {
            "bos_token": "<|begin_of_text|>",
            "eos_token": "<|eot_id|>",
            "chat_template": """{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}{% endif %}"""
        },
        "chatml": {
            "bos_token": "",
            "eos_token": "<|im_end|>",
            "chat_template": """{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}"""
        },
        "mistral": {
            "bos_token": "<s>",
            "eos_token": "</s>",
            "chat_template": """{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}"""
        }
    }

    @classmethod
    def apply_chat_template(
        cls,
        tokenizer,
        messages: List[Dict[str, str]],
        template_name: Optional[str] = None,
        add_generation_prompt: bool = True
    ) -> str:
        """
        Apply chat template to messages.

        Args:
            tokenizer: HuggingFace tokenizer
            messages: List of message dicts
            template_name: Template to use (or auto-detect)
            add_generation_prompt: Whether to add generation prompt

        Returns:
            Formatted string
        """
        if tokenizer.chat_template:
            # Use tokenizer's built-in template
            return tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=add_generation_prompt
            )
        elif template_name and template_name in cls.TEMPLATES:
            # Apply custom template
            from jinja2 import Template
            template = Template(cls.TEMPLATES[template_name]["chat_template"])
            return template.render(
                messages=messages,
                add_generation_prompt=add_generation_prompt,
                bos_token=cls.TEMPLATES[template_name]["bos_token"],
                eos_token=cls.TEMPLATES[template_name]["eos_token"]
            )
        else:
            raise ValueError(f"No chat template available for {template_name}")

    @classmethod
    def format_system_prompt(
        cls,
        system_content: str,
        template_name: str = "llama3"
    ) -> Dict[str, str]:
        """
        Format system prompt as a message.
        """
        return {"role": "system", "content": system_content}

# Response formatting examples
RESPONSE_FORMAT_EXAMPLES = {
    "json_output": {
        "system": "You are a helpful assistant that always responds in valid JSON format.",
        "example_input": "Extract the name and age from: John is 25 years old.",
        "example_output": '{"name": "John", "age": 25}'
    },
    "chain_of_thought": {
        "system": "You are a helpful assistant. Think step by step before providing your final answer.",
        "example_input": "What is 15% of 80?",
        "example_output": """Let me solve this step by step:

1. To find 15% of 80, I need to multiply 80 by 0.15
2. 80 × 0.15 = 12

Therefore, 15% of 80 is 12."""
    },
    "structured_analysis": {
        "system": "Analyze the given text and provide a structured response with sections: Summary, Key Points, and Recommendations.",
        "example_input": "Analyze this business report...",
        "example_output": """## Summary
[Brief overview]

## Key Points
- Point 1
- Point 2
- Point 3

## Recommendations
1. Recommendation 1
2. Recommendation 2"""
    }
}
```

---

## 3.1.3 Training Configuration

### Learning Rate Selection

```python
"""
Training configuration for SFT
"""
import torch
from transformers import TrainingArguments, Trainer
from dataclasses import dataclass
from typing import Optional, Dict, List

@dataclass
class SFTConfig:
    """Configuration for supervised fine-tuning"""

    # Model
    model_name_or_path: str
    trust_remote_code: bool = False

    # Training
    num_epochs: int = 3
    per_device_train_batch_size: int = 4
    per_device_eval_batch_size: int = 4
    gradient_accumulation_steps: int = 4
    gradient_checkpointing: bool = True

    # Optimizer
    learning_rate: float = 2e-5
    weight_decay: float = 0.01
    warmup_ratio: float = 0.1
    lr_scheduler_type: str = "cosine"

    # Regularization
    max_grad_norm: float = 1.0
    label_smoothing: float = 0.0

    # Data
    max_seq_length: int = 2048
    packing: bool = False  # Pack multiple examples into single sequence

    # Evaluation
    eval_strategy: str = "steps"
    eval_steps: int = 500
    save_strategy: str = "steps"
    save_steps: int = 500
    save_total_limit: int = 3

    # Precision
    bf16: bool = True
    tf32: bool = True

# Learning rate guidelines by model size
LEARNING_RATE_GUIDELINES = {
    "1B": {"min": 5e-5, "recommended": 1e-4, "max": 2e-4},
    "7B": {"min": 1e-5, "recommended": 2e-5, "max": 5e-5},
    "13B": {"min": 5e-6, "recommended": 1e-5, "max": 2e-5},
    "70B": {"min": 1e-6, "recommended": 5e-6, "max": 1e-5},
}

def get_recommended_config(
    model_size: str,
    dataset_size: int,
    gpu_memory_gb: int = 80
) -> SFTConfig:
    """
    Get recommended SFT configuration based on model and data.

    Args:
        model_size: Model size (e.g., "7B")
        dataset_size: Number of training examples
        gpu_memory_gb: Available GPU memory

    Returns:
        Recommended SFTConfig
    """
    lr_config = LEARNING_RATE_GUIDELINES.get(model_size, LEARNING_RATE_GUIDELINES["7B"])

    # Calculate epochs based on dataset size
    if dataset_size < 1000:
        num_epochs = 5
    elif dataset_size < 10000:
        num_epochs = 3
    else:
        num_epochs = 1

    # Batch size based on GPU memory
    if gpu_memory_gb >= 80:
        batch_size = 4
        grad_accum = 4
    elif gpu_memory_gb >= 40:
        batch_size = 2
        grad_accum = 8
    else:
        batch_size = 1
        grad_accum = 16

    return SFTConfig(
        learning_rate=lr_config["recommended"],
        num_epochs=num_epochs,
        per_device_train_batch_size=batch_size,
        gradient_accumulation_steps=grad_accum,
        gradient_checkpointing=gpu_memory_gb < 80
    )

def create_training_arguments(config: SFTConfig, output_dir: str) -> TrainingArguments:
    """
    Create HuggingFace TrainingArguments from SFTConfig.
    """
    return TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=config.num_epochs,
        per_device_train_batch_size=config.per_device_train_batch_size,
        per_device_eval_batch_size=config.per_device_eval_batch_size,
        gradient_accumulation_steps=config.gradient_accumulation_steps,
        gradient_checkpointing=config.gradient_checkpointing,
        learning_rate=config.learning_rate,
        weight_decay=config.weight_decay,
        warmup_ratio=config.warmup_ratio,
        lr_scheduler_type=config.lr_scheduler_type,
        max_grad_norm=config.max_grad_norm,
        label_smoothing_factor=config.label_smoothing,
        eval_strategy=config.eval_strategy,
        eval_steps=config.eval_steps,
        save_strategy=config.save_strategy,
        save_steps=config.save_steps,
        save_total_limit=config.save_total_limit,
        bf16=config.bf16,
        tf32=config.tf32,
        logging_steps=10,
        report_to=["wandb"],
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False
    )
```

### Complete Training Script

```python
"""
Complete SFT training script
"""
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import load_dataset, Dataset
from peft import LoraConfig, get_peft_model
import wandb
from typing import Dict, List, Optional
import os

class SFTTrainer:
    """
    Complete SFT training pipeline.
    """

    def __init__(
        self,
        model_name: str,
        output_dir: str,
        config: SFTConfig
    ):
        self.model_name = model_name
        self.output_dir = output_dir
        self.config = config

        self.tokenizer = None
        self.model = None

    def load_model_and_tokenizer(self):
        """Load and prepare model and tokenizer"""
        # Load tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=self.config.trust_remote_code,
            use_fast=True
        )

        # Ensure pad token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id

        # Load model
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            trust_remote_code=self.config.trust_remote_code,
            torch_dtype=torch.bfloat16 if self.config.bf16 else torch.float32,
            attn_implementation="flash_attention_2",
            device_map="auto"
        )

        # Enable gradient checkpointing
        if self.config.gradient_checkpointing:
            self.model.gradient_checkpointing_enable()
            self.model.config.use_cache = False

        return self.model, self.tokenizer

    def prepare_dataset(
        self,
        data: List[Dict],
        format_fn: callable
    ) -> Dataset:
        """
        Prepare dataset for training.

        Args:
            data: List of training examples
            format_fn: Function to format each example

        Returns:
            HuggingFace Dataset
        """
        formatted_data = []

        for example in data:
            formatted = format_fn(example)
            tokenized = self.tokenizer(
                formatted,
                truncation=True,
                max_length=self.config.max_seq_length,
                padding=False
            )
            formatted_data.append(tokenized)

        return Dataset.from_list(formatted_data)

    def train(
        self,
        train_dataset: Dataset,
        eval_dataset: Optional[Dataset] = None
    ):
        """
        Run SFT training.

        Args:
            train_dataset: Training dataset
            eval_dataset: Optional evaluation dataset
        """
        # Create data collator
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False
        )

        # Create training arguments
        training_args = create_training_arguments(self.config, self.output_dir)

        # Create trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            data_collator=data_collator,
            tokenizer=self.tokenizer
        )

        # Train
        train_result = trainer.train()

        # Save final model
        trainer.save_model()
        self.tokenizer.save_pretrained(self.output_dir)

        # Log metrics
        metrics = train_result.metrics
        trainer.log_metrics("train", metrics)
        trainer.save_metrics("train", metrics)

        return trainer, metrics

# TRL SFTTrainer example (recommended for simplicity)
def train_with_trl(
    model_name: str,
    dataset_name: str,
    output_dir: str,
    config: SFTConfig
):
    """
    Train using TRL's SFTTrainer (simplified API).
    """
    from trl import SFTTrainer as TRLSFTTrainer, SFTConfig as TRLSFTConfig

    # Load dataset
    dataset = load_dataset(dataset_name)

    # Configure
    sft_config = TRLSFTConfig(
        output_dir=output_dir,
        num_train_epochs=config.num_epochs,
        per_device_train_batch_size=config.per_device_train_batch_size,
        gradient_accumulation_steps=config.gradient_accumulation_steps,
        learning_rate=config.learning_rate,
        max_seq_length=config.max_seq_length,
        packing=config.packing,
        dataset_text_field="text",  # Field containing formatted text
        bf16=config.bf16,
    )

    # Create trainer
    trainer = TRLSFTTrainer(
        model=model_name,
        args=sft_config,
        train_dataset=dataset["train"],
        eval_dataset=dataset.get("validation"),
    )

    # Train
    trainer.train()
    trainer.save_model()

    return trainer
```

---

## 3.1.4 Loss Function Variants

### Standard and Modified Loss Functions

```python
"""
Loss function variants for SFT
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple

class ResponseOnlyLoss(nn.Module):
    """
    Compute loss only on assistant/response tokens.
    Masks loss on system prompts, user messages, and special tokens.
    """

    def __init__(
        self,
        tokenizer,
        assistant_marker: str = "<|start_header_id|>assistant<|end_header_id|>",
        ignore_index: int = -100
    ):
        super().__init__()
        self.tokenizer = tokenizer
        self.assistant_marker = assistant_marker
        self.ignore_index = ignore_index

        # Get marker token ids
        self.assistant_marker_ids = tokenizer.encode(
            assistant_marker,
            add_special_tokens=False
        )

    def create_labels(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor
    ) -> torch.Tensor:
        """
        Create labels with user tokens masked.

        Args:
            input_ids: Token ids [batch_size, seq_length]
            attention_mask: Attention mask

        Returns:
            Labels with user tokens set to ignore_index
        """
        labels = input_ids.clone()
        batch_size, seq_length = input_ids.shape

        for batch_idx in range(batch_size):
            # Find assistant response starts
            in_response = False

            for pos in range(seq_length):
                # Check if we're at assistant marker
                if self._is_at_marker(input_ids[batch_idx], pos):
                    in_response = True
                    # Mask the marker itself
                    labels[batch_idx, pos] = self.ignore_index
                elif in_response:
                    # Check for end of response (next user turn or EOS)
                    if self._is_end_of_response(input_ids[batch_idx], pos):
                        in_response = False
                        labels[batch_idx, pos] = self.ignore_index
                else:
                    # Mask non-response tokens
                    labels[batch_idx, pos] = self.ignore_index

        # Also mask padding
        labels[attention_mask == 0] = self.ignore_index

        return labels

    def _is_at_marker(self, ids: torch.Tensor, pos: int) -> bool:
        """Check if position is at assistant marker"""
        marker_len = len(self.assistant_marker_ids)
        if pos + marker_len > len(ids):
            return False

        return ids[pos:pos + marker_len].tolist() == self.assistant_marker_ids

    def _is_end_of_response(self, ids: torch.Tensor, pos: int) -> bool:
        """Check if position is end of response"""
        return ids[pos].item() == self.tokenizer.eos_token_id

class LabelSmoothingLoss(nn.Module):
    """
    Cross-entropy loss with label smoothing.
    Prevents overconfident predictions.
    """

    def __init__(
        self,
        smoothing: float = 0.1,
        ignore_index: int = -100
    ):
        super().__init__()
        self.smoothing = smoothing
        self.ignore_index = ignore_index

    def forward(
        self,
        logits: torch.Tensor,
        labels: torch.Tensor
    ) -> torch.Tensor:
        """
        Compute label-smoothed loss.

        Args:
            logits: Model logits [batch_size, seq_length, vocab_size]
            labels: Target labels [batch_size, seq_length]

        Returns:
            Scalar loss
        """
        vocab_size = logits.size(-1)

        # Reshape for easier computation
        logits = logits.view(-1, vocab_size)
        labels = labels.view(-1)

        # Create smoothed labels
        with torch.no_grad():
            # One-hot encoding
            smoothed_labels = torch.zeros_like(logits)
            smoothed_labels.fill_(self.smoothing / (vocab_size - 1))
            smoothed_labels.scatter_(1, labels.unsqueeze(1), 1 - self.smoothing)

        # Compute loss (only on non-ignored positions)
        mask = (labels != self.ignore_index)
        log_probs = F.log_softmax(logits, dim=-1)

        loss = -(smoothed_labels * log_probs).sum(dim=-1)
        loss = loss[mask].mean()

        return loss

class LengthNormalizedLoss(nn.Module):
    """
    Normalize loss by sequence length to prevent bias toward short sequences.
    """

    def __init__(self, ignore_index: int = -100):
        super().__init__()
        self.ignore_index = ignore_index
        self.ce_loss = nn.CrossEntropyLoss(
            ignore_index=ignore_index,
            reduction='none'
        )

    def forward(
        self,
        logits: torch.Tensor,
        labels: torch.Tensor
    ) -> torch.Tensor:
        """
        Compute length-normalized loss.

        Args:
            logits: [batch_size, seq_length, vocab_size]
            labels: [batch_size, seq_length]

        Returns:
            Scalar loss (averaged across batch, normalized by sequence length)
        """
        batch_size, seq_length, vocab_size = logits.shape

        # Compute per-token loss
        logits_flat = logits.view(-1, vocab_size)
        labels_flat = labels.view(-1)

        per_token_loss = self.ce_loss(logits_flat, labels_flat)
        per_token_loss = per_token_loss.view(batch_size, seq_length)

        # Count valid tokens per sequence
        valid_mask = (labels != self.ignore_index)
        valid_counts = valid_mask.sum(dim=1).float()

        # Sum loss per sequence
        seq_loss = (per_token_loss * valid_mask).sum(dim=1)

        # Normalize by sequence length
        normalized_loss = seq_loss / valid_counts.clamp(min=1)

        return normalized_loss.mean()

class WeightedExampleLoss(nn.Module):
    """
    Weight loss by example difficulty or importance.
    """

    def __init__(
        self,
        difficulty_field: str = "difficulty",
        importance_field: str = "importance"
    ):
        super().__init__()
        self.difficulty_field = difficulty_field
        self.importance_field = importance_field
        self.ce_loss = nn.CrossEntropyLoss(reduction='none')

    def forward(
        self,
        logits: torch.Tensor,
        labels: torch.Tensor,
        weights: torch.Tensor
    ) -> torch.Tensor:
        """
        Compute weighted loss.

        Args:
            logits: [batch_size, seq_length, vocab_size]
            labels: [batch_size, seq_length]
            weights: [batch_size] per-example weights

        Returns:
            Weighted scalar loss
        """
        batch_size, seq_length, vocab_size = logits.shape

        # Compute per-token loss
        logits_flat = logits.view(-1, vocab_size)
        labels_flat = labels.view(-1)

        per_token_loss = self.ce_loss(logits_flat, labels_flat)
        per_token_loss = per_token_loss.view(batch_size, seq_length)

        # Average loss per sequence
        seq_loss = per_token_loss.mean(dim=1)

        # Apply weights
        weighted_loss = seq_loss * weights

        return weighted_loss.mean()
```

---

## 3.1.5 Infrastructure Requirements

### GPU Memory Calculator

```python
"""
Infrastructure requirements and memory estimation
"""
from dataclasses import dataclass
from typing import Dict, Optional
import math

@dataclass
class GPUMemoryEstimate:
    """GPU memory requirement estimation"""
    model_params_memory: float  # GB
    optimizer_memory: float     # GB
    gradient_memory: float      # GB
    activation_memory: float    # GB
    total_memory: float         # GB
    recommended_gpu: str

def estimate_sft_memory(
    model_params_billions: float,
    batch_size: int,
    sequence_length: int,
    gradient_checkpointing: bool = True,
    precision: str = "bf16",  # "fp32", "fp16", "bf16"
    optimizer: str = "adamw"  # "adamw", "adam", "sgd"
) -> GPUMemoryEstimate:
    """
    Estimate GPU memory requirements for SFT.

    Args:
        model_params_billions: Model size in billions of parameters
        batch_size: Batch size per GPU
        sequence_length: Maximum sequence length
        gradient_checkpointing: Whether gradient checkpointing is enabled
        precision: Training precision
        optimizer: Optimizer type

    Returns:
        GPUMemoryEstimate with breakdown
    """
    params = model_params_billions * 1e9

    # Bytes per parameter
    param_bytes = {"fp32": 4, "fp16": 2, "bf16": 2}[precision]

    # Model parameters memory
    model_memory = params * param_bytes / 1e9  # GB

    # Optimizer state memory
    if optimizer in ["adam", "adamw"]:
        # Adam stores m and v (2 states per param) in fp32
        optimizer_memory = params * 4 * 2 / 1e9
    else:  # SGD
        optimizer_memory = params * 4 / 1e9

    # Gradient memory (same as model in same precision)
    gradient_memory = model_memory

    # Activation memory (rough estimate)
    # Depends heavily on architecture, this is approximate
    hidden_size = int(math.sqrt(params / 12))  # Rough estimate
    num_layers = int(params / (12 * hidden_size ** 2))

    if gradient_checkpointing:
        # Only store activations at checkpoint boundaries
        activation_memory = (
            batch_size * sequence_length * hidden_size * param_bytes *
            math.sqrt(num_layers) / 1e9
        )
    else:
        # Store all activations
        activation_memory = (
            batch_size * sequence_length * hidden_size * param_bytes *
            num_layers * 2 / 1e9  # *2 for attention + FFN
        )

    total_memory = model_memory + optimizer_memory + gradient_memory + activation_memory

    # Add 20% buffer for other allocations
    total_memory *= 1.2

    # Recommend GPU
    if total_memory <= 24:
        recommended_gpu = "NVIDIA RTX 4090 (24GB)"
    elif total_memory <= 40:
        recommended_gpu = "NVIDIA A100 40GB"
    elif total_memory <= 80:
        recommended_gpu = "NVIDIA A100 80GB / H100"
    else:
        recommended_gpu = f"Multiple GPUs required ({math.ceil(total_memory / 80)} x H100)"

    return GPUMemoryEstimate(
        model_params_memory=model_memory,
        optimizer_memory=optimizer_memory,
        gradient_memory=gradient_memory,
        activation_memory=activation_memory,
        total_memory=total_memory,
        recommended_gpu=recommended_gpu
    )

# Memory requirements by model size
SFT_MEMORY_REQUIREMENTS = {
    "1B": {
        "full_ft_bf16": "~8GB",
        "full_ft_fp32": "~16GB",
        "lora_bf16": "~4GB",
        "qlora_4bit": "~2GB"
    },
    "7B": {
        "full_ft_bf16": "~56GB",
        "full_ft_fp32": "~112GB",
        "lora_bf16": "~16GB",
        "qlora_4bit": "~6GB"
    },
    "13B": {
        "full_ft_bf16": "~104GB",
        "full_ft_fp32": "~208GB",
        "lora_bf16": "~28GB",
        "qlora_4bit": "~10GB"
    },
    "70B": {
        "full_ft_bf16": "~560GB (8x H100)",
        "full_ft_fp32": "Not practical",
        "lora_bf16": "~150GB (2x H100)",
        "qlora_4bit": "~40GB"
    }
}

# Example
estimate = estimate_sft_memory(
    model_params_billions=7,
    batch_size=4,
    sequence_length=2048,
    gradient_checkpointing=True,
    precision="bf16"
)

print(f"Memory Estimate for 7B model SFT:")
print(f"  Model: {estimate.model_params_memory:.1f} GB")
print(f"  Optimizer: {estimate.optimizer_memory:.1f} GB")
print(f"  Gradients: {estimate.gradient_memory:.1f} GB")
print(f"  Activations: {estimate.activation_memory:.1f} GB")
print(f"  Total: {estimate.total_memory:.1f} GB")
print(f"  Recommended: {estimate.recommended_gpu}")
```

---

## 3.1.6 Evaluation During SFT

### Evaluation Framework

```python
"""
SFT evaluation framework
"""
import torch
from transformers import GenerationConfig
from typing import List, Dict, Optional, Callable
from dataclasses import dataclass
import numpy as np

@dataclass
class EvaluationResult:
    """Results from SFT evaluation"""
    loss: float
    perplexity: float
    generation_metrics: Dict[str, float]
    task_metrics: Dict[str, float]

class SFTEvaluator:
    """
    Comprehensive evaluator for SFT models.
    """

    def __init__(
        self,
        model,
        tokenizer,
        eval_tasks: List[str] = None
    ):
        self.model = model
        self.tokenizer = tokenizer
        self.eval_tasks = eval_tasks or ["helpfulness", "format_adherence", "safety"]

    def evaluate_loss(
        self,
        eval_dataloader: torch.utils.data.DataLoader
    ) -> Dict[str, float]:
        """
        Compute evaluation loss and perplexity.
        """
        self.model.eval()
        total_loss = 0.0
        total_tokens = 0

        with torch.no_grad():
            for batch in eval_dataloader:
                batch = {k: v.to(self.model.device) for k, v in batch.items()}
                outputs = self.model(**batch)

                # Count valid tokens
                valid_tokens = (batch["labels"] != -100).sum().item()
                total_loss += outputs.loss.item() * valid_tokens
                total_tokens += valid_tokens

        avg_loss = total_loss / total_tokens
        perplexity = np.exp(avg_loss)

        return {"loss": avg_loss, "perplexity": perplexity}

    def evaluate_generation(
        self,
        test_prompts: List[str],
        generation_config: Optional[GenerationConfig] = None
    ) -> Dict[str, float]:
        """
        Evaluate generation quality.
        """
        if generation_config is None:
            generation_config = GenerationConfig(
                max_new_tokens=512,
                temperature=0.7,
                do_sample=True,
                top_p=0.9
            )

        self.model.eval()
        generations = []

        for prompt in test_prompts:
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt"
            ).to(self.model.device)

            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    generation_config=generation_config
                )

            response = self.tokenizer.decode(
                outputs[0][inputs["input_ids"].shape[1]:],
                skip_special_tokens=True
            )
            generations.append(response)

        # Compute generation metrics
        metrics = {}

        # Average length
        metrics["avg_response_length"] = np.mean([len(g) for g in generations])

        # Non-empty rate
        metrics["non_empty_rate"] = np.mean([len(g.strip()) > 0 for g in generations])

        # Repetition rate (simple n-gram)
        def repetition_rate(text: str, n: int = 3) -> float:
            words = text.split()
            if len(words) < n:
                return 0.0
            ngrams = [tuple(words[i:i+n]) for i in range(len(words) - n + 1)]
            unique = len(set(ngrams))
            return 1 - (unique / len(ngrams)) if ngrams else 0.0

        metrics["avg_repetition_rate"] = np.mean([repetition_rate(g) for g in generations])

        return metrics

    def evaluate_task_performance(
        self,
        task_datasets: Dict[str, List[Dict]]
    ) -> Dict[str, float]:
        """
        Evaluate on held-out task benchmarks.
        """
        results = {}

        for task_name, examples in task_datasets.items():
            correct = 0

            for example in examples:
                # Generate response
                inputs = self.tokenizer(
                    example["prompt"],
                    return_tensors="pt"
                ).to(self.model.device)

                with torch.no_grad():
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=256,
                        temperature=0.1
                    )

                response = self.tokenizer.decode(
                    outputs[0][inputs["input_ids"].shape[1]:],
                    skip_special_tokens=True
                )

                # Check if correct (exact match or contains)
                if example.get("exact_match"):
                    correct += int(response.strip() == example["answer"].strip())
                else:
                    correct += int(example["answer"].lower() in response.lower())

            results[f"{task_name}_accuracy"] = correct / len(examples)

        return results

    def detect_overfitting(
        self,
        train_loss_history: List[float],
        eval_loss_history: List[float],
        threshold: float = 0.1
    ) -> Dict[str, bool]:
        """
        Detect signs of overfitting.

        Args:
            train_loss_history: Training loss per step/epoch
            eval_loss_history: Evaluation loss per evaluation
            threshold: Difference threshold for overfitting detection

        Returns:
            Dictionary with overfitting indicators
        """
        results = {}

        # Check if eval loss increasing while train decreasing
        if len(eval_loss_history) >= 3:
            recent_eval = eval_loss_history[-3:]
            eval_trend = np.polyfit(range(len(recent_eval)), recent_eval, 1)[0]
            results["eval_loss_increasing"] = eval_trend > 0

            # Check gap between train and eval
            train_loss_at_eval = train_loss_history[-1]
            eval_loss = eval_loss_history[-1]
            gap = eval_loss - train_loss_at_eval

            results["large_train_eval_gap"] = gap > threshold
            results["likely_overfitting"] = (
                results["eval_loss_increasing"] and results["large_train_eval_gap"]
            )
        else:
            results["eval_loss_increasing"] = False
            results["large_train_eval_gap"] = False
            results["likely_overfitting"] = False

        return results

# Evaluation schedule
EVALUATION_SCHEDULE = {
    "every_n_steps": 500,
    "metrics_to_track": [
        "eval_loss",
        "eval_perplexity",
        "generation_quality",
        "format_adherence"
    ],
    "early_stopping": {
        "patience": 3,
        "metric": "eval_loss",
        "mode": "min",
        "min_delta": 0.01
    }
}
```

---

## Troubleshooting

### Common SFT Issues

| Issue | Symptoms | Solution |
|-------|----------|----------|
| Overfitting | Train loss decreasing, eval loss increasing | Reduce epochs, add regularization, use more data |
| Catastrophic forgetting | Loss on general tasks increases | Add data replay, use lower learning rate |
| Format inconsistency | Model doesn't follow expected format | Add more format examples, use response-only loss |
| Repetitive outputs | Model generates loops | Lower temperature, add repetition penalty |
| Training divergence | Loss goes to NaN | Lower learning rate, check data quality |

### Debugging Checklist

```markdown
## SFT Debugging Checklist

### Data Quality
- [ ] Verify data format is correct
- [ ] Check for encoding issues
- [ ] Validate special tokens are preserved
- [ ] Ensure responses don't exceed max length
- [ ] Check for duplicate examples

### Training Configuration
- [ ] Learning rate in recommended range
- [ ] Batch size fits in memory
- [ ] Gradient accumulation correct
- [ ] Warmup steps appropriate
- [ ] Weight decay not too high

### Evaluation
- [ ] Validation set is representative
- [ ] Not evaluating on training data
- [ ] Generation config matches use case
- [ ] Monitoring right metrics

### Model Behavior
- [ ] Model follows system prompt
- [ ] Responses in expected format
- [ ] No repetition issues
- [ ] Appropriate response length
```

---

## Glossary

| Term | Definition |
|------|------------|
| **SFT** | Supervised Fine-Tuning - training on input-output pairs |
| **Instruction Tuning** | Fine-tuning to follow instructions |
| **Chat Template** | Format for multi-turn conversations |
| **Response Masking** | Only computing loss on assistant responses |
| **Label Smoothing** | Regularization by softening target distribution |
| **Catastrophic Forgetting** | Loss of pre-training knowledge during fine-tuning |

---

## References

1. Wei, J., et al. (2021). "Finetuned Language Models Are Zero-Shot Learners"
2. Chung, H., et al. (2022). "Scaling Instruction-Finetuned Language Models"
3. Wang, Y., et al. (2023). "Self-Instruct: Aligning Language Models with Self-Generated Instructions"
4. Taori, R., et al. (2023). "Stanford Alpaca: An Instruction-following LLaMA model"
5. HuggingFace. (2024). "SFT Trainer Documentation"
6. Tunstall, L., et al. (2023). "Zephyr: Direct Distillation of LM Alignment"

---

> **Navigation**
> [← 2.6 Ray Train](../02_model_training/2.6_ray_train_guide.md) | **[Index](../README.md#15-repository-structure)** | [3.2 PEFT →](3.2_parameter_efficient_fine_tuning.md)
