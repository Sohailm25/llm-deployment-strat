> **Navigation** | [← 7.3 Chunking](7.3_chunking_strategies_guide.md) | [7.5 RAG Evaluation →](7.5_rag_evaluation_guide.md)
>
> | | |
> |---|---|
> | **Prerequisites** | [7.1-7.3 RAG Foundations](7.1_vector_database_guide.md) |
> | **Related** | [7.5 RAG Evaluation](7.5_rag_evaluation_guide.md) &#124; [5.1 Evaluation Framework](../05_evaluation_testing/5.1_llm_evaluation_framework.md) |
> | **Next** | [7.5 RAG Evaluation](7.5_rag_evaluation_guide.md) |

# Document 7.4: Retrieval & Reranking Guide

## Executive Summary

Effective RAG systems require optimized retrieval pipelines that balance precision, recall, and latency. This guide covers retrieval strategies (dense, sparse, hybrid), reranking techniques (cross-encoders, ColBERT, LLM-based), query processing (expansion, decomposition), and production optimization. Hybrid search with reranking typically improves NDCG@10 by 10-25% over single-method retrieval.

## Prerequisites

- Understanding of embedding models and vector search
- Familiarity with BM25 and traditional IR
- Knowledge of transformer architectures
- Experience with vector databases
- Completion of documents 7.1-7.3 recommended

---

## 7.4.1 Retrieval Fundamentals

### Retrieval Strategies Overview

```python
"""
ABOUTME: Core retrieval strategies for RAG systems.
ABOUTME: Implements dense, sparse, and hybrid retrieval methods.
"""

import numpy as np
from typing import List, Dict, Any, Optional, Tuple, Union
from dataclasses import dataclass, field
from enum import Enum
from abc import ABC, abstractmethod
import math


class RetrievalMethod(Enum):
    """Available retrieval methods."""
    DENSE = "dense"                 # Vector similarity search
    SPARSE = "sparse"               # BM25/keyword search
    HYBRID = "hybrid"               # Combined dense + sparse
    MULTI_VECTOR = "multi_vector"   # ColBERT-style
    LEARNED_SPARSE = "learned_sparse"  # SPLADE


@dataclass
class RetrievalConfig:
    """Configuration for retrieval pipeline."""
    # Method selection
    method: RetrievalMethod = RetrievalMethod.HYBRID
    dense_weight: float = 0.7      # For hybrid fusion
    sparse_weight: float = 0.3

    # Search parameters
    top_k: int = 100               # Initial retrieval
    final_k: int = 10              # After reranking

    # Dense search
    similarity_metric: str = "cosine"
    ef_search: int = 100           # HNSW parameter

    # Sparse search
    bm25_k1: float = 1.5
    bm25_b: float = 0.75

    # Reranking
    use_reranking: bool = True
    reranker_model: str = "cross-encoder/ms-marco-MiniLM-L-6-v2"


@dataclass
class RetrievalResult:
    """Single retrieval result."""
    id: str
    content: str
    score: float
    metadata: Dict[str, Any] = field(default_factory=dict)
    dense_score: Optional[float] = None
    sparse_score: Optional[float] = None
    rerank_score: Optional[float] = None


class BaseRetriever(ABC):
    """Abstract base class for retrievers."""

    @abstractmethod
    def retrieve(
        self,
        query: str,
        top_k: int = 10,
        filter: Optional[Dict[str, Any]] = None
    ) -> List[RetrievalResult]:
        """Retrieve relevant documents."""
        pass

    @abstractmethod
    def index(self, documents: List[Dict[str, Any]]) -> int:
        """Index documents for retrieval."""
        pass


class DenseRetriever(BaseRetriever):
    """
    Dense vector retrieval using embedding similarity.

    Encodes queries and documents into dense vectors,
    retrieves by nearest neighbor search.
    """

    def __init__(
        self,
        embedding_model: Any,
        vector_store: Any,
        config: RetrievalConfig
    ):
        self.embedding_model = embedding_model
        self.vector_store = vector_store
        self.config = config

    def retrieve(
        self,
        query: str,
        top_k: int = 10,
        filter: Optional[Dict[str, Any]] = None
    ) -> List[RetrievalResult]:
        """Retrieve by dense vector similarity."""
        # Encode query
        query_embedding = self.embedding_model.encode_queries([query])[0]

        # Search vector store
        results = self.vector_store.search(
            query_vector=query_embedding,
            top_k=top_k,
            filter=filter
        )

        return [
            RetrievalResult(
                id=r.id,
                content=r.text or r.metadata.get('text', ''),
                score=r.score,
                metadata=r.metadata,
                dense_score=r.score
            )
            for r in results
        ]

    def index(self, documents: List[Dict[str, Any]]) -> int:
        """Index documents with dense embeddings."""
        texts = [d.get('content', d.get('text', '')) for d in documents]
        embeddings = self.embedding_model.encode_documents(texts)

        records = [
            {
                'id': d.get('id', str(i)),
                'vector': embeddings[i],
                'metadata': {k: v for k, v in d.items() if k not in ['content', 'text']},
                'text': texts[i]
            }
            for i, d in enumerate(documents)
        ]

        return self.vector_store.insert(records)


class BM25Retriever(BaseRetriever):
    """
    Sparse retrieval using BM25 algorithm.

    BM25 scores based on term frequency, document length,
    and inverse document frequency.
    """

    def __init__(self, config: RetrievalConfig):
        self.config = config
        self.k1 = config.bm25_k1
        self.b = config.bm25_b

        self.documents = []
        self.doc_freqs = {}      # term -> doc count
        self.doc_lens = []
        self.avg_doc_len = 0
        self.inverted_index = {}  # term -> [(doc_id, term_freq)]

    def _tokenize(self, text: str) -> List[str]:
        """Simple tokenization."""
        import re
        text = text.lower()
        tokens = re.findall(r'\b\w+\b', text)
        return tokens

    def index(self, documents: List[Dict[str, Any]]) -> int:
        """Build BM25 index."""
        self.documents = []
        self.inverted_index = {}
        self.doc_freqs = {}
        self.doc_lens = []

        for i, doc in enumerate(documents):
            text = doc.get('content', doc.get('text', ''))
            tokens = self._tokenize(text)

            self.documents.append({
                'id': doc.get('id', str(i)),
                'text': text,
                'metadata': {k: v for k, v in doc.items() if k not in ['content', 'text']}
            })
            self.doc_lens.append(len(tokens))

            # Build inverted index
            term_freqs = {}
            for token in tokens:
                term_freqs[token] = term_freqs.get(token, 0) + 1

            for term, freq in term_freqs.items():
                if term not in self.inverted_index:
                    self.inverted_index[term] = []
                    self.doc_freqs[term] = 0
                self.inverted_index[term].append((i, freq))
                self.doc_freqs[term] += 1

        self.avg_doc_len = sum(self.doc_lens) / len(self.doc_lens) if self.doc_lens else 0

        return len(documents)

    def retrieve(
        self,
        query: str,
        top_k: int = 10,
        filter: Optional[Dict[str, Any]] = None
    ) -> List[RetrievalResult]:
        """Retrieve using BM25 scoring."""
        query_tokens = self._tokenize(query)
        n_docs = len(self.documents)

        scores = {}

        for token in query_tokens:
            if token not in self.inverted_index:
                continue

            # IDF component
            df = self.doc_freqs[token]
            idf = math.log((n_docs - df + 0.5) / (df + 0.5) + 1)

            for doc_id, tf in self.inverted_index[token]:
                # BM25 score
                doc_len = self.doc_lens[doc_id]
                numerator = tf * (self.k1 + 1)
                denominator = tf + self.k1 * (1 - self.b + self.b * doc_len / self.avg_doc_len)
                score = idf * numerator / denominator

                scores[doc_id] = scores.get(doc_id, 0) + score

        # Sort by score
        ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_k]

        results = []
        for doc_id, score in ranked:
            doc = self.documents[doc_id]

            # Apply filter if provided
            if filter:
                match = all(
                    doc.get('metadata', {}).get(k) == v
                    for k, v in filter.items()
                )
                if not match:
                    continue

            results.append(RetrievalResult(
                id=doc['id'],
                content=doc['text'],
                score=score,
                metadata=doc.get('metadata', {}),
                sparse_score=score
            ))

        return results


class HybridRetriever(BaseRetriever):
    """
    Hybrid retrieval combining dense and sparse methods.

    Fuses results using Reciprocal Rank Fusion (RRF) or
    weighted score combination.
    """

    def __init__(
        self,
        dense_retriever: DenseRetriever,
        sparse_retriever: BM25Retriever,
        config: RetrievalConfig,
        fusion_method: str = "rrf"  # rrf or weighted
    ):
        self.dense = dense_retriever
        self.sparse = sparse_retriever
        self.config = config
        self.fusion_method = fusion_method

    def retrieve(
        self,
        query: str,
        top_k: int = 10,
        filter: Optional[Dict[str, Any]] = None
    ) -> List[RetrievalResult]:
        """
        Retrieve using both dense and sparse methods.

        Fusion strategies:
        - RRF: Reciprocal Rank Fusion (position-based)
        - Weighted: Score-based combination
        """
        # Get results from both retrievers
        dense_k = min(top_k * 3, self.config.top_k)
        sparse_k = min(top_k * 3, self.config.top_k)

        dense_results = self.dense.retrieve(query, dense_k, filter)
        sparse_results = self.sparse.retrieve(query, sparse_k, filter)

        if self.fusion_method == "rrf":
            return self._rrf_fusion(dense_results, sparse_results, top_k)
        else:
            return self._weighted_fusion(dense_results, sparse_results, top_k)

    def _rrf_fusion(
        self,
        dense_results: List[RetrievalResult],
        sparse_results: List[RetrievalResult],
        top_k: int,
        k: int = 60  # RRF constant
    ) -> List[RetrievalResult]:
        """
        Reciprocal Rank Fusion.

        RRF score = sum(1 / (k + rank)) for each result list
        """
        scores = {}
        results_by_id = {}

        # Score from dense results
        for rank, result in enumerate(dense_results):
            scores[result.id] = scores.get(result.id, 0) + 1 / (k + rank + 1)
            if result.id not in results_by_id:
                results_by_id[result.id] = result

        # Score from sparse results
        for rank, result in enumerate(sparse_results):
            scores[result.id] = scores.get(result.id, 0) + 1 / (k + rank + 1)
            if result.id not in results_by_id:
                results_by_id[result.id] = result
            else:
                # Update with sparse score
                results_by_id[result.id].sparse_score = result.sparse_score

        # Rank by RRF score
        ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_k]

        final_results = []
        for doc_id, rrf_score in ranked:
            result = results_by_id[doc_id]
            result.score = rrf_score
            final_results.append(result)

        return final_results

    def _weighted_fusion(
        self,
        dense_results: List[RetrievalResult],
        sparse_results: List[RetrievalResult],
        top_k: int
    ) -> List[RetrievalResult]:
        """
        Weighted score fusion.

        Normalizes scores and combines with configured weights.
        """
        # Normalize dense scores
        dense_scores = [r.dense_score for r in dense_results if r.dense_score]
        if dense_scores:
            dense_min, dense_max = min(dense_scores), max(dense_scores)
            dense_range = dense_max - dense_min or 1

        # Normalize sparse scores
        sparse_scores = [r.sparse_score for r in sparse_results if r.sparse_score]
        if sparse_scores:
            sparse_min, sparse_max = min(sparse_scores), max(sparse_scores)
            sparse_range = sparse_max - sparse_min or 1

        scores = {}
        results_by_id = {}

        for result in dense_results:
            norm_score = (result.dense_score - dense_min) / dense_range if dense_scores else 0
            scores[result.id] = self.config.dense_weight * norm_score
            results_by_id[result.id] = result

        for result in sparse_results:
            norm_score = (result.sparse_score - sparse_min) / sparse_range if sparse_scores else 0
            scores[result.id] = scores.get(result.id, 0) + self.config.sparse_weight * norm_score
            if result.id not in results_by_id:
                results_by_id[result.id] = result
            else:
                results_by_id[result.id].sparse_score = result.sparse_score

        ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_k]

        final_results = []
        for doc_id, combined_score in ranked:
            result = results_by_id[doc_id]
            result.score = combined_score
            final_results.append(result)

        return final_results

    def index(self, documents: List[Dict[str, Any]]) -> int:
        """Index documents in both retrievers."""
        dense_count = self.dense.index(documents)
        sparse_count = self.sparse.index(documents)
        return dense_count
```

---

## 7.4.2 Reranking

### Cross-Encoder Reranking

```python
"""
ABOUTME: Reranking implementations for improving retrieval precision.
ABOUTME: Covers cross-encoders, ColBERT, and LLM-based reranking.
"""

class CrossEncoderReranker:
    """
    Cross-encoder reranking for improved relevance.

    Cross-encoders jointly encode query and document,
    capturing fine-grained interactions but at higher cost.
    """

    def __init__(
        self,
        model_name: str = "cross-encoder/ms-marco-MiniLM-L-6-v2"
    ):
        from sentence_transformers import CrossEncoder
        self.model = CrossEncoder(model_name)

    def rerank(
        self,
        query: str,
        results: List[RetrievalResult],
        top_k: Optional[int] = None
    ) -> List[RetrievalResult]:
        """
        Rerank results using cross-encoder.

        Cross-encoder processes (query, document) pairs together,
        enabling richer interaction modeling than bi-encoders.
        """
        if not results:
            return results

        # Create pairs
        pairs = [(query, r.content) for r in results]

        # Get cross-encoder scores
        scores = self.model.predict(pairs)

        # Update results with rerank scores
        for result, score in zip(results, scores):
            result.rerank_score = float(score)

        # Sort by rerank score
        reranked = sorted(results, key=lambda x: x.rerank_score, reverse=True)

        if top_k:
            reranked = reranked[:top_k]

        # Update final scores
        for i, result in enumerate(reranked):
            result.score = result.rerank_score

        return reranked


class ColBERTReranker:
    """
    ColBERT-style late interaction reranking.

    More efficient than cross-encoders while capturing
    token-level interactions.
    """

    def __init__(
        self,
        model_name: str = "colbert-ir/colbertv2.0"
    ):
        self.model_name = model_name
        self._init_model()

    def _init_model(self):
        """Initialize ColBERT model."""
        try:
            from colbert import Searcher
            from colbert.infra import ColBERTConfig

            self.config = ColBERTConfig(
                checkpoint=self.model_name
            )
            # Note: Full ColBERT requires index setup
            # This is a simplified version
            from sentence_transformers import SentenceTransformer
            self.encoder = SentenceTransformer(
                'sentence-transformers/all-MiniLM-L6-v2'
            )
        except ImportError:
            from sentence_transformers import SentenceTransformer
            self.encoder = SentenceTransformer(
                'sentence-transformers/all-MiniLM-L6-v2'
            )

    def rerank(
        self,
        query: str,
        results: List[RetrievalResult],
        top_k: Optional[int] = None
    ) -> List[RetrievalResult]:
        """
        Rerank using late interaction scoring.

        ColBERT computes token-level embeddings and uses
        MaxSim scoring for fine-grained matching.
        """
        if not results:
            return results

        # Encode query tokens
        query_embedding = self.encoder.encode(
            query,
            output_value='token_embeddings'
        ) if hasattr(self.encoder, 'encode') else self.encoder.encode([query])[0]

        for result in results:
            # Encode document tokens
            doc_embedding = self.encoder.encode(
                result.content,
                output_value='token_embeddings'
            ) if hasattr(self.encoder, 'encode') else self.encoder.encode([result.content])[0]

            # MaxSim scoring (simplified)
            # For each query token, find max similarity with any doc token
            if isinstance(query_embedding, np.ndarray) and len(query_embedding.shape) == 2:
                # Token-level embeddings
                similarity_matrix = query_embedding @ doc_embedding.T
                max_sims = similarity_matrix.max(axis=1)
                result.rerank_score = float(max_sims.mean())
            else:
                # Fallback to single vector similarity
                similarity = np.dot(query_embedding, doc_embedding) / (
                    np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding)
                )
                result.rerank_score = float(similarity)

        # Sort by rerank score
        reranked = sorted(results, key=lambda x: x.rerank_score, reverse=True)

        if top_k:
            reranked = reranked[:top_k]

        for result in reranked:
            result.score = result.rerank_score

        return reranked


class LLMReranker:
    """
    LLM-based reranking using relevance judgments.

    Uses LLM to assess query-document relevance,
    highest quality but slowest.
    """

    def __init__(
        self,
        llm_client: Any,
        batch_size: int = 5
    ):
        self.llm = llm_client
        self.batch_size = batch_size

    def rerank(
        self,
        query: str,
        results: List[RetrievalResult],
        top_k: Optional[int] = None
    ) -> List[RetrievalResult]:
        """
        Rerank using LLM relevance scoring.
        """
        if not results:
            return results

        # Score in batches
        for i in range(0, len(results), self.batch_size):
            batch = results[i:i + self.batch_size]
            scores = self._score_batch(query, batch)

            for result, score in zip(batch, scores):
                result.rerank_score = score

        # Sort by score
        reranked = sorted(
            results,
            key=lambda x: x.rerank_score if x.rerank_score else 0,
            reverse=True
        )

        if top_k:
            reranked = reranked[:top_k]

        for result in reranked:
            result.score = result.rerank_score

        return reranked

    def _score_batch(
        self,
        query: str,
        results: List[RetrievalResult]
    ) -> List[float]:
        """Score a batch of results."""
        prompt = f"""Rate the relevance of each passage to the query on a scale of 0-10.

Query: {query}

"""
        for i, result in enumerate(results):
            prompt += f"Passage {i+1}: {result.content[:500]}...\n\n"

        prompt += """
Respond with only the scores in format:
1: [score]
2: [score]
..."""

        response = self.llm.generate(prompt)

        # Parse scores
        scores = []
        import re
        for line in response.split('\n'):
            match = re.search(r'(\d+):\s*(\d+(?:\.\d+)?)', line)
            if match:
                scores.append(float(match.group(2)) / 10)  # Normalize to 0-1

        # Pad with zeros if parsing failed
        while len(scores) < len(results):
            scores.append(0.0)

        return scores[:len(results)]


class RerankingPipeline:
    """
    Multi-stage reranking pipeline.

    Combines multiple rerankers for optimal quality-latency tradeoff.
    """

    def __init__(
        self,
        stages: List[Tuple[Any, int]]  # [(reranker, top_k), ...]
    ):
        self.stages = stages

    def rerank(
        self,
        query: str,
        results: List[RetrievalResult]
    ) -> List[RetrievalResult]:
        """Apply multi-stage reranking."""
        current_results = results

        for reranker, top_k in self.stages:
            current_results = reranker.rerank(query, current_results, top_k)

        return current_results
```

---

## 7.4.3 Query Processing

### Query Enhancement

```python
"""
ABOUTME: Query processing and enhancement for improved retrieval.
ABOUTME: Implements query expansion, decomposition, and rewriting.
"""

class QueryExpander:
    """
    Expand queries for better recall.

    Techniques:
    - Synonym expansion
    - LLM-based expansion
    - Pseudo-relevance feedback
    """

    def __init__(
        self,
        llm_client: Optional[Any] = None,
        embedding_model: Optional[Any] = None
    ):
        self.llm = llm_client
        self.embedding_model = embedding_model

    def expand_with_synonyms(
        self,
        query: str,
        num_synonyms: int = 2
    ) -> str:
        """Expand query with synonyms."""
        try:
            from nltk.corpus import wordnet
            import nltk
            nltk.download('wordnet', quiet=True)

            words = query.split()
            expanded_words = []

            for word in words:
                synsets = wordnet.synsets(word)
                synonyms = set()

                for syn in synsets[:2]:
                    for lemma in syn.lemmas()[:num_synonyms]:
                        if lemma.name() != word:
                            synonyms.add(lemma.name().replace('_', ' '))

                if synonyms:
                    expanded_words.append(f"({word} OR {' OR '.join(list(synonyms)[:num_synonyms])})")
                else:
                    expanded_words.append(word)

            return ' '.join(expanded_words)

        except ImportError:
            return query

    def expand_with_llm(
        self,
        query: str,
        num_expansions: int = 3
    ) -> List[str]:
        """
        Use LLM to generate query variations.

        Returns multiple query formulations for multi-query retrieval.
        """
        if not self.llm:
            return [query]

        prompt = f"""Generate {num_expansions} alternative search queries that capture the same information need as the original query.

Original query: {query}

Return only the alternative queries, one per line."""

        response = self.llm.generate(prompt)

        expansions = [query]  # Include original
        for line in response.strip().split('\n'):
            line = line.strip().lstrip('0123456789.-) ')
            if line and line != query:
                expansions.append(line)

        return expansions[:num_expansions + 1]

    def pseudo_relevance_feedback(
        self,
        query: str,
        retriever: BaseRetriever,
        top_k: int = 3,
        num_terms: int = 5
    ) -> str:
        """
        Expand query using top retrieved documents.

        Rocchio-style pseudo-relevance feedback.
        """
        # Get initial results
        results = retriever.retrieve(query, top_k)

        if not results:
            return query

        # Extract key terms from top results
        from collections import Counter
        import re

        term_counts = Counter()
        query_terms = set(query.lower().split())

        for result in results:
            words = re.findall(r'\b\w+\b', result.content.lower())
            for word in words:
                if word not in query_terms and len(word) > 3:
                    term_counts[word] += 1

        # Get top expansion terms
        expansion_terms = [term for term, _ in term_counts.most_common(num_terms)]

        # Combine with original query
        expanded = f"{query} {' '.join(expansion_terms)}"

        return expanded


class QueryDecomposer:
    """
    Decompose complex queries into sub-queries.

    Useful for multi-hop questions that require
    information from multiple sources.
    """

    def __init__(self, llm_client: Any):
        self.llm = llm_client

    def decompose(
        self,
        query: str,
        max_sub_queries: int = 3
    ) -> List[str]:
        """
        Decompose complex query into simpler sub-queries.
        """
        prompt = f"""Break down this complex question into simpler sub-questions that can be answered independently.

Question: {query}

Return up to {max_sub_queries} sub-questions, one per line. If the question is already simple, return it as-is."""

        response = self.llm.generate(prompt)

        sub_queries = []
        for line in response.strip().split('\n'):
            line = line.strip().lstrip('0123456789.-) ')
            if line and '?' in line or len(line) > 10:
                sub_queries.append(line)

        return sub_queries[:max_sub_queries] if sub_queries else [query]

    def decompose_with_dependencies(
        self,
        query: str
    ) -> List[Dict[str, Any]]:
        """
        Decompose with dependency tracking.

        Returns sub-queries with their dependencies on previous answers.
        """
        prompt = f"""Analyze this question and break it into steps. For each step, note if it depends on previous steps.

Question: {query}

Format:
Step 1: [sub-question]
Depends on: None
Step 2: [sub-question]
Depends on: Step 1
..."""

        response = self.llm.generate(prompt)

        steps = []
        current_step = {}

        for line in response.strip().split('\n'):
            if line.startswith('Step'):
                if current_step:
                    steps.append(current_step)
                current_step = {
                    'query': line.split(':', 1)[1].strip() if ':' in line else '',
                    'depends_on': []
                }
            elif line.startswith('Depends on:'):
                deps = line.split(':', 1)[1].strip()
                if deps.lower() != 'none':
                    # Parse step numbers
                    import re
                    dep_nums = re.findall(r'Step (\d+)', deps)
                    current_step['depends_on'] = [int(n) - 1 for n in dep_nums]

        if current_step:
            steps.append(current_step)

        return steps


class QueryRewriter:
    """
    Rewrite queries for better retrieval.

    Transforms queries to match document language
    or fix common issues.
    """

    def __init__(self, llm_client: Optional[Any] = None):
        self.llm = llm_client

    def rewrite_for_retrieval(
        self,
        query: str,
        context: Optional[str] = None
    ) -> str:
        """
        Rewrite conversational query for retrieval.

        Handles:
        - Pronoun resolution
        - Query clarification
        - Adding implicit context
        """
        if not self.llm:
            return query

        prompt = f"""Rewrite this query to be more suitable for document retrieval. Make it clear, specific, and self-contained.

Query: {query}
"""
        if context:
            prompt += f"\nConversation context: {context}"

        prompt += "\n\nRewritten query:"

        response = self.llm.generate(prompt)
        return response.strip()

    def hypothetical_document(
        self,
        query: str
    ) -> str:
        """
        Generate hypothetical document that would answer the query.

        HyDE (Hypothetical Document Embeddings) technique.
        """
        if not self.llm:
            return query

        prompt = f"""Write a short passage that would perfectly answer this question:

Question: {query}

Passage:"""

        response = self.llm.generate(prompt)
        return response.strip()
```

---

## 7.4.4 Advanced Retrieval Patterns

### Multi-Stage Retrieval

```python
"""
ABOUTME: Advanced retrieval patterns for production RAG systems.
ABOUTME: Implements multi-stage, contextual, and iterative retrieval.
"""

class MultiStageRetriever:
    """
    Multi-stage retrieval pipeline.

    Stage 1: Fast, high-recall retrieval (BM25 or approximate NN)
    Stage 2: Precise reranking (cross-encoder)
    Stage 3: Optional LLM filtering
    """

    def __init__(
        self,
        first_stage: BaseRetriever,
        reranker: Any,
        llm_filter: Optional[Any] = None,
        config: RetrievalConfig = None
    ):
        self.first_stage = first_stage
        self.reranker = reranker
        self.llm_filter = llm_filter
        self.config = config or RetrievalConfig()

    def retrieve(
        self,
        query: str,
        filter: Optional[Dict[str, Any]] = None
    ) -> List[RetrievalResult]:
        """Execute multi-stage retrieval."""
        # Stage 1: High recall retrieval
        candidates = self.first_stage.retrieve(
            query,
            top_k=self.config.top_k,
            filter=filter
        )

        # Stage 2: Reranking
        reranked = self.reranker.rerank(
            query,
            candidates,
            top_k=self.config.final_k * 2  # Get more for potential filtering
        )

        # Stage 3: LLM filtering (optional)
        if self.llm_filter:
            filtered = self._llm_filter(query, reranked)
            return filtered[:self.config.final_k]

        return reranked[:self.config.final_k]

    def _llm_filter(
        self,
        query: str,
        results: List[RetrievalResult]
    ) -> List[RetrievalResult]:
        """Filter results using LLM relevance check."""
        filtered = []

        for result in results:
            prompt = f"""Is this passage relevant to answering the question?

Question: {query}
Passage: {result.content[:500]}

Answer only YES or NO."""

            response = self.llm_filter.generate(prompt)

            if 'YES' in response.upper():
                filtered.append(result)

        return filtered


class ContextualRetriever:
    """
    Context-aware retrieval that considers conversation history.
    """

    def __init__(
        self,
        base_retriever: BaseRetriever,
        query_rewriter: QueryRewriter
    ):
        self.retriever = base_retriever
        self.rewriter = query_rewriter
        self.conversation_history = []

    def retrieve(
        self,
        query: str,
        include_history: bool = True
    ) -> List[RetrievalResult]:
        """Retrieve with conversation context."""
        # Rewrite query with context
        if include_history and self.conversation_history:
            context = " | ".join(self.conversation_history[-3:])
            rewritten_query = self.rewriter.rewrite_for_retrieval(query, context)
        else:
            rewritten_query = query

        # Retrieve
        results = self.retriever.retrieve(rewritten_query)

        # Update history
        self.conversation_history.append(query)

        return results

    def reset_context(self):
        """Clear conversation history."""
        self.conversation_history = []


class IterativeRetriever:
    """
    Iterative retrieval that refines results based on feedback.
    """

    def __init__(
        self,
        base_retriever: BaseRetriever,
        llm_client: Any,
        max_iterations: int = 3
    ):
        self.retriever = base_retriever
        self.llm = llm_client
        self.max_iterations = max_iterations

    def retrieve(
        self,
        query: str,
        answer_check: Optional[Callable] = None
    ) -> Tuple[List[RetrievalResult], List[str]]:
        """
        Iteratively retrieve and refine until answer is found.

        Returns:
            Tuple of (final results, query history)
        """
        current_query = query
        all_results = []
        query_history = [query]

        for iteration in range(self.max_iterations):
            # Retrieve
            results = self.retriever.retrieve(current_query)
            all_results.extend(results)

            # Check if we have enough to answer
            if answer_check and answer_check(results):
                break

            # Generate refined query
            refined = self._refine_query(
                original_query=query,
                current_query=current_query,
                results=results
            )

            if refined == current_query:
                break  # No improvement

            current_query = refined
            query_history.append(refined)

        # Deduplicate results
        seen = set()
        unique_results = []
        for r in all_results:
            if r.id not in seen:
                seen.add(r.id)
                unique_results.append(r)

        return unique_results, query_history

    def _refine_query(
        self,
        original_query: str,
        current_query: str,
        results: List[RetrievalResult]
    ) -> str:
        """Generate refined query based on results."""
        results_summary = "\n".join([
            f"- {r.content[:200]}..." for r in results[:3]
        ])

        prompt = f"""The original question was: {original_query}

We searched for: {current_query}

Results found:
{results_summary}

If these results don't fully answer the original question, suggest a different search query to find the missing information. If the results are sufficient, return "DONE".

New query:"""

        response = self.llm.generate(prompt)

        if "DONE" in response.upper():
            return current_query

        return response.strip()
```

---

## 7.4.5 Production Optimization

```python
"""
ABOUTME: Production optimization for retrieval pipelines.
ABOUTME: Covers caching, batching, and performance monitoring.
"""

class RetrievalCache:
    """
    Cache for retrieval results.

    Reduces latency for repeated or similar queries.
    """

    def __init__(
        self,
        embedding_model: Optional[Any] = None,
        similarity_threshold: float = 0.95,
        max_cache_size: int = 10000,
        ttl_seconds: int = 3600
    ):
        self.embedding_model = embedding_model
        self.similarity_threshold = similarity_threshold
        self.max_cache_size = max_cache_size
        self.ttl_seconds = ttl_seconds

        self.cache = {}  # query_hash -> (results, timestamp, embedding)
        self.query_embeddings = []
        self.query_hashes = []

    def get(
        self,
        query: str
    ) -> Optional[List[RetrievalResult]]:
        """Get cached results for query."""
        import time

        # Exact match
        query_hash = hash(query)
        if query_hash in self.cache:
            results, timestamp, _ = self.cache[query_hash]
            if time.time() - timestamp < self.ttl_seconds:
                return results
            else:
                del self.cache[query_hash]

        # Semantic similarity match
        if self.embedding_model and self.query_embeddings:
            query_embedding = self.embedding_model.encode([query])[0]

            similarities = [
                np.dot(query_embedding, emb) / (
                    np.linalg.norm(query_embedding) * np.linalg.norm(emb)
                )
                for emb in self.query_embeddings
            ]

            max_sim_idx = np.argmax(similarities)
            if similarities[max_sim_idx] >= self.similarity_threshold:
                cached_hash = self.query_hashes[max_sim_idx]
                if cached_hash in self.cache:
                    results, timestamp, _ = self.cache[cached_hash]
                    if time.time() - timestamp < self.ttl_seconds:
                        return results

        return None

    def set(
        self,
        query: str,
        results: List[RetrievalResult]
    ):
        """Cache results for query."""
        import time

        # Evict if at capacity
        if len(self.cache) >= self.max_cache_size:
            self._evict_oldest()

        query_hash = hash(query)
        embedding = None

        if self.embedding_model:
            embedding = self.embedding_model.encode([query])[0]
            self.query_embeddings.append(embedding)
            self.query_hashes.append(query_hash)

        self.cache[query_hash] = (results, time.time(), embedding)

    def _evict_oldest(self):
        """Evict oldest cache entries."""
        if not self.cache:
            return

        # Find oldest
        oldest_hash = min(
            self.cache.keys(),
            key=lambda h: self.cache[h][1]
        )

        # Remove from cache
        del self.cache[oldest_hash]

        # Remove from embedding lists
        if oldest_hash in self.query_hashes:
            idx = self.query_hashes.index(oldest_hash)
            self.query_hashes.pop(idx)
            self.query_embeddings.pop(idx)


class RetrievalMetrics:
    """
    Track retrieval performance metrics.
    """

    def __init__(self):
        self.latencies = []
        self.cache_hits = 0
        self.cache_misses = 0
        self.result_counts = []

    def record_retrieval(
        self,
        latency_ms: float,
        num_results: int,
        cache_hit: bool
    ):
        """Record metrics for a retrieval."""
        self.latencies.append(latency_ms)
        self.result_counts.append(num_results)

        if cache_hit:
            self.cache_hits += 1
        else:
            self.cache_misses += 1

    def get_summary(self) -> Dict[str, float]:
        """Get metrics summary."""
        if not self.latencies:
            return {}

        return {
            'avg_latency_ms': np.mean(self.latencies),
            'p50_latency_ms': np.percentile(self.latencies, 50),
            'p95_latency_ms': np.percentile(self.latencies, 95),
            'p99_latency_ms': np.percentile(self.latencies, 99),
            'cache_hit_rate': self.cache_hits / (self.cache_hits + self.cache_misses) if (self.cache_hits + self.cache_misses) > 0 else 0,
            'avg_results': np.mean(self.result_counts),
            'total_queries': len(self.latencies)
        }


class ProductionRetrievalPipeline:
    """
    Production-ready retrieval pipeline with all optimizations.
    """

    def __init__(
        self,
        retriever: BaseRetriever,
        reranker: Optional[Any] = None,
        cache: Optional[RetrievalCache] = None,
        config: RetrievalConfig = None
    ):
        self.retriever = retriever
        self.reranker = reranker
        self.cache = cache
        self.config = config or RetrievalConfig()
        self.metrics = RetrievalMetrics()

    def retrieve(
        self,
        query: str,
        filter: Optional[Dict[str, Any]] = None,
        use_cache: bool = True
    ) -> List[RetrievalResult]:
        """Execute production retrieval with all optimizations."""
        import time
        start = time.perf_counter()

        cache_hit = False

        # Check cache
        if use_cache and self.cache:
            cached = self.cache.get(query)
            if cached:
                cache_hit = True
                results = cached
            else:
                results = self._execute_retrieval(query, filter)
                self.cache.set(query, results)
        else:
            results = self._execute_retrieval(query, filter)

        latency = (time.perf_counter() - start) * 1000
        self.metrics.record_retrieval(latency, len(results), cache_hit)

        return results

    def _execute_retrieval(
        self,
        query: str,
        filter: Optional[Dict[str, Any]]
    ) -> List[RetrievalResult]:
        """Execute retrieval pipeline."""
        # First stage retrieval
        results = self.retriever.retrieve(
            query,
            top_k=self.config.top_k,
            filter=filter
        )

        # Reranking
        if self.reranker and results:
            results = self.reranker.rerank(
                query,
                results,
                top_k=self.config.final_k
            )

        return results[:self.config.final_k]
```

---

## Appendix A: Troubleshooting Guide

### Common Issues

| Issue | Cause | Solution |
|-------|-------|----------|
| Low recall | Only using dense search | Add BM25 hybrid |
| Poor precision | No reranking | Add cross-encoder reranker |
| High latency | Reranking all results | Reduce top_k before reranking |
| Inconsistent results | No caching | Add query cache |
| Missing keywords | Dense misses exact matches | Increase sparse weight |

---

## Appendix B: Glossary

| Term | Definition |
|------|------------|
| **BM25** | Best Matching 25 - probabilistic retrieval |
| **Cross-Encoder** | Model that jointly encodes query and document |
| **ColBERT** | Contextualized Late Interaction over BERT |
| **RRF** | Reciprocal Rank Fusion |
| **HyDE** | Hypothetical Document Embeddings |
| **Reranking** | Re-scoring initial results for precision |

---

## References

1. [Hybrid Search with Qdrant](https://qdrant.tech/articles/hybrid-search/)
2. [Contextual RAG with Hybrid Search](https://www.analyticsvidhya.com/blog/2024/12/contextual-rag-systems-with-hybrid-search-and-reranking/)
3. [Reranking in RAG - EY/KA Lab](https://eyka.com/blog/reranking-in-rag-enhancing-accuracy-with-cross-encoders/)
4. [Optimizing RAG - Superlinked](https://superlinked.com/vectorhub/articles/optimizing-rag-with-hybrid-search-reranking)
5. [ColBERT: Efficient Passage Search](https://arxiv.org/abs/2004.12832)
6. [Cross-Encoder Models - Sentence Transformers](https://www.sbert.net/examples/applications/cross-encoder/README.html)
---

> **Navigation**
> [← 7.3 Chunking](7.3_chunking_strategies_guide.md) | **[Index](../README.md#15-repository-structure)** | [7.5 RAG Evaluation →](7.5_rag_evaluation_guide.md)
