> **Navigation** | [← 7.1 Vector Database](7.1_vector_database_guide.md) | [7.3 Chunking Strategies →](7.3_chunking_strategies_guide.md)
>
> | | |
> |---|---|
> | **Prerequisites** | [7.1 Vector Database](7.1_vector_database_guide.md) &#124; Transformer fundamentals |
> | **Related** | [7.3 Chunking](7.3_chunking_strategies_guide.md) &#124; [1.1 Data Collection](../01_data_pipeline/1.1_data_collection_sourcing.md) |
> | **Next** | [7.3 Chunking Strategies](7.3_chunking_strategies_guide.md) |

# Document 7.2: Embedding Model Selection & Fine-tuning Guide

## Executive Summary

Embedding models transform text into dense vector representations that enable semantic similarity search. This guide covers embedding model selection criteria (dimension, performance, cost), evaluation on retrieval benchmarks (MTEB), and fine-tuning techniques using contrastive learning (triplet loss, InfoNCE). Proper embedding model selection and domain-specific fine-tuning can improve RAG retrieval accuracy by 10-30%.

## Prerequisites

- Understanding of transformer architectures and attention mechanisms
- Familiarity with PyTorch and Hugging Face libraries
- Knowledge of similarity metrics and vector databases
- Experience with sentence-transformers library
- Completion of document 7.1 (Vector Database Guide) recommended

---

## 7.2.1 Embedding Model Fundamentals

### Core Concepts

```python
"""
ABOUTME: Embedding model fundamentals and evaluation framework.
ABOUTME: Provides unified interface for multiple embedding models.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import List, Dict, Any, Optional, Tuple, Union
from dataclasses import dataclass, field
from enum import Enum
from abc import ABC, abstractmethod


class PoolingStrategy(Enum):
    """Strategies for pooling token embeddings to sentence embeddings."""
    CLS = "cls"                    # Use [CLS] token
    MEAN = "mean"                  # Average all tokens
    MAX = "max"                    # Max pool
    MEAN_SQRT_LEN = "mean_sqrt"    # Length-normalized mean
    LAST_TOKEN = "last"            # Last token (for decoder models)
    WEIGHTED_MEAN = "weighted"     # Attention-weighted mean


@dataclass
class EmbeddingModelConfig:
    """Configuration for embedding models."""
    model_name: str
    dimension: int
    max_seq_length: int
    pooling: PoolingStrategy = PoolingStrategy.MEAN
    normalize: bool = True

    # Model characteristics
    model_type: str = "encoder"  # encoder, decoder, encoder-decoder
    parameters: int = 0

    # Performance characteristics
    mteb_avg_score: float = 0.0
    inference_speed_tokens_per_sec: int = 0


@dataclass
class EmbeddingModelProfile:
    """Profile of an embedding model's characteristics."""
    name: str
    provider: str
    dimension: int
    max_seq_length: int
    mteb_retrieval_score: float
    cost_per_1m_tokens: float
    latency_ms: float  # Per document
    strengths: List[str]
    limitations: List[str]
    best_for: List[str]


class EmbeddingModelCatalog:
    """
    Catalog of popular embedding models.
    """

    MODELS = {
        # OpenAI Models
        'text-embedding-3-small': EmbeddingModelProfile(
            name='text-embedding-3-small',
            provider='OpenAI',
            dimension=1536,
            max_seq_length=8191,
            mteb_retrieval_score=62.3,
            cost_per_1m_tokens=0.02,
            latency_ms=50,
            strengths=[
                'Low cost',
                'Good general performance',
                'High throughput API',
                'Multilingual'
            ],
            limitations=[
                'API dependency',
                'Lower accuracy than large',
                'No fine-tuning'
            ],
            best_for=[
                'Cost-sensitive applications',
                'General semantic search',
                'Prototyping'
            ]
        ),
        'text-embedding-3-large': EmbeddingModelProfile(
            name='text-embedding-3-large',
            provider='OpenAI',
            dimension=3072,
            max_seq_length=8191,
            mteb_retrieval_score=64.6,
            cost_per_1m_tokens=0.13,
            latency_ms=80,
            strengths=[
                'High accuracy',
                'Multilingual',
                'Matryoshka representations',
                'Dimension reduction support'
            ],
            limitations=[
                'Higher cost',
                'API dependency',
                'No fine-tuning'
            ],
            best_for=[
                'Production RAG systems',
                'High-accuracy retrieval',
                'Multilingual applications'
            ]
        ),

        # Open Source Models
        'bge-large-en-v1.5': EmbeddingModelProfile(
            name='BAAI/bge-large-en-v1.5',
            provider='BAAI',
            dimension=1024,
            max_seq_length=512,
            mteb_retrieval_score=63.5,
            cost_per_1m_tokens=0.0,  # Self-hosted
            latency_ms=15,
            strengths=[
                'Open source',
                'High quality',
                'Self-hosted',
                'Fine-tunable'
            ],
            limitations=[
                'English only',
                'Shorter context',
                'Requires GPU'
            ],
            best_for=[
                'English retrieval',
                'Privacy-sensitive deployments',
                'Fine-tuning projects'
            ]
        ),
        'gte-large-en-v1.5': EmbeddingModelProfile(
            name='Alibaba-NLP/gte-large-en-v1.5',
            provider='Alibaba',
            dimension=1024,
            max_seq_length=8192,
            mteb_retrieval_score=65.4,
            cost_per_1m_tokens=0.0,
            latency_ms=18,
            strengths=[
                'State-of-the-art MTEB',
                'Long context (8K)',
                'Open source',
                'Strong generalization'
            ],
            limitations=[
                'English only',
                'Larger model size'
            ],
            best_for=[
                'Long document retrieval',
                'Research applications',
                'High-accuracy requirements'
            ]
        ),
        'e5-large-v2': EmbeddingModelProfile(
            name='intfloat/e5-large-v2',
            provider='Microsoft',
            dimension=1024,
            max_seq_length=512,
            mteb_retrieval_score=62.0,
            cost_per_1m_tokens=0.0,
            latency_ms=14,
            strengths=[
                'Well-documented',
                'Instruction-tuned variant',
                'Consistent performance',
                'Good baseline'
            ],
            limitations=[
                'Requires query prefix',
                'Shorter context'
            ],
            best_for=[
                'Semantic search',
                'Question answering',
                'Document retrieval'
            ]
        ),
        'all-MiniLM-L6-v2': EmbeddingModelProfile(
            name='sentence-transformers/all-MiniLM-L6-v2',
            provider='Sentence-Transformers',
            dimension=384,
            max_seq_length=256,
            mteb_retrieval_score=41.0,
            cost_per_1m_tokens=0.0,
            latency_ms=3,
            strengths=[
                'Very fast',
                'Low memory',
                'Good for prototyping',
                'CPU-friendly'
            ],
            limitations=[
                'Lower accuracy',
                'Short context',
                'Smaller dimension'
            ],
            best_for=[
                'Prototyping',
                'Resource-constrained environments',
                'High-throughput with lower accuracy'
            ]
        ),

        # Cohere Models
        'embed-english-v3.0': EmbeddingModelProfile(
            name='embed-english-v3.0',
            provider='Cohere',
            dimension=1024,
            max_seq_length=512,
            mteb_retrieval_score=64.5,
            cost_per_1m_tokens=0.10,
            latency_ms=40,
            strengths=[
                'Input type specification',
                'Compression support',
                'Strong retrieval',
                'Enterprise support'
            ],
            limitations=[
                'API dependency',
                'English only v3'
            ],
            best_for=[
                'Enterprise deployments',
                'Mixed query/document workloads'
            ]
        )
    }

    @classmethod
    def get_model(cls, name: str) -> Optional[EmbeddingModelProfile]:
        """Get model profile by name."""
        return cls.MODELS.get(name)

    @classmethod
    def list_by_criteria(
        cls,
        min_mteb_score: float = 0,
        max_cost: float = float('inf'),
        max_latency_ms: float = float('inf'),
        providers: Optional[List[str]] = None
    ) -> List[EmbeddingModelProfile]:
        """List models matching criteria."""
        matches = []

        for name, profile in cls.MODELS.items():
            if profile.mteb_retrieval_score < min_mteb_score:
                continue
            if profile.cost_per_1m_tokens > max_cost:
                continue
            if profile.latency_ms > max_latency_ms:
                continue
            if providers and profile.provider not in providers:
                continue
            matches.append(profile)

        # Sort by MTEB score descending
        matches.sort(key=lambda x: x.mteb_retrieval_score, reverse=True)

        return matches


class EmbeddingModel(ABC):
    """Abstract base class for embedding models."""

    @abstractmethod
    def encode(
        self,
        texts: List[str],
        batch_size: int = 32,
        show_progress: bool = False
    ) -> np.ndarray:
        """Encode texts to embeddings."""
        pass

    @abstractmethod
    def encode_queries(
        self,
        queries: List[str],
        batch_size: int = 32
    ) -> np.ndarray:
        """Encode queries (may use different prompt/pooling)."""
        pass

    @abstractmethod
    def encode_documents(
        self,
        documents: List[str],
        batch_size: int = 32
    ) -> np.ndarray:
        """Encode documents."""
        pass


class SentenceTransformerWrapper(EmbeddingModel):
    """
    Wrapper for sentence-transformers models.

    Provides unified interface for local embedding models.
    """

    def __init__(
        self,
        model_name: str,
        device: str = 'cuda',
        normalize: bool = True,
        query_prefix: str = "",
        document_prefix: str = ""
    ):
        from sentence_transformers import SentenceTransformer

        self.model = SentenceTransformer(model_name, device=device)
        self.normalize = normalize
        self.query_prefix = query_prefix
        self.document_prefix = document_prefix
        self.dimension = self.model.get_sentence_embedding_dimension()

    def encode(
        self,
        texts: List[str],
        batch_size: int = 32,
        show_progress: bool = False
    ) -> np.ndarray:
        """Encode texts to embeddings."""
        embeddings = self.model.encode(
            texts,
            batch_size=batch_size,
            show_progress_bar=show_progress,
            normalize_embeddings=self.normalize
        )
        return embeddings

    def encode_queries(
        self,
        queries: List[str],
        batch_size: int = 32
    ) -> np.ndarray:
        """Encode queries with optional prefix."""
        if self.query_prefix:
            queries = [f"{self.query_prefix}{q}" for q in queries]
        return self.encode(queries, batch_size)

    def encode_documents(
        self,
        documents: List[str],
        batch_size: int = 32
    ) -> np.ndarray:
        """Encode documents with optional prefix."""
        if self.document_prefix:
            documents = [f"{self.document_prefix}{d}" for d in documents]
        return self.encode(documents, batch_size)


class OpenAIEmbedding(EmbeddingModel):
    """
    OpenAI embedding API wrapper.
    """

    def __init__(
        self,
        model_name: str = "text-embedding-3-small",
        api_key: Optional[str] = None
    ):
        import openai

        self.client = openai.OpenAI(api_key=api_key)
        self.model_name = model_name

        # Dimension mapping
        self.dimension_map = {
            'text-embedding-3-small': 1536,
            'text-embedding-3-large': 3072,
            'text-embedding-ada-002': 1536
        }
        self.dimension = self.dimension_map.get(model_name, 1536)

    def encode(
        self,
        texts: List[str],
        batch_size: int = 100,  # OpenAI limit
        show_progress: bool = False
    ) -> np.ndarray:
        """Encode texts using OpenAI API."""
        all_embeddings = []

        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]

            response = self.client.embeddings.create(
                model=self.model_name,
                input=batch
            )

            batch_embeddings = [
                item.embedding for item in response.data
            ]
            all_embeddings.extend(batch_embeddings)

        return np.array(all_embeddings)

    def encode_queries(
        self,
        queries: List[str],
        batch_size: int = 100
    ) -> np.ndarray:
        return self.encode(queries, batch_size)

    def encode_documents(
        self,
        documents: List[str],
        batch_size: int = 100
    ) -> np.ndarray:
        return self.encode(documents, batch_size)
```

### Pooling Strategies

```python
class PoolingModule(nn.Module):
    """
    Flexible pooling for creating sentence embeddings.

    Different pooling strategies work better for different tasks:
    - CLS: Good for classification-oriented models
    - Mean: Most common for retrieval, robust
    - Weighted Mean: Better for longer sequences
    """

    def __init__(self, strategy: PoolingStrategy):
        super().__init__()
        self.strategy = strategy

    def forward(
        self,
        token_embeddings: torch.Tensor,
        attention_mask: torch.Tensor
    ) -> torch.Tensor:
        """
        Pool token embeddings to sentence embedding.

        Args:
            token_embeddings: [batch, seq_len, hidden_dim]
            attention_mask: [batch, seq_len]

        Returns:
            Sentence embeddings [batch, hidden_dim]
        """
        if self.strategy == PoolingStrategy.CLS:
            return token_embeddings[:, 0, :]

        elif self.strategy == PoolingStrategy.MEAN:
            # Expand mask for broadcasting
            mask_expanded = attention_mask.unsqueeze(-1).expand(
                token_embeddings.size()
            ).float()

            sum_embeddings = torch.sum(token_embeddings * mask_expanded, dim=1)
            sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)

            return sum_embeddings / sum_mask

        elif self.strategy == PoolingStrategy.MAX:
            # Set padded positions to large negative
            mask_expanded = attention_mask.unsqueeze(-1).expand(
                token_embeddings.size()
            )
            token_embeddings[~mask_expanded.bool()] = -1e9

            return torch.max(token_embeddings, dim=1)[0]

        elif self.strategy == PoolingStrategy.MEAN_SQRT_LEN:
            mask_expanded = attention_mask.unsqueeze(-1).expand(
                token_embeddings.size()
            ).float()

            sum_embeddings = torch.sum(token_embeddings * mask_expanded, dim=1)
            seq_lengths = attention_mask.sum(dim=1, keepdim=True).float()

            return sum_embeddings / torch.sqrt(seq_lengths)

        elif self.strategy == PoolingStrategy.LAST_TOKEN:
            # Get position of last non-pad token
            seq_lengths = attention_mask.sum(dim=1) - 1
            batch_indices = torch.arange(token_embeddings.size(0))

            return token_embeddings[batch_indices, seq_lengths]

        else:
            raise ValueError(f"Unknown pooling strategy: {self.strategy}")
```

---

## 7.2.2 Model Selection

### Selection Framework

```python
"""
ABOUTME: Embedding model selection framework and comparison.
ABOUTME: Guides selection based on requirements and benchmarks.
"""

@dataclass
class SelectionRequirements:
    """Requirements for embedding model selection."""
    # Performance requirements
    min_retrieval_accuracy: float = 0.7  # NDCG@10
    max_latency_ms: float = 100

    # Resource constraints
    max_cost_per_1m_tokens: float = 0.5
    max_memory_gb: float = 8
    gpu_available: bool = True

    # Deployment constraints
    allow_api_dependency: bool = True
    require_fine_tuning: bool = False

    # Content characteristics
    avg_document_length: int = 500  # tokens
    languages: List[str] = field(default_factory=lambda: ['english'])
    domain: str = "general"


class EmbeddingModelSelector:
    """
    Select optimal embedding model based on requirements.
    """

    def __init__(self):
        self.catalog = EmbeddingModelCatalog()

    def recommend(
        self,
        requirements: SelectionRequirements
    ) -> List[Tuple[str, float, List[str]]]:
        """
        Recommend embedding models based on requirements.

        Returns:
            List of (model_name, score, reasons) sorted by score
        """
        scores = {}

        for name, profile in self.catalog.MODELS.items():
            score = 0.0
            reasons = []
            disqualified = False

            # Check hard constraints
            if not requirements.allow_api_dependency:
                if profile.cost_per_1m_tokens > 0:
                    disqualified = True
                    continue

            if requirements.require_fine_tuning:
                if profile.provider in ['OpenAI', 'Cohere']:
                    disqualified = True
                    continue

            if requirements.max_cost_per_1m_tokens < profile.cost_per_1m_tokens:
                disqualified = True
                continue

            if disqualified:
                continue

            # Score based on retrieval quality
            quality_score = profile.mteb_retrieval_score / 70  # Normalize
            score += quality_score * 3  # Weight quality highly
            if quality_score > 0.9:
                reasons.append("Excellent retrieval quality")

            # Score based on latency
            if profile.latency_ms <= requirements.max_latency_ms:
                latency_score = 1 - (profile.latency_ms / requirements.max_latency_ms)
                score += latency_score
                if latency_score > 0.8:
                    reasons.append("Fast inference")

            # Score based on cost
            if profile.cost_per_1m_tokens == 0:
                score += 1.5
                reasons.append("No API costs")
            elif profile.cost_per_1m_tokens < 0.05:
                score += 1
                reasons.append("Low cost")

            # Context length bonus
            if profile.max_seq_length >= requirements.avg_document_length * 2:
                score += 0.5
                reasons.append("Handles document length")

            # Domain-specific considerations
            if requirements.domain == "code":
                if "code" in name.lower() or profile.provider == "OpenAI":
                    score += 0.5
                    reasons.append("Good for code")

            # Fine-tuning capability
            if requirements.require_fine_tuning and profile.cost_per_1m_tokens == 0:
                score += 1
                reasons.append("Fine-tunable")

            scores[name] = (score, reasons)

        # Sort by score
        ranked = sorted(
            [(name, score, reasons) for name, (score, reasons) in scores.items()],
            key=lambda x: x[1],
            reverse=True
        )

        return ranked

    def compare_models(
        self,
        model_names: List[str],
        eval_data: List[Tuple[str, List[str], List[str]]]  # (query, pos_docs, neg_docs)
    ) -> Dict[str, Dict[str, float]]:
        """
        Compare models on evaluation data.

        Returns metrics for each model.
        """
        results = {}

        for model_name in model_names:
            # Load model
            if 'text-embedding' in model_name:
                model = OpenAIEmbedding(model_name)
            else:
                model = SentenceTransformerWrapper(model_name)

            # Evaluate
            metrics = self._evaluate_retrieval(model, eval_data)
            results[model_name] = metrics

        return results

    def _evaluate_retrieval(
        self,
        model: EmbeddingModel,
        eval_data: List[Tuple[str, List[str], List[str]]]
    ) -> Dict[str, float]:
        """Evaluate model on retrieval task."""
        import time

        # Compute embeddings
        all_queries = [d[0] for d in eval_data]
        all_pos_docs = [doc for d in eval_data for doc in d[1]]
        all_neg_docs = [doc for d in eval_data for doc in d[2]]

        start = time.perf_counter()
        query_embs = model.encode_queries(all_queries)
        doc_embs = model.encode_documents(all_pos_docs + all_neg_docs)
        encoding_time = time.perf_counter() - start

        # Compute retrieval metrics
        mrr_sum = 0.0
        recall_at_10_sum = 0.0

        doc_idx = 0
        for i, (query, pos_docs, neg_docs) in enumerate(eval_data):
            query_emb = query_embs[i]

            # Get relevant doc embeddings
            num_pos = len(pos_docs)
            num_neg = len(neg_docs)

            pos_embs = doc_embs[doc_idx:doc_idx + num_pos]
            neg_embs = doc_embs[doc_idx + num_pos:doc_idx + num_pos + num_neg]
            doc_idx += num_pos + num_neg

            # Compute similarities
            all_doc_embs = np.vstack([pos_embs, neg_embs])
            similarities = query_emb @ all_doc_embs.T

            # Rank
            ranking = np.argsort(-similarities)

            # MRR (first positive doc rank)
            for rank, idx in enumerate(ranking):
                if idx < num_pos:
                    mrr_sum += 1.0 / (rank + 1)
                    break

            # Recall@10
            top_10 = ranking[:10]
            recall_at_10_sum += len([i for i in top_10 if i < num_pos]) / num_pos

        n = len(eval_data)

        return {
            'mrr': mrr_sum / n,
            'recall@10': recall_at_10_sum / n,
            'encoding_time_s': encoding_time,
            'tokens_per_second': sum(len(d[0].split()) + sum(len(doc.split()) for doc in d[1] + d[2]) for d in eval_data) / encoding_time
        }
```

---

## 7.2.3 Fine-tuning Embedding Models

### Contrastive Learning

```python
"""
ABOUTME: Embedding model fine-tuning with contrastive learning.
ABOUTME: Implements triplet loss, InfoNCE, and training loops.
"""

class ContrastiveLoss(nn.Module):
    """
    InfoNCE contrastive loss for embedding fine-tuning.

    For each anchor, we have one positive and multiple negatives.
    Loss encourages anchor to be closer to positive than negatives.
    """

    def __init__(self, temperature: float = 0.05):
        super().__init__()
        self.temperature = temperature

    def forward(
        self,
        anchor: torch.Tensor,      # [batch, dim]
        positive: torch.Tensor,    # [batch, dim]
        negatives: torch.Tensor    # [batch, num_neg, dim] or [num_neg, dim]
    ) -> torch.Tensor:
        """
        Compute InfoNCE loss.

        Loss = -log(exp(sim(a,p)/t) / (exp(sim(a,p)/t) + sum(exp(sim(a,n)/t))))
        """
        batch_size = anchor.size(0)

        # Normalize embeddings
        anchor = F.normalize(anchor, p=2, dim=-1)
        positive = F.normalize(positive, p=2, dim=-1)
        negatives = F.normalize(negatives, p=2, dim=-1)

        # Positive similarity [batch]
        pos_sim = torch.sum(anchor * positive, dim=-1) / self.temperature

        # Negative similarities
        if negatives.dim() == 3:
            # Per-sample negatives [batch, num_neg]
            neg_sim = torch.bmm(
                negatives,
                anchor.unsqueeze(-1)
            ).squeeze(-1) / self.temperature
        else:
            # Shared negatives [batch, num_neg]
            neg_sim = torch.mm(anchor, negatives.t()) / self.temperature

        # Combine and compute loss
        # Logits: [batch, 1 + num_neg]
        logits = torch.cat([pos_sim.unsqueeze(-1), neg_sim], dim=-1)

        # Labels: positive is always at index 0
        labels = torch.zeros(batch_size, dtype=torch.long, device=anchor.device)

        loss = F.cross_entropy(logits, labels)

        return loss


class TripletLoss(nn.Module):
    """
    Triplet margin loss for embedding fine-tuning.

    Ensures distance(anchor, positive) + margin < distance(anchor, negative)
    """

    def __init__(self, margin: float = 0.5, distance: str = "cosine"):
        super().__init__()
        self.margin = margin
        self.distance = distance

    def forward(
        self,
        anchor: torch.Tensor,    # [batch, dim]
        positive: torch.Tensor,  # [batch, dim]
        negative: torch.Tensor   # [batch, dim]
    ) -> torch.Tensor:
        """Compute triplet loss."""
        if self.distance == "cosine":
            # Cosine distance = 1 - cosine_similarity
            pos_dist = 1 - F.cosine_similarity(anchor, positive)
            neg_dist = 1 - F.cosine_similarity(anchor, negative)
        else:
            # L2 distance
            pos_dist = F.pairwise_distance(anchor, positive)
            neg_dist = F.pairwise_distance(anchor, negative)

        # Triplet loss with margin
        loss = F.relu(pos_dist - neg_dist + self.margin)

        return loss.mean()


class MultipleNegativesRankingLoss(nn.Module):
    """
    Multiple Negatives Ranking Loss (MNRL).

    Uses in-batch negatives efficiently - each positive in the batch
    serves as a negative for other samples.

    This is the most common loss for embedding model training.
    """

    def __init__(self, temperature: float = 0.05):
        super().__init__()
        self.temperature = temperature
        self.cross_entropy = nn.CrossEntropyLoss()

    def forward(
        self,
        query_embeds: torch.Tensor,    # [batch, dim]
        doc_embeds: torch.Tensor       # [batch, dim]
    ) -> torch.Tensor:
        """
        Compute MNRL with in-batch negatives.

        Each query is paired with its positive document.
        All other documents in the batch serve as negatives.
        """
        # Normalize
        query_embeds = F.normalize(query_embeds, p=2, dim=-1)
        doc_embeds = F.normalize(doc_embeds, p=2, dim=-1)

        # Compute all pairwise similarities [batch, batch]
        similarities = torch.mm(query_embeds, doc_embeds.t()) / self.temperature

        # Labels: diagonal (query i matches doc i)
        batch_size = query_embeds.size(0)
        labels = torch.arange(batch_size, device=query_embeds.device)

        # Cross entropy loss
        loss = self.cross_entropy(similarities, labels)

        return loss


class EmbeddingFineTuner:
    """
    Fine-tune embedding models for domain-specific retrieval.
    """

    def __init__(
        self,
        model_name: str,
        device: str = 'cuda'
    ):
        from sentence_transformers import SentenceTransformer

        self.model = SentenceTransformer(model_name, device=device)
        self.device = device

        # Loss function
        self.loss_fn = MultipleNegativesRankingLoss()

    def prepare_training_data(
        self,
        data: List[Dict[str, Any]]
    ) -> 'InputExampleDataset':
        """
        Prepare data for training.

        Expected format:
        [
            {
                'query': 'What is RAG?',
                'positive': 'RAG stands for Retrieval-Augmented Generation...',
                'negative': 'Machine learning is...'  # Optional
            },
            ...
        ]
        """
        from sentence_transformers import InputExample

        examples = []

        for item in data:
            query = item['query']
            positive = item['positive']

            # Create pair (MNRL uses in-batch negatives)
            examples.append(InputExample(texts=[query, positive]))

            # Add explicit negative if provided
            if 'negative' in item:
                examples.append(InputExample(
                    texts=[query, positive, item['negative']]
                ))

        return examples

    def train(
        self,
        train_data: List[Dict],
        eval_data: Optional[List[Dict]] = None,
        output_path: str = './fine_tuned_model',
        num_epochs: int = 3,
        batch_size: int = 32,
        learning_rate: float = 2e-5,
        warmup_ratio: float = 0.1
    ):
        """
        Fine-tune the embedding model.
        """
        from sentence_transformers import losses, evaluation
        from torch.utils.data import DataLoader

        # Prepare training data
        train_examples = self.prepare_training_data(train_data)
        train_dataloader = DataLoader(
            train_examples,
            shuffle=True,
            batch_size=batch_size
        )

        # Loss
        train_loss = losses.MultipleNegativesRankingLoss(self.model)

        # Evaluator (if eval data provided)
        evaluator = None
        if eval_data:
            eval_queries = [d['query'] for d in eval_data]
            eval_corpus = list(set(
                [d['positive'] for d in eval_data] +
                [d.get('negative', '') for d in eval_data if d.get('negative')]
            ))
            eval_relevant = {
                i: {eval_corpus.index(d['positive'])}
                for i, d in enumerate(eval_data)
            }

            evaluator = evaluation.InformationRetrievalEvaluator(
                queries=dict(enumerate(eval_queries)),
                corpus=dict(enumerate(eval_corpus)),
                relevant_docs=eval_relevant
            )

        # Training arguments
        warmup_steps = int(len(train_dataloader) * num_epochs * warmup_ratio)

        # Train
        self.model.fit(
            train_objectives=[(train_dataloader, train_loss)],
            evaluator=evaluator,
            epochs=num_epochs,
            warmup_steps=warmup_steps,
            output_path=output_path,
            optimizer_params={'lr': learning_rate},
            show_progress_bar=True
        )

        return output_path


class HardNegativeMiner:
    """
    Mine hard negatives for contrastive learning.

    Hard negatives are documents that are semantically similar
    but not relevant - they're harder to distinguish from positives.
    """

    def __init__(self, embedding_model: EmbeddingModel):
        self.model = embedding_model

    def mine_hard_negatives(
        self,
        queries: List[str],
        positives: List[str],
        corpus: List[str],
        num_negatives: int = 10,
        exclude_top_n: int = 3
    ) -> List[List[str]]:
        """
        Mine hard negatives from corpus.

        Strategy: For each query, find top-k similar documents
        that are NOT the positive, excluding very top matches
        (which might be near-duplicates).

        Args:
            queries: Query strings
            positives: Positive documents (1:1 with queries)
            corpus: Document corpus to mine from
            num_negatives: Number of negatives per query
            exclude_top_n: Exclude top N matches (may be near-duplicates)

        Returns:
            List of negative document lists for each query
        """
        # Encode everything
        query_embeds = self.model.encode_queries(queries)
        corpus_embeds = self.model.encode_documents(corpus)
        positive_embeds = self.model.encode_documents(positives)

        # Create positive set for filtering
        positive_set = set(positives)

        all_negatives = []

        for i, (query_emb, positive_emb) in enumerate(
            zip(query_embeds, positive_embeds)
        ):
            # Compute similarities to corpus
            similarities = query_emb @ corpus_embeds.T

            # Get ranking
            ranking = np.argsort(-similarities)

            # Filter negatives
            negatives = []
            for rank in ranking:
                doc = corpus[rank]

                # Skip if it's the positive
                if doc == positives[i]:
                    continue

                # Skip top N (potential near-duplicates)
                if len(negatives) == 0 and rank < exclude_top_n:
                    continue

                negatives.append(doc)

                if len(negatives) >= num_negatives:
                    break

            all_negatives.append(negatives)

        return all_negatives

    def create_training_triplets(
        self,
        queries: List[str],
        positives: List[str],
        corpus: List[str],
        num_negatives_per_query: int = 5
    ) -> List[Dict[str, str]]:
        """Create training triplets with hard negatives."""
        hard_negs = self.mine_hard_negatives(
            queries, positives, corpus, num_negatives_per_query
        )

        triplets = []
        for query, positive, negatives in zip(queries, positives, hard_negs):
            for negative in negatives:
                triplets.append({
                    'query': query,
                    'positive': positive,
                    'negative': negative
                })

        return triplets
```

### Advanced Fine-tuning

```python
"""
ABOUTME: Advanced fine-tuning techniques for embedding models.
ABOUTME: Includes distillation, domain adaptation, and multi-task learning.
"""

class EmbeddingDistillation:
    """
    Distill knowledge from large embedding model to smaller one.
    """

    def __init__(
        self,
        teacher_model: EmbeddingModel,
        student_model_name: str,
        device: str = 'cuda'
    ):
        from sentence_transformers import SentenceTransformer

        self.teacher = teacher_model
        self.student = SentenceTransformer(student_model_name, device=device)
        self.device = device

    def distill(
        self,
        train_texts: List[str],
        output_path: str,
        num_epochs: int = 3,
        batch_size: int = 32,
        learning_rate: float = 2e-5
    ):
        """
        Distill teacher embeddings to student.

        Loss: MSE between teacher and student embeddings
        """
        from torch.utils.data import DataLoader, Dataset

        # Get teacher embeddings
        teacher_embeddings = self.teacher.encode(train_texts)
        teacher_embeddings = torch.tensor(teacher_embeddings)

        # Training
        self.student.train()
        optimizer = torch.optim.AdamW(
            self.student.parameters(),
            lr=learning_rate
        )

        dataset = list(zip(train_texts, teacher_embeddings))
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

        for epoch in range(num_epochs):
            total_loss = 0

            for texts, targets in dataloader:
                optimizer.zero_grad()

                # Student forward
                student_out = self.student.encode(
                    list(texts),
                    convert_to_tensor=True
                )

                # MSE loss
                targets = targets.to(self.device)
                loss = F.mse_loss(student_out, targets)

                loss.backward()
                optimizer.step()

                total_loss += loss.item()

            print(f"Epoch {epoch + 1}: Loss = {total_loss / len(dataloader):.4f}")

        # Save
        self.student.save(output_path)
        return output_path


class DomainAdaptiveFineTuning:
    """
    Adapt embedding model to specific domain using unlabeled data.

    Uses:
    1. MLM pre-training on domain corpus
    2. Contrastive learning with domain-specific pairs
    """

    def __init__(
        self,
        model_name: str,
        device: str = 'cuda'
    ):
        from sentence_transformers import SentenceTransformer

        self.model = SentenceTransformer(model_name, device=device)
        self.device = device

    def continue_pretraining(
        self,
        domain_texts: List[str],
        output_path: str,
        num_epochs: int = 1
    ):
        """
        Continue MLM pre-training on domain corpus.

        This adapts the base model to domain vocabulary and patterns.
        """
        from transformers import (
            AutoModelForMaskedLM,
            AutoTokenizer,
            DataCollatorForLanguageModeling,
            Trainer,
            TrainingArguments
        )
        from datasets import Dataset

        # Get base model
        base_model_name = self.model[0].auto_model.config._name_or_path

        tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        model = AutoModelForMaskedLM.from_pretrained(base_model_name)

        # Tokenize
        def tokenize(examples):
            return tokenizer(
                examples['text'],
                truncation=True,
                max_length=512
            )

        dataset = Dataset.from_dict({'text': domain_texts})
        tokenized = dataset.map(tokenize, batched=True, remove_columns=['text'])

        # Data collator for MLM
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=tokenizer,
            mlm=True,
            mlm_probability=0.15
        )

        # Training
        training_args = TrainingArguments(
            output_dir=output_path,
            num_train_epochs=num_epochs,
            per_device_train_batch_size=16,
            save_strategy='no',
            logging_steps=100
        )

        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized,
            data_collator=data_collator
        )

        trainer.train()

        # Save adapted base model
        model.save_pretrained(output_path)
        tokenizer.save_pretrained(output_path)

        return output_path

    def generate_synthetic_pairs(
        self,
        documents: List[str],
        num_pairs: int = 10000
    ) -> List[Dict[str, str]]:
        """
        Generate synthetic training pairs from domain documents.

        Strategies:
        1. Same-document pairs (different chunks)
        2. Adjacent sentence pairs
        3. Title-body pairs
        """
        pairs = []

        for doc in documents:
            sentences = doc.split('. ')

            if len(sentences) < 2:
                continue

            # Adjacent sentence pairs
            for i in range(len(sentences) - 1):
                if len(pairs) >= num_pairs:
                    break

                pairs.append({
                    'query': sentences[i],
                    'positive': sentences[i + 1]
                })

        return pairs[:num_pairs]
```

---

## 7.2.4 Evaluation

### MTEB Evaluation

```python
"""
ABOUTME: Embedding model evaluation on standard benchmarks.
ABOUTME: Implements MTEB-style evaluation and custom retrieval metrics.
"""

class EmbeddingEvaluator:
    """
    Evaluate embedding models on retrieval benchmarks.
    """

    def __init__(self, model: EmbeddingModel):
        self.model = model

    def evaluate_retrieval(
        self,
        queries: List[str],
        corpus: List[str],
        relevance_judgments: Dict[int, List[int]],  # query_idx -> [relevant_doc_indices]
        top_k: List[int] = [1, 5, 10, 100]
    ) -> Dict[str, float]:
        """
        Evaluate retrieval performance.

        Metrics:
        - NDCG@k: Normalized Discounted Cumulative Gain
        - MRR: Mean Reciprocal Rank
        - Recall@k: Proportion of relevant docs in top-k
        - MAP: Mean Average Precision
        """
        # Encode
        query_embs = self.model.encode_queries(queries)
        corpus_embs = self.model.encode_documents(corpus)

        # Compute all similarities
        similarities = query_embs @ corpus_embs.T  # [num_queries, num_docs]

        metrics = {}

        # Recall@k
        for k in top_k:
            recall_sum = 0
            for q_idx, rel_docs in relevance_judgments.items():
                top_k_docs = np.argsort(-similarities[q_idx])[:k]
                hits = len(set(top_k_docs) & set(rel_docs))
                recall_sum += hits / len(rel_docs) if rel_docs else 0

            metrics[f'recall@{k}'] = recall_sum / len(relevance_judgments)

        # MRR
        mrr_sum = 0
        for q_idx, rel_docs in relevance_judgments.items():
            ranking = np.argsort(-similarities[q_idx])
            for rank, doc_idx in enumerate(ranking):
                if doc_idx in rel_docs:
                    mrr_sum += 1 / (rank + 1)
                    break

        metrics['mrr'] = mrr_sum / len(relevance_judgments)

        # NDCG@10
        ndcg_sum = 0
        k = 10
        for q_idx, rel_docs in relevance_judgments.items():
            ranking = np.argsort(-similarities[q_idx])[:k]

            # DCG
            dcg = 0
            for i, doc_idx in enumerate(ranking):
                if doc_idx in rel_docs:
                    dcg += 1 / np.log2(i + 2)

            # Ideal DCG
            ideal_dcg = sum(1 / np.log2(i + 2) for i in range(min(k, len(rel_docs))))

            ndcg_sum += dcg / ideal_dcg if ideal_dcg > 0 else 0

        metrics['ndcg@10'] = ndcg_sum / len(relevance_judgments)

        # MAP
        ap_sum = 0
        for q_idx, rel_docs in relevance_judgments.items():
            ranking = np.argsort(-similarities[q_idx])

            precisions = []
            hits = 0
            for i, doc_idx in enumerate(ranking):
                if doc_idx in rel_docs:
                    hits += 1
                    precisions.append(hits / (i + 1))

            ap_sum += sum(precisions) / len(rel_docs) if rel_docs else 0

        metrics['map'] = ap_sum / len(relevance_judgments)

        return metrics

    def evaluate_semantic_similarity(
        self,
        sentence_pairs: List[Tuple[str, str]],
        gold_scores: List[float]
    ) -> Dict[str, float]:
        """
        Evaluate semantic similarity (STS benchmark).

        Measures correlation between model similarities and human judgments.
        """
        from scipy.stats import spearmanr, pearsonr

        # Get embeddings
        sentences1 = [p[0] for p in sentence_pairs]
        sentences2 = [p[1] for p in sentence_pairs]

        embs1 = self.model.encode(sentences1)
        embs2 = self.model.encode(sentences2)

        # Compute cosine similarities
        similarities = [
            float(np.dot(e1, e2) / (np.linalg.norm(e1) * np.linalg.norm(e2)))
            for e1, e2 in zip(embs1, embs2)
        ]

        # Correlations with gold scores
        spearman_corr, _ = spearmanr(similarities, gold_scores)
        pearson_corr, _ = pearsonr(similarities, gold_scores)

        return {
            'spearman': spearman_corr,
            'pearson': pearson_corr
        }

    def benchmark_efficiency(
        self,
        test_texts: List[str],
        batch_sizes: List[int] = [1, 8, 32, 128]
    ) -> Dict[str, Dict[str, float]]:
        """Benchmark encoding efficiency."""
        import time

        results = {}

        for batch_size in batch_sizes:
            # Warm up
            _ = self.model.encode(test_texts[:batch_size])

            # Benchmark
            start = time.perf_counter()
            _ = self.model.encode(test_texts, batch_size=batch_size)
            elapsed = time.perf_counter() - start

            results[f'batch_{batch_size}'] = {
                'total_time_s': elapsed,
                'texts_per_second': len(test_texts) / elapsed,
                'ms_per_text': elapsed * 1000 / len(test_texts)
            }

        return results
```

---

## Appendix A: Troubleshooting Guide

### Common Issues

| Issue | Cause | Solution |
|-------|-------|----------|
| Low retrieval accuracy | Wrong metric/model | Verify cosine similarity; try different model |
| Embeddings all similar | Not normalized | Ensure L2 normalization |
| OOM during encoding | Batch too large | Reduce batch size |
| Slow fine-tuning | No GPU | Use GPU or reduce model size |
| Poor domain transfer | Need fine-tuning | Fine-tune on domain data |
| Inconsistent results | Pooling mismatch | Verify pooling strategy matches model |

---

## Appendix B: Glossary

| Term | Definition |
|------|------------|
| **Embedding** | Dense vector representation of text |
| **MTEB** | Massive Text Embedding Benchmark |
| **InfoNCE** | Noise Contrastive Estimation loss |
| **Triplet Loss** | Loss ensuring anchor-positive closer than anchor-negative |
| **Hard Negative** | Negative sample difficult to distinguish from positive |
| **Pooling** | Combining token embeddings into sentence embedding |
| **MRR** | Mean Reciprocal Rank |
| **NDCG** | Normalized Discounted Cumulative Gain |

---

## References

1. [Sentence-Transformers Documentation](https://sbert.net/)
2. [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)
3. [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)
4. [GTE: General Text Embeddings](https://arxiv.org/abs/2308.03281)
5. [E5: Text Embeddings by Weakly-Supervised Contrastive Pre-training](https://arxiv.org/abs/2212.03533)
6. [Contrastive Learning of Sentence Embeddings](https://dl.acm.org/doi/10.1145/3593590)
7. [Fine-tuning Sentence Transformers](https://huggingface.co/blog/train-sparse-encoder)
8. [SimCSE: Simple Contrastive Learning of Sentence Embeddings](https://arxiv.org/abs/2104.08821)
---

> **Navigation**
> [← 7.1 Vector Database](7.1_vector_database_guide.md) | **[Index](../README.md#15-repository-structure)** | [7.3 Chunking →](7.3_chunking_strategies_guide.md)
