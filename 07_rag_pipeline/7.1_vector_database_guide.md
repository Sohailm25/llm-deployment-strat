> **Navigation** | [← 6.4 Speculative Decoding](../06_model_optimization/6.4_speculative_decoding_guide.md) | [7.2 Embedding Models →](7.2_embedding_model_guide.md)
>
> | | |
> |---|---|
> | **Prerequisites** | Embedding fundamentals &#124; [Category 6 Optimization](../06_model_optimization/) |
> | **Related** | [7.2 Embedding Models](7.2_embedding_model_guide.md) &#124; [7.3 Chunking](7.3_chunking_strategies_guide.md) &#124; [7.4 Retrieval](7.4_retrieval_reranking_guide.md) |
> | **Next** | [7.2 Embedding Model Selection](7.2_embedding_model_guide.md) |

# Document 7.1: Vector Database Selection & Operations Guide

## Executive Summary

Vector databases are purpose-built systems for storing, indexing, and querying high-dimensional embedding vectors at scale. This guide covers selection criteria for major vector databases (Pinecone, Weaviate, Milvus, Qdrant, pgvector), indexing algorithms (HNSW, IVF), operational best practices, performance optimization, and production deployment patterns. Proper vector database selection and tuning is critical for RAG system performance.

## Prerequisites

- Understanding of embedding models and vector representations
- Familiarity with database operations and distributed systems
- Experience with Python and REST APIs
- Knowledge of similarity metrics (cosine, L2, inner product)
- Completion of Category 6 (Model Optimization) recommended

---

## 7.1.1 Vector Database Fundamentals

### Core Concepts

Vector databases enable efficient similarity search over millions to billions of vectors:

```python
"""
ABOUTME: Vector database fundamentals and core abstractions.
ABOUTME: Provides unified interface for multiple vector database backends.
"""

import numpy as np
from typing import List, Dict, Any, Optional, Tuple, Union
from dataclasses import dataclass, field
from enum import Enum
from abc import ABC, abstractmethod
import time


class DistanceMetric(Enum):
    """Supported distance/similarity metrics."""
    COSINE = "cosine"           # 1 - cos(a, b), range [0, 2]
    L2 = "l2"                   # Euclidean distance
    INNER_PRODUCT = "ip"        # Dot product (max = most similar)
    HAMMING = "hamming"         # For binary vectors


class IndexType(Enum):
    """Vector index algorithms."""
    FLAT = "flat"               # Exact search, O(n)
    HNSW = "hnsw"              # Hierarchical Navigable Small World
    IVF = "ivf"                # Inverted File Index
    IVF_PQ = "ivf_pq"          # IVF with Product Quantization
    ANNOY = "annoy"            # Approximate Nearest Neighbors Oh Yeah
    SCANN = "scann"            # Scalable Nearest Neighbors


@dataclass
class VectorDBConfig:
    """Configuration for vector database connection."""
    # Connection
    host: str = "localhost"
    port: int = 6333
    api_key: Optional[str] = None

    # Collection settings
    collection_name: str = "default"
    dimension: int = 768
    metric: DistanceMetric = DistanceMetric.COSINE

    # Index settings
    index_type: IndexType = IndexType.HNSW
    hnsw_m: int = 16              # HNSW connections per node
    hnsw_ef_construct: int = 200  # HNSW build-time beam width
    hnsw_ef_search: int = 100     # HNSW query-time beam width

    # Performance
    batch_size: int = 100
    num_replicas: int = 1
    num_shards: int = 1


@dataclass
class VectorRecord:
    """A single vector with metadata."""
    id: str
    vector: np.ndarray
    metadata: Dict[str, Any] = field(default_factory=dict)
    text: Optional[str] = None  # Original text if available


@dataclass
class SearchResult:
    """Result from vector similarity search."""
    id: str
    score: float                 # Similarity/distance score
    vector: Optional[np.ndarray] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    text: Optional[str] = None


class VectorDatabaseClient(ABC):
    """Abstract base class for vector database clients."""

    @abstractmethod
    def create_collection(
        self,
        name: str,
        dimension: int,
        metric: DistanceMetric = DistanceMetric.COSINE,
        **kwargs
    ) -> bool:
        """Create a new collection/index."""
        pass

    @abstractmethod
    def insert(
        self,
        records: List[VectorRecord],
        collection: Optional[str] = None
    ) -> int:
        """Insert vectors into collection."""
        pass

    @abstractmethod
    def search(
        self,
        query_vector: np.ndarray,
        top_k: int = 10,
        filter: Optional[Dict[str, Any]] = None,
        collection: Optional[str] = None
    ) -> List[SearchResult]:
        """Search for similar vectors."""
        pass

    @abstractmethod
    def delete(
        self,
        ids: List[str],
        collection: Optional[str] = None
    ) -> int:
        """Delete vectors by ID."""
        pass

    @abstractmethod
    def get_collection_stats(
        self,
        collection: Optional[str] = None
    ) -> Dict[str, Any]:
        """Get collection statistics."""
        pass


class HNSWExplainer:
    """
    Explains HNSW (Hierarchical Navigable Small World) index.

    HNSW is the most common index for production vector search:
    - Build time: O(n log n)
    - Query time: O(log n) with high recall
    - Memory: O(n * M) where M is connections per node
    """

    @staticmethod
    def explain_parameters():
        """Explain HNSW parameters and their effects."""
        return {
            'M': {
                'description': 'Number of bidirectional links per node',
                'range': '8-64',
                'default': 16,
                'effect': 'Higher M = better recall, more memory, slower build',
                'recommendation': '16 for balanced, 32-64 for high recall'
            },
            'ef_construction': {
                'description': 'Beam width during index building',
                'range': '100-500',
                'default': 200,
                'effect': 'Higher = better graph quality, slower build',
                'recommendation': '200-400 for production'
            },
            'ef_search': {
                'description': 'Beam width during search',
                'range': '50-500',
                'default': 100,
                'effect': 'Higher = better recall, slower search',
                'recommendation': 'Start at 100, tune based on recall/latency'
            }
        }

    @staticmethod
    def estimate_memory(
        num_vectors: int,
        dimension: int,
        M: int = 16
    ) -> Dict[str, float]:
        """Estimate HNSW memory requirements."""
        # Vector storage
        vector_bytes = num_vectors * dimension * 4  # float32

        # Graph storage (bidirectional links)
        # Each node has ~M links on average across layers
        # Each link is 4 bytes (int32 ID)
        graph_bytes = num_vectors * M * 2 * 4  # x2 for bidirectional

        # Overhead (metadata, node structures, ~20%)
        overhead = (vector_bytes + graph_bytes) * 0.2

        total = vector_bytes + graph_bytes + overhead

        return {
            'vectors_gb': vector_bytes / 1e9,
            'graph_gb': graph_bytes / 1e9,
            'overhead_gb': overhead / 1e9,
            'total_gb': total / 1e9,
            'vectors': num_vectors,
            'dimension': dimension,
            'M': M
        }


class IVFExplainer:
    """
    Explains IVF (Inverted File Index) for vector search.

    IVF partitions vectors into clusters:
    - Build: K-means clustering
    - Query: Search only relevant clusters
    - Trade-off: nprobe clusters searched vs recall
    """

    @staticmethod
    def explain_parameters():
        return {
            'nlist': {
                'description': 'Number of clusters/partitions',
                'range': 'sqrt(n) to 4*sqrt(n)',
                'effect': 'More clusters = faster search, lower recall',
                'recommendation': '4*sqrt(num_vectors) for large datasets'
            },
            'nprobe': {
                'description': 'Number of clusters to search',
                'range': '1 to nlist',
                'effect': 'Higher = better recall, slower search',
                'recommendation': 'nlist/16 to nlist/4'
            }
        }

    @staticmethod
    def optimal_nlist(num_vectors: int) -> int:
        """Calculate optimal nlist value."""
        if num_vectors < 1_000_000:
            return int(4 * np.sqrt(num_vectors))
        else:
            # Cap at reasonable value for very large datasets
            return min(65536, int(4 * np.sqrt(num_vectors)))
```

### Distance Metrics

Understanding when to use each metric:

```python
class DistanceMetricSelector:
    """
    Select appropriate distance metric for your use case.
    """

    METRIC_GUIDE = {
        DistanceMetric.COSINE: {
            'use_when': [
                'Text embeddings (OpenAI, Cohere, etc.)',
                'When vector magnitude doesn\'t matter',
                'Comparing semantic similarity',
                'Most common choice for RAG'
            ],
            'range': '[0, 2] where 0 = identical',
            'normalized': True,
            'note': 'Equivalent to IP on normalized vectors'
        },
        DistanceMetric.L2: {
            'use_when': [
                'Image embeddings',
                'When absolute distance matters',
                'Clustering applications',
                'Scientific/sensor data'
            ],
            'range': '[0, inf) where 0 = identical',
            'normalized': False,
            'note': 'Euclidean distance, sensitive to magnitude'
        },
        DistanceMetric.INNER_PRODUCT: {
            'use_when': [
                'Maximum inner product search (MIPS)',
                'Recommendation systems',
                'When vectors are pre-normalized',
                'Equivalent to cosine for unit vectors'
            ],
            'range': '(-inf, inf) where higher = more similar',
            'normalized': False,
            'note': 'Higher score = more similar (opposite of distance)'
        }
    }

    @staticmethod
    def recommend_metric(
        embedding_model: str,
        use_case: str
    ) -> Tuple[DistanceMetric, str]:
        """Recommend metric based on model and use case."""

        # Model-specific recommendations
        model_metrics = {
            'openai': DistanceMetric.COSINE,
            'cohere': DistanceMetric.COSINE,
            'sentence-transformers': DistanceMetric.COSINE,
            'clip': DistanceMetric.COSINE,
            'imagenet': DistanceMetric.L2,
            'colbert': DistanceMetric.INNER_PRODUCT
        }

        for model_key, metric in model_metrics.items():
            if model_key in embedding_model.lower():
                return metric, f"Recommended for {model_key} models"

        # Use case fallback
        use_case_metrics = {
            'semantic_search': DistanceMetric.COSINE,
            'rag': DistanceMetric.COSINE,
            'recommendation': DistanceMetric.INNER_PRODUCT,
            'image_similarity': DistanceMetric.L2,
            'clustering': DistanceMetric.L2
        }

        for case_key, metric in use_case_metrics.items():
            if case_key in use_case.lower():
                return metric, f"Recommended for {case_key}"

        return DistanceMetric.COSINE, "Default recommendation"
```

---

## 7.1.2 Database Selection

### Comparison Framework

```python
"""
ABOUTME: Vector database comparison and selection framework.
ABOUTME: Covers Pinecone, Weaviate, Milvus, Qdrant, and pgvector.
"""

@dataclass
class VectorDBProfile:
    """Profile of a vector database's characteristics."""
    name: str
    deployment: str              # managed, self-hosted, both
    license: str                 # open-source, proprietary
    max_vectors: int            # Recommended max vectors
    latency_p99_ms: float       # P99 query latency
    qps: int                    # Queries per second
    features: List[str]
    strengths: List[str]
    limitations: List[str]
    pricing_model: str
    best_for: List[str]


class VectorDBComparison:
    """
    Comprehensive comparison of vector databases.
    """

    PROFILES = {
        'pinecone': VectorDBProfile(
            name='Pinecone',
            deployment='managed',
            license='proprietary',
            max_vectors=10_000_000_000,
            latency_p99_ms=50,
            qps=10000,
            features=[
                'Serverless scaling',
                'Metadata filtering',
                'Namespaces',
                'Hybrid search',
                'Sparse-dense vectors'
            ],
            strengths=[
                'Zero operational overhead',
                'Excellent documentation',
                'Simple API',
                'Automatic scaling',
                'SOC2/HIPAA compliance'
            ],
            limitations=[
                'Vendor lock-in',
                'Higher cost at scale',
                'Limited customization',
                'No self-hosted option'
            ],
            pricing_model='Usage-based (read/write units)',
            best_for=[
                'Startups wanting speed to production',
                'Teams without infrastructure expertise',
                'Variable workloads',
                'Enterprise compliance requirements'
            ]
        ),

        'weaviate': VectorDBProfile(
            name='Weaviate',
            deployment='both',
            license='BSD-3-Clause',
            max_vectors=1_000_000_000,
            latency_p99_ms=50,
            qps=5000,
            features=[
                'GraphQL API',
                'Built-in vectorizers',
                'Multi-modal support',
                'Hybrid search (BM25 + vector)',
                'Modules ecosystem'
            ],
            strengths=[
                'Schema-first design',
                'Built-in embedding modules',
                'Strong filtering',
                'Knowledge graph features',
                'Active community'
            ],
            limitations=[
                'Higher memory usage',
                'Complex configuration',
                'Learning curve for GraphQL'
            ],
            pricing_model='Open source / Weaviate Cloud',
            best_for=[
                'Knowledge graph + vector search',
                'Multi-tenant SaaS',
                'Complex filtering requirements',
                'Teams comfortable with GraphQL'
            ]
        ),

        'milvus': VectorDBProfile(
            name='Milvus',
            deployment='both',
            license='Apache-2.0',
            max_vectors=10_000_000_000,
            latency_p99_ms=10,
            qps=20000,
            features=[
                'GPU acceleration',
                'Multiple index types',
                'Scalar filtering',
                'Time travel',
                'Multi-vector search'
            ],
            strengths=[
                'Highest throughput',
                'GPU support',
                'Billion-scale proven',
                'Index flexibility',
                'Cloud-native architecture'
            ],
            limitations=[
                'Complex deployment',
                'Resource intensive',
                'Steeper learning curve'
            ],
            pricing_model='Open source / Zilliz Cloud',
            best_for=[
                'Billion-scale deployments',
                'GPU-accelerated workloads',
                'Maximum throughput needs',
                'Research/experimentation'
            ]
        ),

        'qdrant': VectorDBProfile(
            name='Qdrant',
            deployment='both',
            license='Apache-2.0',
            max_vectors=1_000_000_000,
            latency_p99_ms=5,
            qps=10000,
            features=[
                'Payload filtering',
                'Quantization built-in',
                'Sparse vectors',
                'Multi-tenancy',
                'Recommendations API'
            ],
            strengths=[
                'Best-in-class filtering',
                'Written in Rust (performance)',
                'Simple deployment',
                'Excellent docs',
                'Built-in quantization'
            ],
            limitations=[
                'Smaller community than Milvus',
                'No GPU support',
                'Newer project'
            ],
            pricing_model='Open source / Qdrant Cloud',
            best_for=[
                'Complex filtering + vector search',
                'Cost-conscious teams',
                'Quick deployments',
                'Recommendation systems'
            ]
        ),

        'pgvector': VectorDBProfile(
            name='pgvector',
            deployment='self-hosted',
            license='PostgreSQL',
            max_vectors=100_000_000,
            latency_p99_ms=100,
            qps=1000,
            features=[
                'PostgreSQL integration',
                'SQL queries',
                'ACID transactions',
                'Existing tooling',
                'HNSW and IVF indexes'
            ],
            strengths=[
                'Use existing PostgreSQL',
                'Full SQL support',
                'Transactional guarantees',
                'Familiar tooling',
                'Cost effective'
            ],
            limitations=[
                'Lower performance than specialized DBs',
                'Limited to single node',
                'Fewer vector-specific features'
            ],
            pricing_model='Open source',
            best_for=[
                'Small to medium scale',
                'Teams already using PostgreSQL',
                'Strong consistency requirements',
                'Simple deployments'
            ]
        )
    }

    @classmethod
    def recommend(
        cls,
        requirements: Dict[str, Any]
    ) -> List[Tuple[str, float, str]]:
        """
        Recommend vector databases based on requirements.

        Args:
            requirements: Dict with keys like:
                - scale: 'small' (<1M), 'medium' (1-100M), 'large' (>100M)
                - deployment: 'managed', 'self-hosted', 'any'
                - budget: 'low', 'medium', 'high'
                - filtering_complexity: 'simple', 'complex'
                - latency_requirement_ms: int
                - existing_stack: List[str]

        Returns:
            List of (db_name, score, reason) sorted by score
        """
        scores = {}

        for name, profile in cls.PROFILES.items():
            score = 0.0
            reasons = []

            # Scale matching
            scale = requirements.get('scale', 'medium')
            if scale == 'small' and profile.max_vectors >= 1_000_000:
                score += 1
                reasons.append("Handles your scale")
            elif scale == 'medium' and profile.max_vectors >= 100_000_000:
                score += 1
                reasons.append("Handles medium scale")
            elif scale == 'large' and profile.max_vectors >= 1_000_000_000:
                score += 2
                reasons.append("Proven at billion scale")

            # Deployment preference
            deployment = requirements.get('deployment', 'any')
            if deployment == 'managed' and profile.deployment in ['managed', 'both']:
                score += 1.5
                reasons.append("Managed option available")
            elif deployment == 'self-hosted' and profile.deployment in ['self-hosted', 'both']:
                score += 1.5
                reasons.append("Self-hosted option")

            # Budget consideration
            budget = requirements.get('budget', 'medium')
            if budget == 'low' and profile.license != 'proprietary':
                score += 1
                reasons.append("Open source, lower cost")
            elif budget == 'high' and profile.deployment == 'managed':
                score += 0.5
                reasons.append("Enterprise support available")

            # Filtering requirements
            filtering = requirements.get('filtering_complexity', 'simple')
            if filtering == 'complex':
                if name in ['qdrant', 'weaviate']:
                    score += 1.5
                    reasons.append("Excellent filtering support")

            # Latency requirements
            latency_req = requirements.get('latency_requirement_ms', 100)
            if profile.latency_p99_ms <= latency_req:
                score += 1
                reasons.append(f"Meets {latency_req}ms latency requirement")

            # Existing stack
            stack = requirements.get('existing_stack', [])
            if 'postgresql' in stack and name == 'pgvector':
                score += 2
                reasons.append("Integrates with existing PostgreSQL")
            if 'kubernetes' in stack and name in ['milvus', 'qdrant', 'weaviate']:
                score += 0.5
                reasons.append("Good Kubernetes support")

            scores[name] = (score, "; ".join(reasons))

        # Sort by score descending
        ranked = sorted(
            [(name, score, reason) for name, (score, reason) in scores.items()],
            key=lambda x: x[1],
            reverse=True
        )

        return ranked
```

---

## 7.1.3 Implementation Examples

### Qdrant Client

```python
"""
ABOUTME: Qdrant vector database client implementation.
ABOUTME: Production-ready patterns for indexing and search.
"""

from qdrant_client import QdrantClient
from qdrant_client.http import models
from qdrant_client.http.models import Distance, VectorParams, PointStruct


class QdrantVectorDB(VectorDatabaseClient):
    """
    Qdrant vector database client.

    Qdrant is an open-source vector database written in Rust,
    known for excellent filtering performance and simple deployment.
    """

    def __init__(self, config: VectorDBConfig):
        self.config = config

        # Initialize client
        if config.api_key:
            self.client = QdrantClient(
                host=config.host,
                port=config.port,
                api_key=config.api_key,
                https=True
            )
        else:
            self.client = QdrantClient(
                host=config.host,
                port=config.port
            )

        self.collection_name = config.collection_name

    def create_collection(
        self,
        name: str,
        dimension: int,
        metric: DistanceMetric = DistanceMetric.COSINE,
        **kwargs
    ) -> bool:
        """Create collection with HNSW index."""

        # Map metric
        distance_map = {
            DistanceMetric.COSINE: Distance.COSINE,
            DistanceMetric.L2: Distance.EUCLID,
            DistanceMetric.INNER_PRODUCT: Distance.DOT
        }

        # HNSW configuration
        hnsw_config = models.HnswConfigDiff(
            m=kwargs.get('hnsw_m', self.config.hnsw_m),
            ef_construct=kwargs.get('ef_construct', self.config.hnsw_ef_construct)
        )

        # Quantization for memory efficiency (optional)
        quantization_config = None
        if kwargs.get('enable_quantization', False):
            quantization_config = models.ScalarQuantization(
                scalar=models.ScalarQuantizationConfig(
                    type=models.ScalarType.INT8,
                    quantile=0.99,
                    always_ram=True
                )
            )

        try:
            self.client.create_collection(
                collection_name=name,
                vectors_config=VectorParams(
                    size=dimension,
                    distance=distance_map[metric],
                    hnsw_config=hnsw_config,
                    quantization_config=quantization_config
                ),
                # Enable for multi-tenancy
                shard_number=kwargs.get('num_shards', self.config.num_shards),
                replication_factor=kwargs.get('num_replicas', self.config.num_replicas)
            )
            return True
        except Exception as e:
            print(f"Error creating collection: {e}")
            return False

    def insert(
        self,
        records: List[VectorRecord],
        collection: Optional[str] = None
    ) -> int:
        """Batch insert vectors with metadata."""
        collection = collection or self.collection_name

        points = [
            PointStruct(
                id=rec.id if rec.id.isdigit() else hash(rec.id) % (10**12),
                vector=rec.vector.tolist(),
                payload={
                    **rec.metadata,
                    'original_id': rec.id,
                    'text': rec.text
                } if rec.text else {
                    **rec.metadata,
                    'original_id': rec.id
                }
            )
            for rec in records
        ]

        # Batch upsert
        self.client.upsert(
            collection_name=collection,
            points=points,
            wait=True
        )

        return len(points)

    def search(
        self,
        query_vector: np.ndarray,
        top_k: int = 10,
        filter: Optional[Dict[str, Any]] = None,
        collection: Optional[str] = None
    ) -> List[SearchResult]:
        """Search with optional metadata filtering."""
        collection = collection or self.collection_name

        # Convert filter to Qdrant format
        qdrant_filter = None
        if filter:
            qdrant_filter = self._build_filter(filter)

        results = self.client.search(
            collection_name=collection,
            query_vector=query_vector.tolist(),
            limit=top_k,
            query_filter=qdrant_filter,
            with_payload=True,
            with_vectors=False,
            search_params=models.SearchParams(
                hnsw_ef=self.config.hnsw_ef_search,
                exact=False
            )
        )

        return [
            SearchResult(
                id=str(r.payload.get('original_id', r.id)),
                score=r.score,
                metadata={k: v for k, v in r.payload.items()
                         if k not in ['original_id', 'text']},
                text=r.payload.get('text')
            )
            for r in results
        ]

    def _build_filter(
        self,
        filter_dict: Dict[str, Any]
    ) -> models.Filter:
        """Convert dict filter to Qdrant filter format."""
        must_conditions = []

        for key, value in filter_dict.items():
            if isinstance(value, dict):
                # Range queries
                if '$gte' in value:
                    must_conditions.append(
                        models.FieldCondition(
                            key=key,
                            range=models.Range(gte=value['$gte'])
                        )
                    )
                if '$lte' in value:
                    must_conditions.append(
                        models.FieldCondition(
                            key=key,
                            range=models.Range(lte=value['$lte'])
                        )
                    )
                if '$in' in value:
                    must_conditions.append(
                        models.FieldCondition(
                            key=key,
                            match=models.MatchAny(any=value['$in'])
                        )
                    )
            else:
                # Exact match
                must_conditions.append(
                    models.FieldCondition(
                        key=key,
                        match=models.MatchValue(value=value)
                    )
                )

        return models.Filter(must=must_conditions)

    def delete(
        self,
        ids: List[str],
        collection: Optional[str] = None
    ) -> int:
        """Delete vectors by ID."""
        collection = collection or self.collection_name

        self.client.delete(
            collection_name=collection,
            points_selector=models.PointIdsList(
                points=[hash(id) % (10**12) for id in ids]
            )
        )

        return len(ids)

    def get_collection_stats(
        self,
        collection: Optional[str] = None
    ) -> Dict[str, Any]:
        """Get collection statistics."""
        collection = collection or self.collection_name

        info = self.client.get_collection(collection_name=collection)

        return {
            'name': collection,
            'vectors_count': info.vectors_count,
            'indexed_vectors_count': info.indexed_vectors_count,
            'points_count': info.points_count,
            'segments_count': len(info.segments) if info.segments else 0,
            'status': info.status,
            'config': {
                'dimension': info.config.params.vectors.size,
                'distance': str(info.config.params.vectors.distance)
            }
        }
```

### Pinecone Client

```python
"""
ABOUTME: Pinecone managed vector database client.
ABOUTME: Serverless deployment with automatic scaling.
"""

import pinecone


class PineconeVectorDB(VectorDatabaseClient):
    """
    Pinecone managed vector database client.

    Pinecone provides a fully managed, serverless vector database
    with automatic scaling and enterprise features.
    """

    def __init__(self, config: VectorDBConfig):
        self.config = config

        # Initialize Pinecone
        pinecone.init(
            api_key=config.api_key,
            environment=config.host  # e.g., 'us-east1-gcp'
        )

        self.index = None

    def create_collection(
        self,
        name: str,
        dimension: int,
        metric: DistanceMetric = DistanceMetric.COSINE,
        **kwargs
    ) -> bool:
        """Create Pinecone index (collection)."""

        metric_map = {
            DistanceMetric.COSINE: 'cosine',
            DistanceMetric.L2: 'euclidean',
            DistanceMetric.INNER_PRODUCT: 'dotproduct'
        }

        try:
            # Check if index exists
            if name not in pinecone.list_indexes():
                pinecone.create_index(
                    name=name,
                    dimension=dimension,
                    metric=metric_map[metric],
                    pods=kwargs.get('pods', 1),
                    replicas=kwargs.get('replicas', 1),
                    pod_type=kwargs.get('pod_type', 'p1.x1'),
                    metadata_config={
                        'indexed': kwargs.get('indexed_fields', [])
                    }
                )

            self.index = pinecone.Index(name)
            return True

        except Exception as e:
            print(f"Error creating index: {e}")
            return False

    def insert(
        self,
        records: List[VectorRecord],
        collection: Optional[str] = None
    ) -> int:
        """Upsert vectors to Pinecone."""
        if self.index is None:
            raise ValueError("Index not initialized")

        # Prepare vectors
        vectors = [
            (
                rec.id,
                rec.vector.tolist(),
                {**rec.metadata, 'text': rec.text} if rec.text else rec.metadata
            )
            for rec in records
        ]

        # Batch upsert (Pinecone limit: 100 per request)
        batch_size = 100
        for i in range(0, len(vectors), batch_size):
            batch = vectors[i:i + batch_size]
            self.index.upsert(vectors=batch)

        return len(vectors)

    def search(
        self,
        query_vector: np.ndarray,
        top_k: int = 10,
        filter: Optional[Dict[str, Any]] = None,
        collection: Optional[str] = None,
        namespace: str = ''
    ) -> List[SearchResult]:
        """Query Pinecone with optional metadata filter."""
        if self.index is None:
            raise ValueError("Index not initialized")

        results = self.index.query(
            vector=query_vector.tolist(),
            top_k=top_k,
            filter=filter,
            include_metadata=True,
            namespace=namespace
        )

        return [
            SearchResult(
                id=match.id,
                score=match.score,
                metadata={k: v for k, v in match.metadata.items() if k != 'text'},
                text=match.metadata.get('text')
            )
            for match in results.matches
        ]

    def delete(
        self,
        ids: List[str],
        collection: Optional[str] = None,
        namespace: str = ''
    ) -> int:
        """Delete vectors by ID."""
        if self.index is None:
            raise ValueError("Index not initialized")

        self.index.delete(ids=ids, namespace=namespace)
        return len(ids)

    def get_collection_stats(
        self,
        collection: Optional[str] = None
    ) -> Dict[str, Any]:
        """Get index statistics."""
        if self.index is None:
            raise ValueError("Index not initialized")

        stats = self.index.describe_index_stats()

        return {
            'total_vector_count': stats.total_vector_count,
            'dimension': stats.dimension,
            'namespaces': dict(stats.namespaces),
            'index_fullness': stats.index_fullness
        }
```

### pgvector Client

```python
"""
ABOUTME: pgvector PostgreSQL extension client.
ABOUTME: Vector search integrated with existing PostgreSQL.
"""

import psycopg2
from psycopg2.extras import execute_values


class PgVectorDB(VectorDatabaseClient):
    """
    pgvector PostgreSQL extension client.

    Enables vector similarity search within PostgreSQL,
    leveraging existing infrastructure and SQL capabilities.
    """

    def __init__(self, config: VectorDBConfig):
        self.config = config
        self.conn = psycopg2.connect(
            host=config.host,
            port=config.port,
            database=config.collection_name,
            user=config.api_key.split(':')[0] if config.api_key else 'postgres',
            password=config.api_key.split(':')[1] if config.api_key and ':' in config.api_key else ''
        )

        # Enable pgvector extension
        with self.conn.cursor() as cur:
            cur.execute("CREATE EXTENSION IF NOT EXISTS vector")
        self.conn.commit()

    def create_collection(
        self,
        name: str,
        dimension: int,
        metric: DistanceMetric = DistanceMetric.COSINE,
        **kwargs
    ) -> bool:
        """Create table with vector column and index."""

        # Operator for distance metric
        op_map = {
            DistanceMetric.COSINE: 'vector_cosine_ops',
            DistanceMetric.L2: 'vector_l2_ops',
            DistanceMetric.INNER_PRODUCT: 'vector_ip_ops'
        }

        try:
            with self.conn.cursor() as cur:
                # Create table
                cur.execute(f"""
                    CREATE TABLE IF NOT EXISTS {name} (
                        id TEXT PRIMARY KEY,
                        embedding vector({dimension}),
                        metadata JSONB,
                        text TEXT,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)

                # Create HNSW index
                index_type = kwargs.get('index_type', 'hnsw')
                if index_type == 'hnsw':
                    m = kwargs.get('hnsw_m', 16)
                    ef_construction = kwargs.get('ef_construction', 64)

                    cur.execute(f"""
                        CREATE INDEX IF NOT EXISTS {name}_embedding_idx
                        ON {name}
                        USING hnsw (embedding {op_map[metric]})
                        WITH (m = {m}, ef_construction = {ef_construction})
                    """)
                elif index_type == 'ivfflat':
                    lists = kwargs.get('lists', 100)
                    cur.execute(f"""
                        CREATE INDEX IF NOT EXISTS {name}_embedding_idx
                        ON {name}
                        USING ivfflat (embedding {op_map[metric]})
                        WITH (lists = {lists})
                    """)

                # Index on metadata for filtering
                cur.execute(f"""
                    CREATE INDEX IF NOT EXISTS {name}_metadata_idx
                    ON {name} USING gin (metadata)
                """)

            self.conn.commit()
            return True

        except Exception as e:
            self.conn.rollback()
            print(f"Error creating collection: {e}")
            return False

    def insert(
        self,
        records: List[VectorRecord],
        collection: Optional[str] = None
    ) -> int:
        """Insert vectors with ON CONFLICT upsert."""
        collection = collection or 'vectors'

        data = [
            (
                rec.id,
                rec.vector.tolist(),
                psycopg2.extras.Json(rec.metadata),
                rec.text
            )
            for rec in records
        ]

        with self.conn.cursor() as cur:
            execute_values(
                cur,
                f"""
                INSERT INTO {collection} (id, embedding, metadata, text)
                VALUES %s
                ON CONFLICT (id)
                DO UPDATE SET
                    embedding = EXCLUDED.embedding,
                    metadata = EXCLUDED.metadata,
                    text = EXCLUDED.text
                """,
                data,
                template="(%s, %s::vector, %s, %s)"
            )

        self.conn.commit()
        return len(records)

    def search(
        self,
        query_vector: np.ndarray,
        top_k: int = 10,
        filter: Optional[Dict[str, Any]] = None,
        collection: Optional[str] = None
    ) -> List[SearchResult]:
        """
        Search using pgvector distance operators.

        Operators:
        - <-> : L2 distance
        - <=> : cosine distance
        - <#> : negative inner product
        """
        collection = collection or 'vectors'

        # Build query with optional filter
        filter_clause = ""
        filter_params = []

        if filter:
            conditions = []
            for key, value in filter.items():
                if isinstance(value, dict):
                    if '$gte' in value:
                        conditions.append(f"(metadata->>'{key}')::float >= %s")
                        filter_params.append(value['$gte'])
                    if '$lte' in value:
                        conditions.append(f"(metadata->>'{key}')::float <= %s")
                        filter_params.append(value['$lte'])
                else:
                    conditions.append(f"metadata->>'{key}' = %s")
                    filter_params.append(str(value))

            if conditions:
                filter_clause = "WHERE " + " AND ".join(conditions)

        # Use cosine distance by default
        query = f"""
            SELECT id, 1 - (embedding <=> %s::vector) as score, metadata, text
            FROM {collection}
            {filter_clause}
            ORDER BY embedding <=> %s::vector
            LIMIT %s
        """

        with self.conn.cursor() as cur:
            cur.execute(
                query,
                [query_vector.tolist()] + filter_params + [query_vector.tolist(), top_k]
            )
            rows = cur.fetchall()

        return [
            SearchResult(
                id=row[0],
                score=float(row[1]),
                metadata=row[2] or {},
                text=row[3]
            )
            for row in rows
        ]

    def delete(
        self,
        ids: List[str],
        collection: Optional[str] = None
    ) -> int:
        """Delete vectors by ID."""
        collection = collection or 'vectors'

        with self.conn.cursor() as cur:
            cur.execute(
                f"DELETE FROM {collection} WHERE id = ANY(%s)",
                (ids,)
            )
            deleted = cur.rowcount

        self.conn.commit()
        return deleted

    def get_collection_stats(
        self,
        collection: Optional[str] = None
    ) -> Dict[str, Any]:
        """Get table statistics."""
        collection = collection or 'vectors'

        with self.conn.cursor() as cur:
            cur.execute(f"SELECT COUNT(*) FROM {collection}")
            count = cur.fetchone()[0]

            cur.execute(f"""
                SELECT pg_size_pretty(pg_total_relation_size('{collection}'))
            """)
            size = cur.fetchone()[0]

        return {
            'name': collection,
            'vectors_count': count,
            'total_size': size
        }
```

---

## 7.1.4 Performance Optimization

### Index Tuning

```python
"""
ABOUTME: Vector database performance optimization techniques.
ABOUTME: Covers index tuning, caching, and query optimization.
"""

class PerformanceOptimizer:
    """
    Optimize vector database performance.
    """

    @staticmethod
    def tune_hnsw_parameters(
        target_recall: float,
        target_latency_ms: float,
        num_vectors: int,
        dimension: int
    ) -> Dict[str, int]:
        """
        Recommend HNSW parameters for performance targets.

        Trade-offs:
        - Higher M = better recall, more memory
        - Higher ef_search = better recall, higher latency
        - Higher ef_construction = better graph, slower build
        """
        # Base recommendations
        params = {
            'M': 16,
            'ef_construction': 200,
            'ef_search': 100
        }

        # Adjust for recall target
        if target_recall >= 0.99:
            params['M'] = 32
            params['ef_search'] = 256
            params['ef_construction'] = 400
        elif target_recall >= 0.95:
            params['M'] = 24
            params['ef_search'] = 150
            params['ef_construction'] = 300
        elif target_recall >= 0.90:
            params['M'] = 16
            params['ef_search'] = 100
            params['ef_construction'] = 200

        # Adjust for latency target
        if target_latency_ms < 10:
            params['ef_search'] = min(params['ef_search'], 64)
        elif target_latency_ms < 50:
            params['ef_search'] = min(params['ef_search'], 128)

        # Adjust for scale
        if num_vectors > 10_000_000:
            params['M'] = min(params['M'], 24)  # Memory considerations

        return params

    @staticmethod
    def optimize_batch_size(
        vector_dimension: int,
        available_memory_gb: float,
        network_bandwidth_gbps: float = 1.0
    ) -> int:
        """
        Calculate optimal batch size for inserts.

        Factors:
        - Memory for vector batch
        - Network transfer time
        - Database processing overhead
        """
        # Memory per vector (float32)
        bytes_per_vector = vector_dimension * 4

        # Leave room for overhead (2x for processing)
        max_batch_by_memory = int(
            (available_memory_gb * 1e9 * 0.5) / bytes_per_vector
        )

        # Network-optimal batch (aim for ~1MB transfers)
        target_transfer_bytes = 1 * 1024 * 1024
        optimal_by_network = int(target_transfer_bytes / bytes_per_vector)

        # Common limits
        common_limits = [100, 500, 1000, 5000]

        # Find best match
        batch_size = min(max_batch_by_memory, optimal_by_network)

        # Round to common value
        for limit in common_limits:
            if batch_size <= limit:
                return limit

        return 5000  # Max reasonable batch


class QueryOptimizer:
    """
    Optimize vector search queries.
    """

    @staticmethod
    def optimize_filter_order(
        filters: Dict[str, Any],
        field_cardinalities: Dict[str, int]
    ) -> List[Tuple[str, Any]]:
        """
        Order filters by selectivity for optimal performance.

        More selective filters (lower cardinality) should be applied first.
        """
        filter_items = list(filters.items())

        # Sort by cardinality (lower = more selective = first)
        filter_items.sort(
            key=lambda x: field_cardinalities.get(x[0], float('inf'))
        )

        return filter_items

    @staticmethod
    def should_use_prefilter(
        filter_selectivity: float,
        num_vectors: int
    ) -> bool:
        """
        Decide between pre-filter and post-filter strategy.

        Pre-filter: Filter first, then vector search on subset
        Post-filter: Vector search first, then filter results

        Pre-filter is better when:
        - Filter is highly selective (< 10% of data)
        - Dataset is very large
        """
        if filter_selectivity < 0.1:
            return True  # Filter removes >90% of data
        elif filter_selectivity > 0.5:
            return False  # Filter is not selective
        else:
            # Gray zone - depends on dataset size
            return num_vectors > 1_000_000


class CachingStrategy:
    """
    Implement caching for vector search.
    """

    def __init__(
        self,
        cache_size_mb: int = 100,
        ttl_seconds: int = 300
    ):
        self.cache_size = cache_size_mb * 1024 * 1024
        self.ttl = ttl_seconds
        self.cache: Dict[str, Tuple[List[SearchResult], float]] = {}
        self.cache_bytes = 0

    def _compute_cache_key(
        self,
        query_vector: np.ndarray,
        top_k: int,
        filter: Optional[Dict] = None
    ) -> str:
        """Create cache key from query parameters."""
        import hashlib

        # Hash vector (quantize for better cache hits)
        quantized = (query_vector * 100).astype(np.int16)
        vector_hash = hashlib.md5(quantized.tobytes()).hexdigest()[:16]

        # Include filter in key
        filter_str = str(sorted(filter.items())) if filter else ''

        return f"{vector_hash}:{top_k}:{filter_str}"

    def get(
        self,
        query_vector: np.ndarray,
        top_k: int,
        filter: Optional[Dict] = None
    ) -> Optional[List[SearchResult]]:
        """Get cached results if available."""
        key = self._compute_cache_key(query_vector, top_k, filter)

        if key in self.cache:
            results, timestamp = self.cache[key]
            if time.time() - timestamp < self.ttl:
                return results
            else:
                # Expired
                del self.cache[key]

        return None

    def set(
        self,
        query_vector: np.ndarray,
        top_k: int,
        filter: Optional[Dict],
        results: List[SearchResult]
    ):
        """Cache search results."""
        key = self._compute_cache_key(query_vector, top_k, filter)

        # Simple LRU eviction
        if len(self.cache) > 10000:
            # Remove oldest entries
            sorted_keys = sorted(
                self.cache.keys(),
                key=lambda k: self.cache[k][1]
            )
            for old_key in sorted_keys[:1000]:
                del self.cache[old_key]

        self.cache[key] = (results, time.time())
```

---

## 7.1.5 Production Operations

### Monitoring and Maintenance

```python
"""
ABOUTME: Production operations for vector databases.
ABOUTME: Covers monitoring, backup, and scaling.
"""

@dataclass
class VectorDBMetrics:
    """Metrics to monitor for vector databases."""
    # Query performance
    query_latency_p50_ms: float
    query_latency_p99_ms: float
    queries_per_second: float

    # Index health
    index_build_progress: float
    index_memory_mb: float
    vector_count: int

    # Resource utilization
    cpu_percent: float
    memory_percent: float
    disk_usage_percent: float

    # Quality metrics
    estimated_recall: float


class VectorDBMonitor:
    """
    Monitor vector database health and performance.
    """

    def __init__(self, client: VectorDatabaseClient):
        self.client = client
        self.metrics_history: List[VectorDBMetrics] = []

    def collect_metrics(self, collection: str) -> VectorDBMetrics:
        """Collect current metrics."""
        stats = self.client.get_collection_stats(collection)

        # Run benchmark queries for latency
        latencies = self._benchmark_latency(collection, num_queries=10)

        return VectorDBMetrics(
            query_latency_p50_ms=np.percentile(latencies, 50),
            query_latency_p99_ms=np.percentile(latencies, 99),
            queries_per_second=1000 / np.mean(latencies) if latencies else 0,
            index_build_progress=1.0,  # Would come from DB
            index_memory_mb=0,  # DB-specific
            vector_count=stats.get('vectors_count', 0),
            cpu_percent=0,  # System metric
            memory_percent=0,  # System metric
            disk_usage_percent=0,  # System metric
            estimated_recall=0.95  # Would need ground truth
        )

    def _benchmark_latency(
        self,
        collection: str,
        num_queries: int = 10
    ) -> List[float]:
        """Run benchmark queries to measure latency."""
        latencies = []

        # Generate random query vectors
        dimension = 768  # Assume
        for _ in range(num_queries):
            query = np.random.randn(dimension).astype(np.float32)

            start = time.perf_counter()
            self.client.search(query, top_k=10, collection=collection)
            latency = (time.perf_counter() - start) * 1000

            latencies.append(latency)

        return latencies

    def check_health(self, collection: str) -> Dict[str, Any]:
        """Check database health status."""
        metrics = self.collect_metrics(collection)

        issues = []
        status = 'healthy'

        # Check latency
        if metrics.query_latency_p99_ms > 100:
            issues.append(f"High P99 latency: {metrics.query_latency_p99_ms:.1f}ms")
            status = 'warning'
        if metrics.query_latency_p99_ms > 500:
            status = 'critical'

        # Check recall
        if metrics.estimated_recall < 0.90:
            issues.append(f"Low recall: {metrics.estimated_recall:.2f}")
            status = 'warning'

        return {
            'status': status,
            'metrics': metrics,
            'issues': issues
        }


class BackupManager:
    """
    Manage vector database backups.
    """

    @staticmethod
    def export_collection(
        client: VectorDatabaseClient,
        collection: str,
        output_path: str,
        batch_size: int = 1000
    ) -> int:
        """Export collection to file."""
        import json

        stats = client.get_collection_stats(collection)
        total_vectors = stats.get('vectors_count', 0)

        exported = 0

        with open(output_path, 'w') as f:
            # Would need scroll/pagination API
            # This is a simplified example
            pass

        return exported

    @staticmethod
    def create_snapshot(
        db_type: str,
        collection: str,
        snapshot_path: str
    ) -> bool:
        """Create database-specific snapshot."""
        # Database-specific snapshot commands
        snapshot_commands = {
            'qdrant': f"curl -X POST 'http://localhost:6333/collections/{collection}/snapshots'",
            'milvus': f"milvus-cli create snapshot -c {collection}",
            'weaviate': f"weaviate backup create {collection}"
        }

        import subprocess

        if db_type in snapshot_commands:
            result = subprocess.run(
                snapshot_commands[db_type],
                shell=True,
                capture_output=True
            )
            return result.returncode == 0

        return False


class ScalingAdvisor:
    """
    Advise on scaling vector database infrastructure.
    """

    @staticmethod
    def recommend_scaling(
        current_vectors: int,
        projected_vectors: int,
        current_qps: float,
        target_qps: float,
        current_latency_ms: float,
        target_latency_ms: float
    ) -> Dict[str, Any]:
        """Recommend scaling actions."""
        recommendations = []

        # Vector count scaling
        if projected_vectors > current_vectors * 2:
            recommendations.append({
                'type': 'horizontal_scale',
                'reason': 'Vector count growth',
                'action': 'Add shards to distribute vectors'
            })

        # QPS scaling
        qps_ratio = target_qps / current_qps if current_qps > 0 else float('inf')
        if qps_ratio > 1.5:
            recommendations.append({
                'type': 'add_replicas',
                'reason': 'QPS requirement increase',
                'action': f'Add {int(qps_ratio)} replicas for read scaling'
            })

        # Latency scaling
        if target_latency_ms < current_latency_ms * 0.5:
            recommendations.append({
                'type': 'index_optimization',
                'reason': 'Latency reduction needed',
                'action': 'Tune HNSW ef_search or add memory'
            })

        return {
            'current_state': {
                'vectors': current_vectors,
                'qps': current_qps,
                'latency_ms': current_latency_ms
            },
            'targets': {
                'vectors': projected_vectors,
                'qps': target_qps,
                'latency_ms': target_latency_ms
            },
            'recommendations': recommendations
        }
```

---

## Appendix A: Troubleshooting Guide

### Common Issues

| Issue | Cause | Solution |
|-------|-------|----------|
| High latency | ef_search too high | Reduce ef_search, accept lower recall |
| Low recall | ef_search too low | Increase ef_search or M |
| OOM during indexing | Not enough RAM | Use disk-based index or quantization |
| Slow inserts | Batch size too small | Increase batch size to 500-1000 |
| Filter timeouts | Unindexed filter fields | Create secondary indexes on filter fields |
| Inconsistent results | Approximate search | Use exact search for determinism |

---

## Appendix B: Glossary

| Term | Definition |
|------|------------|
| **HNSW** | Hierarchical Navigable Small World - graph-based ANN index |
| **IVF** | Inverted File Index - cluster-based ANN index |
| **ANN** | Approximate Nearest Neighbors |
| **Recall** | Fraction of true nearest neighbors found |
| **QPS** | Queries Per Second |
| **Embedding** | Dense vector representation of data |
| **Cosine Similarity** | Angle-based similarity measure |
| **Sharding** | Distributing data across multiple nodes |

---

## References

1. [Best Vector Databases 2025 - Firecrawl](https://www.firecrawl.dev/blog/best-vector-databases-2025)
2. [Vector Database Comparison - LiquidMetal AI](https://liquidmetal.ai/casesAndBlogs/vector-comparison/)
3. [Qdrant Documentation](https://qdrant.tech/documentation/)
4. [Pinecone Documentation](https://docs.pinecone.io/)
5. [Milvus Documentation](https://milvus.io/docs)
6. [Weaviate Documentation](https://weaviate.io/developers/weaviate)
7. [pgvector Documentation](https://github.com/pgvector/pgvector)
8. [HNSW Paper: Efficient and robust approximate nearest neighbor search](https://arxiv.org/abs/1603.09320)
---

> **Navigation**
> [← 6.4 Speculative Decoding](../06_model_optimization/6.4_speculative_decoding_guide.md) | **[Index](../README.md#15-repository-structure)** | [7.2 Embedding Models →](7.2_embedding_model_guide.md)
