# Document 7.5: RAG Evaluation Guide

## Executive Summary

RAG system evaluation requires measuring both retrieval quality (context precision, recall) and generation quality (faithfulness, answer relevance). This guide covers the RAGAS framework, component-level evaluation, end-to-end testing, human evaluation protocols, and continuous monitoring. Comprehensive RAG evaluation goes beyond traditional metrics like BLEU/ROUGE to capture factual accuracy, context utilization, and answer completeness.

## Prerequisites

- Understanding of RAG system architecture
- Familiarity with retrieval and generation metrics
- Knowledge of LLM-as-Judge evaluation
- Experience with evaluation frameworks
- Completion of documents 7.1-7.4 recommended

---

## 7.5.1 RAG Evaluation Fundamentals

### Evaluation Framework Overview

```python
"""
ABOUTME: Core RAG evaluation framework and metrics.
ABOUTME: Provides comprehensive assessment of retrieval and generation.
"""

import numpy as np
from typing import List, Dict, Any, Optional, Tuple, Callable
from dataclasses import dataclass, field
from enum import Enum
from abc import ABC, abstractmethod


class EvaluationComponent(Enum):
    """Components that can be evaluated in RAG."""
    RETRIEVER = "retriever"
    GENERATOR = "generator"
    END_TO_END = "end_to_end"
    CHUNKING = "chunking"


@dataclass
class RAGSample:
    """A single RAG evaluation sample."""
    query: str
    retrieved_contexts: List[str]
    generated_answer: str
    ground_truth_answer: Optional[str] = None
    ground_truth_contexts: Optional[List[str]] = None
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class EvaluationResult:
    """Results from RAG evaluation."""
    # Core RAGAS metrics
    faithfulness: float = 0.0
    answer_relevancy: float = 0.0
    context_precision: float = 0.0
    context_recall: float = 0.0

    # Additional metrics
    context_relevancy: float = 0.0
    answer_correctness: float = 0.0
    answer_similarity: float = 0.0

    # Aggregate
    ragas_score: float = 0.0

    # Per-sample details
    sample_scores: List[Dict[str, float]] = field(default_factory=list)

    def compute_ragas_score(self):
        """Compute overall RAGAS score."""
        metrics = [
            self.faithfulness,
            self.answer_relevancy,
            self.context_precision,
            self.context_recall
        ]
        self.ragas_score = np.mean([m for m in metrics if m > 0])


class RAGEvaluator:
    """
    Comprehensive RAG system evaluator.

    Implements RAGAS-style metrics plus additional
    retrieval and generation quality measures.
    """

    def __init__(
        self,
        llm_judge: Any,
        embedding_model: Optional[Any] = None
    ):
        self.llm = llm_judge
        self.embedding_model = embedding_model

    def evaluate(
        self,
        samples: List[RAGSample],
        metrics: Optional[List[str]] = None
    ) -> EvaluationResult:
        """
        Evaluate RAG samples on specified metrics.

        Default metrics:
        - faithfulness
        - answer_relevancy
        - context_precision
        - context_recall
        """
        if metrics is None:
            metrics = [
                'faithfulness',
                'answer_relevancy',
                'context_precision',
                'context_recall'
            ]

        result = EvaluationResult()
        sample_scores = []

        for sample in samples:
            scores = {}

            if 'faithfulness' in metrics:
                scores['faithfulness'] = self._compute_faithfulness(sample)

            if 'answer_relevancy' in metrics:
                scores['answer_relevancy'] = self._compute_answer_relevancy(sample)

            if 'context_precision' in metrics:
                scores['context_precision'] = self._compute_context_precision(sample)

            if 'context_recall' in metrics:
                scores['context_recall'] = self._compute_context_recall(sample)

            if 'context_relevancy' in metrics:
                scores['context_relevancy'] = self._compute_context_relevancy(sample)

            if 'answer_correctness' in metrics:
                scores['answer_correctness'] = self._compute_answer_correctness(sample)

            sample_scores.append(scores)

        # Aggregate scores
        result.sample_scores = sample_scores

        for metric in metrics:
            metric_scores = [s.get(metric, 0) for s in sample_scores if metric in s]
            if metric_scores:
                setattr(result, metric, np.mean(metric_scores))

        result.compute_ragas_score()

        return result

    def _compute_faithfulness(self, sample: RAGSample) -> float:
        """
        Compute faithfulness score.

        Measures factual consistency between answer and context.
        Score = (statements supported by context) / (total statements)
        """
        # Extract claims from answer
        claims_prompt = f"""Extract factual claims from this answer as a numbered list.

Answer: {sample.generated_answer}

List each distinct factual claim:"""

        claims_response = self.llm.generate(claims_prompt)
        claims = self._parse_numbered_list(claims_response)

        if not claims:
            return 1.0  # No claims to verify

        # Verify each claim against context
        context = "\n\n".join(sample.retrieved_contexts)
        supported = 0

        for claim in claims:
            verify_prompt = f"""Based ONLY on the provided context, determine if this claim is supported.

Context:
{context}

Claim: {claim}

Answer with only SUPPORTED, CONTRADICTED, or NOT_FOUND:"""

            verdict = self.llm.generate(verify_prompt).strip().upper()

            if 'SUPPORTED' in verdict:
                supported += 1

        return supported / len(claims)

    def _compute_answer_relevancy(self, sample: RAGSample) -> float:
        """
        Compute answer relevancy score.

        Measures how well the answer addresses the query.
        """
        # Generate questions that the answer would address
        gen_questions_prompt = f"""Given this answer, generate 3 questions that this answer would be a good response to.

Answer: {sample.generated_answer}

Questions:"""

        gen_questions = self.llm.generate(gen_questions_prompt)
        generated_qs = self._parse_numbered_list(gen_questions)

        if not generated_qs or not self.embedding_model:
            # Fallback to direct relevancy check
            return self._direct_relevancy_check(sample)

        # Compute similarity between original query and generated questions
        original_emb = self.embedding_model.encode([sample.query])[0]
        gen_embs = self.embedding_model.encode(generated_qs)

        similarities = [
            np.dot(original_emb, gen_emb) / (
                np.linalg.norm(original_emb) * np.linalg.norm(gen_emb)
            )
            for gen_emb in gen_embs
        ]

        return float(np.mean(similarities))

    def _direct_relevancy_check(self, sample: RAGSample) -> float:
        """Direct LLM-based relevancy scoring."""
        prompt = f"""Rate how relevant this answer is to the question on a scale of 0-10.

Question: {sample.query}
Answer: {sample.generated_answer}

Score (0-10):"""

        response = self.llm.generate(prompt)

        try:
            import re
            match = re.search(r'(\d+(?:\.\d+)?)', response)
            if match:
                return float(match.group(1)) / 10
        except:
            pass

        return 0.5  # Default

    def _compute_context_precision(self, sample: RAGSample) -> float:
        """
        Compute context precision score.

        Measures if relevant contexts are ranked higher.
        Precision@K with relevance weighting.
        """
        if not sample.retrieved_contexts:
            return 0.0

        # Score relevance of each context
        relevance_scores = []

        for i, context in enumerate(sample.retrieved_contexts):
            prompt = f"""Rate how relevant this context is to answering the question.

Question: {sample.query}
Context: {context[:500]}

Score (0-10):"""

            response = self.llm.generate(prompt)

            try:
                import re
                match = re.search(r'(\d+(?:\.\d+)?)', response)
                if match:
                    score = float(match.group(1)) / 10
                else:
                    score = 0.5
            except:
                score = 0.5

            relevance_scores.append(score)

        # Compute precision with rank weighting
        # Higher ranked contexts should be more relevant
        n = len(relevance_scores)
        weighted_sum = 0
        cumulative_relevant = 0

        for i, score in enumerate(relevance_scores):
            if score >= 0.5:  # Consider relevant if score >= 0.5
                cumulative_relevant += 1
                weighted_sum += cumulative_relevant / (i + 1)

        if cumulative_relevant == 0:
            return 0.0

        return weighted_sum / cumulative_relevant

    def _compute_context_recall(self, sample: RAGSample) -> float:
        """
        Compute context recall score.

        Measures if all necessary information is retrieved.
        Requires ground truth answer or contexts.
        """
        if sample.ground_truth_answer is None:
            return self._estimate_recall_without_ground_truth(sample)

        # Extract key information from ground truth
        extract_prompt = f"""Extract the key facts needed to answer this question from the ground truth.

Question: {sample.query}
Ground Truth Answer: {sample.ground_truth_answer}

List key facts:"""

        key_facts = self._parse_numbered_list(self.llm.generate(extract_prompt))

        if not key_facts:
            return 1.0

        # Check if each fact is present in retrieved contexts
        context = "\n\n".join(sample.retrieved_contexts)
        found = 0

        for fact in key_facts:
            check_prompt = f"""Is this fact present or supported in the context?

Context:
{context}

Fact: {fact}

Answer YES or NO:"""

            response = self.llm.generate(check_prompt).strip().upper()

            if 'YES' in response:
                found += 1

        return found / len(key_facts)

    def _estimate_recall_without_ground_truth(self, sample: RAGSample) -> float:
        """Estimate recall when ground truth is not available."""
        # Check if answer seems complete based on context
        prompt = f"""Does the retrieved context contain enough information to fully answer the question?

Question: {sample.query}

Retrieved Context:
{' '.join(sample.retrieved_contexts)[:2000]}

Rate completeness (0-10):"""

        response = self.llm.generate(prompt)

        try:
            import re
            match = re.search(r'(\d+(?:\.\d+)?)', response)
            if match:
                return float(match.group(1)) / 10
        except:
            pass

        return 0.5

    def _compute_context_relevancy(self, sample: RAGSample) -> float:
        """
        Compute overall context relevancy.

        Measures how much of retrieved context is useful.
        """
        if not sample.retrieved_contexts:
            return 0.0

        useful_sentences = 0
        total_sentences = 0

        context = " ".join(sample.retrieved_contexts)
        sentences = context.split('. ')

        for sentence in sentences:
            if len(sentence.strip()) < 10:
                continue

            total_sentences += 1

            prompt = f"""Is this sentence useful for answering the question?

Question: {sample.query}
Sentence: {sentence}

Answer YES or NO:"""

            response = self.llm.generate(prompt).strip().upper()

            if 'YES' in response:
                useful_sentences += 1

        if total_sentences == 0:
            return 0.0

        return useful_sentences / total_sentences

    def _compute_answer_correctness(self, sample: RAGSample) -> float:
        """
        Compute answer correctness against ground truth.
        """
        if sample.ground_truth_answer is None:
            return -1  # Cannot compute without ground truth

        prompt = f"""Compare the generated answer to the ground truth answer.

Question: {sample.query}
Generated Answer: {sample.generated_answer}
Ground Truth: {sample.ground_truth_answer}

Rate correctness (0-10) considering:
- Factual accuracy
- Completeness
- No contradictions

Score:"""

        response = self.llm.generate(prompt)

        try:
            import re
            match = re.search(r'(\d+(?:\.\d+)?)', response)
            if match:
                return float(match.group(1)) / 10
        except:
            pass

        return 0.5

    def _parse_numbered_list(self, text: str) -> List[str]:
        """Parse numbered list from LLM response."""
        import re

        items = []
        for line in text.strip().split('\n'):
            # Remove numbering and clean
            cleaned = re.sub(r'^\d+[\.\)]\s*', '', line.strip())
            if cleaned and len(cleaned) > 5:
                items.append(cleaned)

        return items
```

---

## 7.5.2 Retriever Evaluation

### Component-Level Retrieval Metrics

```python
"""
ABOUTME: Retriever-specific evaluation metrics.
ABOUTME: Measures retrieval quality independent of generation.
"""

@dataclass
class RetrievalSample:
    """Sample for retrieval evaluation."""
    query: str
    retrieved_docs: List[str]
    retrieved_scores: List[float]
    relevant_docs: List[str]  # Ground truth
    doc_ids: Optional[List[str]] = None


class RetrieverEvaluator:
    """
    Evaluate retrieval component of RAG.

    Metrics:
    - Precision@K
    - Recall@K
    - NDCG@K
    - MRR
    - Hit Rate
    """

    def evaluate(
        self,
        samples: List[RetrievalSample],
        k_values: List[int] = [1, 3, 5, 10]
    ) -> Dict[str, float]:
        """Evaluate retrieval on standard metrics."""
        metrics = {}

        for k in k_values:
            precisions = []
            recalls = []
            ndcgs = []
            hits = []

            for sample in samples:
                # Precision@K
                p = self._precision_at_k(sample, k)
                precisions.append(p)

                # Recall@K
                r = self._recall_at_k(sample, k)
                recalls.append(r)

                # NDCG@K
                n = self._ndcg_at_k(sample, k)
                ndcgs.append(n)

                # Hit@K
                h = self._hit_at_k(sample, k)
                hits.append(h)

            metrics[f'precision@{k}'] = np.mean(precisions)
            metrics[f'recall@{k}'] = np.mean(recalls)
            metrics[f'ndcg@{k}'] = np.mean(ndcgs)
            metrics[f'hit@{k}'] = np.mean(hits)

        # MRR
        mrrs = [self._mrr(sample) for sample in samples]
        metrics['mrr'] = np.mean(mrrs)

        return metrics

    def _precision_at_k(self, sample: RetrievalSample, k: int) -> float:
        """Precision at K."""
        retrieved_k = sample.retrieved_docs[:k]
        relevant_set = set(sample.relevant_docs)

        hits = sum(1 for doc in retrieved_k if doc in relevant_set)
        return hits / k if k > 0 else 0

    def _recall_at_k(self, sample: RetrievalSample, k: int) -> float:
        """Recall at K."""
        retrieved_k = sample.retrieved_docs[:k]
        relevant_set = set(sample.relevant_docs)

        if not relevant_set:
            return 1.0

        hits = sum(1 for doc in retrieved_k if doc in relevant_set)
        return hits / len(relevant_set)

    def _ndcg_at_k(self, sample: RetrievalSample, k: int) -> float:
        """Normalized Discounted Cumulative Gain at K."""
        relevant_set = set(sample.relevant_docs)

        # DCG
        dcg = 0
        for i, doc in enumerate(sample.retrieved_docs[:k]):
            if doc in relevant_set:
                dcg += 1 / np.log2(i + 2)

        # Ideal DCG
        ideal_k = min(k, len(sample.relevant_docs))
        idcg = sum(1 / np.log2(i + 2) for i in range(ideal_k))

        return dcg / idcg if idcg > 0 else 0

    def _hit_at_k(self, sample: RetrievalSample, k: int) -> float:
        """Binary: any relevant doc in top K."""
        retrieved_k = set(sample.retrieved_docs[:k])
        relevant_set = set(sample.relevant_docs)

        return 1.0 if retrieved_k & relevant_set else 0.0

    def _mrr(self, sample: RetrievalSample) -> float:
        """Mean Reciprocal Rank."""
        relevant_set = set(sample.relevant_docs)

        for i, doc in enumerate(sample.retrieved_docs):
            if doc in relevant_set:
                return 1 / (i + 1)

        return 0.0


class SemanticRetrievalEvaluator:
    """
    Evaluate retrieval using semantic similarity.

    Useful when exact doc matching isn't appropriate.
    """

    def __init__(self, embedding_model: Any):
        self.embedding_model = embedding_model

    def evaluate(
        self,
        samples: List[RetrievalSample],
        similarity_threshold: float = 0.8
    ) -> Dict[str, float]:
        """Evaluate with semantic matching."""
        metrics = {
            'semantic_precision': [],
            'semantic_recall': [],
            'avg_relevance': []
        }

        for sample in samples:
            # Encode all docs
            retrieved_embs = self.embedding_model.encode(sample.retrieved_docs)
            relevant_embs = self.embedding_model.encode(sample.relevant_docs)

            # For each retrieved doc, check if semantically similar to any relevant
            semantic_hits = 0
            relevance_scores = []

            for ret_emb in retrieved_embs:
                max_sim = 0
                for rel_emb in relevant_embs:
                    sim = np.dot(ret_emb, rel_emb) / (
                        np.linalg.norm(ret_emb) * np.linalg.norm(rel_emb)
                    )
                    max_sim = max(max_sim, sim)

                relevance_scores.append(max_sim)

                if max_sim >= similarity_threshold:
                    semantic_hits += 1

            metrics['semantic_precision'].append(
                semantic_hits / len(sample.retrieved_docs) if sample.retrieved_docs else 0
            )
            metrics['avg_relevance'].append(np.mean(relevance_scores) if relevance_scores else 0)

            # Semantic recall
            recalled = 0
            for rel_emb in relevant_embs:
                max_sim = max(
                    np.dot(rel_emb, ret_emb) / (
                        np.linalg.norm(rel_emb) * np.linalg.norm(ret_emb)
                    )
                    for ret_emb in retrieved_embs
                ) if retrieved_embs.size > 0 else 0

                if max_sim >= similarity_threshold:
                    recalled += 1

            metrics['semantic_recall'].append(
                recalled / len(sample.relevant_docs) if sample.relevant_docs else 1
            )

        return {k: np.mean(v) for k, v in metrics.items()}
```

---

## 7.5.3 Generator Evaluation

### Generation Quality Metrics

```python
"""
ABOUTME: Generator-specific evaluation for RAG systems.
ABOUTME: Measures answer quality, hallucination, and completeness.
"""

@dataclass
class GenerationSample:
    """Sample for generation evaluation."""
    query: str
    context: str
    generated_answer: str
    ground_truth_answer: Optional[str] = None


class GeneratorEvaluator:
    """
    Evaluate generation component of RAG.

    Metrics:
    - Faithfulness (no hallucination)
    - Completeness
    - Coherence
    - Fluency
    """

    def __init__(self, llm_judge: Any):
        self.llm = llm_judge

    def evaluate(
        self,
        samples: List[GenerationSample]
    ) -> Dict[str, float]:
        """Evaluate generation quality."""
        metrics = {
            'faithfulness': [],
            'completeness': [],
            'coherence': [],
            'fluency': [],
            'hallucination_rate': []
        }

        for sample in samples:
            # Faithfulness
            faith = self._evaluate_faithfulness(sample)
            metrics['faithfulness'].append(faith)

            # Hallucination detection
            halluc = self._detect_hallucination(sample)
            metrics['hallucination_rate'].append(1 if halluc else 0)

            # Completeness
            comp = self._evaluate_completeness(sample)
            metrics['completeness'].append(comp)

            # Coherence
            coh = self._evaluate_coherence(sample)
            metrics['coherence'].append(coh)

            # Fluency
            flu = self._evaluate_fluency(sample)
            metrics['fluency'].append(flu)

        return {k: np.mean(v) for k, v in metrics.items()}

    def _evaluate_faithfulness(self, sample: GenerationSample) -> float:
        """Check if answer is faithful to context."""
        prompt = f"""Evaluate if the answer contains only information that can be found in or inferred from the context.

Context: {sample.context[:2000]}

Answer: {sample.generated_answer}

Rate faithfulness (0-10):
- 10: Every claim is directly supported by context
- 5: Some claims supported, some not
- 0: Contains significant unsupported claims

Score:"""

        response = self.llm.generate(prompt)

        try:
            import re
            match = re.search(r'(\d+(?:\.\d+)?)', response)
            if match:
                return float(match.group(1)) / 10
        except:
            pass

        return 0.5

    def _detect_hallucination(self, sample: GenerationSample) -> bool:
        """Detect if answer contains hallucinations."""
        prompt = f"""Does this answer contain any information that is NOT supported by the context?

Context: {sample.context[:2000]}

Answer: {sample.generated_answer}

Reply with only YES (contains hallucination) or NO (faithful to context):"""

        response = self.llm.generate(prompt).strip().upper()

        return 'YES' in response

    def _evaluate_completeness(self, sample: GenerationSample) -> float:
        """Evaluate if answer fully addresses the question."""
        prompt = f"""Does the answer fully address all aspects of the question?

Question: {sample.query}
Answer: {sample.generated_answer}

Rate completeness (0-10):
- 10: Addresses all aspects thoroughly
- 5: Addresses main points but misses some details
- 0: Fails to address the question

Score:"""

        response = self.llm.generate(prompt)

        try:
            import re
            match = re.search(r'(\d+(?:\.\d+)?)', response)
            if match:
                return float(match.group(1)) / 10
        except:
            pass

        return 0.5

    def _evaluate_coherence(self, sample: GenerationSample) -> float:
        """Evaluate logical coherence of answer."""
        prompt = f"""Evaluate the logical coherence of this answer.

Answer: {sample.generated_answer}

Rate coherence (0-10):
- 10: Perfectly logical flow, well-structured
- 5: Generally coherent with minor issues
- 0: Incoherent, contradictory, or confusing

Score:"""

        response = self.llm.generate(prompt)

        try:
            import re
            match = re.search(r'(\d+(?:\.\d+)?)', response)
            if match:
                return float(match.group(1)) / 10
        except:
            pass

        return 0.5

    def _evaluate_fluency(self, sample: GenerationSample) -> float:
        """Evaluate language fluency."""
        prompt = f"""Rate the language quality and fluency of this text.

Text: {sample.generated_answer}

Rate fluency (0-10):
- 10: Perfect grammar, natural flow
- 5: Minor errors, still readable
- 0: Poor grammar, hard to understand

Score:"""

        response = self.llm.generate(prompt)

        try:
            import re
            match = re.search(r'(\d+(?:\.\d+)?)', response)
            if match:
                return float(match.group(1)) / 10
        except:
            pass

        return 0.5


class HallucinationDetector:
    """
    Specialized hallucination detection for RAG.
    """

    def __init__(self, llm_judge: Any, embedding_model: Optional[Any] = None):
        self.llm = llm_judge
        self.embedding_model = embedding_model

    def detect_hallucinations(
        self,
        answer: str,
        context: str
    ) -> Dict[str, Any]:
        """
        Detect and classify hallucinations.

        Types:
        - Intrinsic: Contradicts context
        - Extrinsic: Not in context (could be true or false)
        - Fabrication: Made up facts
        """
        # Extract claims
        claims_prompt = f"""Extract all factual claims from this text as a numbered list.

Text: {answer}

Claims:"""

        claims_response = self.llm.generate(claims_prompt)
        claims = self._parse_claims(claims_response)

        results = {
            'total_claims': len(claims),
            'supported_claims': 0,
            'contradicted_claims': 0,
            'extrinsic_claims': 0,
            'hallucinated_claims': [],
            'hallucination_rate': 0.0
        }

        for claim in claims:
            verdict = self._verify_claim(claim, context)

            if verdict == 'SUPPORTED':
                results['supported_claims'] += 1
            elif verdict == 'CONTRADICTED':
                results['contradicted_claims'] += 1
                results['hallucinated_claims'].append({
                    'claim': claim,
                    'type': 'intrinsic'
                })
            else:  # NOT_FOUND
                results['extrinsic_claims'] += 1
                results['hallucinated_claims'].append({
                    'claim': claim,
                    'type': 'extrinsic'
                })

        if claims:
            results['hallucination_rate'] = (
                results['contradicted_claims'] + results['extrinsic_claims']
            ) / len(claims)

        return results

    def _parse_claims(self, text: str) -> List[str]:
        """Parse claims from response."""
        import re
        claims = []
        for line in text.strip().split('\n'):
            cleaned = re.sub(r'^\d+[\.\)]\s*', '', line.strip())
            if cleaned and len(cleaned) > 10:
                claims.append(cleaned)
        return claims

    def _verify_claim(self, claim: str, context: str) -> str:
        """Verify single claim against context."""
        prompt = f"""Based ONLY on the context below, classify this claim:

Context: {context[:2000]}

Claim: {claim}

Respond with exactly one of:
- SUPPORTED: Claim is directly supported by context
- CONTRADICTED: Claim contradicts the context
- NOT_FOUND: Claim is not mentioned in context

Classification:"""

        response = self.llm.generate(prompt).strip().upper()

        if 'SUPPORTED' in response:
            return 'SUPPORTED'
        elif 'CONTRADICTED' in response:
            return 'CONTRADICTED'
        else:
            return 'NOT_FOUND'
```

---

## 7.5.4 End-to-End Evaluation

### Comprehensive RAG Testing

```python
"""
ABOUTME: End-to-end RAG system evaluation.
ABOUTME: Tests complete pipeline from query to answer.
"""

@dataclass
class E2ESample:
    """End-to-end test sample."""
    query: str
    ground_truth_answer: str
    expected_sources: Optional[List[str]] = None
    difficulty: str = "medium"
    category: str = "general"


class EndToEndEvaluator:
    """
    Evaluate complete RAG pipeline.
    """

    def __init__(
        self,
        rag_system: Any,
        llm_judge: Any,
        embedding_model: Optional[Any] = None
    ):
        self.rag = rag_system
        self.llm = llm_judge
        self.embedding_model = embedding_model

    def evaluate(
        self,
        test_samples: List[E2ESample],
        verbose: bool = False
    ) -> Dict[str, Any]:
        """Run end-to-end evaluation."""
        results = {
            'overall_score': 0.0,
            'answer_quality': [],
            'retrieval_quality': [],
            'latencies': [],
            'by_category': {},
            'by_difficulty': {},
            'failures': []
        }

        for sample in test_samples:
            try:
                import time
                start = time.perf_counter()

                # Run RAG
                rag_result = self.rag.query(sample.query)
                latency = time.perf_counter() - start

                results['latencies'].append(latency)

                # Evaluate answer
                answer_score = self._score_answer(
                    query=sample.query,
                    generated=rag_result.answer,
                    ground_truth=sample.ground_truth_answer
                )
                results['answer_quality'].append(answer_score)

                # Evaluate retrieval if we have expected sources
                if sample.expected_sources and rag_result.contexts:
                    retrieval_score = self._score_retrieval(
                        retrieved=rag_result.contexts,
                        expected=sample.expected_sources
                    )
                    results['retrieval_quality'].append(retrieval_score)

                # Track by category
                if sample.category not in results['by_category']:
                    results['by_category'][sample.category] = []
                results['by_category'][sample.category].append(answer_score)

                # Track by difficulty
                if sample.difficulty not in results['by_difficulty']:
                    results['by_difficulty'][sample.difficulty] = []
                results['by_difficulty'][sample.difficulty].append(answer_score)

                if verbose:
                    print(f"Query: {sample.query[:50]}...")
                    print(f"Answer score: {answer_score:.2f}")
                    print(f"Latency: {latency:.2f}s")
                    print("-" * 50)

            except Exception as e:
                results['failures'].append({
                    'query': sample.query,
                    'error': str(e)
                })

        # Compute aggregates
        results['overall_score'] = np.mean(results['answer_quality']) if results['answer_quality'] else 0
        results['avg_latency'] = np.mean(results['latencies']) if results['latencies'] else 0
        results['failure_rate'] = len(results['failures']) / len(test_samples)

        # Aggregate by category and difficulty
        for cat, scores in results['by_category'].items():
            results['by_category'][cat] = np.mean(scores)

        for diff, scores in results['by_difficulty'].items():
            results['by_difficulty'][diff] = np.mean(scores)

        return results

    def _score_answer(
        self,
        query: str,
        generated: str,
        ground_truth: str
    ) -> float:
        """Score generated answer against ground truth."""
        prompt = f"""Compare the generated answer to the reference answer.

Question: {query}

Generated Answer: {generated}

Reference Answer: {ground_truth}

Score the generated answer (0-10) based on:
- Factual correctness
- Completeness
- Relevance to question

Provide only the numeric score:"""

        response = self.llm.generate(prompt)

        try:
            import re
            match = re.search(r'(\d+(?:\.\d+)?)', response)
            if match:
                return float(match.group(1)) / 10
        except:
            pass

        return 0.5

    def _score_retrieval(
        self,
        retrieved: List[str],
        expected: List[str]
    ) -> float:
        """Score retrieval against expected sources."""
        if not self.embedding_model:
            # Exact match fallback
            ret_set = set(s[:100] for s in retrieved)
            exp_set = set(s[:100] for s in expected)
            overlap = len(ret_set & exp_set)
            return overlap / len(exp_set) if exp_set else 0

        # Semantic matching
        ret_embs = self.embedding_model.encode(retrieved)
        exp_embs = self.embedding_model.encode(expected)

        matched = 0
        for exp_emb in exp_embs:
            sims = [
                np.dot(exp_emb, ret_emb) / (
                    np.linalg.norm(exp_emb) * np.linalg.norm(ret_emb)
                )
                for ret_emb in ret_embs
            ]
            if max(sims) > 0.8:
                matched += 1

        return matched / len(expected)


class RegressionTester:
    """
    Track RAG performance over time for regression detection.
    """

    def __init__(
        self,
        evaluator: EndToEndEvaluator,
        baseline_results: Optional[Dict] = None
    ):
        self.evaluator = evaluator
        self.baseline = baseline_results
        self.history = []

    def run_regression_test(
        self,
        test_samples: List[E2ESample],
        threshold: float = 0.05  # 5% regression threshold
    ) -> Dict[str, Any]:
        """Run regression test."""
        current = self.evaluator.evaluate(test_samples)
        self.history.append(current)

        result = {
            'current_score': current['overall_score'],
            'baseline_score': self.baseline['overall_score'] if self.baseline else None,
            'regression_detected': False,
            'regression_amount': 0.0,
            'improved_categories': [],
            'regressed_categories': []
        }

        if self.baseline:
            delta = self.baseline['overall_score'] - current['overall_score']
            result['regression_amount'] = delta

            if delta > threshold:
                result['regression_detected'] = True

            # Check by category
            for cat, score in current['by_category'].items():
                if cat in self.baseline['by_category']:
                    baseline_cat = self.baseline['by_category'][cat]
                    if score > baseline_cat + 0.05:
                        result['improved_categories'].append(cat)
                    elif score < baseline_cat - threshold:
                        result['regressed_categories'].append(cat)

        return result

    def update_baseline(self, results: Dict):
        """Update baseline to current results."""
        self.baseline = results
```

---

## 7.5.5 Human Evaluation

### Human Evaluation Protocol

```python
"""
ABOUTME: Human evaluation protocols for RAG systems.
ABOUTME: Structured approaches for collecting human judgments.
"""

@dataclass
class HumanEvalTask:
    """Task for human evaluators."""
    task_id: str
    query: str
    answer: str
    contexts: List[str]
    reference_answer: Optional[str] = None
    evaluation_criteria: List[str] = field(default_factory=list)


class HumanEvaluationManager:
    """
    Manage human evaluation of RAG outputs.
    """

    def __init__(
        self,
        criteria: Optional[List[str]] = None
    ):
        self.criteria = criteria or [
            'relevance',
            'accuracy',
            'completeness',
            'coherence',
            'helpfulness'
        ]

    def create_evaluation_tasks(
        self,
        rag_outputs: List[Dict],
        num_evaluators: int = 3
    ) -> List[HumanEvalTask]:
        """Create evaluation tasks from RAG outputs."""
        tasks = []

        for i, output in enumerate(rag_outputs):
            for eval_num in range(num_evaluators):
                task = HumanEvalTask(
                    task_id=f"task_{i}_eval_{eval_num}",
                    query=output['query'],
                    answer=output['answer'],
                    contexts=output.get('contexts', []),
                    reference_answer=output.get('reference'),
                    evaluation_criteria=self.criteria
                )
                tasks.append(task)

        return tasks

    def generate_evaluation_form(
        self,
        task: HumanEvalTask
    ) -> Dict[str, Any]:
        """Generate evaluation form for a task."""
        form = {
            'task_id': task.task_id,
            'query': task.query,
            'answer': task.answer,
            'contexts_preview': [c[:200] + '...' for c in task.contexts[:3]],
            'questions': []
        }

        # Add rating questions for each criterion
        for criterion in task.evaluation_criteria:
            form['questions'].append({
                'criterion': criterion,
                'question': self._get_question(criterion),
                'scale': '1-5',
                'guidelines': self._get_guidelines(criterion)
            })

        # Add free-form questions
        form['questions'].append({
            'criterion': 'issues',
            'question': 'What issues, if any, did you notice with this answer?',
            'type': 'text'
        })

        return form

    def _get_question(self, criterion: str) -> str:
        """Get evaluation question for criterion."""
        questions = {
            'relevance': 'How relevant is this answer to the question?',
            'accuracy': 'How accurate is the information in this answer?',
            'completeness': 'How completely does this answer address the question?',
            'coherence': 'How well-organized and coherent is this answer?',
            'helpfulness': 'How helpful would this answer be to someone with this question?'
        }
        return questions.get(criterion, f'Rate the {criterion} of this answer.')

    def _get_guidelines(self, criterion: str) -> str:
        """Get evaluation guidelines for criterion."""
        guidelines = {
            'relevance': '5=Directly addresses question, 1=Completely irrelevant',
            'accuracy': '5=All facts correct, 1=Contains major errors',
            'completeness': '5=Fully addresses all aspects, 1=Misses key points',
            'coherence': '5=Well-structured, logical flow, 1=Disorganized, hard to follow',
            'helpfulness': '5=Very useful, 1=Not helpful at all'
        }
        return guidelines.get(criterion, '')

    def compute_agreement(
        self,
        ratings: List[Dict[str, int]]
    ) -> Dict[str, float]:
        """
        Compute inter-annotator agreement.

        Uses Krippendorff's alpha for ordinal data.
        """
        from collections import defaultdict

        if len(ratings) < 2:
            return {'agreement': 1.0}

        # Group ratings by criterion
        by_criterion = defaultdict(list)
        for rating in ratings:
            for criterion, score in rating.items():
                if isinstance(score, (int, float)):
                    by_criterion[criterion].append(score)

        agreements = {}
        for criterion, scores in by_criterion.items():
            if len(scores) >= 2:
                # Simple agreement: correlation
                mean_score = np.mean(scores)
                variance = np.var(scores)
                # Cohen's Kappa approximation for ordinal
                agreements[criterion] = 1 - variance / 2 if variance < 2 else 0

        agreements['overall'] = np.mean(list(agreements.values())) if agreements else 0

        return agreements

    def aggregate_human_scores(
        self,
        all_ratings: Dict[str, List[Dict]]  # task_id -> list of ratings
    ) -> Dict[str, Dict[str, float]]:
        """Aggregate ratings across evaluators."""
        aggregated = {}

        for task_id, ratings in all_ratings.items():
            task_scores = {}

            # Get all criteria
            criteria = set()
            for rating in ratings:
                criteria.update(rating.keys())

            for criterion in criteria:
                scores = [r.get(criterion) for r in ratings if criterion in r]
                scores = [s for s in scores if isinstance(s, (int, float))]

                if scores:
                    task_scores[criterion] = {
                        'mean': np.mean(scores),
                        'std': np.std(scores),
                        'min': min(scores),
                        'max': max(scores)
                    }

            aggregated[task_id] = task_scores

        return aggregated
```

---

## 7.5.6 Continuous Evaluation

### Production Monitoring

```python
"""
ABOUTME: Continuous evaluation for production RAG systems.
ABOUTME: Implements online monitoring and quality tracking.
"""

class ContinuousEvaluator:
    """
    Continuous evaluation for production RAG.

    Samples queries and evaluates quality over time.
    """

    def __init__(
        self,
        llm_judge: Any,
        sample_rate: float = 0.1,  # Evaluate 10% of queries
        alert_threshold: float = 0.7
    ):
        self.llm = llm_judge
        self.sample_rate = sample_rate
        self.alert_threshold = alert_threshold

        self.evaluation_log = []
        self.alerts = []

    def should_evaluate(self) -> bool:
        """Determine if this query should be evaluated."""
        import random
        return random.random() < self.sample_rate

    def evaluate_query(
        self,
        query: str,
        answer: str,
        contexts: List[str]
    ) -> Dict[str, float]:
        """Evaluate a single query-answer pair."""
        scores = {}

        # Quick faithfulness check
        scores['faithfulness'] = self._quick_faithfulness(answer, contexts)

        # Relevance check
        scores['relevance'] = self._quick_relevance(query, answer)

        # Log result
        self.evaluation_log.append({
            'timestamp': self._get_timestamp(),
            'query': query[:100],
            'scores': scores
        })

        # Check for alerts
        avg_score = np.mean(list(scores.values()))
        if avg_score < self.alert_threshold:
            self._trigger_alert(query, answer, scores)

        return scores

    def _quick_faithfulness(
        self,
        answer: str,
        contexts: List[str]
    ) -> float:
        """Quick faithfulness check."""
        context = " ".join(contexts)[:1500]

        prompt = f"""Quick check: Is this answer supported by the context?

Context: {context}
Answer: {answer}

Score 0-10:"""

        response = self.llm.generate(prompt)

        try:
            import re
            match = re.search(r'(\d+)', response)
            if match:
                return float(match.group(1)) / 10
        except:
            pass

        return 0.5

    def _quick_relevance(
        self,
        query: str,
        answer: str
    ) -> float:
        """Quick relevance check."""
        prompt = f"""Does this answer address the question?

Question: {query}
Answer: {answer}

Score 0-10:"""

        response = self.llm.generate(prompt)

        try:
            import re
            match = re.search(r'(\d+)', response)
            if match:
                return float(match.group(1)) / 10
        except:
            pass

        return 0.5

    def _trigger_alert(
        self,
        query: str,
        answer: str,
        scores: Dict[str, float]
    ):
        """Trigger quality alert."""
        self.alerts.append({
            'timestamp': self._get_timestamp(),
            'query': query,
            'answer': answer[:200],
            'scores': scores,
            'severity': 'high' if min(scores.values()) < 0.3 else 'medium'
        })

    def _get_timestamp(self) -> str:
        """Get current timestamp."""
        from datetime import datetime
        return datetime.now().isoformat()

    def get_quality_summary(
        self,
        window_hours: int = 24
    ) -> Dict[str, Any]:
        """Get quality summary for time window."""
        from datetime import datetime, timedelta

        cutoff = datetime.now() - timedelta(hours=window_hours)

        recent_logs = [
            log for log in self.evaluation_log
            if datetime.fromisoformat(log['timestamp']) > cutoff
        ]

        if not recent_logs:
            return {'message': 'No evaluations in time window'}

        # Aggregate metrics
        metrics = {}
        for log in recent_logs:
            for metric, score in log['scores'].items():
                if metric not in metrics:
                    metrics[metric] = []
                metrics[metric].append(score)

        summary = {
            'num_evaluations': len(recent_logs),
            'time_window_hours': window_hours,
            'metrics': {
                m: {
                    'mean': np.mean(s),
                    'std': np.std(s),
                    'min': min(s)
                }
                for m, s in metrics.items()
            },
            'num_alerts': len([
                a for a in self.alerts
                if datetime.fromisoformat(a['timestamp']) > cutoff
            ])
        }

        return summary
```

---

## Appendix A: Troubleshooting Guide

### Common Issues

| Issue | Cause | Solution |
|-------|-------|----------|
| Low faithfulness | Hallucination in generation | Check context quality, adjust prompt |
| Low context precision | Poor retrieval | Tune retriever, add reranking |
| Low context recall | Missing relevant docs | Expand retrieval, check indexing |
| Inconsistent scores | LLM judge variance | Use majority voting, calibrate |
| High latency eval | Too many LLM calls | Batch evaluations, sample |

---

## Appendix B: Glossary

| Term | Definition |
|------|------------|
| **RAGAS** | RAG Assessment framework |
| **Faithfulness** | Answer supported by context |
| **Context Precision** | Relevant contexts ranked high |
| **Context Recall** | All needed info retrieved |
| **Hallucination** | Generated info not in context |
| **NDCG** | Normalized Discounted Cumulative Gain |

---

## References

1. [RAGAS Documentation](https://docs.ragas.io/)
2. [RAGAS Evaluation Guide - Confident AI](https://www.confident-ai.com/blog/rag-evaluation-metrics-answer-relevancy-faithfulness-and-more)
3. [Evaluating RAG with RAGAS - Medium](https://medium.com/data-science/evaluating-rag-applications-with-ragas-81d67b0ee31a)
4. [RAG Evaluation Best Practices - Qdrant](https://qdrant.tech/blog/rag-evaluation-guide/)
5. [Evaluating the Evaluators - Tweag](https://www.tweag.io/blog/2025-02-27-rag-evaluation/)
6. [RAG Evaluation with Langfuse](https://langfuse.com/guides/cookbook/evaluation_of_rag_with_ragas)
