> **Navigation** | [← 7.2 Embedding Models](7.2_embedding_model_guide.md) | [7.4 Retrieval & Reranking →](7.4_retrieval_reranking_guide.md)
>
> | | |
> |---|---|
> | **Prerequisites** | [7.1 Vector Database](7.1_vector_database_guide.md) &#124; [7.2 Embedding Models](7.2_embedding_model_guide.md) |
> | **Related** | [7.4 Retrieval](7.4_retrieval_reranking_guide.md) &#124; [1.2 Data Cleaning](../01_data_pipeline/1.2_data_cleaning_preprocessing.md) |
> | **Next** | [7.4 Retrieval & Reranking](7.4_retrieval_reranking_guide.md) |

# Document 7.3: Chunking Strategies Guide

## Executive Summary

Document chunking is the process of splitting documents into smaller segments for vector indexing and retrieval. The choice of chunking strategy significantly impacts RAG system performance - it's arguably the most important factor for retrieval quality. This guide covers fixed-size, recursive, semantic, document-aware, and hierarchical chunking strategies, along with evaluation methods and optimization techniques.

## Prerequisites

- Understanding of text processing and tokenization
- Familiarity with embedding models and vector databases
- Knowledge of RAG system architecture
- Experience with LangChain or similar frameworks
- Completion of documents 7.1 (Vector Database) and 7.2 (Embedding Models) recommended

---

## 7.3.1 Chunking Fundamentals

### Why Chunking Matters

```python
"""
ABOUTME: Chunking fundamentals and core concepts for RAG systems.
ABOUTME: Provides base classes and utilities for document chunking.
"""

from typing import List, Dict, Any, Optional, Tuple, Callable
from dataclasses import dataclass, field
from enum import Enum
from abc import ABC, abstractmethod
import re


class ChunkingStrategy(Enum):
    """Available chunking strategies."""
    FIXED_SIZE = "fixed_size"           # Fixed character/token count
    RECURSIVE = "recursive"             # Hierarchical separator-based
    SENTENCE = "sentence"               # Sentence-level
    SEMANTIC = "semantic"               # Embedding similarity-based
    DOCUMENT_AWARE = "document_aware"   # Structure-aware (MD, HTML, etc.)
    HIERARCHICAL = "hierarchical"       # Parent-child relationships
    AGENTIC = "agentic"                # LLM-guided chunking


@dataclass
class ChunkingConfig:
    """Configuration for chunking strategies."""
    # Size parameters
    chunk_size: int = 512              # Target chunk size (chars or tokens)
    chunk_overlap: int = 50            # Overlap between chunks
    size_unit: str = "tokens"          # "chars" or "tokens"

    # Strategy-specific
    separators: List[str] = field(default_factory=lambda: ["\n\n", "\n", ". ", " "])
    min_chunk_size: int = 100          # Minimum chunk size
    max_chunk_size: int = 1024         # Maximum chunk size

    # Semantic chunking
    similarity_threshold: float = 0.5   # For semantic boundaries
    buffer_size: int = 1               # Sentences to buffer

    # Metadata
    include_metadata: bool = True
    metadata_fields: List[str] = field(default_factory=lambda: ["source", "page", "section"])


@dataclass
class Chunk:
    """Represents a document chunk with metadata."""
    content: str
    index: int                         # Position in document
    metadata: Dict[str, Any] = field(default_factory=dict)

    # Optional relationships
    parent_id: Optional[str] = None
    child_ids: List[str] = field(default_factory=list)

    # Chunk boundaries
    start_char: int = 0
    end_char: int = 0

    @property
    def char_count(self) -> int:
        return len(self.content)

    @property
    def word_count(self) -> int:
        return len(self.content.split())


class BaseChunker(ABC):
    """Abstract base class for document chunkers."""

    def __init__(self, config: ChunkingConfig):
        self.config = config

    @abstractmethod
    def chunk(self, text: str, metadata: Optional[Dict] = None) -> List[Chunk]:
        """Split text into chunks."""
        pass

    def chunk_documents(
        self,
        documents: List[Dict[str, Any]]
    ) -> List[Chunk]:
        """Chunk multiple documents."""
        all_chunks = []

        for doc in documents:
            text = doc.get('content', doc.get('text', ''))
            doc_metadata = {k: v for k, v in doc.items() if k not in ['content', 'text']}

            chunks = self.chunk(text, doc_metadata)
            all_chunks.extend(chunks)

        return all_chunks

    def _count_tokens(self, text: str) -> int:
        """Estimate token count (rough approximation)."""
        # Rough estimate: ~4 chars per token for English
        return len(text) // 4

    def _truncate_to_size(
        self,
        text: str,
        max_size: int
    ) -> str:
        """Truncate text to maximum size."""
        if self.config.size_unit == "tokens":
            # Rough truncation by estimated tokens
            max_chars = max_size * 4
            return text[:max_chars]
        else:
            return text[:max_size]


class ChunkingImpactAnalyzer:
    """
    Analyze the impact of chunking strategy on retrieval.

    Chunking decisions affect:
    1. Retrieval precision (smaller chunks = more precise)
    2. Context completeness (larger chunks = more context)
    3. Embedding quality (coherent chunks = better embeddings)
    4. Token efficiency (overlap = more tokens to embed)
    """

    @staticmethod
    def analyze_tradeoffs(
        chunk_size: int,
        overlap: int
    ) -> Dict[str, str]:
        """Analyze tradeoffs for given parameters."""
        analysis = {}

        # Size tradeoffs
        if chunk_size < 256:
            analysis['precision'] = "High - very focused chunks"
            analysis['context'] = "Low - may lose important context"
            analysis['use_case'] = "FAQ, specific fact retrieval"
        elif chunk_size < 512:
            analysis['precision'] = "Good - balanced chunks"
            analysis['context'] = "Moderate - reasonable context"
            analysis['use_case'] = "General RAG, Q&A"
        else:
            analysis['precision'] = "Lower - broader chunks"
            analysis['context'] = "High - full context available"
            analysis['use_case'] = "Summarization, complex reasoning"

        # Overlap tradeoffs
        overlap_ratio = overlap / chunk_size if chunk_size > 0 else 0
        if overlap_ratio < 0.1:
            analysis['boundary_handling'] = "Poor - may cut mid-sentence"
        elif overlap_ratio < 0.2:
            analysis['boundary_handling'] = "Good - reasonable overlap"
        else:
            analysis['boundary_handling'] = "Excellent - but higher storage cost"

        return analysis

    @staticmethod
    def recommend_config(
        document_type: str,
        query_type: str,
        avg_query_length: int = 50
    ) -> ChunkingConfig:
        """Recommend chunking configuration."""
        configs = {
            ('technical_docs', 'specific'): ChunkingConfig(
                chunk_size=256,
                chunk_overlap=50,
                separators=["\n\n", "\n", ". "]
            ),
            ('technical_docs', 'broad'): ChunkingConfig(
                chunk_size=512,
                chunk_overlap=100,
                separators=["\n\n", "\n", ". "]
            ),
            ('articles', 'specific'): ChunkingConfig(
                chunk_size=384,
                chunk_overlap=64,
                separators=["\n\n", "\n", ". ", "! ", "? "]
            ),
            ('code', 'any'): ChunkingConfig(
                chunk_size=512,
                chunk_overlap=100,
                separators=["\n\n\n", "\n\n", "\n"]
            ),
            ('legal', 'specific'): ChunkingConfig(
                chunk_size=256,
                chunk_overlap=50,
                min_chunk_size=100
            )
        }

        key = (document_type, query_type)
        return configs.get(key, ChunkingConfig())
```

---

## 7.3.2 Fixed-Size and Recursive Chunking

### Fixed-Size Chunking

```python
"""
ABOUTME: Fixed-size and recursive chunking implementations.
ABOUTME: Covers character-based, token-based, and hierarchical splitting.
"""

class FixedSizeChunker(BaseChunker):
    """
    Fixed-size chunking with configurable overlap.

    Simplest strategy - splits by character or token count.
    Fast but may cut mid-sentence or mid-concept.
    """

    def chunk(
        self,
        text: str,
        metadata: Optional[Dict] = None
    ) -> List[Chunk]:
        """Split text into fixed-size chunks."""
        chunks = []
        metadata = metadata or {}

        chunk_size = self.config.chunk_size
        overlap = self.config.chunk_overlap

        if self.config.size_unit == "tokens":
            # Convert to approximate character count
            chunk_size = chunk_size * 4
            overlap = overlap * 4

        start = 0
        chunk_idx = 0

        while start < len(text):
            end = min(start + chunk_size, len(text))

            # Try to end at word boundary
            if end < len(text):
                # Look for space within last 20% of chunk
                search_start = end - int(chunk_size * 0.2)
                last_space = text.rfind(' ', search_start, end)
                if last_space > search_start:
                    end = last_space + 1

            chunk_text = text[start:end].strip()

            if len(chunk_text) >= self.config.min_chunk_size:
                chunks.append(Chunk(
                    content=chunk_text,
                    index=chunk_idx,
                    metadata={
                        **metadata,
                        'chunk_method': 'fixed_size',
                        'chunk_size': len(chunk_text)
                    },
                    start_char=start,
                    end_char=end
                ))
                chunk_idx += 1

            start = end - overlap

        return chunks


class RecursiveChunker(BaseChunker):
    """
    Recursive character text splitter.

    Tries separators in order until chunks are small enough.
    Preserves logical document structure better than fixed-size.

    Based on LangChain's RecursiveCharacterTextSplitter.
    """

    def chunk(
        self,
        text: str,
        metadata: Optional[Dict] = None
    ) -> List[Chunk]:
        """Recursively split text using hierarchical separators."""
        metadata = metadata or {}

        # Start recursive splitting
        chunks_text = self._split_recursive(
            text,
            self.config.separators
        )

        # Convert to Chunk objects
        chunks = []
        current_pos = 0

        for i, chunk_text in enumerate(chunks_text):
            # Find position in original text
            start = text.find(chunk_text, current_pos)
            if start == -1:
                start = current_pos
            end = start + len(chunk_text)

            chunks.append(Chunk(
                content=chunk_text,
                index=i,
                metadata={
                    **metadata,
                    'chunk_method': 'recursive',
                    'chunk_size': len(chunk_text)
                },
                start_char=start,
                end_char=end
            ))

            current_pos = end

        return chunks

    def _split_recursive(
        self,
        text: str,
        separators: List[str]
    ) -> List[str]:
        """Recursively split text."""
        if not text:
            return []

        # Get chunk size in chars
        target_size = self.config.chunk_size
        if self.config.size_unit == "tokens":
            target_size = target_size * 4

        # Base case: text is small enough
        if len(text) <= target_size:
            return [text] if len(text) >= self.config.min_chunk_size else []

        # Find best separator
        separator = None
        for sep in separators:
            if sep in text:
                separator = sep
                break

        if separator is None:
            # No separator found, force split
            return self._force_split(text, target_size)

        # Split by separator
        splits = text.split(separator)

        # Merge small chunks
        chunks = []
        current_chunk = ""

        for split in splits:
            split = split.strip()
            if not split:
                continue

            # Check if adding this split exceeds target
            potential_chunk = current_chunk + separator + split if current_chunk else split

            if len(potential_chunk) <= target_size:
                current_chunk = potential_chunk
            else:
                # Save current chunk if large enough
                if current_chunk and len(current_chunk) >= self.config.min_chunk_size:
                    chunks.append(current_chunk)

                # Check if split itself needs recursive splitting
                if len(split) > target_size:
                    # Recurse with remaining separators
                    remaining_seps = separators[separators.index(separator) + 1:]
                    if remaining_seps:
                        sub_chunks = self._split_recursive(split, remaining_seps)
                        chunks.extend(sub_chunks)
                    else:
                        chunks.extend(self._force_split(split, target_size))
                    current_chunk = ""
                else:
                    current_chunk = split

        # Don't forget last chunk
        if current_chunk and len(current_chunk) >= self.config.min_chunk_size:
            chunks.append(current_chunk)

        # Add overlap
        chunks = self._add_overlap(chunks)

        return chunks

    def _force_split(self, text: str, target_size: int) -> List[str]:
        """Force split when no separator works."""
        chunks = []
        start = 0

        while start < len(text):
            end = min(start + target_size, len(text))

            # Try to split at word boundary
            if end < len(text):
                last_space = text.rfind(' ', start, end)
                if last_space > start:
                    end = last_space

            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)

            start = end

        return chunks

    def _add_overlap(self, chunks: List[str]) -> List[str]:
        """Add overlap between chunks."""
        if self.config.chunk_overlap <= 0 or len(chunks) <= 1:
            return chunks

        overlap = self.config.chunk_overlap
        if self.config.size_unit == "tokens":
            overlap = overlap * 4

        overlapped = []
        for i, chunk in enumerate(chunks):
            if i > 0:
                # Get end of previous chunk for overlap
                prev_end = chunks[i-1][-overlap:]
                chunk = prev_end + " " + chunk

            overlapped.append(chunk)

        return overlapped
```

---

## 7.3.3 Sentence and Semantic Chunking

### Sentence-Based Chunking

```python
"""
ABOUTME: Sentence-level and semantic chunking implementations.
ABOUTME: Uses NLP and embeddings for intelligent boundary detection.
"""

import numpy as np


class SentenceChunker(BaseChunker):
    """
    Sentence-based chunking using NLP sentence detection.

    Groups sentences to reach target chunk size while
    respecting sentence boundaries.
    """

    def __init__(self, config: ChunkingConfig):
        super().__init__(config)
        self._init_sentence_splitter()

    def _init_sentence_splitter(self):
        """Initialize sentence splitting."""
        try:
            import spacy
            self.nlp = spacy.load("en_core_web_sm")
            self.use_spacy = True
        except:
            self.use_spacy = False

    def _split_sentences(self, text: str) -> List[str]:
        """Split text into sentences."""
        if self.use_spacy:
            doc = self.nlp(text)
            return [sent.text.strip() for sent in doc.sents]
        else:
            # Fallback: regex-based splitting
            pattern = r'(?<=[.!?])\s+(?=[A-Z])'
            sentences = re.split(pattern, text)
            return [s.strip() for s in sentences if s.strip()]

    def chunk(
        self,
        text: str,
        metadata: Optional[Dict] = None
    ) -> List[Chunk]:
        """Split text respecting sentence boundaries."""
        metadata = metadata or {}
        sentences = self._split_sentences(text)

        chunks = []
        current_sentences = []
        current_size = 0

        target_size = self.config.chunk_size
        if self.config.size_unit == "chars":
            target_size = target_size // 4  # Convert to approximate tokens

        for sentence in sentences:
            sentence_size = len(sentence.split())  # Approximate tokens

            if current_size + sentence_size > target_size and current_sentences:
                # Save current chunk
                chunk_text = " ".join(current_sentences)
                chunks.append(Chunk(
                    content=chunk_text,
                    index=len(chunks),
                    metadata={
                        **metadata,
                        'chunk_method': 'sentence',
                        'num_sentences': len(current_sentences)
                    }
                ))

                # Start new chunk with overlap
                overlap_sentences = self._get_overlap_sentences(current_sentences)
                current_sentences = overlap_sentences
                current_size = sum(len(s.split()) for s in current_sentences)

            current_sentences.append(sentence)
            current_size += sentence_size

        # Final chunk
        if current_sentences:
            chunk_text = " ".join(current_sentences)
            if len(chunk_text) >= self.config.min_chunk_size:
                chunks.append(Chunk(
                    content=chunk_text,
                    index=len(chunks),
                    metadata={
                        **metadata,
                        'chunk_method': 'sentence',
                        'num_sentences': len(current_sentences)
                    }
                ))

        return chunks

    def _get_overlap_sentences(
        self,
        sentences: List[str]
    ) -> List[str]:
        """Get sentences for overlap."""
        if self.config.chunk_overlap <= 0:
            return []

        # Get last N sentences for overlap
        overlap_tokens = 0
        overlap_sentences = []

        for sentence in reversed(sentences):
            sentence_tokens = len(sentence.split())
            if overlap_tokens + sentence_tokens > self.config.chunk_overlap:
                break
            overlap_sentences.insert(0, sentence)
            overlap_tokens += sentence_tokens

        return overlap_sentences


class SemanticChunker(BaseChunker):
    """
    Semantic chunking based on embedding similarity.

    Splits text where semantic similarity between consecutive
    sentences drops below a threshold. Creates more coherent chunks
    that respect topic boundaries.

    Reference: Greg Kamradt's semantic chunking approach.
    """

    def __init__(
        self,
        config: ChunkingConfig,
        embedding_model: Optional[Any] = None
    ):
        super().__init__(config)
        self.embedding_model = embedding_model
        self._init_embedding_model()
        self._init_sentence_splitter()

    def _init_embedding_model(self):
        """Initialize embedding model."""
        if self.embedding_model is None:
            try:
                from sentence_transformers import SentenceTransformer
                self.embedding_model = SentenceTransformer(
                    'all-MiniLM-L6-v2'
                )
            except ImportError:
                raise ImportError(
                    "sentence-transformers required for semantic chunking"
                )

    def _init_sentence_splitter(self):
        """Initialize sentence splitting."""
        try:
            import spacy
            self.nlp = spacy.load("en_core_web_sm")
        except:
            self.nlp = None

    def _split_sentences(self, text: str) -> List[str]:
        """Split text into sentences."""
        if self.nlp:
            doc = self.nlp(text)
            return [sent.text.strip() for sent in doc.sents]
        else:
            pattern = r'(?<=[.!?])\s+(?=[A-Z])'
            return [s.strip() for s in re.split(pattern, text) if s.strip()]

    def chunk(
        self,
        text: str,
        metadata: Optional[Dict] = None
    ) -> List[Chunk]:
        """
        Split text at semantic boundaries.

        Algorithm:
        1. Split into sentences
        2. Compute embeddings for each sentence
        3. Calculate similarity between consecutive sentences
        4. Split where similarity drops below threshold
        5. Merge small chunks to meet minimum size
        """
        metadata = metadata or {}
        sentences = self._split_sentences(text)

        if len(sentences) <= 1:
            return [Chunk(
                content=text,
                index=0,
                metadata={**metadata, 'chunk_method': 'semantic'}
            )]

        # Compute sentence embeddings
        embeddings = self.embedding_model.encode(sentences)

        # Compute similarities between consecutive sentences
        similarities = []
        for i in range(len(embeddings) - 1):
            sim = self._cosine_similarity(embeddings[i], embeddings[i + 1])
            similarities.append(sim)

        # Find split points where similarity drops
        split_points = self._find_split_points(
            similarities,
            self.config.similarity_threshold
        )

        # Create chunks from split points
        chunks = []
        start_idx = 0

        for split_idx in split_points + [len(sentences)]:
            chunk_sentences = sentences[start_idx:split_idx]
            chunk_text = " ".join(chunk_sentences)

            if len(chunk_text) >= self.config.min_chunk_size:
                chunks.append(Chunk(
                    content=chunk_text,
                    index=len(chunks),
                    metadata={
                        **metadata,
                        'chunk_method': 'semantic',
                        'num_sentences': len(chunk_sentences),
                        'semantic_coherence': self._compute_coherence(
                            embeddings[start_idx:split_idx]
                        )
                    }
                ))

            start_idx = split_idx

        # Merge very small chunks
        chunks = self._merge_small_chunks(chunks)

        # Ensure max size
        chunks = self._split_large_chunks(chunks)

        return chunks

    def _cosine_similarity(
        self,
        a: np.ndarray,
        b: np.ndarray
    ) -> float:
        """Compute cosine similarity."""
        return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))

    def _find_split_points(
        self,
        similarities: List[float],
        threshold: float
    ) -> List[int]:
        """Find indices where to split based on similarity drops."""
        # Compute local minima below threshold
        split_points = []

        for i, sim in enumerate(similarities):
            is_below_threshold = sim < threshold

            # Check if local minimum (lower than neighbors)
            is_local_min = True
            if i > 0 and similarities[i-1] < sim:
                is_local_min = False
            if i < len(similarities) - 1 and similarities[i+1] < sim:
                is_local_min = False

            if is_below_threshold or is_local_min:
                split_points.append(i + 1)  # Split after this sentence

        return split_points

    def _compute_coherence(
        self,
        embeddings: np.ndarray
    ) -> float:
        """Compute average pairwise similarity within chunk."""
        if len(embeddings) <= 1:
            return 1.0

        similarities = []
        for i in range(len(embeddings)):
            for j in range(i + 1, len(embeddings)):
                sim = self._cosine_similarity(embeddings[i], embeddings[j])
                similarities.append(sim)

        return float(np.mean(similarities))

    def _merge_small_chunks(self, chunks: List[Chunk]) -> List[Chunk]:
        """Merge chunks smaller than minimum size."""
        if not chunks:
            return chunks

        merged = []
        current = chunks[0]

        for chunk in chunks[1:]:
            if len(current.content) < self.config.min_chunk_size:
                # Merge with next
                current = Chunk(
                    content=current.content + " " + chunk.content,
                    index=current.index,
                    metadata=current.metadata
                )
            else:
                merged.append(current)
                current = chunk

        merged.append(current)
        return merged

    def _split_large_chunks(self, chunks: List[Chunk]) -> List[Chunk]:
        """Split chunks larger than maximum size."""
        result = []

        for chunk in chunks:
            if len(chunk.content) <= self.config.max_chunk_size:
                result.append(chunk)
            else:
                # Use recursive chunker for large chunks
                recursive = RecursiveChunker(self.config)
                sub_chunks = recursive.chunk(chunk.content, chunk.metadata)
                result.extend(sub_chunks)

        return result
```

---

## 7.3.4 Document-Aware Chunking

### Structure-Aware Chunking

```python
"""
ABOUTME: Document structure-aware chunking implementations.
ABOUTME: Handles Markdown, HTML, code, and other structured formats.
"""

class MarkdownChunker(BaseChunker):
    """
    Markdown-aware chunking that respects document structure.

    Splits at header boundaries, code blocks, and sections
    while maintaining document hierarchy in metadata.
    """

    def __init__(self, config: ChunkingConfig):
        super().__init__(config)

        # Header pattern
        self.header_pattern = re.compile(r'^(#{1,6})\s+(.+)$', re.MULTILINE)
        # Code block pattern
        self.code_block_pattern = re.compile(r'```[\s\S]*?```', re.MULTILINE)

    def chunk(
        self,
        text: str,
        metadata: Optional[Dict] = None
    ) -> List[Chunk]:
        """Split Markdown document respecting structure."""
        metadata = metadata or {}

        # Extract code blocks (preserve them intact)
        code_blocks = []
        def save_code_block(match):
            code_blocks.append(match.group(0))
            return f"__CODE_BLOCK_{len(code_blocks)-1}__"

        text_without_code = self.code_block_pattern.sub(save_code_block, text)

        # Split by headers
        sections = self._split_by_headers(text_without_code)

        chunks = []

        for section in sections:
            header_info = section.get('header', {})
            content = section['content']

            # Restore code blocks
            for i, code_block in enumerate(code_blocks):
                content = content.replace(f"__CODE_BLOCK_{i}__", code_block)

            # Check if section needs further splitting
            if len(content) > self.config.max_chunk_size:
                # Recursively chunk large sections
                sub_chunks = self._split_section(content)
                for sub_chunk in sub_chunks:
                    chunks.append(Chunk(
                        content=sub_chunk,
                        index=len(chunks),
                        metadata={
                            **metadata,
                            'chunk_method': 'markdown',
                            **header_info
                        }
                    ))
            elif len(content) >= self.config.min_chunk_size:
                chunks.append(Chunk(
                    content=content,
                    index=len(chunks),
                    metadata={
                        **metadata,
                        'chunk_method': 'markdown',
                        **header_info
                    }
                ))

        return chunks

    def _split_by_headers(self, text: str) -> List[Dict[str, Any]]:
        """Split text by Markdown headers."""
        sections = []
        current_section = {'header': {}, 'content': ''}
        header_stack = []  # Track header hierarchy

        lines = text.split('\n')
        current_content = []

        for line in lines:
            header_match = self.header_pattern.match(line)

            if header_match:
                # Save previous section
                if current_content:
                    current_section['content'] = '\n'.join(current_content).strip()
                    if current_section['content']:
                        sections.append(current_section)

                # Parse header
                level = len(header_match.group(1))
                title = header_match.group(2)

                # Update header stack
                while header_stack and header_stack[-1][0] >= level:
                    header_stack.pop()
                header_stack.append((level, title))

                # Start new section
                current_section = {
                    'header': {
                        'level': level,
                        'title': title,
                        'path': ' > '.join(h[1] for h in header_stack)
                    },
                    'content': ''
                }
                current_content = [line]
            else:
                current_content.append(line)

        # Final section
        if current_content:
            current_section['content'] = '\n'.join(current_content).strip()
            if current_section['content']:
                sections.append(current_section)

        return sections

    def _split_section(self, content: str) -> List[str]:
        """Split large section into smaller chunks."""
        # Use recursive chunker
        recursive_config = ChunkingConfig(
            chunk_size=self.config.chunk_size,
            chunk_overlap=self.config.chunk_overlap,
            separators=["\n\n", "\n", ". ", " "],
            min_chunk_size=self.config.min_chunk_size
        )
        chunker = RecursiveChunker(recursive_config)
        chunks = chunker.chunk(content)
        return [c.content for c in chunks]


class CodeChunker(BaseChunker):
    """
    Code-aware chunking for source code files.

    Splits at function/class boundaries while preserving
    complete code units.
    """

    def __init__(
        self,
        config: ChunkingConfig,
        language: str = "python"
    ):
        super().__init__(config)
        self.language = language

        # Language-specific patterns
        self.patterns = {
            'python': {
                'function': r'^(async\s+)?def\s+\w+',
                'class': r'^class\s+\w+',
                'separator': '\n\n'
            },
            'javascript': {
                'function': r'^(async\s+)?function\s+\w+|^(const|let|var)\s+\w+\s*=\s*(async\s+)?\(',
                'class': r'^class\s+\w+',
                'separator': '\n\n'
            },
            'java': {
                'function': r'^\s*(public|private|protected)?\s*(static)?\s*\w+\s+\w+\s*\(',
                'class': r'^\s*(public|private)?\s*class\s+\w+',
                'separator': '\n\n'
            }
        }

    def chunk(
        self,
        text: str,
        metadata: Optional[Dict] = None
    ) -> List[Chunk]:
        """Split code respecting function/class boundaries."""
        metadata = metadata or {}
        patterns = self.patterns.get(self.language, self.patterns['python'])

        # Split into top-level blocks
        blocks = self._split_into_blocks(text, patterns)

        chunks = []

        for block in blocks:
            block_type = block.get('type', 'code')
            content = block['content']

            # Check if block needs splitting
            if len(content) > self.config.max_chunk_size:
                # Split large blocks
                sub_chunks = self._split_large_block(content)
                for sub_content in sub_chunks:
                    chunks.append(Chunk(
                        content=sub_content,
                        index=len(chunks),
                        metadata={
                            **metadata,
                            'chunk_method': 'code',
                            'block_type': block_type,
                            'language': self.language
                        }
                    ))
            elif len(content) >= self.config.min_chunk_size:
                chunks.append(Chunk(
                    content=content,
                    index=len(chunks),
                    metadata={
                        **metadata,
                        'chunk_method': 'code',
                        'block_type': block_type,
                        'language': self.language
                    }
                ))

        return chunks

    def _split_into_blocks(
        self,
        text: str,
        patterns: Dict[str, str]
    ) -> List[Dict[str, Any]]:
        """Split code into logical blocks."""
        lines = text.split('\n')
        blocks = []
        current_block = {'type': 'code', 'content': '', 'lines': []}

        func_pattern = re.compile(patterns['function'], re.MULTILINE)
        class_pattern = re.compile(patterns['class'], re.MULTILINE)

        for line in lines:
            # Check for function start
            if func_pattern.match(line):
                if current_block['lines']:
                    current_block['content'] = '\n'.join(current_block['lines'])
                    blocks.append(current_block)
                current_block = {'type': 'function', 'content': '', 'lines': [line]}

            # Check for class start
            elif class_pattern.match(line):
                if current_block['lines']:
                    current_block['content'] = '\n'.join(current_block['lines'])
                    blocks.append(current_block)
                current_block = {'type': 'class', 'content': '', 'lines': [line]}

            else:
                current_block['lines'].append(line)

        # Final block
        if current_block['lines']:
            current_block['content'] = '\n'.join(current_block['lines'])
            blocks.append(current_block)

        return blocks

    def _split_large_block(self, content: str) -> List[str]:
        """Split large code block."""
        # For code, try to split at double newlines
        chunks = []
        current = ""

        for line in content.split('\n'):
            if len(current) + len(line) > self.config.max_chunk_size:
                if current:
                    chunks.append(current)
                current = line
            else:
                current = current + '\n' + line if current else line

        if current:
            chunks.append(current)

        return chunks


class HTMLChunker(BaseChunker):
    """
    HTML-aware chunking that respects DOM structure.
    """

    def chunk(
        self,
        text: str,
        metadata: Optional[Dict] = None
    ) -> List[Chunk]:
        """Split HTML respecting tag boundaries."""
        from html.parser import HTMLParser
        import html

        metadata = metadata or {}

        # Extract text content from HTML
        class TextExtractor(HTMLParser):
            def __init__(self):
                super().__init__()
                self.sections = []
                self.current_section = {'tag': 'root', 'text': ''}
                self.header_tags = {'h1', 'h2', 'h3', 'h4', 'h5', 'h6'}

            def handle_starttag(self, tag, attrs):
                if tag in self.header_tags or tag in {'section', 'article', 'div'}:
                    if self.current_section['text'].strip():
                        self.sections.append(self.current_section)
                    self.current_section = {'tag': tag, 'text': ''}

            def handle_data(self, data):
                self.current_section['text'] += data

            def handle_endtag(self, tag):
                pass

        extractor = TextExtractor()
        extractor.feed(text)

        if extractor.current_section['text'].strip():
            extractor.sections.append(extractor.current_section)

        # Convert sections to chunks
        chunks = []
        for section in extractor.sections:
            content = section['text'].strip()
            if len(content) >= self.config.min_chunk_size:
                chunks.append(Chunk(
                    content=content,
                    index=len(chunks),
                    metadata={
                        **metadata,
                        'chunk_method': 'html',
                        'source_tag': section['tag']
                    }
                ))

        return chunks
```

---

## 7.3.5 Hierarchical and Advanced Chunking

### Parent-Child Hierarchical Chunking

```python
"""
ABOUTME: Hierarchical chunking with parent-child relationships.
ABOUTME: Enables multi-level retrieval and context expansion.
"""

import uuid


class HierarchicalChunker(BaseChunker):
    """
    Hierarchical chunking creating parent-child chunk relationships.

    Enables:
    1. Fine-grained retrieval with small chunks
    2. Context expansion by retrieving parent chunks
    3. Multi-level indexing for different query types
    """

    def __init__(
        self,
        config: ChunkingConfig,
        parent_chunk_size: int = 2048,
        child_chunk_size: int = 512
    ):
        super().__init__(config)
        self.parent_chunk_size = parent_chunk_size
        self.child_chunk_size = child_chunk_size

    def chunk(
        self,
        text: str,
        metadata: Optional[Dict] = None
    ) -> Tuple[List[Chunk], List[Chunk]]:
        """
        Create hierarchical chunks.

        Returns:
            Tuple of (parent_chunks, child_chunks)
        """
        metadata = metadata or {}

        # Create parent chunks
        parent_config = ChunkingConfig(
            chunk_size=self.parent_chunk_size,
            chunk_overlap=100,
            min_chunk_size=self.parent_chunk_size // 2
        )
        parent_chunker = RecursiveChunker(parent_config)
        parent_chunks = parent_chunker.chunk(text, metadata)

        # Assign IDs to parents
        for i, parent in enumerate(parent_chunks):
            parent.metadata['chunk_id'] = f"parent_{i}_{uuid.uuid4().hex[:8]}"
            parent.metadata['chunk_level'] = 'parent'

        # Create child chunks from each parent
        child_config = ChunkingConfig(
            chunk_size=self.child_chunk_size,
            chunk_overlap=50,
            min_chunk_size=self.child_chunk_size // 2
        )
        child_chunker = RecursiveChunker(child_config)

        all_child_chunks = []

        for parent in parent_chunks:
            children = child_chunker.chunk(
                parent.content,
                {**parent.metadata, 'chunk_level': 'child'}
            )

            for j, child in enumerate(children):
                child.parent_id = parent.metadata['chunk_id']
                child.metadata['chunk_id'] = f"child_{j}_{uuid.uuid4().hex[:8]}"
                parent.child_ids.append(child.metadata['chunk_id'])

            all_child_chunks.extend(children)

        return parent_chunks, all_child_chunks


class ContextEnrichedChunker(BaseChunker):
    """
    Chunk with contextual enrichment.

    Adds surrounding context and metadata to each chunk
    to improve retrieval without losing context.
    """

    def __init__(
        self,
        config: ChunkingConfig,
        context_window: int = 1  # Number of chunks to include as context
    ):
        super().__init__(config)
        self.context_window = context_window

    def chunk(
        self,
        text: str,
        metadata: Optional[Dict] = None
    ) -> List[Chunk]:
        """Create chunks with contextual enrichment."""
        metadata = metadata or {}

        # First, create base chunks
        base_chunker = RecursiveChunker(self.config)
        base_chunks = base_chunker.chunk(text, metadata)

        # Enrich each chunk with context
        enriched_chunks = []

        for i, chunk in enumerate(base_chunks):
            # Get preceding context
            preceding = []
            for j in range(max(0, i - self.context_window), i):
                summary = self._summarize_chunk(base_chunks[j].content)
                preceding.append(summary)

            # Get following context
            following = []
            for j in range(i + 1, min(len(base_chunks), i + self.context_window + 1)):
                summary = self._summarize_chunk(base_chunks[j].content)
                following.append(summary)

            # Create enriched content
            context_prefix = ""
            if preceding:
                context_prefix = f"[Previous: {' | '.join(preceding)}]\n\n"

            context_suffix = ""
            if following:
                context_suffix = f"\n\n[Next: {' | '.join(following)}]"

            enriched_content = context_prefix + chunk.content + context_suffix

            enriched_chunks.append(Chunk(
                content=enriched_content,
                index=i,
                metadata={
                    **chunk.metadata,
                    'original_content': chunk.content,
                    'has_context': True,
                    'context_window': self.context_window
                }
            ))

        return enriched_chunks

    def _summarize_chunk(self, content: str, max_length: int = 100) -> str:
        """Create brief summary of chunk for context."""
        # Simple truncation - could use LLM for better summaries
        words = content.split()
        if len(words) <= 20:
            return content
        return " ".join(words[:20]) + "..."


class AgenticChunker:
    """
    LLM-guided chunking for optimal semantic boundaries.

    Uses an LLM to determine optimal split points based on
    content understanding.
    """

    def __init__(
        self,
        llm_client: Any,
        config: ChunkingConfig
    ):
        self.llm = llm_client
        self.config = config

    def chunk(
        self,
        text: str,
        metadata: Optional[Dict] = None
    ) -> List[Chunk]:
        """Use LLM to determine chunk boundaries."""
        metadata = metadata or {}

        # First, get rough chunks
        rough_config = ChunkingConfig(
            chunk_size=self.config.chunk_size * 2,
            chunk_overlap=0
        )
        rough_chunker = RecursiveChunker(rough_config)
        rough_chunks = rough_chunker.chunk(text, metadata)

        final_chunks = []

        for rough_chunk in rough_chunks:
            # Ask LLM where to split
            split_points = self._get_llm_split_points(rough_chunk.content)

            if split_points:
                sub_chunks = self._split_at_points(
                    rough_chunk.content,
                    split_points
                )
                for sub_content in sub_chunks:
                    if len(sub_content) >= self.config.min_chunk_size:
                        final_chunks.append(Chunk(
                            content=sub_content,
                            index=len(final_chunks),
                            metadata={
                                **metadata,
                                'chunk_method': 'agentic'
                            }
                        ))
            else:
                final_chunks.append(Chunk(
                    content=rough_chunk.content,
                    index=len(final_chunks),
                    metadata={**metadata, 'chunk_method': 'agentic'}
                ))

        return final_chunks

    def _get_llm_split_points(self, text: str) -> List[int]:
        """Ask LLM for optimal split points."""
        prompt = f"""Analyze this text and identify the best points to split it into coherent chunks for retrieval.

Text:
{text}

Return a JSON list of character positions where the text should be split.
Each split should be at a natural topic or concept boundary.
Target chunk size: {self.config.chunk_size} characters.

Response format: [100, 500, 850]"""

        response = self.llm.generate(prompt)

        try:
            import json
            return json.loads(response)
        except:
            return []

    def _split_at_points(
        self,
        text: str,
        points: List[int]
    ) -> List[str]:
        """Split text at specified character positions."""
        chunks = []
        start = 0

        for point in sorted(points):
            if point > start and point < len(text):
                chunks.append(text[start:point].strip())
                start = point

        # Final chunk
        if start < len(text):
            chunks.append(text[start:].strip())

        return chunks
```

---

## 7.3.6 Evaluation and Optimization

```python
"""
ABOUTME: Chunking evaluation and optimization tools.
ABOUTME: Measures chunking quality and impact on retrieval.
"""

class ChunkingEvaluator:
    """
    Evaluate chunking strategy quality.
    """

    def __init__(self, embedding_model: Optional[Any] = None):
        self.embedding_model = embedding_model

    def evaluate_chunks(
        self,
        chunks: List[Chunk],
        queries: Optional[List[str]] = None,
        ground_truth: Optional[Dict[str, List[int]]] = None
    ) -> Dict[str, float]:
        """
        Evaluate chunking quality.

        Metrics:
        - Size distribution statistics
        - Semantic coherence within chunks
        - Retrieval performance (if ground truth provided)
        """
        metrics = {}

        # Size statistics
        sizes = [len(c.content) for c in chunks]
        metrics['avg_chunk_size'] = np.mean(sizes)
        metrics['std_chunk_size'] = np.std(sizes)
        metrics['min_chunk_size'] = np.min(sizes)
        metrics['max_chunk_size'] = np.max(sizes)
        metrics['num_chunks'] = len(chunks)

        # Semantic coherence (if embedding model available)
        if self.embedding_model:
            coherence_scores = []
            for chunk in chunks:
                coherence = self._compute_chunk_coherence(chunk.content)
                coherence_scores.append(coherence)

            metrics['avg_coherence'] = np.mean(coherence_scores)
            metrics['min_coherence'] = np.min(coherence_scores)

        # Retrieval evaluation (if ground truth provided)
        if queries and ground_truth and self.embedding_model:
            retrieval_metrics = self._evaluate_retrieval(
                chunks, queries, ground_truth
            )
            metrics.update(retrieval_metrics)

        return metrics

    def _compute_chunk_coherence(self, text: str) -> float:
        """Compute semantic coherence within chunk."""
        sentences = text.split('. ')
        if len(sentences) <= 1:
            return 1.0

        embeddings = self.embedding_model.encode(sentences)

        # Average pairwise similarity
        similarities = []
        for i in range(len(embeddings)):
            for j in range(i + 1, len(embeddings)):
                sim = np.dot(embeddings[i], embeddings[j]) / (
                    np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[j])
                )
                similarities.append(sim)

        return float(np.mean(similarities))

    def _evaluate_retrieval(
        self,
        chunks: List[Chunk],
        queries: List[str],
        ground_truth: Dict[str, List[int]]
    ) -> Dict[str, float]:
        """Evaluate retrieval performance."""
        # Encode chunks and queries
        chunk_texts = [c.content for c in chunks]
        chunk_embeddings = self.embedding_model.encode(chunk_texts)
        query_embeddings = self.embedding_model.encode(queries)

        # Compute retrieval metrics
        recall_at_5 = 0.0
        mrr = 0.0

        for q_idx, query in enumerate(queries):
            # Get similarities
            sims = query_embeddings[q_idx] @ chunk_embeddings.T
            ranking = np.argsort(-sims)

            # Ground truth chunks
            relevant = ground_truth.get(query, [])

            # Recall@5
            top_5 = ranking[:5]
            hits = len(set(top_5) & set(relevant))
            recall_at_5 += hits / len(relevant) if relevant else 0

            # MRR
            for rank, idx in enumerate(ranking):
                if idx in relevant:
                    mrr += 1 / (rank + 1)
                    break

        n_queries = len(queries)
        return {
            'recall@5': recall_at_5 / n_queries,
            'mrr': mrr / n_queries
        }

    def compare_strategies(
        self,
        text: str,
        strategies: List[Tuple[str, BaseChunker]],
        queries: List[str],
        ground_truth: Dict[str, List[int]]
    ) -> Dict[str, Dict[str, float]]:
        """Compare multiple chunking strategies."""
        results = {}

        for name, chunker in strategies:
            chunks = chunker.chunk(text)
            metrics = self.evaluate_chunks(chunks, queries, ground_truth)
            results[name] = metrics

        return results
```

---

## Appendix A: Troubleshooting Guide

### Common Issues

| Issue | Cause | Solution |
|-------|-------|----------|
| Chunks too large | Separators not matching | Add more separators to hierarchy |
| Lost context | Chunks too small | Increase chunk size or add overlap |
| Mid-sentence splits | No sentence awareness | Use sentence or semantic chunking |
| Poor retrieval | Bad boundaries | Try semantic chunking |
| High latency | Many small chunks | Consolidate or use hierarchical |
| Memory issues | Large overlap | Reduce overlap percentage |

---

## Appendix B: Glossary

| Term | Definition |
|------|------------|
| **Chunking** | Splitting documents into smaller pieces |
| **Overlap** | Shared content between consecutive chunks |
| **Semantic Chunking** | Splitting based on meaning boundaries |
| **Hierarchical Chunking** | Parent-child chunk relationships |
| **Token** | Basic unit for LLM processing |
| **Coherence** | Semantic consistency within chunk |

---

## References

1. [Best Chunking Strategies 2025 - Firecrawl](https://www.firecrawl.dev/blog/best-chunking-strategies-rag-2025)
2. [Chunking in RAG - Stack Overflow](https://stackoverflow.blog/2024/12/27/breaking-up-is-hard-to-do-chunking-in-rag-applications/)
3. [Semantic Chunking - Weaviate](https://weaviate.io/blog/chunking-strategies-for-rag)
4. [LangChain Text Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/)
5. [NVIDIA Chunking Benchmark](https://developer.nvidia.com/blog/rag-101-chunking-strategies/)
