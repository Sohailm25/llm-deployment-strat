> **Navigation** | [← 7.5 RAG Evaluation](7.5_rag_evaluation_guide.md) | [8.1 Model Registry →](../08_mlops_lifecycle/8.1_model_registry_guide.md)
>
> | | |
> |---|---|
> | **Prerequisites** | [7.1-7.5 RAG Pipeline](7.1_vector_database_guide.md) &#124; Agent frameworks (LangChain, LlamaIndex) |
> | **Related** | [9.2 Serving Architecture](../09_inference_serving/9.2_serving_architecture_patterns_guide.md) &#124; [12.2 SDK Design](../12_user_developer_experience/12.2_sdk_client_library_guide.md) |
> | **Next** | [8.1 Model Registry](../08_mlops_lifecycle/8.1_model_registry_guide.md) |

# 7.6 Advanced RAG Patterns Guide

## Document Purpose

This guide covers advanced RAG architectures beyond naive retrieve-then-generate, including modular RAG, agentic RAG, multi-modal RAG, conversational RAG, and structured data RAG patterns for production systems.

## Prerequisites

- Understanding of basic RAG pipelines (Document 7.1-7.5)
- Familiarity with LLM APIs and prompt engineering
- Experience with vector databases and embedding models
- Basic understanding of agent frameworks (LangChain, LlamaIndex)

## Target Audience

- ML Engineers building production RAG systems
- Backend Engineers implementing complex retrieval patterns
- AI Architects designing enterprise RAG solutions

---

## 1. RAG Architecture Evolution

### 1.1 RAG Paradigm Overview

```python
"""
RAG architecture taxonomy and selection framework
"""
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional, Callable
from enum import Enum
from abc import ABC, abstractmethod
import asyncio


class RAGParadigm(Enum):
    NAIVE = "naive"           # Simple retrieve-then-generate
    ADVANCED = "advanced"     # Query transformation, iterative retrieval
    MODULAR = "modular"       # Pluggable components, routing
    AGENTIC = "agentic"       # Tool-using, multi-hop reasoning


@dataclass
class RAGCapability:
    """Capability requirements for RAG paradigm selection"""
    multi_hop_reasoning: bool = False
    query_decomposition: bool = False
    self_reflection: bool = False
    tool_usage: bool = False
    multi_modal: bool = False
    conversational: bool = False
    structured_data: bool = False


class RAGParadigmSelector:
    """Select appropriate RAG paradigm based on requirements"""

    def select_paradigm(self, capabilities: RAGCapability) -> RAGParadigm:
        """Determine the optimal RAG paradigm"""
        # Agentic RAG for complex requirements
        if (capabilities.multi_hop_reasoning and
            capabilities.tool_usage and
            capabilities.self_reflection):
            return RAGParadigm.AGENTIC

        # Modular RAG for routing and composition
        if capabilities.query_decomposition or capabilities.multi_modal:
            return RAGParadigm.MODULAR

        # Advanced RAG for query transformation
        if (capabilities.query_decomposition or
            capabilities.self_reflection):
            return RAGParadigm.ADVANCED

        # Default to naive RAG
        return RAGParadigm.NAIVE

    def get_paradigm_components(
        self,
        paradigm: RAGParadigm
    ) -> Dict[str, List[str]]:
        """Get required components for each paradigm"""
        components = {
            RAGParadigm.NAIVE: {
                "required": ["retriever", "generator"],
                "optional": ["reranker"]
            },
            RAGParadigm.ADVANCED: {
                "required": ["query_transformer", "retriever", "generator"],
                "optional": ["reranker", "context_compressor", "self_checker"]
            },
            RAGParadigm.MODULAR: {
                "required": ["router", "retriever_pool", "generator"],
                "optional": ["query_analyzer", "fusion_module", "cache"]
            },
            RAGParadigm.AGENTIC: {
                "required": ["agent", "tool_registry", "retriever", "generator"],
                "optional": ["planner", "memory", "self_critic"]
            }
        }
        return components[paradigm]


# Naive RAG baseline
class NaiveRAG:
    """Simple retrieve-then-generate pattern"""

    def __init__(
        self,
        retriever,
        generator,
        top_k: int = 5
    ):
        self.retriever = retriever
        self.generator = generator
        self.top_k = top_k

    def query(self, question: str) -> Dict[str, Any]:
        """Basic RAG query"""
        # Retrieve
        documents = self.retriever.retrieve(question, top_k=self.top_k)

        # Generate
        context = self._format_context(documents)
        answer = self.generator.generate(
            question=question,
            context=context
        )

        return {
            "answer": answer,
            "documents": documents,
            "paradigm": "naive"
        }

    def _format_context(self, documents: List[Dict]) -> str:
        """Format documents into context string"""
        context_parts = []
        for i, doc in enumerate(documents, 1):
            context_parts.append(f"[{i}] {doc['content']}")
        return "\n\n".join(context_parts)
```

### 1.2 Naive RAG Limitations

```python
"""
Understanding naive RAG failure modes
"""
from dataclasses import dataclass
from typing import List, Optional


@dataclass
class RAGFailureMode:
    """Categorization of RAG failure modes"""
    name: str
    description: str
    detection_method: str
    mitigation_strategy: str


class RAGFailureModeAnalyzer:
    """Analyze and categorize RAG failures"""

    FAILURE_MODES = [
        RAGFailureMode(
            name="irrelevant_retrieval",
            description="Retrieved documents don't contain answer",
            detection_method="Low semantic similarity between query and docs",
            mitigation_strategy="Query expansion, hybrid retrieval"
        ),
        RAGFailureMode(
            name="context_overflow",
            description="Too much context, key info buried",
            detection_method="Answer quality degrades with more docs",
            mitigation_strategy="Context compression, reranking"
        ),
        RAGFailureMode(
            name="hallucination_despite_context",
            description="Model ignores or contradicts context",
            detection_method="Faithfulness score < threshold",
            mitigation_strategy="Self-RAG, citation enforcement"
        ),
        RAGFailureMode(
            name="multi_hop_failure",
            description="Answer requires combining multiple docs",
            detection_method="Question requires reasoning chains",
            mitigation_strategy="Iterative retrieval, agentic RAG"
        ),
        RAGFailureMode(
            name="temporal_confusion",
            description="Outdated information in knowledge base",
            detection_method="Timestamp comparison, freshness check",
            mitigation_strategy="Date filtering, freshness ranking"
        ),
        RAGFailureMode(
            name="contradictory_sources",
            description="Retrieved documents contain conflicting info",
            detection_method="Semantic contradiction detection",
            mitigation_strategy="Source authority ranking, conflict resolution"
        )
    ]

    def analyze_failure(
        self,
        query: str,
        retrieved_docs: List[Dict],
        generated_answer: str,
        ground_truth: Optional[str] = None
    ) -> List[RAGFailureMode]:
        """Detect potential failure modes in RAG output"""
        detected_failures = []

        # Check for irrelevant retrieval
        if self._check_irrelevant_retrieval(query, retrieved_docs):
            detected_failures.append(self.FAILURE_MODES[0])

        # Check for context overflow
        if self._check_context_overflow(retrieved_docs):
            detected_failures.append(self.FAILURE_MODES[1])

        # Check for hallucination
        if self._check_hallucination(generated_answer, retrieved_docs):
            detected_failures.append(self.FAILURE_MODES[2])

        # Check for multi-hop requirement
        if self._requires_multi_hop(query):
            detected_failures.append(self.FAILURE_MODES[3])

        return detected_failures

    def _check_irrelevant_retrieval(
        self,
        query: str,
        docs: List[Dict]
    ) -> bool:
        """Check if retrieved docs are relevant to query"""
        # Implement semantic similarity check
        avg_similarity = sum(
            doc.get('score', 0) for doc in docs
        ) / len(docs) if docs else 0
        return avg_similarity < 0.5

    def _check_context_overflow(self, docs: List[Dict]) -> bool:
        """Check for potential context overflow"""
        total_tokens = sum(
            len(doc.get('content', '').split())
            for doc in docs
        )
        return total_tokens > 4000  # Threshold for context size

    def _check_hallucination(
        self,
        answer: str,
        docs: List[Dict]
    ) -> bool:
        """Check if answer is grounded in documents"""
        # Simplified check - in production use NLI model
        context = " ".join(doc.get('content', '') for doc in docs)
        # Check if key claims in answer appear in context
        answer_words = set(answer.lower().split())
        context_words = set(context.lower().split())
        overlap = len(answer_words & context_words) / len(answer_words)
        return overlap < 0.3

    def _requires_multi_hop(self, query: str) -> bool:
        """Detect if query requires multi-hop reasoning"""
        multi_hop_indicators = [
            "compare", "difference between", "relationship",
            "how does X affect Y", "why did X cause Y",
            "based on", "considering", "given that"
        ]
        query_lower = query.lower()
        return any(
            indicator in query_lower
            for indicator in multi_hop_indicators
        )
```

---

## 2. Advanced RAG Patterns

### 2.1 Query Transformation

```python
"""
Query transformation techniques for improved retrieval
"""
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
import json


@dataclass
class TransformedQuery:
    """Result of query transformation"""
    original: str
    transformed: List[str]
    method: str
    metadata: Dict[str, Any] = None


class QueryTransformer:
    """Transform queries for better retrieval"""

    def __init__(self, llm_client):
        self.llm = llm_client

    def expand_query(self, query: str) -> TransformedQuery:
        """Expand query with related terms and paraphrases"""
        prompt = f"""Generate 3 alternative phrasings of this query that
        might help find relevant information. Keep the same meaning but
        use different words and perspectives.

        Original query: {query}

        Return as JSON array of strings."""

        response = self.llm.generate(prompt)
        alternatives = json.loads(response)

        return TransformedQuery(
            original=query,
            transformed=[query] + alternatives,
            method="expansion"
        )

    def decompose_query(self, query: str) -> TransformedQuery:
        """Decompose complex query into sub-queries"""
        prompt = f"""Break down this complex question into simpler
        sub-questions that can be answered independently, then
        combined to answer the original.

        Question: {query}

        Return as JSON with:
        - sub_questions: list of simpler questions
        - combination_strategy: how to combine answers
        """

        response = self.llm.generate(prompt)
        result = json.loads(response)

        return TransformedQuery(
            original=query,
            transformed=result['sub_questions'],
            method="decomposition",
            metadata={"combination_strategy": result['combination_strategy']}
        )

    def hyde_transform(self, query: str) -> TransformedQuery:
        """
        Hypothetical Document Embeddings (HyDE):
        Generate a hypothetical answer to use for retrieval
        """
        prompt = f"""Answer this question as if you had access to
        the relevant documents. Generate a detailed, factual response
        that would appear in a knowledge base. Do not include
        citations or hedging language.

        Question: {query}

        Hypothetical document:"""

        hypothetical_doc = self.llm.generate(prompt)

        return TransformedQuery(
            original=query,
            transformed=[hypothetical_doc],
            method="hyde",
            metadata={"use_for": "embedding_similarity"}
        )

    def step_back_prompting(self, query: str) -> TransformedQuery:
        """
        Step-back prompting: Ask a more general question first
        """
        prompt = f"""Given this specific question, generate a more
        general "step-back" question that would help understand
        the broader context needed to answer the original.

        Specific question: {query}

        Return JSON with:
        - step_back_question: the more general question
        - reasoning: why this helps answer the original
        """

        response = self.llm.generate(prompt)
        result = json.loads(response)

        return TransformedQuery(
            original=query,
            transformed=[result['step_back_question'], query],
            method="step_back",
            metadata={"reasoning": result['reasoning']}
        )


class MultiQueryRetriever:
    """Retrieve using multiple query transformations"""

    def __init__(
        self,
        retriever,
        transformer: QueryTransformer,
        dedup_threshold: float = 0.9
    ):
        self.retriever = retriever
        self.transformer = transformer
        self.dedup_threshold = dedup_threshold

    def retrieve(
        self,
        query: str,
        top_k: int = 10,
        transform_methods: List[str] = ["expansion"]
    ) -> List[Dict]:
        """Retrieve using multiple transformed queries"""
        all_docs = []
        seen_ids = set()

        # Transform query using specified methods
        for method in transform_methods:
            if method == "expansion":
                transformed = self.transformer.expand_query(query)
            elif method == "decomposition":
                transformed = self.transformer.decompose_query(query)
            elif method == "hyde":
                transformed = self.transformer.hyde_transform(query)
            elif method == "step_back":
                transformed = self.transformer.step_back_prompting(query)
            else:
                continue

            # Retrieve for each transformed query
            for sub_query in transformed.transformed:
                docs = self.retriever.retrieve(sub_query, top_k=top_k)

                for doc in docs:
                    doc_id = doc.get('id', hash(doc.get('content', '')))
                    if doc_id not in seen_ids:
                        seen_ids.add(doc_id)
                        doc['source_query'] = sub_query
                        doc['transform_method'] = method
                        all_docs.append(doc)

        # Deduplicate and rank
        ranked_docs = self._rank_and_deduplicate(all_docs, query)

        return ranked_docs[:top_k]

    def _rank_and_deduplicate(
        self,
        docs: List[Dict],
        original_query: str
    ) -> List[Dict]:
        """Rank documents by relevance and remove duplicates"""
        # Group by content similarity
        unique_docs = []
        for doc in docs:
            is_duplicate = False
            for unique in unique_docs:
                if self._is_similar(doc, unique):
                    # Keep higher scoring version
                    if doc.get('score', 0) > unique.get('score', 0):
                        unique_docs.remove(unique)
                        unique_docs.append(doc)
                    is_duplicate = True
                    break
            if not is_duplicate:
                unique_docs.append(doc)

        # Sort by score
        return sorted(
            unique_docs,
            key=lambda x: x.get('score', 0),
            reverse=True
        )

    def _is_similar(self, doc1: Dict, doc2: Dict) -> bool:
        """Check if two documents are semantically similar"""
        # Simplified - use embedding similarity in production
        content1 = doc1.get('content', '').lower()
        content2 = doc2.get('content', '').lower()

        words1 = set(content1.split())
        words2 = set(content2.split())

        if not words1 or not words2:
            return False

        overlap = len(words1 & words2) / min(len(words1), len(words2))
        return overlap > self.dedup_threshold
```

### 2.2 Self-RAG and Corrective RAG

```python
"""
Self-RAG and Corrective RAG (CRAG) implementations
"""
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum


class RetrievalDecision(Enum):
    RETRIEVE = "retrieve"
    NO_RETRIEVE = "no_retrieve"
    INSUFFICIENT = "insufficient"


class SupportLevel(Enum):
    FULLY_SUPPORTED = "fully_supported"
    PARTIALLY_SUPPORTED = "partially_supported"
    NOT_SUPPORTED = "not_supported"


class SelfRAG:
    """
    Self-RAG: Learning to retrieve, generate, and critique.
    Model decides when to retrieve and verifies its outputs.
    """

    def __init__(
        self,
        retriever,
        generator,
        critic_model=None
    ):
        self.retriever = retriever
        self.generator = generator
        self.critic = critic_model or generator

    def query(
        self,
        question: str,
        top_k: int = 5
    ) -> Dict[str, Any]:
        """Self-RAG query with retrieval decision and self-critique"""

        # Step 1: Decide if retrieval is needed
        retrieval_decision = self._decide_retrieval(question)

        if retrieval_decision == RetrievalDecision.NO_RETRIEVE:
            # Generate without retrieval
            answer = self._generate_without_retrieval(question)
            return {
                "answer": answer,
                "retrieval_used": False,
                "documents": []
            }

        # Step 2: Retrieve documents
        documents = self.retriever.retrieve(question, top_k=top_k)

        # Step 3: Check document relevance
        relevant_docs = self._filter_relevant_docs(question, documents)

        if not relevant_docs:
            # No relevant docs found, generate with caveat
            answer = self._generate_with_uncertainty(question)
            return {
                "answer": answer,
                "retrieval_used": True,
                "documents": documents,
                "relevance_check": "no_relevant_docs"
            }

        # Step 4: Generate answer
        answer = self._generate_with_context(question, relevant_docs)

        # Step 5: Self-critique
        critique_result = self._critique_answer(
            question, answer, relevant_docs
        )

        # Step 6: Regenerate if needed
        if critique_result['needs_revision']:
            answer = self._revise_answer(
                question, answer, relevant_docs, critique_result['feedback']
            )

        return {
            "answer": answer,
            "retrieval_used": True,
            "documents": relevant_docs,
            "critique": critique_result,
            "support_level": critique_result['support_level']
        }

    def _decide_retrieval(self, question: str) -> RetrievalDecision:
        """Decide whether retrieval is needed for this question"""
        prompt = f"""Analyze this question and decide if external
        knowledge retrieval is needed to answer it accurately.

        Question: {question}

        Consider:
        - Is this factual and needs up-to-date information?
        - Is this about specific domain knowledge?
        - Could you confidently answer from general knowledge?

        Respond with EXACTLY one of: RETRIEVE, NO_RETRIEVE
        """

        decision = self.critic.generate(prompt).strip().upper()

        if "NO_RETRIEVE" in decision:
            return RetrievalDecision.NO_RETRIEVE
        return RetrievalDecision.RETRIEVE

    def _filter_relevant_docs(
        self,
        question: str,
        documents: List[Dict]
    ) -> List[Dict]:
        """Filter documents by relevance to question"""
        relevant = []

        for doc in documents:
            prompt = f"""Is this document relevant to answering the question?

            Question: {question}

            Document: {doc['content'][:500]}

            Answer YES or NO only.
            """

            relevance = self.critic.generate(prompt).strip().upper()
            if "YES" in relevance:
                relevant.append(doc)

        return relevant

    def _critique_answer(
        self,
        question: str,
        answer: str,
        documents: List[Dict]
    ) -> Dict[str, Any]:
        """Self-critique the generated answer"""
        context = "\n".join(doc['content'] for doc in documents)

        prompt = f"""Evaluate this answer for accuracy and support.

        Question: {question}
        Answer: {answer}

        Context documents:
        {context}

        Evaluate:
        1. Support level: Is the answer FULLY_SUPPORTED,
           PARTIALLY_SUPPORTED, or NOT_SUPPORTED by the context?
        2. Factual accuracy: Any factual errors?
        3. Completeness: Does it fully address the question?

        Return JSON:
        {{
            "support_level": "FULLY_SUPPORTED|PARTIALLY_SUPPORTED|NOT_SUPPORTED",
            "has_errors": true/false,
            "is_complete": true/false,
            "feedback": "specific feedback for improvement"
        }}
        """

        import json
        result = json.loads(self.critic.generate(prompt))

        result['needs_revision'] = (
            result['support_level'] != "FULLY_SUPPORTED" or
            result['has_errors'] or
            not result['is_complete']
        )

        return result

    def _generate_without_retrieval(self, question: str) -> str:
        """Generate answer using only model knowledge"""
        prompt = f"""Answer this question using your knowledge.
        If uncertain, acknowledge the uncertainty.

        Question: {question}
        """
        return self.generator.generate(prompt)

    def _generate_with_context(
        self,
        question: str,
        documents: List[Dict]
    ) -> str:
        """Generate answer using retrieved context"""
        context = "\n\n".join(
            f"[{i+1}] {doc['content']}"
            for i, doc in enumerate(documents)
        )

        prompt = f"""Answer the question using ONLY the provided context.
        Cite sources using [1], [2], etc.

        Context:
        {context}

        Question: {question}
        """
        return self.generator.generate(prompt)

    def _generate_with_uncertainty(self, question: str) -> str:
        """Generate answer acknowledging lack of relevant info"""
        prompt = f"""Answer this question, but note that no highly
        relevant sources were found in the knowledge base. Be cautious
        about factual claims.

        Question: {question}
        """
        return self.generator.generate(prompt)

    def _revise_answer(
        self,
        question: str,
        original_answer: str,
        documents: List[Dict],
        feedback: str
    ) -> str:
        """Revise answer based on self-critique feedback"""
        context = "\n".join(doc['content'] for doc in documents)

        prompt = f"""Revise this answer based on the feedback.

        Question: {question}
        Original answer: {original_answer}

        Feedback: {feedback}

        Context for reference:
        {context}

        Provide an improved answer:
        """
        return self.generator.generate(prompt)


class CorrectiveRAG:
    """
    CRAG: Corrective Retrieval Augmented Generation.
    Evaluates retrieval quality and takes corrective actions.
    """

    def __init__(
        self,
        retriever,
        generator,
        web_search=None,
        confidence_threshold: float = 0.7
    ):
        self.retriever = retriever
        self.generator = generator
        self.web_search = web_search
        self.confidence_threshold = confidence_threshold

    def query(self, question: str, top_k: int = 5) -> Dict[str, Any]:
        """CRAG query with corrective retrieval"""

        # Step 1: Initial retrieval
        documents = self.retriever.retrieve(question, top_k=top_k)

        # Step 2: Evaluate retrieval quality
        quality_assessment = self._assess_retrieval_quality(
            question, documents
        )

        # Step 3: Take corrective action based on quality
        if quality_assessment['action'] == 'correct':
            # Retrieval is good, proceed with generation
            final_docs = documents

        elif quality_assessment['action'] == 'refine':
            # Partially relevant, refine documents
            final_docs = self._refine_documents(
                question, documents, quality_assessment
            )

        elif quality_assessment['action'] == 'web_search':
            # Poor retrieval, try web search
            if self.web_search:
                web_results = self.web_search.search(question)
                final_docs = self._merge_results(documents, web_results)
            else:
                final_docs = documents

        else:  # 'ambiguous'
            # Mixed quality, combine strategies
            refined = self._refine_documents(
                question, documents, quality_assessment
            )
            if self.web_search:
                web_results = self.web_search.search(question)
                final_docs = self._merge_results(refined, web_results)
            else:
                final_docs = refined

        # Step 4: Generate answer
        answer = self._generate_answer(question, final_docs)

        return {
            "answer": answer,
            "documents": final_docs,
            "quality_assessment": quality_assessment,
            "corrective_action": quality_assessment['action']
        }

    def _assess_retrieval_quality(
        self,
        question: str,
        documents: List[Dict]
    ) -> Dict[str, Any]:
        """Assess quality of retrieved documents"""

        # Evaluate each document
        doc_assessments = []
        for doc in documents:
            assessment = self._evaluate_document(question, doc)
            doc_assessments.append(assessment)

        # Calculate overall quality
        relevant_count = sum(
            1 for a in doc_assessments if a['relevance'] == 'relevant'
        )
        total_count = len(doc_assessments)

        if total_count == 0:
            relevance_ratio = 0
        else:
            relevance_ratio = relevant_count / total_count

        # Determine corrective action
        if relevance_ratio >= 0.7:
            action = 'correct'  # Good quality, proceed
        elif relevance_ratio >= 0.4:
            action = 'refine'   # Partial quality, refine
        elif relevance_ratio >= 0.2:
            action = 'ambiguous'  # Mixed, combine strategies
        else:
            action = 'web_search'  # Poor, use web search

        return {
            'action': action,
            'relevance_ratio': relevance_ratio,
            'doc_assessments': doc_assessments
        }

    def _evaluate_document(
        self,
        question: str,
        document: Dict
    ) -> Dict[str, str]:
        """Evaluate single document relevance"""
        prompt = f"""Evaluate if this document is relevant to the question.

        Question: {question}
        Document: {document['content'][:500]}

        Rate as: RELEVANT, PARTIALLY_RELEVANT, or IRRELEVANT
        """

        relevance = self.generator.generate(prompt).strip().lower()

        if 'irrelevant' in relevance:
            return {'relevance': 'irrelevant', 'doc_id': document.get('id')}
        elif 'partially' in relevance:
            return {'relevance': 'partial', 'doc_id': document.get('id')}
        else:
            return {'relevance': 'relevant', 'doc_id': document.get('id')}

    def _refine_documents(
        self,
        question: str,
        documents: List[Dict],
        assessment: Dict
    ) -> List[Dict]:
        """Refine documents by extracting relevant parts"""
        refined = []

        for doc, doc_assessment in zip(
            documents, assessment['doc_assessments']
        ):
            if doc_assessment['relevance'] == 'relevant':
                refined.append(doc)
            elif doc_assessment['relevance'] == 'partial':
                # Extract only relevant portions
                extracted = self._extract_relevant_content(
                    question, doc['content']
                )
                if extracted:
                    refined.append({
                        **doc,
                        'content': extracted,
                        'refined': True
                    })

        return refined

    def _extract_relevant_content(
        self,
        question: str,
        content: str
    ) -> Optional[str]:
        """Extract relevant portions from document"""
        prompt = f"""Extract ONLY the portions of this document that
        are relevant to answering the question. If nothing is relevant,
        respond with "NONE".

        Question: {question}
        Document: {content}

        Relevant portions:
        """

        extracted = self.generator.generate(prompt)
        if "NONE" in extracted.upper():
            return None
        return extracted

    def _merge_results(
        self,
        retrieval_docs: List[Dict],
        web_docs: List[Dict]
    ) -> List[Dict]:
        """Merge retrieval and web search results"""
        # Deduplicate by content similarity
        seen_content = set()
        merged = []

        # Prioritize retrieval results
        for doc in retrieval_docs:
            content_hash = hash(doc['content'][:100])
            if content_hash not in seen_content:
                seen_content.add(content_hash)
                doc['source'] = 'retrieval'
                merged.append(doc)

        # Add web results
        for doc in web_docs:
            content_hash = hash(doc.get('content', '')[:100])
            if content_hash not in seen_content:
                seen_content.add(content_hash)
                doc['source'] = 'web_search'
                merged.append(doc)

        return merged

    def _generate_answer(
        self,
        question: str,
        documents: List[Dict]
    ) -> str:
        """Generate final answer from documents"""
        context = "\n\n".join(
            f"[Source: {doc.get('source', 'unknown')}] {doc['content']}"
            for doc in documents
        )

        prompt = f"""Answer the question using the provided sources.

        Sources:
        {context}

        Question: {question}

        Answer:
        """
        return self.generator.generate(prompt)
```

---

## 3. Modular RAG Architecture

### 3.1 Module-Based Design

```python
"""
Modular RAG with pluggable components and routing
"""
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional, Callable
from dataclasses import dataclass, field
from enum import Enum
import asyncio


# Base module interface
class RAGModule(ABC):
    """Base class for all RAG modules"""

    @abstractmethod
    def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Process input and return output"""
        pass

    @property
    @abstractmethod
    def name(self) -> str:
        """Module name for routing"""
        pass


# Query Analysis Module
class QueryAnalyzer(RAGModule):
    """Analyze and classify incoming queries"""

    def __init__(self, llm_client):
        self.llm = llm_client

    @property
    def name(self) -> str:
        return "query_analyzer"

    def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        query = input_data['query']

        analysis = self._analyze_query(query)

        return {
            **input_data,
            'query_analysis': analysis,
            'query_type': analysis['type'],
            'complexity': analysis['complexity'],
            'domains': analysis['domains']
        }

    def _analyze_query(self, query: str) -> Dict[str, Any]:
        """Analyze query characteristics"""
        import json

        prompt = f"""Analyze this query for RAG routing:

        Query: {query}

        Return JSON:
        {{
            "type": "factual|analytical|comparative|procedural|conversational",
            "complexity": "simple|moderate|complex",
            "domains": ["list of relevant domains"],
            "requires_structured_data": true/false,
            "requires_web_search": true/false,
            "requires_multi_hop": true/false
        }}
        """

        result = self.llm.generate(prompt)
        return json.loads(result)


# Router Module
class QueryRouter(RAGModule):
    """Route queries to appropriate retrieval pipelines"""

    def __init__(
        self,
        routes: Dict[str, Callable],
        default_route: str = "general"
    ):
        self.routes = routes
        self.default_route = default_route

    @property
    def name(self) -> str:
        return "router"

    def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        # Get routing decision based on query analysis
        query_type = input_data.get('query_type', 'factual')
        complexity = input_data.get('complexity', 'simple')
        domains = input_data.get('domains', [])

        route = self._select_route(query_type, complexity, domains)

        return {
            **input_data,
            'selected_route': route,
            'pipeline': self.routes.get(route, self.routes[self.default_route])
        }

    def _select_route(
        self,
        query_type: str,
        complexity: str,
        domains: List[str]
    ) -> str:
        """Select appropriate route based on query characteristics"""

        # Domain-specific routing
        for domain in domains:
            if domain in self.routes:
                return domain

        # Complexity-based routing
        if complexity == "complex":
            return "agentic"

        # Type-based routing
        type_routes = {
            "factual": "dense",
            "analytical": "hybrid",
            "comparative": "multi_index",
            "procedural": "structured",
            "conversational": "conversational"
        }

        return type_routes.get(query_type, self.default_route)


# Retriever Pool
class RetrieverPool(RAGModule):
    """Pool of specialized retrievers"""

    def __init__(self, retrievers: Dict[str, Any]):
        self.retrievers = retrievers

    @property
    def name(self) -> str:
        return "retriever_pool"

    def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        route = input_data.get('selected_route', 'general')
        query = input_data['query']

        # Get appropriate retriever
        retriever = self.retrievers.get(
            route,
            self.retrievers.get('general')
        )

        # Retrieve documents
        documents = retriever.retrieve(
            query,
            top_k=input_data.get('top_k', 10)
        )

        return {
            **input_data,
            'documents': documents,
            'retriever_used': route
        }


# Fusion Module
class ResultFusion(RAGModule):
    """Fuse results from multiple retrievers"""

    def __init__(self, fusion_method: str = "rrf"):
        self.fusion_method = fusion_method

    @property
    def name(self) -> str:
        return "fusion"

    def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        result_sets = input_data.get('multi_results', [])

        if not result_sets or len(result_sets) == 1:
            # No fusion needed
            return input_data

        if self.fusion_method == "rrf":
            fused = self._reciprocal_rank_fusion(result_sets)
        elif self.fusion_method == "weighted":
            weights = input_data.get('fusion_weights', None)
            fused = self._weighted_fusion(result_sets, weights)
        else:
            fused = self._simple_union(result_sets)

        return {
            **input_data,
            'documents': fused,
            'fusion_method': self.fusion_method
        }

    def _reciprocal_rank_fusion(
        self,
        result_sets: List[List[Dict]],
        k: int = 60
    ) -> List[Dict]:
        """Reciprocal Rank Fusion"""
        doc_scores = {}
        doc_data = {}

        for results in result_sets:
            for rank, doc in enumerate(results):
                doc_id = doc.get('id', hash(doc.get('content', '')))
                rrf_score = 1 / (k + rank + 1)

                if doc_id in doc_scores:
                    doc_scores[doc_id] += rrf_score
                else:
                    doc_scores[doc_id] = rrf_score
                    doc_data[doc_id] = doc

        # Sort by fused score
        sorted_ids = sorted(
            doc_scores.keys(),
            key=lambda x: doc_scores[x],
            reverse=True
        )

        fused = []
        for doc_id in sorted_ids:
            doc = doc_data[doc_id].copy()
            doc['rrf_score'] = doc_scores[doc_id]
            fused.append(doc)

        return fused

    def _weighted_fusion(
        self,
        result_sets: List[List[Dict]],
        weights: Optional[List[float]] = None
    ) -> List[Dict]:
        """Weighted score fusion"""
        if weights is None:
            weights = [1.0] * len(result_sets)

        doc_scores = {}
        doc_data = {}

        for results, weight in zip(result_sets, weights):
            max_score = max(
                (doc.get('score', 0) for doc in results),
                default=1
            )

            for doc in results:
                doc_id = doc.get('id', hash(doc.get('content', '')))
                normalized_score = doc.get('score', 0) / max_score
                weighted_score = normalized_score * weight

                if doc_id in doc_scores:
                    doc_scores[doc_id] += weighted_score
                else:
                    doc_scores[doc_id] = weighted_score
                    doc_data[doc_id] = doc

        sorted_ids = sorted(
            doc_scores.keys(),
            key=lambda x: doc_scores[x],
            reverse=True
        )

        return [doc_data[doc_id] for doc_id in sorted_ids]

    def _simple_union(
        self,
        result_sets: List[List[Dict]]
    ) -> List[Dict]:
        """Simple union with deduplication"""
        seen = set()
        union = []

        for results in result_sets:
            for doc in results:
                doc_id = doc.get('id', hash(doc.get('content', '')))
                if doc_id not in seen:
                    seen.add(doc_id)
                    union.append(doc)

        return union


# Modular RAG Pipeline
class ModularRAGPipeline:
    """Orchestrate modular RAG components"""

    def __init__(self, modules: List[RAGModule]):
        self.modules = {m.name: m for m in modules}
        self.pipeline_order = [m.name for m in modules]

    def execute(
        self,
        query: str,
        config: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """Execute the modular RAG pipeline"""

        # Initialize data
        data = {
            'query': query,
            'config': config or {},
            'execution_trace': []
        }

        # Execute each module in order
        for module_name in self.pipeline_order:
            module = self.modules[module_name]

            # Process
            data = module.process(data)

            # Record execution
            data['execution_trace'].append({
                'module': module_name,
                'output_keys': list(data.keys())
            })

        return data

    async def execute_async(
        self,
        query: str,
        config: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """Async execution with parallel modules where possible"""
        data = {
            'query': query,
            'config': config or {},
            'execution_trace': []
        }

        # Execute modules (could parallelize independent modules)
        for module_name in self.pipeline_order:
            module = self.modules[module_name]

            # Check if module has async process
            if hasattr(module, 'process_async'):
                data = await module.process_async(data)
            else:
                data = module.process(data)

            data['execution_trace'].append({'module': module_name})

        return data
```

### 3.2 Dynamic Pipeline Configuration

```python
"""
Dynamic pipeline configuration for adaptive RAG
"""
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional
import yaml


@dataclass
class PipelineConfig:
    """Configuration for RAG pipeline"""
    name: str
    modules: List[str]
    routing_rules: Dict[str, str] = field(default_factory=dict)
    module_configs: Dict[str, Dict] = field(default_factory=dict)


class DynamicPipelineBuilder:
    """Build pipelines from configuration"""

    def __init__(self, module_registry: Dict[str, type]):
        self.registry = module_registry

    def from_yaml(self, yaml_path: str) -> ModularRAGPipeline:
        """Build pipeline from YAML config"""
        with open(yaml_path) as f:
            config = yaml.safe_load(f)

        return self.from_config(PipelineConfig(**config))

    def from_config(self, config: PipelineConfig) -> ModularRAGPipeline:
        """Build pipeline from PipelineConfig"""
        modules = []

        for module_name in config.modules:
            module_class = self.registry.get(module_name)
            if module_class is None:
                raise ValueError(f"Unknown module: {module_name}")

            module_config = config.module_configs.get(module_name, {})
            module = module_class(**module_config)
            modules.append(module)

        return ModularRAGPipeline(modules)


# Example YAML configuration
EXAMPLE_PIPELINE_CONFIG = """
name: adaptive_rag
modules:
  - query_analyzer
  - router
  - retriever_pool
  - reranker
  - context_compressor
  - generator

routing_rules:
  factual: dense_retriever
  analytical: hybrid_retriever
  code: code_retriever
  default: general_retriever

module_configs:
  retriever_pool:
    default_top_k: 20
  reranker:
    model: cross-encoder/ms-marco-MiniLM-L-12-v2
    top_k: 10
  context_compressor:
    max_tokens: 4000
    method: extractive
"""
```

---

## 4. Agentic RAG

### 4.1 Agent-Based Retrieval

```python
"""
Agentic RAG with tool-using agents and multi-hop reasoning
"""
from typing import List, Dict, Any, Optional, Callable
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
import json


@dataclass
class Tool:
    """Definition of a tool available to the agent"""
    name: str
    description: str
    function: Callable
    parameters: Dict[str, Any]


@dataclass
class AgentAction:
    """Action taken by the agent"""
    tool: str
    tool_input: Dict[str, Any]
    reasoning: str


@dataclass
class AgentObservation:
    """Result of agent action"""
    tool: str
    result: Any


class AgenticRAG:
    """
    Agentic RAG: Agent decides when and how to retrieve
    """

    def __init__(
        self,
        llm_client,
        tools: List[Tool],
        max_iterations: int = 5
    ):
        self.llm = llm_client
        self.tools = {tool.name: tool for tool in tools}
        self.max_iterations = max_iterations

    def query(self, question: str) -> Dict[str, Any]:
        """Execute agentic RAG query"""

        # Initialize state
        history = []
        iteration = 0

        while iteration < self.max_iterations:
            # Decide next action
            action = self._decide_action(question, history)

            # Check if we should finish
            if action.tool == "finish":
                return {
                    "answer": action.tool_input.get("answer", ""),
                    "history": history,
                    "iterations": iteration
                }

            # Execute tool
            observation = self._execute_tool(action)

            # Add to history
            history.append({
                "action": action,
                "observation": observation
            })

            iteration += 1

        # Max iterations reached, synthesize answer
        return self._synthesize_answer(question, history)

    def _decide_action(
        self,
        question: str,
        history: List[Dict]
    ) -> AgentAction:
        """Decide next action based on question and history"""

        # Format tool descriptions
        tool_descriptions = "\n".join(
            f"- {name}: {tool.description}"
            for name, tool in self.tools.items()
        )

        # Format history
        history_str = ""
        for step in history:
            action = step['action']
            obs = step['observation']
            history_str += f"\nAction: {action.tool}({action.tool_input})"
            history_str += f"\nObservation: {obs.result}"

        prompt = f"""You are an AI agent that answers questions by
        using tools. Decide what to do next.

        Available tools:
        {tool_descriptions}
        - finish: Use when you have enough information to answer

        Question: {question}

        Previous actions and observations:
        {history_str if history_str else "None yet"}

        Think step by step about what information you need and
        which tool would help. Then respond with JSON:
        {{
            "reasoning": "your thought process",
            "tool": "tool_name or finish",
            "tool_input": {{...}}
        }}

        If using finish, tool_input should be {{"answer": "your answer"}}
        """

        response = self.llm.generate(prompt)
        result = json.loads(response)

        return AgentAction(
            tool=result['tool'],
            tool_input=result['tool_input'],
            reasoning=result['reasoning']
        )

    def _execute_tool(self, action: AgentAction) -> AgentObservation:
        """Execute the selected tool"""

        tool = self.tools.get(action.tool)
        if tool is None:
            return AgentObservation(
                tool=action.tool,
                result=f"Error: Unknown tool {action.tool}"
            )

        try:
            result = tool.function(**action.tool_input)
            return AgentObservation(tool=action.tool, result=result)
        except Exception as e:
            return AgentObservation(
                tool=action.tool,
                result=f"Error: {str(e)}"
            )

    def _synthesize_answer(
        self,
        question: str,
        history: List[Dict]
    ) -> Dict[str, Any]:
        """Synthesize answer from history when max iterations reached"""

        observations = "\n".join(
            f"- {step['action'].tool}: {step['observation'].result}"
            for step in history
        )

        prompt = f"""Based on the information gathered, answer the question.

        Question: {question}

        Information gathered:
        {observations}

        Provide a comprehensive answer based on this information.
        """

        answer = self.llm.generate(prompt)

        return {
            "answer": answer,
            "history": history,
            "iterations": len(history),
            "status": "max_iterations_reached"
        }


# Example tools for agentic RAG
def create_rag_tools(
    vector_retriever,
    keyword_retriever,
    web_search=None
) -> List[Tool]:
    """Create standard RAG tools"""

    tools = [
        Tool(
            name="semantic_search",
            description="Search the knowledge base using semantic similarity. Use for conceptual questions.",
            function=lambda query, top_k=5: vector_retriever.retrieve(query, top_k),
            parameters={"query": "str", "top_k": "int (optional)"}
        ),
        Tool(
            name="keyword_search",
            description="Search using exact keyword matching. Use for specific terms, names, or codes.",
            function=lambda query, top_k=5: keyword_retriever.retrieve(query, top_k),
            parameters={"query": "str", "top_k": "int (optional)"}
        ),
        Tool(
            name="lookup_document",
            description="Retrieve a specific document by ID. Use when you know the document ID.",
            function=lambda doc_id: vector_retriever.get_document(doc_id),
            parameters={"doc_id": "str"}
        )
    ]

    if web_search:
        tools.append(Tool(
            name="web_search",
            description="Search the web for recent information. Use for current events or when knowledge base lacks info.",
            function=lambda query: web_search.search(query),
            parameters={"query": "str"}
        ))

    return tools
```

### 4.2 Multi-Hop Reasoning

```python
"""
Multi-hop reasoning for complex questions
"""
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
import json


@dataclass
class ReasoningStep:
    """A single step in multi-hop reasoning"""
    question: str
    thought: str
    action: str
    result: Any
    is_final: bool = False


class MultiHopReasoner:
    """
    Multi-hop reasoning with iterative retrieval and reasoning
    """

    def __init__(
        self,
        llm_client,
        retriever,
        max_hops: int = 5
    ):
        self.llm = llm_client
        self.retriever = retriever
        self.max_hops = max_hops

    def reason(self, question: str) -> Dict[str, Any]:
        """Perform multi-hop reasoning"""

        steps = []
        context = []
        hop = 0

        while hop < self.max_hops:
            # Decompose: what do we need to find out?
            step = self._reason_step(question, steps, context)
            steps.append(step)

            if step.is_final:
                break

            # Retrieve information for sub-question
            retrieved = self.retriever.retrieve(
                step.question,
                top_k=3
            )
            context.extend(retrieved)

            # Update step with retrieval result
            step.result = self._summarize_retrieval(
                step.question, retrieved
            )

            hop += 1

        # Generate final answer
        answer = self._generate_final_answer(question, steps, context)

        return {
            "answer": answer,
            "reasoning_steps": steps,
            "hops": hop,
            "context_documents": context
        }

    def _reason_step(
        self,
        original_question: str,
        previous_steps: List[ReasoningStep],
        context: List[Dict]
    ) -> ReasoningStep:
        """Determine next reasoning step"""

        # Format previous reasoning
        prev_reasoning = ""
        for i, step in enumerate(previous_steps, 1):
            prev_reasoning += f"\nStep {i}: {step.thought}"
            if step.result:
                prev_reasoning += f"\nResult: {step.result}"

        # Format available context
        context_str = "\n".join(
            f"- {doc.get('content', '')[:200]}..."
            for doc in context[-5:]  # Last 5 documents
        )

        prompt = f"""You are solving a complex question through
        step-by-step reasoning. Decide the next step.

        Original question: {original_question}

        Previous reasoning:
        {prev_reasoning if prev_reasoning else "None yet"}

        Available context:
        {context_str if context_str else "None yet"}

        Decide:
        1. Do you have enough information to answer? If yes, provide final answer.
        2. If not, what specific sub-question needs to be answered?

        Respond with JSON:
        {{
            "thought": "your reasoning about what's needed",
            "is_final": true/false,
            "sub_question": "specific question to search for" (if not final),
            "final_answer": "answer" (if final)
        }}
        """

        response = self.llm.generate(prompt)
        result = json.loads(response)

        return ReasoningStep(
            question=result.get('sub_question', ''),
            thought=result['thought'],
            action="retrieve" if not result['is_final'] else "answer",
            result=result.get('final_answer'),
            is_final=result['is_final']
        )

    def _summarize_retrieval(
        self,
        question: str,
        documents: List[Dict]
    ) -> str:
        """Summarize retrieved documents for the sub-question"""

        doc_content = "\n\n".join(
            doc.get('content', '') for doc in documents
        )

        prompt = f"""Summarize the key information from these documents
        that answers the question.

        Question: {question}

        Documents:
        {doc_content}

        Key findings:
        """

        return self.llm.generate(prompt)

    def _generate_final_answer(
        self,
        question: str,
        steps: List[ReasoningStep],
        context: List[Dict]
    ) -> str:
        """Generate final answer from reasoning chain"""

        # Check if last step has final answer
        if steps and steps[-1].is_final and steps[-1].result:
            return steps[-1].result

        # Otherwise synthesize from reasoning chain
        reasoning_chain = "\n".join(
            f"- {step.thought}: {step.result or 'No result'}"
            for step in steps
        )

        context_str = "\n".join(
            doc.get('content', '')[:300] for doc in context[:5]
        )

        prompt = f"""Based on the following reasoning chain and
        context, provide a comprehensive answer.

        Question: {question}

        Reasoning chain:
        {reasoning_chain}

        Supporting context:
        {context_str}

        Final answer:
        """

        return self.llm.generate(prompt)


class QueryPlanner:
    """Plan complex queries into executable steps"""

    def __init__(self, llm_client):
        self.llm = llm_client

    def plan(self, question: str) -> List[Dict[str, Any]]:
        """Create execution plan for complex question"""

        prompt = f"""Create a step-by-step plan to answer this question.
        Each step should be a specific, searchable query.

        Question: {question}

        Create a plan with JSON:
        {{
            "analysis": "brief analysis of question complexity",
            "steps": [
                {{
                    "step_number": 1,
                    "purpose": "what this step accomplishes",
                    "query": "specific search query",
                    "depends_on": [] // list of step numbers this depends on
                }},
                ...
            ],
            "synthesis_strategy": "how to combine results"
        }}
        """

        result = json.loads(self.llm.generate(prompt))
        return result
```

---

## 5. Multi-Modal RAG

### 5.1 Image and Document Understanding

```python
"""
Multi-modal RAG with vision models and ColPali
"""
from typing import List, Dict, Any, Optional, Union
from dataclasses import dataclass
import base64
from pathlib import Path


@dataclass
class MultiModalDocument:
    """Document with text and visual content"""
    id: str
    text_content: Optional[str]
    image_content: Optional[bytes]  # Raw image bytes
    image_embeddings: Optional[List[List[float]]]  # ColPali patch embeddings
    metadata: Dict[str, Any]


class ColPaliRetriever:
    """
    ColPali-based retrieval using vision language models.
    Embeds document pages as images for visually-rich retrieval.
    """

    def __init__(
        self,
        model_name: str = "vidore/colpali-v1.2",
        vector_db_client=None,
        device: str = "cuda"
    ):
        self.model_name = model_name
        self.db = vector_db_client
        self.device = device
        self._load_model()

    def _load_model(self):
        """Load ColPali model"""
        from colpali_engine.models import ColPali, ColPaliProcessor

        self.model = ColPali.from_pretrained(
            self.model_name,
            torch_dtype="auto",
            device_map=self.device
        ).eval()

        self.processor = ColPaliProcessor.from_pretrained(self.model_name)

    def embed_page(self, image_path: str) -> List[List[float]]:
        """
        Embed a document page image into multi-vector representation.
        Returns patch embeddings (many vectors per image).
        """
        from PIL import Image
        import torch

        # Load image
        image = Image.open(image_path)

        # Process through ColPali
        inputs = self.processor(images=[image], return_tensors="pt").to(self.device)

        with torch.no_grad():
            embeddings = self.model(**inputs)

        # Return patch embeddings
        return embeddings[0].cpu().numpy().tolist()

    def embed_query(self, query: str) -> List[List[float]]:
        """Embed query into multi-vector representation"""
        import torch

        inputs = self.processor(text=[query], return_tensors="pt").to(self.device)

        with torch.no_grad():
            embeddings = self.model(**inputs)

        return embeddings[0].cpu().numpy().tolist()

    def retrieve(
        self,
        query: str,
        top_k: int = 10
    ) -> List[MultiModalDocument]:
        """
        Retrieve documents using late interaction scoring.
        Uses MaxSim between query and document patch embeddings.
        """
        # Embed query
        query_embeddings = self.embed_query(query)

        # Search using late interaction
        # For each document, compute MaxSim score
        results = self._late_interaction_search(query_embeddings, top_k)

        return results

    def _late_interaction_search(
        self,
        query_embeddings: List[List[float]],
        top_k: int
    ) -> List[MultiModalDocument]:
        """
        Late interaction search using MaxSim scoring.
        For each query token, find max similarity to any document patch,
        then sum across all query tokens.
        """
        import numpy as np

        query_vecs = np.array(query_embeddings)

        # Get all document embeddings from DB
        # In production, use approximate nearest neighbor
        all_docs = self.db.get_all_documents()

        scored_docs = []
        for doc in all_docs:
            if doc.image_embeddings is None:
                continue

            doc_vecs = np.array(doc.image_embeddings)

            # MaxSim scoring
            # For each query token, find max similarity to doc patches
            similarity_matrix = np.dot(query_vecs, doc_vecs.T)
            max_sims = similarity_matrix.max(axis=1)  # Max over doc patches
            score = max_sims.sum()  # Sum over query tokens

            scored_docs.append((doc, score))

        # Sort by score
        scored_docs.sort(key=lambda x: x[1], reverse=True)

        return [doc for doc, _ in scored_docs[:top_k]]

    def index_pdf(self, pdf_path: str, doc_id: str):
        """Index a PDF document by rendering pages as images"""
        import fitz  # PyMuPDF
        from PIL import Image
        import io

        pdf = fitz.open(pdf_path)

        for page_num in range(len(pdf)):
            page = pdf[page_num]

            # Render page as image
            pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # 2x zoom
            img_bytes = pix.tobytes("png")

            # Embed page
            img = Image.open(io.BytesIO(img_bytes))

            # Save temporarily for embedding
            temp_path = f"/tmp/page_{page_num}.png"
            img.save(temp_path)

            embeddings = self.embed_page(temp_path)

            # Store in vector DB
            doc = MultiModalDocument(
                id=f"{doc_id}_page_{page_num}",
                text_content=page.get_text(),
                image_content=img_bytes,
                image_embeddings=embeddings,
                metadata={
                    "source": pdf_path,
                    "page": page_num,
                    "total_pages": len(pdf)
                }
            )

            self.db.insert(doc)

        pdf.close()


class MultiModalRAG:
    """
    RAG system supporting text, images, tables, and charts
    """

    def __init__(
        self,
        text_retriever,
        image_retriever: ColPaliRetriever,
        vision_model,
        text_model
    ):
        self.text_retriever = text_retriever
        self.image_retriever = image_retriever
        self.vision_model = vision_model
        self.text_model = text_model

    def query(
        self,
        question: str,
        include_images: bool = True,
        top_k: int = 5
    ) -> Dict[str, Any]:
        """Multi-modal RAG query"""

        results = {
            "text_results": [],
            "image_results": []
        }

        # Text retrieval
        text_docs = self.text_retriever.retrieve(question, top_k=top_k)
        results["text_results"] = text_docs

        # Image retrieval (if enabled)
        if include_images:
            image_docs = self.image_retriever.retrieve(question, top_k=top_k)
            results["image_results"] = image_docs

        # Process images with vision model
        image_descriptions = []
        for img_doc in results["image_results"]:
            if img_doc.image_content:
                description = self._describe_image(
                    img_doc.image_content,
                    question
                )
                image_descriptions.append({
                    "doc_id": img_doc.id,
                    "description": description
                })

        # Generate answer combining text and image context
        answer = self._generate_multimodal_answer(
            question,
            text_docs,
            image_descriptions
        )

        return {
            "answer": answer,
            "text_sources": text_docs,
            "image_sources": results["image_results"],
            "image_descriptions": image_descriptions
        }

    def _describe_image(
        self,
        image_bytes: bytes,
        question: str
    ) -> str:
        """Use vision model to describe image in context of question"""

        # Encode image
        image_b64 = base64.b64encode(image_bytes).decode()

        prompt = f"""Analyze this image in the context of the question.
        Extract any relevant information including:
        - Text visible in the image
        - Data from tables or charts
        - Key visual elements

        Question context: {question}

        Relevant information from image:
        """

        return self.vision_model.generate(
            prompt,
            images=[image_b64]
        )

    def _generate_multimodal_answer(
        self,
        question: str,
        text_docs: List[Dict],
        image_descriptions: List[Dict]
    ) -> str:
        """Generate answer from text and image context"""

        # Format text context
        text_context = "\n\n".join(
            f"[Text {i+1}] {doc.get('content', '')}"
            for i, doc in enumerate(text_docs)
        )

        # Format image context
        image_context = "\n\n".join(
            f"[Image {i+1}] {desc['description']}"
            for i, desc in enumerate(image_descriptions)
        )

        prompt = f"""Answer the question using the provided text and
        image information. Cite sources as [Text N] or [Image N].

        Text Sources:
        {text_context}

        Image Sources:
        {image_context}

        Question: {question}

        Answer:
        """

        return self.text_model.generate(prompt)
```

---

## 6. Conversational RAG

### 6.1 Context Management

```python
"""
Conversational RAG with memory and context carryover
"""
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime
from collections import deque


@dataclass
class ConversationTurn:
    """Single turn in conversation"""
    role: str  # "user" or "assistant"
    content: str
    timestamp: datetime = field(default_factory=datetime.now)
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class ConversationContext:
    """Full conversation context"""
    session_id: str
    turns: List[ConversationTurn] = field(default_factory=list)
    summary: Optional[str] = None
    entities: Dict[str, Any] = field(default_factory=dict)
    retrieval_history: List[Dict] = field(default_factory=list)


class ConversationMemory:
    """
    Memory management for conversational RAG.
    Implements buffer window, summary, and entity extraction.
    """

    def __init__(
        self,
        llm_client,
        buffer_size: int = 10,
        summarize_threshold: int = 20
    ):
        self.llm = llm_client
        self.buffer_size = buffer_size
        self.summarize_threshold = summarize_threshold
        self.contexts: Dict[str, ConversationContext] = {}

    def get_or_create_context(self, session_id: str) -> ConversationContext:
        """Get existing or create new conversation context"""
        if session_id not in self.contexts:
            self.contexts[session_id] = ConversationContext(
                session_id=session_id
            )
        return self.contexts[session_id]

    def add_turn(
        self,
        session_id: str,
        role: str,
        content: str,
        metadata: Optional[Dict] = None
    ):
        """Add a turn to conversation history"""
        context = self.get_or_create_context(session_id)

        turn = ConversationTurn(
            role=role,
            content=content,
            metadata=metadata or {}
        )
        context.turns.append(turn)

        # Extract entities from user message
        if role == "user":
            entities = self._extract_entities(content)
            context.entities.update(entities)

        # Summarize if needed
        if len(context.turns) >= self.summarize_threshold:
            self._update_summary(context)

    def get_context_window(
        self,
        session_id: str,
        include_summary: bool = True
    ) -> str:
        """Get formatted context window for RAG"""
        context = self.get_or_create_context(session_id)

        parts = []

        # Add summary if available
        if include_summary and context.summary:
            parts.append(f"Previous conversation summary:\n{context.summary}")

        # Add recent turns
        recent_turns = context.turns[-self.buffer_size:]
        for turn in recent_turns:
            parts.append(f"{turn.role.capitalize()}: {turn.content}")

        return "\n\n".join(parts)

    def get_entities(self, session_id: str) -> Dict[str, Any]:
        """Get extracted entities for context enrichment"""
        context = self.get_or_create_context(session_id)
        return context.entities

    def _extract_entities(self, text: str) -> Dict[str, Any]:
        """Extract named entities and key concepts from text"""
        prompt = f"""Extract named entities and key concepts from this text.

        Text: {text}

        Return JSON with:
        {{
            "people": ["list of person names"],
            "organizations": ["list of org names"],
            "locations": ["list of places"],
            "dates": ["list of dates/times"],
            "topics": ["list of key topics discussed"],
            "references": ["things the user referred to"]
        }}
        """

        import json
        try:
            result = json.loads(self.llm.generate(prompt))
            return result
        except:
            return {}

    def _update_summary(self, context: ConversationContext):
        """Summarize older turns to compress context"""

        # Keep recent turns, summarize older ones
        older_turns = context.turns[:-self.buffer_size]

        if not older_turns:
            return

        # Format turns for summarization
        turns_text = "\n".join(
            f"{t.role}: {t.content}" for t in older_turns
        )

        prompt = f"""Summarize this conversation, preserving:
        - Key topics discussed
        - Important decisions or conclusions
        - Relevant context for future questions

        Conversation:
        {turns_text}

        Previous summary (if any):
        {context.summary or "None"}

        Updated summary:
        """

        context.summary = self.llm.generate(prompt)

        # Remove summarized turns
        context.turns = context.turns[-self.buffer_size:]


class ConversationalRAG:
    """
    RAG with conversation context and coreference resolution
    """

    def __init__(
        self,
        retriever,
        generator,
        memory: ConversationMemory
    ):
        self.retriever = retriever
        self.generator = generator
        self.memory = memory

    def query(
        self,
        session_id: str,
        user_message: str,
        top_k: int = 5
    ) -> Dict[str, Any]:
        """Process conversational RAG query"""

        # Add user message to memory
        self.memory.add_turn(session_id, "user", user_message)

        # Resolve coreferences and expand query
        resolved_query = self._resolve_coreferences(session_id, user_message)

        # Retrieve with resolved query
        documents = self.retriever.retrieve(resolved_query, top_k=top_k)

        # Get conversation context
        conversation_context = self.memory.get_context_window(session_id)
        entities = self.memory.get_entities(session_id)

        # Generate response
        response = self._generate_response(
            user_message,
            resolved_query,
            documents,
            conversation_context,
            entities
        )

        # Add response to memory
        self.memory.add_turn(session_id, "assistant", response)

        return {
            "response": response,
            "resolved_query": resolved_query,
            "documents": documents,
            "session_id": session_id
        }

    def _resolve_coreferences(
        self,
        session_id: str,
        user_message: str
    ) -> str:
        """Resolve pronouns and references in user message"""

        context = self.memory.get_context_window(session_id)
        entities = self.memory.get_entities(session_id)

        prompt = f"""Resolve any coreferences in the user's message.
        Replace pronouns and references with explicit mentions.

        Conversation context:
        {context}

        Known entities:
        {entities}

        User message: {user_message}

        Rewritten message with resolved references:
        """

        resolved = self.generator.generate(prompt)
        return resolved.strip()

    def _generate_response(
        self,
        original_query: str,
        resolved_query: str,
        documents: List[Dict],
        conversation_context: str,
        entities: Dict[str, Any]
    ) -> str:
        """Generate contextual response"""

        # Format document context
        doc_context = "\n\n".join(
            f"[{i+1}] {doc.get('content', '')}"
            for i, doc in enumerate(documents)
        )

        prompt = f"""You are a helpful assistant in a conversation.
        Answer based on the documents and conversation history.

        Conversation history:
        {conversation_context}

        Retrieved documents:
        {doc_context}

        Current question: {original_query}
        (Resolved as: {resolved_query})

        Provide a helpful, contextual response. Reference the
        conversation history when relevant. Cite documents as [N].

        Response:
        """

        return self.generator.generate(prompt)

    def reset_session(self, session_id: str):
        """Clear conversation history for a session"""
        if session_id in self.memory.contexts:
            del self.memory.contexts[session_id]
```

---

## 7. Structured Data RAG

### 7.1 Text-to-SQL Integration

```python
"""
RAG with structured data sources (SQL, knowledge graphs)
"""
from typing import List, Dict, Any, Optional, Union
from dataclasses import dataclass
import json


@dataclass
class StructuredResult:
    """Result from structured data query"""
    source_type: str  # "sql", "kg", "api"
    query: str
    result: Any
    schema_info: Optional[Dict] = None


class TextToSQLEngine:
    """
    Text-to-SQL for structured data retrieval
    """

    def __init__(
        self,
        llm_client,
        db_connection,
        schema_info: Dict[str, Any]
    ):
        self.llm = llm_client
        self.db = db_connection
        self.schema = schema_info

    def query(self, natural_language_query: str) -> StructuredResult:
        """Convert natural language to SQL and execute"""

        # Generate SQL
        sql = self._generate_sql(natural_language_query)

        # Validate SQL (basic safety checks)
        if not self._validate_sql(sql):
            return StructuredResult(
                source_type="sql",
                query=sql,
                result={"error": "SQL validation failed"},
                schema_info=self.schema
            )

        # Execute
        try:
            result = self.db.execute(sql)
            return StructuredResult(
                source_type="sql",
                query=sql,
                result=result,
                schema_info=self.schema
            )
        except Exception as e:
            return StructuredResult(
                source_type="sql",
                query=sql,
                result={"error": str(e)},
                schema_info=self.schema
            )

    def _generate_sql(self, question: str) -> str:
        """Generate SQL from natural language"""

        schema_str = self._format_schema()

        prompt = f"""Generate a SQL query to answer this question.

        Database schema:
        {schema_str}

        Question: {question}

        Important:
        - Use only tables and columns from the schema
        - Use appropriate JOINs when needed
        - Add LIMIT clauses for safety
        - Return only the SQL query, no explanation

        SQL:
        """

        sql = self.llm.generate(prompt).strip()

        # Clean up
        if sql.startswith("```"):
            sql = sql.split("```")[1]
            if sql.startswith("sql"):
                sql = sql[3:]

        return sql.strip()

    def _format_schema(self) -> str:
        """Format schema for prompt"""
        lines = []
        for table, columns in self.schema.items():
            cols = ", ".join(
                f"{c['name']} ({c['type']})"
                for c in columns
            )
            lines.append(f"{table}: {cols}")
        return "\n".join(lines)

    def _validate_sql(self, sql: str) -> bool:
        """Basic SQL safety validation"""
        sql_lower = sql.lower()

        # Block dangerous operations
        dangerous_keywords = [
            "drop", "delete", "truncate", "insert",
            "update", "alter", "create", "grant"
        ]

        for keyword in dangerous_keywords:
            if keyword in sql_lower:
                return False

        return True


class KnowledgeGraphRetriever:
    """
    Retrieve from knowledge graphs using natural language
    """

    def __init__(
        self,
        llm_client,
        kg_client,  # Neo4j, etc.
        schema: Dict[str, Any]
    ):
        self.llm = llm_client
        self.kg = kg_client
        self.schema = schema

    def query(self, question: str) -> StructuredResult:
        """Query knowledge graph with natural language"""

        # Generate Cypher query (for Neo4j)
        cypher = self._generate_cypher(question)

        # Execute
        try:
            result = self.kg.run(cypher)
            return StructuredResult(
                source_type="kg",
                query=cypher,
                result=list(result),
                schema_info=self.schema
            )
        except Exception as e:
            return StructuredResult(
                source_type="kg",
                query=cypher,
                result={"error": str(e)},
                schema_info=self.schema
            )

    def _generate_cypher(self, question: str) -> str:
        """Generate Cypher query from natural language"""

        schema_str = self._format_kg_schema()

        prompt = f"""Generate a Cypher query to answer this question.

        Knowledge Graph Schema:
        Node types: {schema_str['nodes']}
        Relationship types: {schema_str['relationships']}

        Question: {question}

        Cypher query:
        """

        return self.llm.generate(prompt).strip()

    def _format_kg_schema(self) -> Dict[str, str]:
        """Format KG schema for prompt"""
        return {
            'nodes': ", ".join(self.schema.get('nodes', [])),
            'relationships': ", ".join(self.schema.get('relationships', []))
        }


class HybridStructuredRAG:
    """
    RAG combining unstructured text and structured data
    """

    def __init__(
        self,
        text_retriever,
        sql_engine: TextToSQLEngine,
        kg_retriever: Optional[KnowledgeGraphRetriever],
        generator
    ):
        self.text_retriever = text_retriever
        self.sql_engine = sql_engine
        self.kg_retriever = kg_retriever
        self.generator = generator

    def query(
        self,
        question: str,
        use_structured: bool = True,
        top_k: int = 5
    ) -> Dict[str, Any]:
        """Query combining structured and unstructured sources"""

        # Analyze query to determine data sources
        sources_needed = self._analyze_query(question)

        results = {
            "text_results": [],
            "sql_results": None,
            "kg_results": None
        }

        # Text retrieval
        if "text" in sources_needed:
            text_docs = self.text_retriever.retrieve(question, top_k=top_k)
            results["text_results"] = text_docs

        # SQL retrieval
        if use_structured and "sql" in sources_needed:
            sql_result = self.sql_engine.query(question)
            results["sql_results"] = sql_result

        # Knowledge graph retrieval
        if use_structured and "kg" in sources_needed and self.kg_retriever:
            kg_result = self.kg_retriever.query(question)
            results["kg_results"] = kg_result

        # Generate answer
        answer = self._generate_hybrid_answer(question, results)

        return {
            "answer": answer,
            **results
        }

    def _analyze_query(self, question: str) -> List[str]:
        """Analyze which data sources are needed"""

        prompt = f"""Analyze which data sources would help answer this question.

        Question: {question}

        Available sources:
        - text: Unstructured documents and articles
        - sql: Structured database with tables
        - kg: Knowledge graph with entities and relationships

        Return JSON list of needed sources, e.g. ["text", "sql"]
        """

        result = self.generator.generate(prompt)
        try:
            return json.loads(result)
        except:
            return ["text"]  # Default to text

    def _generate_hybrid_answer(
        self,
        question: str,
        results: Dict[str, Any]
    ) -> str:
        """Generate answer from multiple source types"""

        context_parts = []

        # Format text results
        if results["text_results"]:
            text_context = "\n".join(
                f"- {doc.get('content', '')[:300]}"
                for doc in results["text_results"][:3]
            )
            context_parts.append(f"Text sources:\n{text_context}")

        # Format SQL results
        if results["sql_results"]:
            sql_result = results["sql_results"]
            if isinstance(sql_result.result, dict) and "error" in sql_result.result:
                context_parts.append(
                    f"SQL query failed: {sql_result.result['error']}"
                )
            else:
                context_parts.append(
                    f"SQL query: {sql_result.query}\n"
                    f"Result: {json.dumps(sql_result.result[:10], indent=2)}"
                )

        # Format KG results
        if results["kg_results"]:
            kg_result = results["kg_results"]
            if isinstance(kg_result.result, dict) and "error" in kg_result.result:
                context_parts.append(
                    f"Knowledge graph query failed: {kg_result.result['error']}"
                )
            else:
                context_parts.append(
                    f"Knowledge graph query: {kg_result.query}\n"
                    f"Result: {json.dumps(kg_result.result[:10], indent=2)}"
                )

        prompt = f"""Answer the question using all available information.

        {chr(10).join(context_parts)}

        Question: {question}

        Provide a comprehensive answer that synthesizes information from
        all sources. Indicate which sources support which claims.

        Answer:
        """

        return self.generator.generate(prompt)
```

---

## 8. Production Patterns

### 8.1 Caching Strategies

```python
"""
Production caching for RAG systems
"""
from typing import Dict, Any, Optional, List
from dataclasses import dataclass
import hashlib
import json
import time
from functools import lru_cache


@dataclass
class CacheEntry:
    """Cache entry with TTL and metadata"""
    key: str
    value: Any
    created_at: float
    ttl: int
    hit_count: int = 0
    metadata: Dict[str, Any] = None


class RAGCache:
    """
    Multi-layer caching for RAG pipelines
    """

    def __init__(
        self,
        query_cache_size: int = 1000,
        embedding_cache_size: int = 10000,
        result_cache_ttl: int = 3600,
        redis_client=None
    ):
        self.query_cache_size = query_cache_size
        self.embedding_cache_size = embedding_cache_size
        self.result_cache_ttl = result_cache_ttl
        self.redis = redis_client

        # Local caches
        self._query_cache: Dict[str, CacheEntry] = {}
        self._embedding_cache: Dict[str, CacheEntry] = {}

    def cache_key(self, query: str, params: Optional[Dict] = None) -> str:
        """Generate cache key from query and parameters"""
        key_data = {"query": query, "params": params or {}}
        key_str = json.dumps(key_data, sort_keys=True)
        return hashlib.sha256(key_str.encode()).hexdigest()[:16]

    # Query Result Cache
    def get_query_result(self, key: str) -> Optional[Dict[str, Any]]:
        """Get cached query result"""
        # Try local cache first
        if key in self._query_cache:
            entry = self._query_cache[key]
            if time.time() - entry.created_at < entry.ttl:
                entry.hit_count += 1
                return entry.value
            else:
                del self._query_cache[key]

        # Try Redis
        if self.redis:
            cached = self.redis.get(f"rag:query:{key}")
            if cached:
                return json.loads(cached)

        return None

    def set_query_result(
        self,
        key: str,
        result: Dict[str, Any],
        ttl: Optional[int] = None
    ):
        """Cache query result"""
        ttl = ttl or self.result_cache_ttl

        # Local cache
        self._query_cache[key] = CacheEntry(
            key=key,
            value=result,
            created_at=time.time(),
            ttl=ttl
        )

        # Evict if too large
        if len(self._query_cache) > self.query_cache_size:
            self._evict_lru(self._query_cache)

        # Redis cache
        if self.redis:
            self.redis.setex(
                f"rag:query:{key}",
                ttl,
                json.dumps(result)
            )

    # Embedding Cache
    def get_embedding(self, text: str) -> Optional[List[float]]:
        """Get cached embedding"""
        key = hashlib.sha256(text.encode()).hexdigest()[:16]

        if key in self._embedding_cache:
            entry = self._embedding_cache[key]
            entry.hit_count += 1
            return entry.value

        if self.redis:
            cached = self.redis.get(f"rag:embed:{key}")
            if cached:
                return json.loads(cached)

        return None

    def set_embedding(self, text: str, embedding: List[float]):
        """Cache embedding"""
        key = hashlib.sha256(text.encode()).hexdigest()[:16]

        self._embedding_cache[key] = CacheEntry(
            key=key,
            value=embedding,
            created_at=time.time(),
            ttl=86400  # 24 hours
        )

        if len(self._embedding_cache) > self.embedding_cache_size:
            self._evict_lru(self._embedding_cache)

        if self.redis:
            self.redis.setex(
                f"rag:embed:{key}",
                86400,
                json.dumps(embedding)
            )

    # Semantic Cache
    def get_similar_query(
        self,
        query: str,
        embedding: List[float],
        threshold: float = 0.95
    ) -> Optional[Dict[str, Any]]:
        """Find semantically similar cached query"""
        # This would use vector similarity search
        # Simplified implementation
        pass

    def _evict_lru(self, cache: Dict[str, CacheEntry]):
        """Evict least recently used entries"""
        # Sort by hit count and created time
        sorted_keys = sorted(
            cache.keys(),
            key=lambda k: (cache[k].hit_count, cache[k].created_at)
        )

        # Remove bottom 10%
        to_remove = len(sorted_keys) // 10
        for key in sorted_keys[:to_remove]:
            del cache[key]

    def get_stats(self) -> Dict[str, Any]:
        """Get cache statistics"""
        return {
            "query_cache_size": len(self._query_cache),
            "embedding_cache_size": len(self._embedding_cache),
            "query_cache_hits": sum(
                e.hit_count for e in self._query_cache.values()
            ),
            "embedding_cache_hits": sum(
                e.hit_count for e in self._embedding_cache.values()
            )
        }


class CachedRAGPipeline:
    """RAG pipeline with integrated caching"""

    def __init__(
        self,
        retriever,
        generator,
        embedder,
        cache: RAGCache
    ):
        self.retriever = retriever
        self.generator = generator
        self.embedder = embedder
        self.cache = cache

    def query(
        self,
        question: str,
        use_cache: bool = True,
        top_k: int = 5
    ) -> Dict[str, Any]:
        """Execute query with caching"""

        cache_key = self.cache.cache_key(question, {"top_k": top_k})

        # Check query cache
        if use_cache:
            cached_result = self.cache.get_query_result(cache_key)
            if cached_result:
                cached_result["cache_hit"] = True
                return cached_result

        # Get or compute embedding
        embedding = self.cache.get_embedding(question)
        if embedding is None:
            embedding = self.embedder.embed(question)
            self.cache.set_embedding(question, embedding)

        # Retrieve
        documents = self.retriever.retrieve_by_embedding(
            embedding, top_k=top_k
        )

        # Generate
        answer = self.generator.generate(
            question=question,
            context=self._format_context(documents)
        )

        result = {
            "answer": answer,
            "documents": documents,
            "cache_hit": False
        }

        # Cache result
        if use_cache:
            self.cache.set_query_result(cache_key, result)

        return result

    def _format_context(self, documents: List[Dict]) -> str:
        """Format documents as context"""
        return "\n\n".join(
            f"[{i+1}] {doc['content']}"
            for i, doc in enumerate(documents)
        )
```

### 8.2 Failure Handling and Graceful Degradation

```python
"""
Failure handling and graceful degradation for production RAG
"""
from typing import Dict, Any, Optional, List, Callable
from dataclasses import dataclass
from enum import Enum
import time
import logging

logger = logging.getLogger(__name__)


class FailureType(Enum):
    RETRIEVAL_EMPTY = "retrieval_empty"
    RETRIEVAL_TIMEOUT = "retrieval_timeout"
    GENERATION_FAILED = "generation_failed"
    CONTEXT_TOO_LONG = "context_too_long"
    LOW_CONFIDENCE = "low_confidence"


@dataclass
class FallbackStrategy:
    """Fallback strategy for failure handling"""
    failure_type: FailureType
    primary_action: Callable
    fallback_action: Callable
    max_retries: int = 2


class ResilientRAG:
    """
    Production RAG with comprehensive failure handling
    """

    def __init__(
        self,
        primary_retriever,
        fallback_retriever,
        generator,
        web_search=None,
        max_retries: int = 2,
        timeout: float = 30.0
    ):
        self.primary_retriever = primary_retriever
        self.fallback_retriever = fallback_retriever
        self.generator = generator
        self.web_search = web_search
        self.max_retries = max_retries
        self.timeout = timeout

    def query(
        self,
        question: str,
        top_k: int = 5
    ) -> Dict[str, Any]:
        """Execute query with resilient error handling"""

        result = {
            "answer": None,
            "documents": [],
            "fallbacks_used": [],
            "errors": []
        }

        # Step 1: Try primary retrieval
        documents = self._resilient_retrieve(question, top_k, result)

        if not documents:
            # Step 2: Handle empty retrieval
            documents = self._handle_empty_retrieval(question, result)

        result["documents"] = documents

        # Step 3: Generate answer
        answer = self._resilient_generate(question, documents, result)
        result["answer"] = answer

        # Step 4: Validate answer quality
        if not self._validate_answer(answer, documents):
            result["warnings"] = ["Answer may have low confidence"]

        return result

    def _resilient_retrieve(
        self,
        question: str,
        top_k: int,
        result: Dict
    ) -> List[Dict]:
        """Retrieve with retry and fallback logic"""

        for attempt in range(self.max_retries):
            try:
                # Try primary retriever
                start_time = time.time()
                documents = self.primary_retriever.retrieve(
                    question, top_k=top_k
                )

                if time.time() - start_time > self.timeout:
                    raise TimeoutError("Retrieval timeout")

                if documents:
                    return documents

            except Exception as e:
                logger.warning(f"Primary retrieval attempt {attempt + 1} failed: {e}")
                result["errors"].append({
                    "type": "retrieval",
                    "attempt": attempt + 1,
                    "error": str(e)
                })

        # Try fallback retriever
        try:
            result["fallbacks_used"].append("fallback_retriever")
            return self.fallback_retriever.retrieve(question, top_k=top_k)
        except Exception as e:
            logger.error(f"Fallback retrieval failed: {e}")
            result["errors"].append({
                "type": "fallback_retrieval",
                "error": str(e)
            })
            return []

    def _handle_empty_retrieval(
        self,
        question: str,
        result: Dict
    ) -> List[Dict]:
        """Handle case when no documents retrieved"""

        fallback_docs = []

        # Try web search if available
        if self.web_search:
            try:
                result["fallbacks_used"].append("web_search")
                web_results = self.web_search.search(question)
                fallback_docs.extend([
                    {
                        "content": r.get("snippet", ""),
                        "source": "web",
                        "url": r.get("url", "")
                    }
                    for r in web_results[:3]
                ])
            except Exception as e:
                logger.warning(f"Web search fallback failed: {e}")

        return fallback_docs

    def _resilient_generate(
        self,
        question: str,
        documents: List[Dict],
        result: Dict
    ) -> str:
        """Generate answer with error handling"""

        if not documents:
            # Generate with caveat about lack of sources
            return self._generate_no_context_answer(question)

        context = self._prepare_context(documents)

        for attempt in range(self.max_retries):
            try:
                answer = self.generator.generate(
                    question=question,
                    context=context
                )
                return answer
            except Exception as e:
                logger.warning(f"Generation attempt {attempt + 1} failed: {e}")
                result["errors"].append({
                    "type": "generation",
                    "attempt": attempt + 1,
                    "error": str(e)
                })

        # All retries failed
        result["fallbacks_used"].append("default_response")
        return self._generate_fallback_response(question)

    def _prepare_context(self, documents: List[Dict]) -> str:
        """Prepare context with length management"""
        max_context_tokens = 4000

        context_parts = []
        current_length = 0

        for doc in documents:
            content = doc.get("content", "")
            # Rough token estimate
            doc_tokens = len(content.split())

            if current_length + doc_tokens > max_context_tokens:
                # Truncate if needed
                remaining = max_context_tokens - current_length
                content = " ".join(content.split()[:remaining])

            context_parts.append(content)
            current_length += len(content.split())

            if current_length >= max_context_tokens:
                break

        return "\n\n".join(context_parts)

    def _generate_no_context_answer(self, question: str) -> str:
        """Generate answer when no context available"""
        prompt = f"""The user asked a question, but no relevant
        documents were found in the knowledge base.

        Acknowledge this limitation and provide what help you can.
        If the question requires specific information, suggest
        how the user might find it.

        Question: {question}
        """
        return self.generator.generate(prompt=prompt)

    def _generate_fallback_response(self, question: str) -> str:
        """Generate fallback response when generation fails"""
        return (
            "I apologize, but I'm having trouble generating a response "
            "to your question right now. Please try again in a moment, "
            "or rephrase your question."
        )

    def _validate_answer(
        self,
        answer: str,
        documents: List[Dict]
    ) -> bool:
        """Validate answer quality"""
        if not answer or len(answer) < 10:
            return False

        if not documents:
            return True  # No documents to validate against

        # Check if answer has some overlap with documents
        answer_words = set(answer.lower().split())
        doc_words = set()
        for doc in documents:
            doc_words.update(doc.get("content", "").lower().split())

        overlap = len(answer_words & doc_words) / len(answer_words)
        return overlap > 0.1  # At least 10% overlap
```

---

## 9. Troubleshooting Guide

### Common Issues and Solutions

| Issue | Symptoms | Solution |
|-------|----------|----------|
| Poor retrieval | Irrelevant docs, low recall | Improve embedding model, add hybrid retrieval |
| Context overflow | Long docs, key info lost | Implement chunking, context compression |
| Hallucination | Claims not in docs | Add self-RAG, citation enforcement |
| Multi-hop failure | Complex Q not answered | Use agentic RAG, query decomposition |
| Slow response | High latency | Add caching, optimize pipeline |
| Conversation drift | Context lost | Improve memory management |

### Debugging Checklist

```python
"""
RAG debugging and diagnostic utilities
"""

class RAGDiagnostics:
    """Diagnostic tools for RAG pipelines"""

    def diagnose_retrieval(
        self,
        query: str,
        retrieved_docs: List[Dict],
        expected_docs: Optional[List[Dict]] = None
    ) -> Dict[str, Any]:
        """Diagnose retrieval quality issues"""

        diagnostics = {
            "query_analysis": self._analyze_query(query),
            "doc_count": len(retrieved_docs),
            "avg_score": sum(d.get('score', 0) for d in retrieved_docs) / len(retrieved_docs) if retrieved_docs else 0,
            "score_distribution": [d.get('score', 0) for d in retrieved_docs],
            "issues": []
        }

        # Check for issues
        if not retrieved_docs:
            diagnostics["issues"].append("NO_RESULTS")
        elif diagnostics["avg_score"] < 0.5:
            diagnostics["issues"].append("LOW_RELEVANCE")

        if expected_docs:
            recall = self._calculate_recall(retrieved_docs, expected_docs)
            diagnostics["recall"] = recall
            if recall < 0.5:
                diagnostics["issues"].append("LOW_RECALL")

        return diagnostics

    def diagnose_generation(
        self,
        question: str,
        context: str,
        answer: str
    ) -> Dict[str, Any]:
        """Diagnose generation quality issues"""

        return {
            "answer_length": len(answer),
            "context_utilization": self._measure_context_use(context, answer),
            "potential_hallucination": self._detect_hallucination(context, answer),
            "issues": []
        }

    def _analyze_query(self, query: str) -> Dict[str, Any]:
        """Analyze query characteristics"""
        return {
            "length": len(query.split()),
            "has_entities": bool(self._extract_entities(query)),
            "complexity": "complex" if len(query.split()) > 20 else "simple"
        }

    def _calculate_recall(
        self,
        retrieved: List[Dict],
        expected: List[Dict]
    ) -> float:
        """Calculate recall against expected documents"""
        if not expected:
            return 1.0

        retrieved_ids = {d.get('id') for d in retrieved}
        expected_ids = {d.get('id') for d in expected}

        return len(retrieved_ids & expected_ids) / len(expected_ids)

    def _measure_context_use(self, context: str, answer: str) -> float:
        """Measure how much of context is reflected in answer"""
        context_words = set(context.lower().split())
        answer_words = set(answer.lower().split())

        if not answer_words:
            return 0.0

        return len(context_words & answer_words) / len(answer_words)

    def _detect_hallucination(self, context: str, answer: str) -> bool:
        """Simple hallucination detection"""
        # Check for claims not in context
        # This is a simplified heuristic
        context_lower = context.lower()

        # Look for specific patterns that might indicate hallucination
        hallucination_indicators = [
            "studies show",
            "research indicates",
            "according to",
            "it is known that"
        ]

        for indicator in hallucination_indicators:
            if indicator in answer.lower():
                # Check if supporting text is in context
                if indicator not in context_lower:
                    return True

        return False

    def _extract_entities(self, text: str) -> List[str]:
        """Extract entities from text (simplified)"""
        # In production, use proper NER
        import re
        # Find capitalized words that might be entities
        return re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', text)
```

---

## 10. References and Resources

### Papers
- "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks" (2024)
- "ColPali: Efficient Document Retrieval with Vision Language Models" (2024)
- "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection"
- "Corrective Retrieval Augmented Generation" (CRAG)

### Frameworks
- LlamaIndex Agentic RAG: https://docs.llamaindex.ai/en/stable/optimizing/agentic_strategies/
- LangGraph for Agentic Workflows: https://langchain-ai.github.io/langgraph/
- ColPali Implementation: https://github.com/illuin-tech/colpali

### Courses
- DeepLearning.AI: Building Agentic RAG with LlamaIndex
- LlamaIndex Documentation: Agentic Strategies

---

## Document Metadata

- **Version**: 1.0
- **Last Updated**: 2024
- **Prerequisites**: Documents 7.1-7.5
- **Estimated Reading Time**: 60 minutes
- **Hands-on Lab Time**: 4-6 hours
