> **Navigation** | [← 13.2 Disaster Recovery](13.2_disaster_recovery_guide.md) | [13.4 On-Call Practices →](13.4_on_call_practices_guide.md)
>
> | | |
> |---|---|
> | **Prerequisites** | Cloud infrastructure &#124; Monitoring/metrics &#124; LLM operations |
> | **Related** | [10.4 Cost Monitoring](../10_monitoring_observability/10.4_cost_monitoring_optimization_guide.md) &#124; [14.1 TCO](../14_cost_capacity_management/14.1_total_cost_ownership_guide.md) |
> | **Next** | [13.4 On-Call Practices](13.4_on_call_practices_guide.md) |

# 13.3 Capacity Planning Guide

## Document Information
- **Version**: 1.0
- **Last Updated**: 2024
- **Owner**: Site Reliability Engineering Team
- **Classification**: Internal

## Purpose and Scope

This guide provides comprehensive frameworks for capacity planning, forecasting, and resource management for LLM platforms. Effective capacity planning ensures system reliability, cost efficiency, and ability to scale with demand.

## Prerequisites

- Understanding of cloud infrastructure and resource types
- Familiarity with monitoring and metrics systems
- Knowledge of LLM operational characteristics
- Access to usage analytics and billing data

---

## 1. Capacity Planning Fundamentals

### 1.1 Resource Types and Metrics

```python
"""
Resource type definitions and capacity metrics.
"""

from enum import Enum
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta


class ResourceType(Enum):
    """Types of resources to plan for."""
    COMPUTE = "compute"
    MEMORY = "memory"
    STORAGE = "storage"
    NETWORK = "network"
    GPU = "gpu"
    API_RATE_LIMIT = "api_rate_limit"
    DATABASE_CONNECTIONS = "database_connections"
    CACHE_MEMORY = "cache_memory"
    QUEUE_DEPTH = "queue_depth"


class CapacityUnit(Enum):
    """Units for capacity measurement."""
    VCPU = "vCPU"
    GB_RAM = "GB RAM"
    GB_STORAGE = "GB Storage"
    MBPS = "Mbps"
    GPU_UNITS = "GPU Units"
    REQUESTS_PER_SECOND = "req/s"
    CONNECTIONS = "connections"
    TOKENS_PER_MINUTE = "tokens/min"
    MESSAGES = "messages"


@dataclass
class ResourceCapacity:
    """Current capacity for a resource type."""
    resource_type: ResourceType
    unit: CapacityUnit
    current_capacity: float
    current_usage: float
    reserved_capacity: float = 0
    max_capacity: float = 0
    usage_percentage: float = 0
    headroom_percentage: float = 0
    last_updated: datetime = field(default_factory=datetime.utcnow)

    def __post_init__(self):
        if self.current_capacity > 0:
            self.usage_percentage = (self.current_usage / self.current_capacity) * 100
            self.headroom_percentage = 100 - self.usage_percentage


@dataclass
class CapacityThreshold:
    """Threshold configuration for capacity alerts."""
    resource_type: ResourceType
    warning_threshold: float  # percentage
    critical_threshold: float  # percentage
    scale_trigger_threshold: float  # percentage
    scale_down_threshold: float  # percentage


class CapacityMetricsCollector:
    """
    Collect and manage capacity metrics.
    """

    def __init__(self):
        self.current_capacity: Dict[str, ResourceCapacity] = {}
        self.thresholds: Dict[ResourceType, CapacityThreshold] = {}
        self.history: Dict[str, List[Dict]] = {}
        self._initialize_thresholds()

    def _initialize_thresholds(self) -> None:
        """Initialize default capacity thresholds."""
        default_thresholds = [
            CapacityThreshold(
                resource_type=ResourceType.COMPUTE,
                warning_threshold=70,
                critical_threshold=85,
                scale_trigger_threshold=75,
                scale_down_threshold=30
            ),
            CapacityThreshold(
                resource_type=ResourceType.MEMORY,
                warning_threshold=75,
                critical_threshold=90,
                scale_trigger_threshold=80,
                scale_down_threshold=40
            ),
            CapacityThreshold(
                resource_type=ResourceType.GPU,
                warning_threshold=80,
                critical_threshold=95,
                scale_trigger_threshold=85,
                scale_down_threshold=50
            ),
            CapacityThreshold(
                resource_type=ResourceType.DATABASE_CONNECTIONS,
                warning_threshold=70,
                critical_threshold=85,
                scale_trigger_threshold=75,
                scale_down_threshold=30
            ),
            CapacityThreshold(
                resource_type=ResourceType.API_RATE_LIMIT,
                warning_threshold=80,
                critical_threshold=95,
                scale_trigger_threshold=85,
                scale_down_threshold=50
            ),
            CapacityThreshold(
                resource_type=ResourceType.STORAGE,
                warning_threshold=75,
                critical_threshold=90,
                scale_trigger_threshold=80,
                scale_down_threshold=50
            )
        ]

        for threshold in default_thresholds:
            self.thresholds[threshold.resource_type] = threshold

    def record_capacity(
        self,
        service: str,
        resource_type: ResourceType,
        unit: CapacityUnit,
        current_capacity: float,
        current_usage: float,
        max_capacity: float = 0
    ) -> ResourceCapacity:
        """Record current capacity metrics."""
        key = f"{service}:{resource_type.value}"

        capacity = ResourceCapacity(
            resource_type=resource_type,
            unit=unit,
            current_capacity=current_capacity,
            current_usage=current_usage,
            max_capacity=max_capacity
        )

        self.current_capacity[key] = capacity

        # Store history
        if key not in self.history:
            self.history[key] = []

        self.history[key].append({
            "timestamp": datetime.utcnow().isoformat(),
            "capacity": current_capacity,
            "usage": current_usage,
            "percentage": capacity.usage_percentage
        })

        # Keep last 30 days of hourly data
        cutoff = datetime.utcnow() - timedelta(days=30)
        self.history[key] = [
            h for h in self.history[key]
            if datetime.fromisoformat(h["timestamp"]) > cutoff
        ]

        return capacity

    def get_capacity_status(self, service: str) -> Dict[str, Any]:
        """Get capacity status for a service."""
        status = {}

        for key, capacity in self.current_capacity.items():
            if key.startswith(f"{service}:"):
                resource_name = key.split(":")[1]
                threshold = self.thresholds.get(capacity.resource_type)

                alert_level = "normal"
                if threshold:
                    if capacity.usage_percentage >= threshold.critical_threshold:
                        alert_level = "critical"
                    elif capacity.usage_percentage >= threshold.warning_threshold:
                        alert_level = "warning"

                status[resource_name] = {
                    "capacity": capacity.current_capacity,
                    "usage": capacity.current_usage,
                    "percentage": round(capacity.usage_percentage, 1),
                    "headroom": round(capacity.headroom_percentage, 1),
                    "unit": capacity.unit.value,
                    "alert_level": alert_level
                }

        return status

    def get_capacity_alerts(self) -> List[Dict[str, Any]]:
        """Get all capacity alerts."""
        alerts = []

        for key, capacity in self.current_capacity.items():
            threshold = self.thresholds.get(capacity.resource_type)
            if not threshold:
                continue

            service, resource = key.split(":")

            if capacity.usage_percentage >= threshold.critical_threshold:
                alerts.append({
                    "service": service,
                    "resource": resource,
                    "level": "critical",
                    "usage_percentage": round(capacity.usage_percentage, 1),
                    "threshold": threshold.critical_threshold,
                    "message": f"{resource} at {capacity.usage_percentage:.1f}% capacity"
                })
            elif capacity.usage_percentage >= threshold.warning_threshold:
                alerts.append({
                    "service": service,
                    "resource": resource,
                    "level": "warning",
                    "usage_percentage": round(capacity.usage_percentage, 1),
                    "threshold": threshold.warning_threshold,
                    "message": f"{resource} at {capacity.usage_percentage:.1f}% capacity"
                })

        return sorted(alerts, key=lambda x: x["usage_percentage"], reverse=True)

    def get_usage_trend(
        self,
        service: str,
        resource_type: ResourceType,
        period_days: int = 7
    ) -> Dict[str, Any]:
        """Get usage trend for a resource."""
        key = f"{service}:{resource_type.value}"
        history = self.history.get(key, [])

        if not history:
            return {"no_data": True}

        cutoff = datetime.utcnow() - timedelta(days=period_days)
        recent = [
            h for h in history
            if datetime.fromisoformat(h["timestamp"]) > cutoff
        ]

        if not recent:
            return {"no_data": True}

        usages = [h["percentage"] for h in recent]

        return {
            "min": min(usages),
            "max": max(usages),
            "average": sum(usages) / len(usages),
            "current": usages[-1] if usages else 0,
            "trend": "increasing" if usages[-1] > usages[0] else "decreasing" if usages[-1] < usages[0] else "stable",
            "data_points": len(recent)
        }
```

### 1.2 Demand Forecasting

```python
"""
Demand forecasting for capacity planning.
"""

from dataclasses import dataclass, field
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
import math


@dataclass
class ForecastDataPoint:
    """A data point for forecasting."""
    timestamp: datetime
    value: float
    is_forecast: bool = False
    confidence_lower: Optional[float] = None
    confidence_upper: Optional[float] = None


@dataclass
class DemandForecast:
    """Demand forecast result."""
    metric: str
    service: str
    generated_at: datetime
    forecast_horizon_days: int
    data_points: List[ForecastDataPoint]
    peak_forecast: float
    average_forecast: float
    growth_rate_daily: float
    confidence_level: float
    model_used: str


class DemandForecaster:
    """
    Forecast demand for capacity planning.
    """

    def __init__(self):
        self.historical_data: Dict[str, List[Dict]] = {}

    def add_historical_data(
        self,
        metric: str,
        timestamp: datetime,
        value: float
    ) -> None:
        """Add historical data point for forecasting."""
        if metric not in self.historical_data:
            self.historical_data[metric] = []

        self.historical_data[metric].append({
            "timestamp": timestamp,
            "value": value
        })

        # Keep sorted
        self.historical_data[metric].sort(key=lambda x: x["timestamp"])

    def forecast(
        self,
        metric: str,
        service: str,
        horizon_days: int = 30,
        confidence_level: float = 0.95
    ) -> DemandForecast:
        """Generate demand forecast."""
        history = self.historical_data.get(metric, [])

        if len(history) < 7:
            raise ValueError(f"Insufficient data for forecasting: {len(history)} points")

        # Extract values
        values = [h["value"] for h in history]
        timestamps = [h["timestamp"] for h in history]

        # Calculate trend using linear regression
        slope, intercept = self._linear_regression(values)

        # Calculate seasonality (weekly pattern)
        seasonality = self._calculate_seasonality(history)

        # Generate forecast
        data_points = []

        # Add historical points
        for h in history[-30:]:  # Last 30 days of history
            data_points.append(ForecastDataPoint(
                timestamp=h["timestamp"],
                value=h["value"],
                is_forecast=False
            ))

        # Generate forecast points
        last_timestamp = timestamps[-1]
        last_index = len(values)

        for day in range(1, horizon_days + 1):
            forecast_timestamp = last_timestamp + timedelta(days=day)
            forecast_index = last_index + day

            # Base forecast from trend
            base_forecast = intercept + slope * forecast_index

            # Apply seasonality
            day_of_week = forecast_timestamp.weekday()
            seasonal_factor = seasonality.get(day_of_week, 1.0)
            forecast_value = base_forecast * seasonal_factor

            # Calculate confidence interval
            std_error = self._calculate_std_error(values, slope, intercept)
            z_score = 1.96  # 95% confidence
            margin = z_score * std_error * math.sqrt(1 + 1/len(values) + (forecast_index - len(values)/2)**2 / sum((i - len(values)/2)**2 for i in range(len(values))))

            data_points.append(ForecastDataPoint(
                timestamp=forecast_timestamp,
                value=max(0, forecast_value),
                is_forecast=True,
                confidence_lower=max(0, forecast_value - margin),
                confidence_upper=forecast_value + margin
            ))

        # Calculate summary metrics
        forecast_values = [dp.value for dp in data_points if dp.is_forecast]

        return DemandForecast(
            metric=metric,
            service=service,
            generated_at=datetime.utcnow(),
            forecast_horizon_days=horizon_days,
            data_points=data_points,
            peak_forecast=max(forecast_values),
            average_forecast=sum(forecast_values) / len(forecast_values),
            growth_rate_daily=slope / (sum(values) / len(values)) * 100,
            confidence_level=confidence_level,
            model_used="linear_regression_with_seasonality"
        )

    def _linear_regression(
        self,
        values: List[float]
    ) -> Tuple[float, float]:
        """Simple linear regression."""
        n = len(values)
        x = list(range(n))

        sum_x = sum(x)
        sum_y = sum(values)
        sum_xy = sum(x[i] * values[i] for i in range(n))
        sum_x2 = sum(xi ** 2 for xi in x)

        slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x ** 2)
        intercept = (sum_y - slope * sum_x) / n

        return slope, intercept

    def _calculate_seasonality(
        self,
        history: List[Dict]
    ) -> Dict[int, float]:
        """Calculate weekly seasonality factors."""
        by_day = {i: [] for i in range(7)}

        for h in history:
            day_of_week = h["timestamp"].weekday()
            by_day[day_of_week].append(h["value"])

        overall_avg = sum(h["value"] for h in history) / len(history)

        seasonality = {}
        for day, values in by_day.items():
            if values:
                day_avg = sum(values) / len(values)
                seasonality[day] = day_avg / overall_avg if overall_avg > 0 else 1.0
            else:
                seasonality[day] = 1.0

        return seasonality

    def _calculate_std_error(
        self,
        values: List[float],
        slope: float,
        intercept: float
    ) -> float:
        """Calculate standard error of regression."""
        n = len(values)
        residuals = [
            values[i] - (intercept + slope * i)
            for i in range(n)
        ]
        ss_res = sum(r ** 2 for r in residuals)
        return math.sqrt(ss_res / (n - 2)) if n > 2 else 0

    def forecast_capacity_needs(
        self,
        service: str,
        current_capacity: float,
        usage_metric: str,
        target_utilization: float = 70,
        horizon_days: int = 90
    ) -> Dict[str, Any]:
        """Forecast capacity needs based on demand forecast."""
        forecast = self.forecast(usage_metric, service, horizon_days)

        # Calculate when capacity threshold will be reached
        capacity_threshold = current_capacity * (target_utilization / 100)

        crossing_day = None
        for dp in forecast.data_points:
            if dp.is_forecast and dp.value >= capacity_threshold:
                crossing_day = (dp.timestamp - datetime.utcnow()).days
                break

        # Calculate required capacity at end of horizon
        end_demand = forecast.data_points[-1].value
        required_capacity = end_demand / (target_utilization / 100)

        return {
            "current_capacity": current_capacity,
            "current_utilization": (forecast.data_points[0].value / current_capacity) * 100,
            "forecast_peak_demand": forecast.peak_forecast,
            "forecast_average_demand": forecast.average_forecast,
            "days_until_threshold": crossing_day,
            "required_capacity_at_horizon": required_capacity,
            "capacity_increase_needed": max(0, required_capacity - current_capacity),
            "growth_rate_daily_percent": forecast.growth_rate_daily,
            "recommendation": self._generate_recommendation(
                current_capacity,
                forecast.peak_forecast,
                crossing_day,
                target_utilization
            )
        }

    def _generate_recommendation(
        self,
        current_capacity: float,
        peak_forecast: float,
        days_until_threshold: Optional[int],
        target_utilization: float
    ) -> str:
        """Generate capacity recommendation."""
        if days_until_threshold is None:
            return "Current capacity is sufficient for the forecast period"

        if days_until_threshold <= 7:
            return f"URGENT: Scale immediately. Capacity threshold will be reached in {days_until_threshold} days"

        if days_until_threshold <= 30:
            return f"Plan scaling within 2 weeks. Capacity threshold in {days_until_threshold} days"

        if days_until_threshold <= 60:
            return f"Schedule scaling for next month. Capacity threshold in {days_until_threshold} days"

        return f"Monitor growth. Capacity threshold in {days_until_threshold} days"
```

---

## 2. Service-Specific Capacity Planning

### 2.1 LLM API Capacity Planning

```python
"""
LLM API specific capacity planning.
"""

from dataclasses import dataclass, field
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from enum import Enum


class TokenType(Enum):
    """Token types for LLM APIs."""
    INPUT = "input"
    OUTPUT = "output"
    TOTAL = "total"


@dataclass
class ModelCapacity:
    """Capacity for a specific model."""
    model_id: str
    provider: str
    tokens_per_minute_limit: int
    requests_per_minute_limit: int
    tokens_per_day_limit: int
    current_tpm_usage: int = 0
    current_rpm_usage: int = 0
    current_daily_usage: int = 0


@dataclass
class ProviderCapacity:
    """Capacity allocation for a provider."""
    provider: str
    total_tpm_limit: int
    total_rpm_limit: int
    total_daily_limit: int
    models: Dict[str, ModelCapacity] = field(default_factory=dict)
    cost_per_1k_input_tokens: float = 0
    cost_per_1k_output_tokens: float = 0


class LLMCapacityPlanner:
    """
    Plan and manage LLM API capacity.
    """

    def __init__(self):
        self.provider_capacity: Dict[str, ProviderCapacity] = {}
        self.usage_history: Dict[str, List[Dict]] = {}
        self._initialize_providers()

    def _initialize_providers(self) -> None:
        """Initialize provider capacity configurations."""
        self.provider_capacity = {
            "openai": ProviderCapacity(
                provider="openai",
                total_tpm_limit=10000000,  # 10M TPM
                total_rpm_limit=10000,
                total_daily_limit=100000000,  # 100M tokens/day
                models={
                    "gpt-4-turbo": ModelCapacity(
                        model_id="gpt-4-turbo",
                        provider="openai",
                        tokens_per_minute_limit=800000,
                        requests_per_minute_limit=500,
                        tokens_per_day_limit=10000000
                    ),
                    "gpt-4o": ModelCapacity(
                        model_id="gpt-4o",
                        provider="openai",
                        tokens_per_minute_limit=30000000,
                        requests_per_minute_limit=10000,
                        tokens_per_day_limit=None  # No daily limit
                    ),
                    "gpt-3.5-turbo": ModelCapacity(
                        model_id="gpt-3.5-turbo",
                        provider="openai",
                        tokens_per_minute_limit=2000000,
                        requests_per_minute_limit=10000,
                        tokens_per_day_limit=None
                    )
                },
                cost_per_1k_input_tokens=0.01,
                cost_per_1k_output_tokens=0.03
            ),
            "anthropic": ProviderCapacity(
                provider="anthropic",
                total_tpm_limit=4000000,
                total_rpm_limit=4000,
                total_daily_limit=50000000,
                models={
                    "claude-3-opus": ModelCapacity(
                        model_id="claude-3-opus",
                        provider="anthropic",
                        tokens_per_minute_limit=400000,
                        requests_per_minute_limit=400,
                        tokens_per_day_limit=10000000
                    ),
                    "claude-3-sonnet": ModelCapacity(
                        model_id="claude-3-sonnet",
                        provider="anthropic",
                        tokens_per_minute_limit=800000,
                        requests_per_minute_limit=800,
                        tokens_per_day_limit=20000000
                    ),
                    "claude-3-haiku": ModelCapacity(
                        model_id="claude-3-haiku",
                        provider="anthropic",
                        tokens_per_minute_limit=4000000,
                        requests_per_minute_limit=4000,
                        tokens_per_day_limit=None
                    )
                },
                cost_per_1k_input_tokens=0.015,
                cost_per_1k_output_tokens=0.075
            )
        }

    def record_usage(
        self,
        provider: str,
        model_id: str,
        input_tokens: int,
        output_tokens: int,
        request_count: int = 1
    ) -> None:
        """Record token usage."""
        key = f"{provider}:{model_id}"

        if key not in self.usage_history:
            self.usage_history[key] = []

        self.usage_history[key].append({
            "timestamp": datetime.utcnow(),
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "total_tokens": input_tokens + output_tokens,
            "requests": request_count
        })

        # Update current usage
        provider_cap = self.provider_capacity.get(provider)
        if provider_cap and model_id in provider_cap.models:
            model_cap = provider_cap.models[model_id]
            model_cap.current_tpm_usage += input_tokens + output_tokens
            model_cap.current_rpm_usage += request_count

    def get_utilization(
        self,
        provider: str,
        model_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """Get current utilization for provider/model."""
        provider_cap = self.provider_capacity.get(provider)
        if not provider_cap:
            return {"error": f"Provider {provider} not found"}

        if model_id:
            model_cap = provider_cap.models.get(model_id)
            if not model_cap:
                return {"error": f"Model {model_id} not found"}

            return {
                "model_id": model_id,
                "provider": provider,
                "tpm_usage": model_cap.current_tpm_usage,
                "tpm_limit": model_cap.tokens_per_minute_limit,
                "tpm_utilization": (model_cap.current_tpm_usage / model_cap.tokens_per_minute_limit) * 100,
                "rpm_usage": model_cap.current_rpm_usage,
                "rpm_limit": model_cap.requests_per_minute_limit,
                "rpm_utilization": (model_cap.current_rpm_usage / model_cap.requests_per_minute_limit) * 100
            }

        # Provider-level utilization
        total_tpm = sum(m.current_tpm_usage for m in provider_cap.models.values())
        total_rpm = sum(m.current_rpm_usage for m in provider_cap.models.values())

        return {
            "provider": provider,
            "total_tpm_usage": total_tpm,
            "total_tpm_limit": provider_cap.total_tpm_limit,
            "total_tpm_utilization": (total_tpm / provider_cap.total_tpm_limit) * 100,
            "total_rpm_usage": total_rpm,
            "total_rpm_limit": provider_cap.total_rpm_limit,
            "total_rpm_utilization": (total_rpm / provider_cap.total_rpm_limit) * 100,
            "models": {
                model_id: {
                    "tpm_utilization": (m.current_tpm_usage / m.tokens_per_minute_limit) * 100,
                    "rpm_utilization": (m.current_rpm_usage / m.requests_per_minute_limit) * 100
                }
                for model_id, m in provider_cap.models.items()
            }
        }

    def calculate_required_capacity(
        self,
        target_requests_per_minute: int,
        avg_input_tokens: int,
        avg_output_tokens: int,
        model_id: str,
        safety_margin: float = 1.5
    ) -> Dict[str, Any]:
        """Calculate required capacity for target load."""
        tokens_per_request = avg_input_tokens + avg_output_tokens
        target_tpm = target_requests_per_minute * tokens_per_request * safety_margin

        # Find provider with model
        for provider, provider_cap in self.provider_capacity.items():
            if model_id in provider_cap.models:
                model_cap = provider_cap.models[model_id]

                rpm_sufficient = target_requests_per_minute * safety_margin <= model_cap.requests_per_minute_limit
                tpm_sufficient = target_tpm <= model_cap.tokens_per_minute_limit

                if not rpm_sufficient or not tpm_sufficient:
                    # Calculate how many rate limit increases needed
                    rpm_increase = max(0, (target_requests_per_minute * safety_margin) - model_cap.requests_per_minute_limit)
                    tpm_increase = max(0, target_tpm - model_cap.tokens_per_minute_limit)

                    return {
                        "model_id": model_id,
                        "provider": provider,
                        "target_rpm": target_requests_per_minute,
                        "target_tpm": target_tpm,
                        "current_rpm_limit": model_cap.requests_per_minute_limit,
                        "current_tpm_limit": model_cap.tokens_per_minute_limit,
                        "sufficient": False,
                        "rpm_increase_needed": rpm_increase,
                        "tpm_increase_needed": tpm_increase,
                        "recommendation": self._generate_limit_recommendation(
                            rpm_increase, tpm_increase, provider
                        )
                    }

                return {
                    "model_id": model_id,
                    "provider": provider,
                    "target_rpm": target_requests_per_minute,
                    "target_tpm": target_tpm,
                    "current_rpm_limit": model_cap.requests_per_minute_limit,
                    "current_tpm_limit": model_cap.tokens_per_minute_limit,
                    "sufficient": True,
                    "headroom_rpm_percent": ((model_cap.requests_per_minute_limit - target_requests_per_minute * safety_margin) / model_cap.requests_per_minute_limit) * 100,
                    "headroom_tpm_percent": ((model_cap.tokens_per_minute_limit - target_tpm) / model_cap.tokens_per_minute_limit) * 100
                }

        return {"error": f"Model {model_id} not found in any provider"}

    def _generate_limit_recommendation(
        self,
        rpm_increase: float,
        tpm_increase: float,
        provider: str
    ) -> str:
        """Generate recommendation for rate limit increase."""
        recommendations = []

        if rpm_increase > 0:
            recommendations.append(f"Request RPM increase of {rpm_increase:.0f} from {provider}")

        if tpm_increase > 0:
            recommendations.append(f"Request TPM increase of {tpm_increase:.0f} from {provider}")

        recommendations.append("Consider multi-provider strategy to distribute load")
        recommendations.append("Implement request queuing to handle burst traffic")

        return ". ".join(recommendations)

    def estimate_costs(
        self,
        requests_per_day: int,
        avg_input_tokens: int,
        avg_output_tokens: int,
        provider: str
    ) -> Dict[str, Any]:
        """Estimate costs for given usage."""
        provider_cap = self.provider_capacity.get(provider)
        if not provider_cap:
            return {"error": f"Provider {provider} not found"}

        daily_input_tokens = requests_per_day * avg_input_tokens
        daily_output_tokens = requests_per_day * avg_output_tokens

        daily_cost = (
            (daily_input_tokens / 1000) * provider_cap.cost_per_1k_input_tokens +
            (daily_output_tokens / 1000) * provider_cap.cost_per_1k_output_tokens
        )

        return {
            "provider": provider,
            "requests_per_day": requests_per_day,
            "total_input_tokens": daily_input_tokens,
            "total_output_tokens": daily_output_tokens,
            "daily_cost": daily_cost,
            "monthly_cost": daily_cost * 30,
            "annual_cost": daily_cost * 365,
            "cost_per_request": daily_cost / requests_per_day,
            "input_token_cost": (daily_input_tokens / 1000) * provider_cap.cost_per_1k_input_tokens,
            "output_token_cost": (daily_output_tokens / 1000) * provider_cap.cost_per_1k_output_tokens
        }
```

### 2.2 Infrastructure Capacity Planning

```python
"""
Infrastructure capacity planning for LLM platforms.
"""

from dataclasses import dataclass, field
from datetime import datetime
from typing import Dict, List, Optional, Any
from enum import Enum


class InstanceType(Enum):
    """Cloud instance types."""
    COMPUTE_SMALL = "compute_small"
    COMPUTE_MEDIUM = "compute_medium"
    COMPUTE_LARGE = "compute_large"
    MEMORY_OPTIMIZED = "memory_optimized"
    GPU_STANDARD = "gpu_standard"
    GPU_HIGH_MEMORY = "gpu_high_memory"


@dataclass
class InstanceSpec:
    """Specification for an instance type."""
    instance_type: InstanceType
    vcpus: int
    memory_gb: float
    gpu_count: int = 0
    gpu_memory_gb: float = 0
    network_bandwidth_gbps: float = 0
    storage_iops: int = 0
    hourly_cost: float = 0


@dataclass
class ServiceResourceRequirements:
    """Resource requirements for a service."""
    service_name: str
    min_instances: int
    max_instances: int
    target_cpu_utilization: float
    target_memory_utilization: float
    requests_per_instance: int
    memory_per_request_mb: float
    cpu_per_request_millicores: float
    preferred_instance_type: InstanceType


class InfrastructureCapacityPlanner:
    """
    Plan infrastructure capacity for LLM platform services.
    """

    def __init__(self):
        self.instance_specs: Dict[InstanceType, InstanceSpec] = {}
        self.service_requirements: Dict[str, ServiceResourceRequirements] = {}
        self._initialize_specs()
        self._initialize_services()

    def _initialize_specs(self) -> None:
        """Initialize instance specifications."""
        self.instance_specs = {
            InstanceType.COMPUTE_SMALL: InstanceSpec(
                instance_type=InstanceType.COMPUTE_SMALL,
                vcpus=2,
                memory_gb=4,
                network_bandwidth_gbps=1,
                storage_iops=3000,
                hourly_cost=0.05
            ),
            InstanceType.COMPUTE_MEDIUM: InstanceSpec(
                instance_type=InstanceType.COMPUTE_MEDIUM,
                vcpus=4,
                memory_gb=8,
                network_bandwidth_gbps=2,
                storage_iops=6000,
                hourly_cost=0.10
            ),
            InstanceType.COMPUTE_LARGE: InstanceSpec(
                instance_type=InstanceType.COMPUTE_LARGE,
                vcpus=8,
                memory_gb=16,
                network_bandwidth_gbps=5,
                storage_iops=12000,
                hourly_cost=0.20
            ),
            InstanceType.MEMORY_OPTIMIZED: InstanceSpec(
                instance_type=InstanceType.MEMORY_OPTIMIZED,
                vcpus=4,
                memory_gb=32,
                network_bandwidth_gbps=5,
                storage_iops=12000,
                hourly_cost=0.25
            ),
            InstanceType.GPU_STANDARD: InstanceSpec(
                instance_type=InstanceType.GPU_STANDARD,
                vcpus=8,
                memory_gb=32,
                gpu_count=1,
                gpu_memory_gb=16,
                network_bandwidth_gbps=10,
                storage_iops=20000,
                hourly_cost=1.50
            ),
            InstanceType.GPU_HIGH_MEMORY: InstanceSpec(
                instance_type=InstanceType.GPU_HIGH_MEMORY,
                vcpus=16,
                memory_gb=64,
                gpu_count=4,
                gpu_memory_gb=80,
                network_bandwidth_gbps=25,
                storage_iops=50000,
                hourly_cost=6.00
            )
        }

    def _initialize_services(self) -> None:
        """Initialize service resource requirements."""
        self.service_requirements = {
            "api-gateway": ServiceResourceRequirements(
                service_name="api-gateway",
                min_instances=3,
                max_instances=50,
                target_cpu_utilization=70,
                target_memory_utilization=75,
                requests_per_instance=1000,
                memory_per_request_mb=10,
                cpu_per_request_millicores=50,
                preferred_instance_type=InstanceType.COMPUTE_MEDIUM
            ),
            "model-router": ServiceResourceRequirements(
                service_name="model-router",
                min_instances=3,
                max_instances=30,
                target_cpu_utilization=70,
                target_memory_utilization=70,
                requests_per_instance=500,
                memory_per_request_mb=20,
                cpu_per_request_millicores=100,
                preferred_instance_type=InstanceType.COMPUTE_MEDIUM
            ),
            "embedding-service": ServiceResourceRequirements(
                service_name="embedding-service",
                min_instances=2,
                max_instances=20,
                target_cpu_utilization=80,
                target_memory_utilization=80,
                requests_per_instance=100,
                memory_per_request_mb=100,
                cpu_per_request_millicores=200,
                preferred_instance_type=InstanceType.GPU_STANDARD
            ),
            "vector-database": ServiceResourceRequirements(
                service_name="vector-database",
                min_instances=3,
                max_instances=12,
                target_cpu_utilization=60,
                target_memory_utilization=70,
                requests_per_instance=2000,
                memory_per_request_mb=5,
                cpu_per_request_millicores=20,
                preferred_instance_type=InstanceType.MEMORY_OPTIMIZED
            ),
            "cache": ServiceResourceRequirements(
                service_name="cache",
                min_instances=3,
                max_instances=9,
                target_cpu_utilization=50,
                target_memory_utilization=80,
                requests_per_instance=10000,
                memory_per_request_mb=0.1,
                cpu_per_request_millicores=5,
                preferred_instance_type=InstanceType.MEMORY_OPTIMIZED
            )
        }

    def calculate_required_instances(
        self,
        service_name: str,
        target_requests_per_second: int
    ) -> Dict[str, Any]:
        """Calculate required instances for target load."""
        requirements = self.service_requirements.get(service_name)
        if not requirements:
            return {"error": f"Service {service_name} not found"}

        spec = self.instance_specs[requirements.preferred_instance_type]

        # Calculate based on request capacity
        instances_for_requests = math.ceil(
            target_requests_per_second / requirements.requests_per_instance
        )

        # Calculate based on CPU
        cpu_per_second = target_requests_per_second * requirements.cpu_per_request_millicores
        available_cpu_per_instance = spec.vcpus * 1000 * (requirements.target_cpu_utilization / 100)
        instances_for_cpu = math.ceil(cpu_per_second / available_cpu_per_instance)

        # Calculate based on memory
        memory_per_second = target_requests_per_second * requirements.memory_per_request_mb
        available_memory_per_instance = spec.memory_gb * 1024 * (requirements.target_memory_utilization / 100)
        instances_for_memory = math.ceil(memory_per_second / available_memory_per_instance)

        required_instances = max(
            instances_for_requests,
            instances_for_cpu,
            instances_for_memory,
            requirements.min_instances
        )

        required_instances = min(required_instances, requirements.max_instances)

        hourly_cost = required_instances * spec.hourly_cost
        monthly_cost = hourly_cost * 24 * 30

        return {
            "service": service_name,
            "target_rps": target_requests_per_second,
            "required_instances": required_instances,
            "instance_type": requirements.preferred_instance_type.value,
            "limiting_factor": self._get_limiting_factor(
                instances_for_requests,
                instances_for_cpu,
                instances_for_memory
            ),
            "utilization_at_target": {
                "cpu": (cpu_per_second / (required_instances * spec.vcpus * 1000)) * 100,
                "memory": (memory_per_second / (required_instances * spec.memory_gb * 1024)) * 100,
                "request_capacity": (target_requests_per_second / (required_instances * requirements.requests_per_instance)) * 100
            },
            "cost": {
                "hourly": hourly_cost,
                "daily": hourly_cost * 24,
                "monthly": monthly_cost,
                "annual": monthly_cost * 12
            },
            "headroom_to_max": {
                "instances": requirements.max_instances - required_instances,
                "additional_rps": (requirements.max_instances - required_instances) * requirements.requests_per_instance
            }
        }

    def _get_limiting_factor(
        self,
        for_requests: int,
        for_cpu: int,
        for_memory: int
    ) -> str:
        """Determine the limiting factor for scaling."""
        max_factor = max(for_requests, for_cpu, for_memory)

        if max_factor == for_cpu:
            return "cpu"
        elif max_factor == for_memory:
            return "memory"
        else:
            return "request_capacity"

    def plan_scaling_schedule(
        self,
        service_name: str,
        traffic_pattern: Dict[int, float]  # hour -> traffic multiplier
    ) -> Dict[str, Any]:
        """Plan scaling schedule based on traffic pattern."""
        requirements = self.service_requirements.get(service_name)
        if not requirements:
            return {"error": f"Service {service_name} not found"}

        base_rps = requirements.requests_per_instance * requirements.min_instances

        schedule = []
        total_instance_hours = 0

        for hour, multiplier in sorted(traffic_pattern.items()):
            target_rps = int(base_rps * multiplier)
            result = self.calculate_required_instances(service_name, target_rps)

            schedule.append({
                "hour": hour,
                "traffic_multiplier": multiplier,
                "target_rps": target_rps,
                "instances": result["required_instances"]
            })

            total_instance_hours += result["required_instances"]

        spec = self.instance_specs[requirements.preferred_instance_type]

        return {
            "service": service_name,
            "schedule": schedule,
            "min_instances": min(s["instances"] for s in schedule),
            "max_instances": max(s["instances"] for s in schedule),
            "average_instances": total_instance_hours / len(schedule),
            "daily_cost": total_instance_hours * spec.hourly_cost,
            "monthly_cost": total_instance_hours * spec.hourly_cost * 30
        }

    def generate_capacity_report(self) -> Dict[str, Any]:
        """Generate comprehensive capacity report."""
        report = {
            "generated_at": datetime.utcnow().isoformat(),
            "services": {}
        }

        for service_name, requirements in self.service_requirements.items():
            spec = self.instance_specs[requirements.preferred_instance_type]

            # Calculate current capacity
            min_rps = requirements.min_instances * requirements.requests_per_instance
            max_rps = requirements.max_instances * requirements.requests_per_instance

            report["services"][service_name] = {
                "instance_type": requirements.preferred_instance_type.value,
                "current_min_instances": requirements.min_instances,
                "current_max_instances": requirements.max_instances,
                "capacity": {
                    "min_rps": min_rps,
                    "max_rps": max_rps
                },
                "cost_at_min": {
                    "hourly": requirements.min_instances * spec.hourly_cost,
                    "monthly": requirements.min_instances * spec.hourly_cost * 24 * 30
                },
                "cost_at_max": {
                    "hourly": requirements.max_instances * spec.hourly_cost,
                    "monthly": requirements.max_instances * spec.hourly_cost * 24 * 30
                },
                "scaling_config": {
                    "target_cpu": requirements.target_cpu_utilization,
                    "target_memory": requirements.target_memory_utilization
                }
            }

        return report


# Import math for ceiling function
import math
```

---

## 3. Capacity Monitoring and Alerting

### 3.1 Capacity Dashboard

```python
"""
Capacity monitoring dashboard components.
"""

from dataclasses import dataclass, field
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any


@dataclass
class CapacityDashboardWidget:
    """Widget for capacity dashboard."""
    id: str
    title: str
    widget_type: str  # gauge, chart, table, alert
    metric: str
    service: str
    threshold_warning: float
    threshold_critical: float
    current_value: float = 0
    trend: str = "stable"


class CapacityDashboard:
    """
    Generate capacity monitoring dashboard.
    """

    def __init__(
        self,
        metrics_collector: CapacityMetricsCollector,
        forecaster: DemandForecaster
    ):
        self.metrics = metrics_collector
        self.forecaster = forecaster

    def get_overview(self) -> Dict[str, Any]:
        """Get capacity overview for all services."""
        overview = {
            "timestamp": datetime.utcnow().isoformat(),
            "overall_status": "healthy",
            "services": {},
            "alerts": [],
            "forecasts": {}
        }

        # Get current capacity status
        alerts = self.metrics.get_capacity_alerts()
        overview["alerts"] = alerts

        if any(a["level"] == "critical" for a in alerts):
            overview["overall_status"] = "critical"
        elif any(a["level"] == "warning" for a in alerts):
            overview["overall_status"] = "warning"

        # Get service-level status
        services = set()
        for key in self.metrics.current_capacity.keys():
            service = key.split(":")[0]
            services.add(service)

        for service in services:
            status = self.metrics.get_capacity_status(service)
            overview["services"][service] = {
                "resources": status,
                "status": self._calculate_service_status(status)
            }

        return overview

    def _calculate_service_status(
        self,
        resource_status: Dict[str, Any]
    ) -> str:
        """Calculate overall service status from resources."""
        statuses = [r.get("alert_level", "normal") for r in resource_status.values()]

        if "critical" in statuses:
            return "critical"
        elif "warning" in statuses:
            return "warning"
        return "healthy"

    def get_resource_details(
        self,
        service: str,
        resource_type: str
    ) -> Dict[str, Any]:
        """Get detailed view of a specific resource."""
        key = f"{service}:{resource_type}"
        capacity = self.metrics.current_capacity.get(key)

        if not capacity:
            return {"error": "Resource not found"}

        # Get trend
        try:
            resource_enum = ResourceType(resource_type)
            trend = self.metrics.get_usage_trend(service, resource_enum, 7)
        except ValueError:
            trend = {"no_data": True}

        return {
            "service": service,
            "resource": resource_type,
            "current": {
                "capacity": capacity.current_capacity,
                "usage": capacity.current_usage,
                "percentage": capacity.usage_percentage,
                "unit": capacity.unit.value
            },
            "trend": trend,
            "thresholds": self._get_thresholds(resource_type),
            "recommendations": self._generate_recommendations(capacity, trend)
        }

    def _get_thresholds(self, resource_type: str) -> Dict[str, float]:
        """Get thresholds for a resource type."""
        try:
            resource_enum = ResourceType(resource_type)
            threshold = self.metrics.thresholds.get(resource_enum)
            if threshold:
                return {
                    "warning": threshold.warning_threshold,
                    "critical": threshold.critical_threshold,
                    "scale_up": threshold.scale_trigger_threshold,
                    "scale_down": threshold.scale_down_threshold
                }
        except ValueError:
            pass

        return {
            "warning": 70,
            "critical": 85,
            "scale_up": 75,
            "scale_down": 30
        }

    def _generate_recommendations(
        self,
        capacity: ResourceCapacity,
        trend: Dict[str, Any]
    ) -> List[str]:
        """Generate recommendations based on capacity and trend."""
        recommendations = []

        if capacity.usage_percentage > 85:
            recommendations.append("URGENT: Scale up immediately to prevent service degradation")

        elif capacity.usage_percentage > 70:
            if trend.get("trend") == "increasing":
                recommendations.append("Plan scaling within 1 week based on increasing trend")
            else:
                recommendations.append("Monitor closely - approaching warning threshold")

        elif capacity.usage_percentage < 30:
            if trend.get("trend") == "decreasing":
                recommendations.append("Consider scaling down to reduce costs")
            else:
                recommendations.append("Current capacity has significant headroom")

        if trend.get("max", 0) > 90:
            recommendations.append("Peak usage exceeded 90% recently - consider increasing capacity buffer")

        return recommendations

    def generate_report(
        self,
        report_type: str = "daily"
    ) -> str:
        """Generate capacity report."""
        overview = self.get_overview()

        report = f"""# Capacity Report - {datetime.utcnow().strftime('%Y-%m-%d')}

## Overall Status: {overview['overall_status'].upper()}

## Active Alerts

"""
        if overview["alerts"]:
            report += "| Service | Resource | Level | Usage | Threshold |\n"
            report += "|---------|----------|-------|-------|----------|\n"
            for alert in overview["alerts"]:
                report += f"| {alert['service']} | {alert['resource']} | {alert['level']} | {alert['usage_percentage']}% | {alert['threshold']}% |\n"
        else:
            report += "No active alerts.\n"

        report += "\n## Service Status\n\n"

        for service, data in overview["services"].items():
            report += f"### {service}\n\n"
            report += f"Status: **{data['status']}**\n\n"

            if data["resources"]:
                report += "| Resource | Usage | Capacity | Utilization |\n"
                report += "|----------|-------|----------|-------------|\n"

                for resource, info in data["resources"].items():
                    report += f"| {resource} | {info['usage']:.1f} | {info['capacity']:.1f} | {info['percentage']:.1f}% |\n"

            report += "\n"

        return report
```

---

## 4. Auto-Scaling Configuration

### 4.1 Horizontal Pod Autoscaler

```python
"""
Auto-scaling configuration for Kubernetes.
"""

from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any
from enum import Enum


class ScalingMetricType(Enum):
    """Types of scaling metrics."""
    CPU = "cpu"
    MEMORY = "memory"
    CUSTOM = "custom"
    EXTERNAL = "external"


@dataclass
class ScalingMetric:
    """A metric for scaling decisions."""
    metric_type: ScalingMetricType
    name: str
    target_value: float
    target_type: str = "Utilization"  # Utilization, Value, AverageValue


@dataclass
class ScalingBehavior:
    """Scaling behavior configuration."""
    scale_up_stabilization_seconds: int = 0
    scale_down_stabilization_seconds: int = 300
    scale_up_policies: List[Dict] = field(default_factory=list)
    scale_down_policies: List[Dict] = field(default_factory=list)


@dataclass
class HPAConfig:
    """Horizontal Pod Autoscaler configuration."""
    name: str
    namespace: str
    target_deployment: str
    min_replicas: int
    max_replicas: int
    metrics: List[ScalingMetric]
    behavior: ScalingBehavior


class AutoScalingManager:
    """
    Manage auto-scaling configurations.
    """

    def __init__(self):
        self.hpa_configs: Dict[str, HPAConfig] = {}
        self._initialize_configs()

    def _initialize_configs(self) -> None:
        """Initialize default HPA configurations."""
        default_configs = [
            HPAConfig(
                name="api-gateway-hpa",
                namespace="production",
                target_deployment="api-gateway",
                min_replicas=3,
                max_replicas=50,
                metrics=[
                    ScalingMetric(
                        metric_type=ScalingMetricType.CPU,
                        name="cpu",
                        target_value=70,
                        target_type="Utilization"
                    ),
                    ScalingMetric(
                        metric_type=ScalingMetricType.MEMORY,
                        name="memory",
                        target_value=75,
                        target_type="Utilization"
                    ),
                    ScalingMetric(
                        metric_type=ScalingMetricType.CUSTOM,
                        name="http_requests_per_second",
                        target_value=1000,
                        target_type="AverageValue"
                    )
                ],
                behavior=ScalingBehavior(
                    scale_up_stabilization_seconds=0,
                    scale_down_stabilization_seconds=300,
                    scale_up_policies=[
                        {"type": "Percent", "value": 100, "periodSeconds": 60},
                        {"type": "Pods", "value": 4, "periodSeconds": 60}
                    ],
                    scale_down_policies=[
                        {"type": "Percent", "value": 10, "periodSeconds": 60}
                    ]
                )
            ),
            HPAConfig(
                name="model-router-hpa",
                namespace="production",
                target_deployment="model-router",
                min_replicas=3,
                max_replicas=30,
                metrics=[
                    ScalingMetric(
                        metric_type=ScalingMetricType.CPU,
                        name="cpu",
                        target_value=70,
                        target_type="Utilization"
                    ),
                    ScalingMetric(
                        metric_type=ScalingMetricType.CUSTOM,
                        name="pending_requests",
                        target_value=10,
                        target_type="AverageValue"
                    )
                ],
                behavior=ScalingBehavior(
                    scale_up_stabilization_seconds=0,
                    scale_down_stabilization_seconds=300,
                    scale_up_policies=[
                        {"type": "Percent", "value": 50, "periodSeconds": 60}
                    ],
                    scale_down_policies=[
                        {"type": "Percent", "value": 10, "periodSeconds": 120}
                    ]
                )
            ),
            HPAConfig(
                name="embedding-service-hpa",
                namespace="production",
                target_deployment="embedding-service",
                min_replicas=2,
                max_replicas=20,
                metrics=[
                    ScalingMetric(
                        metric_type=ScalingMetricType.CUSTOM,
                        name="gpu_utilization",
                        target_value=80,
                        target_type="AverageValue"
                    ),
                    ScalingMetric(
                        metric_type=ScalingMetricType.CUSTOM,
                        name="queue_depth",
                        target_value=50,
                        target_type="AverageValue"
                    )
                ],
                behavior=ScalingBehavior(
                    scale_up_stabilization_seconds=60,
                    scale_down_stabilization_seconds=600,
                    scale_up_policies=[
                        {"type": "Pods", "value": 2, "periodSeconds": 120}
                    ],
                    scale_down_policies=[
                        {"type": "Pods", "value": 1, "periodSeconds": 300}
                    ]
                )
            )
        ]

        for config in default_configs:
            self.hpa_configs[config.name] = config

    def generate_hpa_manifest(self, config_name: str) -> str:
        """Generate Kubernetes HPA manifest."""
        config = self.hpa_configs.get(config_name)
        if not config:
            raise ValueError(f"Config {config_name} not found")

        metrics_yaml = self._generate_metrics_yaml(config.metrics)
        behavior_yaml = self._generate_behavior_yaml(config.behavior)

        return f"""apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: {config.name}
  namespace: {config.namespace}
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: {config.target_deployment}
  minReplicas: {config.min_replicas}
  maxReplicas: {config.max_replicas}
{metrics_yaml}
{behavior_yaml}
"""

    def _generate_metrics_yaml(self, metrics: List[ScalingMetric]) -> str:
        """Generate metrics section of HPA manifest."""
        yaml = "  metrics:\n"

        for metric in metrics:
            if metric.metric_type == ScalingMetricType.CPU:
                yaml += f"""  - type: Resource
    resource:
      name: cpu
      target:
        type: {metric.target_type}
        averageUtilization: {int(metric.target_value)}
"""
            elif metric.metric_type == ScalingMetricType.MEMORY:
                yaml += f"""  - type: Resource
    resource:
      name: memory
      target:
        type: {metric.target_type}
        averageUtilization: {int(metric.target_value)}
"""
            elif metric.metric_type == ScalingMetricType.CUSTOM:
                yaml += f"""  - type: Pods
    pods:
      metric:
        name: {metric.name}
      target:
        type: {metric.target_type}
        averageValue: {int(metric.target_value)}
"""
            elif metric.metric_type == ScalingMetricType.EXTERNAL:
                yaml += f"""  - type: External
    external:
      metric:
        name: {metric.name}
      target:
        type: {metric.target_type}
        value: {int(metric.target_value)}
"""

        return yaml

    def _generate_behavior_yaml(self, behavior: ScalingBehavior) -> str:
        """Generate behavior section of HPA manifest."""
        yaml = f"""  behavior:
    scaleUp:
      stabilizationWindowSeconds: {behavior.scale_up_stabilization_seconds}
      policies:
"""
        for policy in behavior.scale_up_policies:
            yaml += f"""      - type: {policy['type']}
        value: {policy['value']}
        periodSeconds: {policy['periodSeconds']}
"""

        yaml += f"""    scaleDown:
      stabilizationWindowSeconds: {behavior.scale_down_stabilization_seconds}
      policies:
"""
        for policy in behavior.scale_down_policies:
            yaml += f"""      - type: {policy['type']}
        value: {policy['value']}
        periodSeconds: {policy['periodSeconds']}
"""

        return yaml

    def get_scaling_recommendations(
        self,
        service: str,
        current_replicas: int,
        current_cpu_util: float,
        current_memory_util: float,
        current_rps: float
    ) -> Dict[str, Any]:
        """Get scaling recommendations based on current state."""
        # Find HPA config for service
        config = None
        for hpa in self.hpa_configs.values():
            if hpa.target_deployment == service:
                config = hpa
                break

        if not config:
            return {"error": f"No HPA config found for {service}"}

        recommendations = []
        target_replicas = current_replicas

        # Check CPU-based scaling
        for metric in config.metrics:
            if metric.metric_type == ScalingMetricType.CPU:
                if current_cpu_util > metric.target_value:
                    scale_factor = current_cpu_util / metric.target_value
                    suggested = int(current_replicas * scale_factor)
                    target_replicas = max(target_replicas, suggested)
                    recommendations.append(f"CPU at {current_cpu_util}% (target: {metric.target_value}%) - suggest scaling up")

            elif metric.metric_type == ScalingMetricType.MEMORY:
                if current_memory_util > metric.target_value:
                    scale_factor = current_memory_util / metric.target_value
                    suggested = int(current_replicas * scale_factor)
                    target_replicas = max(target_replicas, suggested)
                    recommendations.append(f"Memory at {current_memory_util}% (target: {metric.target_value}%) - suggest scaling up")

        # Apply limits
        target_replicas = max(config.min_replicas, min(config.max_replicas, target_replicas))

        action = "none"
        if target_replicas > current_replicas:
            action = "scale_up"
        elif target_replicas < current_replicas and current_cpu_util < 30 and current_memory_util < 30:
            target_replicas = max(config.min_replicas, current_replicas - 1)
            action = "scale_down"

        return {
            "service": service,
            "current_replicas": current_replicas,
            "recommended_replicas": target_replicas,
            "action": action,
            "recommendations": recommendations,
            "limits": {
                "min": config.min_replicas,
                "max": config.max_replicas
            }
        }
```

---

## Summary

This Capacity Planning Guide provides comprehensive frameworks for:

1. **Resource Metrics** - Tracking capacity across compute, memory, storage, GPU, and API limits

2. **Demand Forecasting** - Predicting future capacity needs with trend analysis

3. **LLM-Specific Planning** - Token limits, provider capacity, and cost estimation

4. **Infrastructure Planning** - Instance sizing, scaling schedules, and cost optimization

5. **Monitoring and Dashboards** - Real-time visibility into capacity utilization

6. **Auto-Scaling** - Kubernetes HPA configurations and scaling policies

Key principles:
- **Proactive planning**: Forecast needs before reaching limits
- **Multi-dimensional**: Consider CPU, memory, network, and application metrics
- **Cost-aware**: Balance performance with infrastructure costs
- **Automated scaling**: Configure auto-scaling for dynamic workloads
- **Regular review**: Continuously refine forecasts based on actual usage

---

> **Navigation**
> [← 13.2 Disaster Recovery](13.2_disaster_recovery_guide.md) | **[Index](../README.md#15-repository-structure)** | [13.4 On-Call Practices →](13.4_on_call_practices_guide.md)
