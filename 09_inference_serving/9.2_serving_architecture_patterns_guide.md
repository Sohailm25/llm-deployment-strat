> **Navigation** | [← 9.1 Inference Engines](9.1_inference_engine_selection_guide.md) | [9.3 API Design →](9.3_api_design_llm_services_guide.md)
>
> | | |
> |---|---|
> | **Prerequisites** | [9.1 Inference Engines](9.1_inference_engine_selection_guide.md) &#124; Kubernetes &#124; Microservices |
> | **Related** | [9.3 API Design](9.3_api_design_llm_services_guide.md) &#124; [13.3 Capacity Planning](../13_operations_reliability/13.3_capacity_planning_guide.md) |
> | **Next** | [9.3 API Design for LLM Services](9.3_api_design_llm_services_guide.md) |

# Document 9.2: Serving Architecture Patterns Guide

## Purpose

This guide provides design patterns for building scalable, reliable LLM serving systems. It covers architecture components, serving patterns (single-model, multi-model, tiered, federated), scaling strategies, high availability configurations, and performance optimization techniques. These patterns enable production-grade LLM deployments that can handle varying loads while maintaining low latency and high throughput.

---

## Prerequisites

- **Infrastructure**: Kubernetes cluster with GPU nodes, load balancer, container registry
- **Tools**: Docker, Helm, Istio/Envoy, Prometheus, Grafana
- **Knowledge**: Microservices architecture, Kubernetes, GPU scheduling
- **Access**: Cloud provider credentials, DNS management

---

## 9.2.1 Architecture Components

```python
"""
Core components of an LLM serving architecture.
"""

from dataclasses import dataclass, field
from enum import Enum
from typing import Optional, Any
from abc import ABC, abstractmethod


class ComponentType(Enum):
    LOAD_BALANCER = "load_balancer"
    API_GATEWAY = "api_gateway"
    REQUEST_ROUTER = "request_router"
    MODEL_SERVER = "model_server"
    QUEUE_MANAGER = "queue_manager"
    CACHE_LAYER = "cache_layer"
    MONITORING = "monitoring"
    AUTOSCALER = "autoscaler"


@dataclass
class ArchitectureComponent:
    """Base class for architecture components."""

    name: str
    component_type: ComponentType
    description: str
    technologies: list[str] = field(default_factory=list)
    scaling_mode: str = "horizontal"  # horizontal, vertical, none
    stateful: bool = False


# Core Components Definition
ARCHITECTURE_COMPONENTS = {
    ComponentType.LOAD_BALANCER: ArchitectureComponent(
        name="Load Balancer",
        component_type=ComponentType.LOAD_BALANCER,
        description="Distributes incoming requests across model server replicas",
        technologies=["NGINX", "HAProxy", "Envoy", "AWS ALB", "GCP GLB"],
        scaling_mode="horizontal",
        stateful=False
    ),

    ComponentType.API_GATEWAY: ArchitectureComponent(
        name="API Gateway",
        component_type=ComponentType.API_GATEWAY,
        description="Handles authentication, rate limiting, request validation",
        technologies=["Kong", "Envoy", "AWS API Gateway", "Apigee"],
        scaling_mode="horizontal",
        stateful=False
    ),

    ComponentType.REQUEST_ROUTER: ArchitectureComponent(
        name="Request Router",
        component_type=ComponentType.REQUEST_ROUTER,
        description="Routes requests to appropriate model servers based on model, load, or user",
        technologies=["Custom service", "Istio VirtualService", "Envoy routing"],
        scaling_mode="horizontal",
        stateful=False
    ),

    ComponentType.MODEL_SERVER: ArchitectureComponent(
        name="Model Server",
        component_type=ComponentType.MODEL_SERVER,
        description="Runs inference on LLM models",
        technologies=["vLLM", "TensorRT-LLM", "SGLang", "TGI"],
        scaling_mode="horizontal",
        stateful=True  # Model weights in GPU memory
    ),

    ComponentType.QUEUE_MANAGER: ArchitectureComponent(
        name="Queue Manager",
        component_type=ComponentType.QUEUE_MANAGER,
        description="Manages request queuing for load smoothing and priority handling",
        technologies=["Redis", "RabbitMQ", "Kafka", "SQS"],
        scaling_mode="horizontal",
        stateful=True
    ),

    ComponentType.CACHE_LAYER: ArchitectureComponent(
        name="Cache Layer",
        component_type=ComponentType.CACHE_LAYER,
        description="Caches responses for repeated queries, KV cache for prefix sharing",
        technologies=["Redis", "Memcached", "Custom KV cache"],
        scaling_mode="horizontal",
        stateful=True
    ),

    ComponentType.MONITORING: ArchitectureComponent(
        name="Monitoring",
        component_type=ComponentType.MONITORING,
        description="Collects metrics, logs, and traces for observability",
        technologies=["Prometheus", "Grafana", "Datadog", "OpenTelemetry"],
        scaling_mode="horizontal",
        stateful=True
    ),

    ComponentType.AUTOSCALER: ArchitectureComponent(
        name="Autoscaler",
        component_type=ComponentType.AUTOSCALER,
        description="Automatically scales model servers based on demand",
        technologies=["Kubernetes HPA", "KEDA", "Custom scaler"],
        scaling_mode="none",
        stateful=False
    )
}


@dataclass
class LLMServingArchitecture:
    """Complete LLM serving architecture definition."""

    name: str
    components: list[ArchitectureComponent]
    traffic_flow: list[tuple[ComponentType, ComponentType]]
    deployment_mode: str  # kubernetes, docker-compose, bare-metal
    gpu_requirements: dict[str, int] = field(default_factory=dict)

    def validate(self) -> list[str]:
        """Validate architecture configuration."""
        issues = []

        # Must have model server
        has_model_server = any(
            c.component_type == ComponentType.MODEL_SERVER
            for c in self.components
        )
        if not has_model_server:
            issues.append("Architecture must include a model server")

        # Should have load balancer for production
        has_lb = any(
            c.component_type == ComponentType.LOAD_BALANCER
            for c in self.components
        )
        if not has_lb:
            issues.append("Production architecture should include load balancer")

        return issues


class ArchitectureBuilder:
    """Builder for LLM serving architectures."""

    def __init__(self, name: str):
        self.name = name
        self.components: list[ArchitectureComponent] = []
        self.traffic_flow: list[tuple[ComponentType, ComponentType]] = []
        self.deployment_mode = "kubernetes"
        self.gpu_requirements = {}

    def add_component(self, component_type: ComponentType) -> "ArchitectureBuilder":
        """Add a component to the architecture."""
        self.components.append(ARCHITECTURE_COMPONENTS[component_type])
        return self

    def add_traffic_flow(
        self,
        from_component: ComponentType,
        to_component: ComponentType
    ) -> "ArchitectureBuilder":
        """Define traffic flow between components."""
        self.traffic_flow.append((from_component, to_component))
        return self

    def set_gpu_requirements(self, model_name: str, gpu_count: int) -> "ArchitectureBuilder":
        """Set GPU requirements for a model."""
        self.gpu_requirements[model_name] = gpu_count
        return self

    def build(self) -> LLMServingArchitecture:
        """Build the architecture."""
        return LLMServingArchitecture(
            name=self.name,
            components=self.components,
            traffic_flow=self.traffic_flow,
            deployment_mode=self.deployment_mode,
            gpu_requirements=self.gpu_requirements
        )


# Standard architecture templates
def create_basic_architecture() -> LLMServingArchitecture:
    """Create a basic single-model serving architecture."""
    return (
        ArchitectureBuilder("basic-llm-serving")
        .add_component(ComponentType.LOAD_BALANCER)
        .add_component(ComponentType.MODEL_SERVER)
        .add_component(ComponentType.MONITORING)
        .add_traffic_flow(ComponentType.LOAD_BALANCER, ComponentType.MODEL_SERVER)
        .set_gpu_requirements("llama-8b", 1)
        .build()
    )


def create_production_architecture() -> LLMServingArchitecture:
    """Create a production-grade serving architecture."""
    return (
        ArchitectureBuilder("production-llm-serving")
        .add_component(ComponentType.LOAD_BALANCER)
        .add_component(ComponentType.API_GATEWAY)
        .add_component(ComponentType.REQUEST_ROUTER)
        .add_component(ComponentType.QUEUE_MANAGER)
        .add_component(ComponentType.CACHE_LAYER)
        .add_component(ComponentType.MODEL_SERVER)
        .add_component(ComponentType.MONITORING)
        .add_component(ComponentType.AUTOSCALER)
        .add_traffic_flow(ComponentType.LOAD_BALANCER, ComponentType.API_GATEWAY)
        .add_traffic_flow(ComponentType.API_GATEWAY, ComponentType.REQUEST_ROUTER)
        .add_traffic_flow(ComponentType.REQUEST_ROUTER, ComponentType.CACHE_LAYER)
        .add_traffic_flow(ComponentType.CACHE_LAYER, ComponentType.QUEUE_MANAGER)
        .add_traffic_flow(ComponentType.QUEUE_MANAGER, ComponentType.MODEL_SERVER)
        .set_gpu_requirements("llama-70b", 8)
        .build()
    )
```

---

## 9.2.2 Serving Patterns

### Single Model Serving

```python
"""
Single model serving pattern - the simplest deployment.
"""

from dataclasses import dataclass, field
from typing import Optional
import asyncio


@dataclass
class SingleModelConfig:
    """Configuration for single model serving."""

    model_id: str
    model_path: str
    replicas: int = 1
    gpu_per_replica: int = 1

    # Resource limits
    memory_limit_gb: int = 32
    cpu_limit: int = 8

    # Serving parameters
    max_batch_size: int = 64
    max_concurrent_requests: int = 256

    # Health check
    health_check_path: str = "/health"
    health_check_interval_seconds: int = 10


class SingleModelServer:
    """
    Single model serving implementation.
    Best for: Simple deployments, dedicated model instances.
    """

    def __init__(self, config: SingleModelConfig):
        self.config = config
        self.model = None
        self.request_count = 0

    async def start(self):
        """Start the model server."""
        # Load model into GPU memory
        self.model = await self._load_model()

    async def _load_model(self):
        """Load the model."""
        # Implementation depends on inference engine
        pass

    async def generate(
        self,
        prompt: str,
        max_tokens: int = 100,
        temperature: float = 0.7
    ) -> str:
        """Generate completion."""
        self.request_count += 1
        # Inference logic
        return await self.model.generate(prompt, max_tokens, temperature)

    async def health_check(self) -> dict:
        """Health check endpoint."""
        return {
            "status": "healthy" if self.model else "unhealthy",
            "model_id": self.config.model_id,
            "request_count": self.request_count
        }


# Kubernetes deployment for single model
SINGLE_MODEL_K8S_DEPLOYMENT = """
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-single-model
  labels:
    app: llm-server
spec:
  replicas: 2
  selector:
    matchLabels:
      app: llm-server
  template:
    metadata:
      labels:
        app: llm-server
    spec:
      containers:
      - name: vllm
        image: vllm/vllm-openai:latest
        args:
          - "--model"
          - "meta-llama/Llama-3.1-8B-Instruct"
          - "--tensor-parallel-size"
          - "1"
        ports:
        - containerPort: 8000
        resources:
          limits:
            nvidia.com/gpu: 1
            memory: 32Gi
            cpu: "8"
          requests:
            nvidia.com/gpu: 1
            memory: 24Gi
            cpu: "4"
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 120
          periodSeconds: 10
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 120
          periodSeconds: 30
---
apiVersion: v1
kind: Service
metadata:
  name: llm-service
spec:
  selector:
    app: llm-server
  ports:
  - port: 80
    targetPort: 8000
  type: ClusterIP
"""
```

### Multi-Model Serving

```python
"""
Multi-model serving pattern - serve multiple models efficiently.
"""

from dataclasses import dataclass, field
from enum import Enum
from typing import Optional
import asyncio
import random


class RoutingStrategy(Enum):
    ROUND_ROBIN = "round_robin"
    LEAST_LOADED = "least_loaded"
    RANDOM = "random"
    HASH_BASED = "hash_based"
    CAPABILITY_BASED = "capability_based"


@dataclass
class ModelEndpoint:
    """Represents a model endpoint in multi-model serving."""

    model_id: str
    url: str
    weight: float = 1.0
    max_concurrent: int = 100
    current_load: int = 0
    capabilities: list[str] = field(default_factory=list)
    healthy: bool = True


@dataclass
class MultiModelConfig:
    """Configuration for multi-model serving."""

    models: list[ModelEndpoint]
    routing_strategy: RoutingStrategy = RoutingStrategy.LEAST_LOADED
    enable_fallback: bool = True
    fallback_model_id: Optional[str] = None
    load_balance_interval_seconds: int = 5


class ModelRouter:
    """
    Routes requests to appropriate models in multi-model setup.
    Supports various routing strategies and load balancing.
    """

    def __init__(self, config: MultiModelConfig):
        self.config = config
        self.models = {m.model_id: m for m in config.models}
        self._round_robin_index = 0

    def route(
        self,
        model_id: Optional[str] = None,
        required_capabilities: Optional[list[str]] = None,
        user_id: Optional[str] = None
    ) -> Optional[ModelEndpoint]:
        """Route request to appropriate model endpoint."""

        # Filter by model_id if specified
        if model_id:
            candidates = [m for m in self.config.models if m.model_id == model_id]
        else:
            candidates = self.config.models.copy()

        # Filter by capabilities
        if required_capabilities:
            candidates = [
                m for m in candidates
                if all(cap in m.capabilities for cap in required_capabilities)
            ]

        # Filter healthy models
        candidates = [m for m in candidates if m.healthy]

        if not candidates:
            if self.config.enable_fallback and self.config.fallback_model_id:
                return self.models.get(self.config.fallback_model_id)
            return None

        # Apply routing strategy
        if self.config.routing_strategy == RoutingStrategy.ROUND_ROBIN:
            return self._round_robin_route(candidates)
        elif self.config.routing_strategy == RoutingStrategy.LEAST_LOADED:
            return self._least_loaded_route(candidates)
        elif self.config.routing_strategy == RoutingStrategy.RANDOM:
            return random.choice(candidates)
        elif self.config.routing_strategy == RoutingStrategy.HASH_BASED:
            return self._hash_route(candidates, user_id or "default")
        else:
            return candidates[0]

    def _round_robin_route(self, candidates: list[ModelEndpoint]) -> ModelEndpoint:
        """Round-robin routing."""
        self._round_robin_index = (self._round_robin_index + 1) % len(candidates)
        return candidates[self._round_robin_index]

    def _least_loaded_route(self, candidates: list[ModelEndpoint]) -> ModelEndpoint:
        """Route to least loaded model."""
        return min(candidates, key=lambda m: m.current_load / m.max_concurrent)

    def _hash_route(self, candidates: list[ModelEndpoint], key: str) -> ModelEndpoint:
        """Consistent hash routing for session affinity."""
        index = hash(key) % len(candidates)
        return candidates[index]

    def update_load(self, model_id: str, delta: int):
        """Update model load count."""
        if model_id in self.models:
            self.models[model_id].current_load += delta

    def mark_unhealthy(self, model_id: str):
        """Mark a model as unhealthy."""
        if model_id in self.models:
            self.models[model_id].healthy = False

    def mark_healthy(self, model_id: str):
        """Mark a model as healthy."""
        if model_id in self.models:
            self.models[model_id].healthy = True


class MultiModelProxy:
    """
    Proxy service for multi-model serving.
    Handles routing, load balancing, and failover.
    """

    def __init__(self, config: MultiModelConfig):
        self.config = config
        self.router = ModelRouter(config)

    async def generate(
        self,
        prompt: str,
        model_id: Optional[str] = None,
        max_tokens: int = 100,
        **kwargs
    ) -> dict:
        """Route and execute generation request."""
        import aiohttp

        endpoint = self.router.route(model_id=model_id)

        if not endpoint:
            raise ValueError(f"No available model for request (model_id={model_id})")

        self.router.update_load(endpoint.model_id, 1)

        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{endpoint.url}/v1/completions",
                    json={
                        "prompt": prompt,
                        "max_tokens": max_tokens,
                        **kwargs
                    },
                    timeout=aiohttp.ClientTimeout(total=60)
                ) as response:
                    result = await response.json()
                    return {
                        "result": result,
                        "model_id": endpoint.model_id,
                        "endpoint": endpoint.url
                    }
        except Exception as e:
            self.router.mark_unhealthy(endpoint.model_id)
            raise
        finally:
            self.router.update_load(endpoint.model_id, -1)


# Kubernetes multi-model deployment with Istio routing
MULTI_MODEL_ISTIO_CONFIG = """
# VirtualService for model routing
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: llm-router
spec:
  hosts:
  - llm-api.example.com
  http:
  - match:
    - headers:
        x-model-id:
          exact: "llama-70b"
    route:
    - destination:
        host: llm-70b-service
        port:
          number: 80
  - match:
    - headers:
        x-model-id:
          exact: "llama-8b"
    route:
    - destination:
        host: llm-8b-service
        port:
          number: 80
  - route:  # Default to 8B model
    - destination:
        host: llm-8b-service
        port:
          number: 80
---
# DestinationRule for load balancing
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: llm-70b-lb
spec:
  host: llm-70b-service
  trafficPolicy:
    loadBalancer:
      simple: LEAST_REQUEST
    connectionPool:
      http:
        maxRequestsPerConnection: 10
        http2MaxRequests: 1000
"""
```

### Tiered Serving

```python
"""
Tiered serving pattern - cascade models for quality vs cost optimization.
"""

from dataclasses import dataclass, field
from enum import Enum
from typing import Optional, Callable
import asyncio


class TierLevel(Enum):
    FAST = "fast"      # Small model, low latency
    STANDARD = "standard"  # Medium model, balanced
    QUALITY = "quality"    # Large model, highest quality


@dataclass
class ModelTier:
    """Represents a model tier in cascaded serving."""

    level: TierLevel
    model_id: str
    endpoint: str
    latency_target_ms: int
    cost_per_1k_tokens: float
    quality_score: float  # 0-1 relative quality
    fallback_tier: Optional[TierLevel] = None


@dataclass
class TieredServingConfig:
    """Configuration for tiered serving."""

    tiers: list[ModelTier]
    default_tier: TierLevel = TierLevel.STANDARD
    enable_quality_routing: bool = True
    enable_cost_routing: bool = True
    quality_threshold: float = 0.8  # Min quality score to use fast tier
    latency_budget_ms: int = 5000


class TierRouter:
    """
    Routes requests based on quality requirements and cost constraints.
    Implements model cascading with fallback.
    """

    def __init__(self, config: TieredServingConfig):
        self.config = config
        self.tiers = {t.level: t for t in config.tiers}

    def select_tier(
        self,
        complexity: float,  # 0-1 estimated task complexity
        latency_budget_ms: Optional[int] = None,
        max_cost: Optional[float] = None,
        min_quality: Optional[float] = None
    ) -> ModelTier:
        """Select appropriate tier based on requirements."""

        latency_budget = latency_budget_ms or self.config.latency_budget_ms
        candidates = list(self.tiers.values())

        # Filter by latency
        candidates = [t for t in candidates if t.latency_target_ms <= latency_budget]

        # Filter by cost
        if max_cost:
            candidates = [t for t in candidates if t.cost_per_1k_tokens <= max_cost]

        # Filter by quality
        if min_quality:
            candidates = [t for t in candidates if t.quality_score >= min_quality]

        if not candidates:
            # Fall back to standard tier
            return self.tiers.get(TierLevel.STANDARD, list(self.tiers.values())[0])

        # Select based on complexity
        if complexity < 0.3:
            # Simple task - use fastest tier
            return min(candidates, key=lambda t: t.latency_target_ms)
        elif complexity > 0.7:
            # Complex task - use highest quality
            return max(candidates, key=lambda t: t.quality_score)
        else:
            # Balanced - use standard
            standard = self.tiers.get(TierLevel.STANDARD)
            if standard in candidates:
                return standard
            return candidates[0]


class CascadeExecutor:
    """
    Executes requests with automatic cascade to higher tiers.
    """

    def __init__(self, config: TieredServingConfig):
        self.config = config
        self.router = TierRouter(config)
        self.tiers = {t.level: t for t in config.tiers}

    async def execute(
        self,
        prompt: str,
        complexity: float = 0.5,
        max_cascades: int = 2,
        quality_check: Optional[Callable[[str], float]] = None
    ) -> dict:
        """Execute with automatic cascade if quality is low."""
        import aiohttp

        current_tier = self.router.select_tier(complexity)
        cascades = 0
        results = []

        while cascades <= max_cascades:
            # Execute on current tier
            result = await self._call_tier(current_tier, prompt)
            results.append({
                "tier": current_tier.level.value,
                "result": result
            })

            # Check quality if function provided
            if quality_check:
                quality = quality_check(result)
                if quality >= self.config.quality_threshold:
                    break  # Quality is acceptable

            # Cascade to higher tier
            if current_tier.fallback_tier:
                current_tier = self.tiers.get(current_tier.fallback_tier)
                if not current_tier:
                    break
                cascades += 1
            else:
                break

        return {
            "final_result": results[-1]["result"],
            "cascade_path": [r["tier"] for r in results],
            "num_cascades": cascades
        }

    async def _call_tier(self, tier: ModelTier, prompt: str) -> str:
        """Call a specific tier."""
        import aiohttp

        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{tier.endpoint}/v1/completions",
                json={"prompt": prompt, "max_tokens": 100}
            ) as response:
                result = await response.json()
                return result["choices"][0]["text"]


class CostOptimizedRouter:
    """
    Routes to minimize cost while meeting quality requirements.
    """

    def __init__(self, config: TieredServingConfig):
        self.config = config
        self.tiers = sorted(config.tiers, key=lambda t: t.cost_per_1k_tokens)
        self.quality_estimator = None

    def set_quality_estimator(self, estimator: Callable[[str], float]):
        """Set function to estimate input complexity."""
        self.quality_estimator = estimator

    def route(self, prompt: str, min_quality: float = 0.7) -> ModelTier:
        """Route to cheapest tier that meets quality requirement."""

        # Estimate complexity if possible
        if self.quality_estimator:
            complexity = self.quality_estimator(prompt)
        else:
            # Default: use prompt length as complexity proxy
            complexity = min(1.0, len(prompt) / 1000)

        # Find cheapest tier that meets quality
        for tier in self.tiers:
            # Estimate effective quality based on complexity
            effective_quality = tier.quality_score * (1 - complexity * 0.2)
            if effective_quality >= min_quality:
                return tier

        # Fall back to highest quality tier
        return max(self.tiers, key=lambda t: t.quality_score)


# Example tier configuration
TIERED_CONFIG_EXAMPLE = """
# Tiered serving configuration
tiers:
  - level: fast
    model_id: llama-3.1-8b-instruct
    endpoint: http://llm-8b:8000
    latency_target_ms: 500
    cost_per_1k_tokens: 0.001
    quality_score: 0.7
    fallback_tier: standard

  - level: standard
    model_id: llama-3.1-70b-instruct
    endpoint: http://llm-70b:8000
    latency_target_ms: 2000
    cost_per_1k_tokens: 0.01
    quality_score: 0.9
    fallback_tier: quality

  - level: quality
    model_id: llama-3.1-405b-instruct
    endpoint: http://llm-405b:8000
    latency_target_ms: 5000
    cost_per_1k_tokens: 0.05
    quality_score: 1.0
    fallback_tier: null

default_tier: standard
quality_threshold: 0.8
"""
```

### Federated Serving

```python
"""
Federated serving pattern - multi-region deployment with data locality.
"""

from dataclasses import dataclass, field
from enum import Enum
from typing import Optional
import asyncio


class Region(Enum):
    US_EAST = "us-east-1"
    US_WEST = "us-west-2"
    EU_WEST = "eu-west-1"
    AP_SOUTHEAST = "ap-southeast-1"
    AP_NORTHEAST = "ap-northeast-1"


@dataclass
class RegionalEndpoint:
    """Regional LLM endpoint."""

    region: Region
    endpoint: str
    models: list[str]
    capacity: int  # Max concurrent requests
    current_load: int = 0
    healthy: bool = True
    data_residency_compliant: bool = True


@dataclass
class FederatedConfig:
    """Configuration for federated serving."""

    endpoints: list[RegionalEndpoint]
    enable_cross_region_routing: bool = True
    data_residency_regions: list[Region] = field(default_factory=list)
    latency_threshold_ms: int = 200  # Max acceptable cross-region latency


class GeographicRouter:
    """
    Routes requests based on geographic location and data residency.
    """

    def __init__(self, config: FederatedConfig):
        self.config = config
        self.endpoints = {e.region: e for e in config.endpoints}
        self.region_latencies = self._estimate_latencies()

    def _estimate_latencies(self) -> dict[tuple[Region, Region], int]:
        """Estimate cross-region latencies."""
        # Simplified latency estimates in ms
        base_latencies = {
            (Region.US_EAST, Region.US_WEST): 60,
            (Region.US_EAST, Region.EU_WEST): 80,
            (Region.US_EAST, Region.AP_SOUTHEAST): 200,
            (Region.US_EAST, Region.AP_NORTHEAST): 180,
            (Region.US_WEST, Region.EU_WEST): 140,
            (Region.US_WEST, Region.AP_SOUTHEAST): 120,
            (Region.US_WEST, Region.AP_NORTHEAST): 100,
            (Region.EU_WEST, Region.AP_SOUTHEAST): 180,
            (Region.EU_WEST, Region.AP_NORTHEAST): 200,
            (Region.AP_SOUTHEAST, Region.AP_NORTHEAST): 60,
        }

        # Make symmetric
        latencies = {}
        for (r1, r2), latency in base_latencies.items():
            latencies[(r1, r2)] = latency
            latencies[(r2, r1)] = latency
            latencies[(r1, r1)] = 5  # Same region
            latencies[(r2, r2)] = 5

        return latencies

    def route(
        self,
        user_region: Region,
        model_id: str,
        require_data_residency: bool = False
    ) -> Optional[RegionalEndpoint]:
        """Route to best regional endpoint."""

        # Get candidate endpoints
        candidates = [
            e for e in self.config.endpoints
            if model_id in e.models and e.healthy
        ]

        # Apply data residency filter
        if require_data_residency:
            candidates = [
                e for e in candidates
                if e.region in self.config.data_residency_regions
                and e.data_residency_compliant
            ]

        if not candidates:
            return None

        # Sort by latency from user region
        def get_latency(endpoint: RegionalEndpoint) -> int:
            return self.region_latencies.get(
                (user_region, endpoint.region),
                999
            )

        candidates.sort(key=get_latency)

        # Filter by latency threshold if cross-region routing is disabled
        if not self.config.enable_cross_region_routing:
            candidates = [
                e for e in candidates
                if get_latency(e) <= self.config.latency_threshold_ms
            ]

        # Select least loaded among low-latency options
        low_latency = [
            e for e in candidates
            if get_latency(e) <= self.config.latency_threshold_ms
        ]

        if low_latency:
            return min(low_latency, key=lambda e: e.current_load / e.capacity)

        # Fall back to any available if allowed
        return candidates[0] if candidates else None


class FederatedProxy:
    """
    Federated serving proxy with cross-region failover.
    """

    def __init__(self, config: FederatedConfig):
        self.config = config
        self.router = GeographicRouter(config)

    async def generate(
        self,
        prompt: str,
        user_region: Region,
        model_id: str,
        require_data_residency: bool = False,
        **kwargs
    ) -> dict:
        """Generate with geographic routing."""
        import aiohttp

        endpoint = self.router.route(
            user_region=user_region,
            model_id=model_id,
            require_data_residency=require_data_residency
        )

        if not endpoint:
            raise ValueError(
                f"No available endpoint for model {model_id} "
                f"in region {user_region.value}"
            )

        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{endpoint.endpoint}/v1/completions",
                json={"prompt": prompt, **kwargs}
            ) as response:
                result = await response.json()
                return {
                    "result": result,
                    "served_by_region": endpoint.region.value,
                    "user_region": user_region.value
                }

    async def replicate_to_regions(
        self,
        model_id: str,
        source_region: Region,
        target_regions: list[Region]
    ):
        """Replicate model to additional regions."""
        # Implementation for cross-region model replication
        pass


# Multi-region Kubernetes deployment
FEDERATED_K8S_CONFIG = """
# US-East deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-us-east
  namespace: llm-serving
  labels:
    region: us-east-1
spec:
  replicas: 3
  selector:
    matchLabels:
      app: llm-server
      region: us-east-1
  template:
    spec:
      nodeSelector:
        topology.kubernetes.io/region: us-east-1
      containers:
      - name: vllm
        image: vllm/vllm-openai:latest
        # ... config
---
# EU-West deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-eu-west
  namespace: llm-serving
  labels:
    region: eu-west-1
spec:
  replicas: 2
  selector:
    matchLabels:
      app: llm-server
      region: eu-west-1
  template:
    spec:
      nodeSelector:
        topology.kubernetes.io/region: eu-west-1
      containers:
      - name: vllm
        image: vllm/vllm-openai:latest
        # ... config
---
# Global load balancer service
apiVersion: v1
kind: Service
metadata:
  name: llm-global
  annotations:
    external-dns.alpha.kubernetes.io/hostname: llm-api.example.com
spec:
  type: LoadBalancer
  externalTrafficPolicy: Local  # Preserve client IP for geo-routing
  selector:
    app: llm-server
  ports:
  - port: 443
    targetPort: 8000
"""
```

---

## 9.2.3 Scaling Strategies

```python
"""
Scaling strategies for LLM serving systems.
"""

from dataclasses import dataclass, field
from enum import Enum
from typing import Optional, Callable
import asyncio
from datetime import datetime


class ScalingStrategy(Enum):
    HORIZONTAL = "horizontal"  # Add replicas
    VERTICAL = "vertical"      # Upgrade GPU
    HYBRID = "hybrid"          # Both


@dataclass
class ScalingPolicy:
    """Policy for auto-scaling decisions."""

    name: str
    strategy: ScalingStrategy

    # Horizontal scaling
    min_replicas: int = 1
    max_replicas: int = 10
    target_utilization: float = 0.7  # GPU or request utilization

    # Vertical scaling
    gpu_upgrade_path: list[str] = field(default_factory=lambda: ["T4", "L4", "A10", "A100", "H100"])

    # Thresholds
    scale_up_threshold: float = 0.8
    scale_down_threshold: float = 0.3
    cooldown_seconds: int = 300

    # Cost optimization
    prefer_spot_instances: bool = True
    max_cost_per_hour: Optional[float] = None


@dataclass
class ScalingMetrics:
    """Metrics for scaling decisions."""

    gpu_utilization: float
    request_queue_depth: int
    average_latency_ms: float
    error_rate: float
    active_requests: int
    total_capacity: int
    timestamp: datetime = field(default_factory=datetime.utcnow)

    @property
    def utilization(self) -> float:
        """Calculate overall utilization."""
        return self.active_requests / max(self.total_capacity, 1)


class HorizontalScaler:
    """
    Horizontal scaling controller.
    Adds/removes replicas based on load.
    """

    def __init__(self, policy: ScalingPolicy, k8s_client=None):
        self.policy = policy
        self.k8s = k8s_client
        self.current_replicas = policy.min_replicas
        self.last_scale_time = None

    def evaluate(self, metrics: ScalingMetrics) -> Optional[int]:
        """Evaluate if scaling is needed. Returns target replicas."""

        # Check cooldown
        if self.last_scale_time:
            elapsed = (datetime.utcnow() - self.last_scale_time).total_seconds()
            if elapsed < self.policy.cooldown_seconds:
                return None

        # Scale up
        if metrics.utilization > self.policy.scale_up_threshold:
            target = min(
                self.current_replicas + 1,
                self.policy.max_replicas
            )
            if target > self.current_replicas:
                return target

        # Scale down
        if metrics.utilization < self.policy.scale_down_threshold:
            target = max(
                self.current_replicas - 1,
                self.policy.min_replicas
            )
            if target < self.current_replicas:
                return target

        return None

    async def scale_to(self, target_replicas: int, deployment_name: str):
        """Scale deployment to target replicas."""
        if not self.k8s:
            return

        await self.k8s.scale_deployment(
            name=deployment_name,
            replicas=target_replicas
        )
        self.current_replicas = target_replicas
        self.last_scale_time = datetime.utcnow()


class VerticalScaler:
    """
    Vertical scaling controller.
    Upgrades/downgrades GPU instances.
    """

    def __init__(self, policy: ScalingPolicy):
        self.policy = policy
        self.current_gpu_index = 0

    def evaluate(self, metrics: ScalingMetrics) -> Optional[str]:
        """Evaluate if GPU upgrade is needed."""

        # Check if GPU is bottleneck
        if metrics.gpu_utilization > 0.9 and metrics.average_latency_ms > 1000:
            # Try to upgrade
            if self.current_gpu_index < len(self.policy.gpu_upgrade_path) - 1:
                return self.policy.gpu_upgrade_path[self.current_gpu_index + 1]

        # Check if we can downgrade
        if metrics.gpu_utilization < 0.3 and self.current_gpu_index > 0:
            return self.policy.gpu_upgrade_path[self.current_gpu_index - 1]

        return None


class PredictiveScaler:
    """
    Predictive scaling based on historical patterns.
    """

    def __init__(self, policy: ScalingPolicy):
        self.policy = policy
        self.history: list[ScalingMetrics] = []
        self.patterns: dict[int, float] = {}  # hour -> expected utilization

    def record_metrics(self, metrics: ScalingMetrics):
        """Record metrics for pattern learning."""
        self.history.append(metrics)

        # Update hourly pattern
        hour = metrics.timestamp.hour
        if hour not in self.patterns:
            self.patterns[hour] = metrics.utilization
        else:
            # Exponential moving average
            self.patterns[hour] = 0.9 * self.patterns[hour] + 0.1 * metrics.utilization

    def predict_load(self, hours_ahead: int = 1) -> float:
        """Predict load for future hour."""
        target_hour = (datetime.utcnow().hour + hours_ahead) % 24
        return self.patterns.get(target_hour, 0.5)

    def recommend_replicas(self, hours_ahead: int = 1) -> int:
        """Recommend replica count for future load."""
        predicted_utilization = self.predict_load(hours_ahead)

        # Calculate needed capacity
        target_utilization = self.policy.target_utilization
        needed_replicas = int(
            predicted_utilization / target_utilization * self.policy.min_replicas
        )

        return max(
            self.policy.min_replicas,
            min(needed_replicas, self.policy.max_replicas)
        )


class CostOptimizedScaler:
    """
    Scaling with cost optimization.
    Uses spot instances and right-sizes GPU allocation.
    """

    def __init__(self, policy: ScalingPolicy):
        self.policy = policy
        self.instance_costs = {
            "T4": 0.50,
            "L4": 0.80,
            "A10": 1.20,
            "A100": 3.00,
            "H100": 5.00
        }
        self.spot_discount = 0.6  # 60% of on-demand

    def find_cheapest_config(
        self,
        min_throughput: int,
        max_latency_ms: int
    ) -> dict:
        """Find cheapest configuration meeting requirements."""

        configurations = []

        for gpu_type in self.policy.gpu_upgrade_path:
            # Estimate capacity per GPU
            capacity = self._estimate_capacity(gpu_type, max_latency_ms)
            replicas_needed = max(1, min_throughput // capacity)

            if replicas_needed > self.policy.max_replicas:
                continue

            # Calculate cost
            cost = self.instance_costs[gpu_type] * replicas_needed
            if self.policy.prefer_spot_instances:
                cost *= self.spot_discount

            configurations.append({
                "gpu_type": gpu_type,
                "replicas": replicas_needed,
                "cost_per_hour": cost,
                "capacity": capacity * replicas_needed
            })

        if not configurations:
            return None

        # Return cheapest
        return min(configurations, key=lambda c: c["cost_per_hour"])

    def _estimate_capacity(self, gpu_type: str, max_latency_ms: int) -> int:
        """Estimate requests per second capacity."""
        base_capacity = {
            "T4": 5,
            "L4": 10,
            "A10": 20,
            "A100": 50,
            "H100": 80
        }
        return base_capacity.get(gpu_type, 10)


# Kubernetes HPA configuration
K8S_HPA_CONFIG = """
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: llm-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: llm-server
  minReplicas: 2
  maxReplicas: 10
  metrics:
  # Scale based on GPU utilization
  - type: Pods
    pods:
      metric:
        name: nvidia_gpu_utilization
      target:
        type: AverageValue
        averageValue: "70"
  # Scale based on request queue depth
  - type: External
    external:
      metric:
        name: llm_request_queue_depth
        selector:
          matchLabels:
            deployment: llm-server
      target:
        type: AverageValue
        averageValue: "50"
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Pods
        value: 2
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
"""

# KEDA ScaledObject for advanced scaling
KEDA_SCALEDOBJECT = """
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: llm-scaledobject
spec:
  scaleTargetRef:
    name: llm-server
  minReplicaCount: 1
  maxReplicaCount: 20
  cooldownPeriod: 300
  triggers:
  - type: prometheus
    metadata:
      serverAddress: http://prometheus:9090
      metricName: llm_request_queue_depth
      threshold: "100"
      query: sum(llm_request_queue_depth{deployment="llm-server"})
  - type: prometheus
    metadata:
      serverAddress: http://prometheus:9090
      metricName: llm_gpu_memory_utilization
      threshold: "0.8"
      query: avg(nvidia_gpu_memory_used_bytes/nvidia_gpu_memory_total_bytes{deployment="llm-server"})
"""
```

---

## 9.2.4 High Availability

```python
"""
High availability patterns for LLM serving.
"""

from dataclasses import dataclass, field
from enum import Enum
from typing import Optional
import asyncio
from datetime import datetime, timedelta


class FailoverStrategy(Enum):
    ACTIVE_PASSIVE = "active_passive"
    ACTIVE_ACTIVE = "active_active"
    N_PLUS_ONE = "n_plus_one"
    MULTI_REGION = "multi_region"


@dataclass
class HAConfig:
    """High availability configuration."""

    strategy: FailoverStrategy
    health_check_interval_seconds: int = 10
    health_check_timeout_seconds: int = 5
    consecutive_failures_for_unhealthy: int = 3
    recovery_probe_interval_seconds: int = 30

    # Replication
    min_healthy_replicas: int = 2
    spread_across_zones: bool = True

    # Circuit breaker
    circuit_breaker_enabled: bool = True
    circuit_breaker_threshold: float = 0.5  # 50% error rate
    circuit_breaker_recovery_seconds: int = 60


@dataclass
class ReplicaHealth:
    """Health status of a replica."""

    replica_id: str
    endpoint: str
    healthy: bool = True
    last_check: Optional[datetime] = None
    consecutive_failures: int = 0
    last_error: Optional[str] = None


class HealthChecker:
    """
    Health checking for LLM serving replicas.
    """

    def __init__(self, config: HAConfig):
        self.config = config
        self.replicas: dict[str, ReplicaHealth] = {}

    async def check_replica(self, replica: ReplicaHealth) -> bool:
        """Check health of a single replica."""
        import aiohttp

        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    f"{replica.endpoint}/health",
                    timeout=aiohttp.ClientTimeout(
                        total=self.config.health_check_timeout_seconds
                    )
                ) as response:
                    if response.status == 200:
                        replica.consecutive_failures = 0
                        replica.healthy = True
                        replica.last_error = None
                    else:
                        self._record_failure(replica, f"Status {response.status}")
        except Exception as e:
            self._record_failure(replica, str(e))

        replica.last_check = datetime.utcnow()
        return replica.healthy

    def _record_failure(self, replica: ReplicaHealth, error: str):
        """Record a health check failure."""
        replica.consecutive_failures += 1
        replica.last_error = error

        if replica.consecutive_failures >= self.config.consecutive_failures_for_unhealthy:
            replica.healthy = False

    async def run_continuous_checks(self):
        """Run continuous health checks."""
        while True:
            tasks = [
                self.check_replica(replica)
                for replica in self.replicas.values()
            ]
            await asyncio.gather(*tasks)
            await asyncio.sleep(self.config.health_check_interval_seconds)

    def get_healthy_replicas(self) -> list[ReplicaHealth]:
        """Get list of healthy replicas."""
        return [r for r in self.replicas.values() if r.healthy]


class CircuitBreaker:
    """
    Circuit breaker for replica protection.
    """

    def __init__(self, config: HAConfig):
        self.config = config
        self.state: dict[str, str] = {}  # replica_id -> state (closed, open, half-open)
        self.failure_counts: dict[str, int] = {}
        self.request_counts: dict[str, int] = {}
        self.open_time: dict[str, datetime] = {}

    def record_success(self, replica_id: str):
        """Record successful request."""
        self.request_counts[replica_id] = self.request_counts.get(replica_id, 0) + 1

        if self.state.get(replica_id) == "half-open":
            self.state[replica_id] = "closed"
            self.failure_counts[replica_id] = 0

    def record_failure(self, replica_id: str):
        """Record failed request."""
        self.failure_counts[replica_id] = self.failure_counts.get(replica_id, 0) + 1
        self.request_counts[replica_id] = self.request_counts.get(replica_id, 0) + 1

        # Check if should open circuit
        if self._should_open(replica_id):
            self.state[replica_id] = "open"
            self.open_time[replica_id] = datetime.utcnow()

    def can_execute(self, replica_id: str) -> bool:
        """Check if request can be executed on replica."""
        state = self.state.get(replica_id, "closed")

        if state == "closed":
            return True

        if state == "open":
            # Check if recovery time has passed
            open_time = self.open_time.get(replica_id)
            if open_time:
                elapsed = (datetime.utcnow() - open_time).total_seconds()
                if elapsed >= self.config.circuit_breaker_recovery_seconds:
                    self.state[replica_id] = "half-open"
                    return True
            return False

        if state == "half-open":
            return True  # Allow test request

        return False

    def _should_open(self, replica_id: str) -> bool:
        """Check if circuit should open."""
        failures = self.failure_counts.get(replica_id, 0)
        requests = self.request_counts.get(replica_id, 0)

        if requests < 10:
            return False  # Not enough data

        error_rate = failures / requests
        return error_rate >= self.config.circuit_breaker_threshold


class FailoverManager:
    """
    Manages failover between replicas and regions.
    """

    def __init__(self, config: HAConfig):
        self.config = config
        self.health_checker = HealthChecker(config)
        self.circuit_breaker = CircuitBreaker(config) if config.circuit_breaker_enabled else None
        self.primary_replica: Optional[str] = None

    def select_replica(self) -> Optional[ReplicaHealth]:
        """Select a healthy replica for request."""
        healthy = self.health_checker.get_healthy_replicas()

        if not healthy:
            return None

        # Filter by circuit breaker
        if self.circuit_breaker:
            healthy = [
                r for r in healthy
                if self.circuit_breaker.can_execute(r.replica_id)
            ]

        if not healthy:
            return None

        # Strategy-based selection
        if self.config.strategy == FailoverStrategy.ACTIVE_PASSIVE:
            # Return primary if healthy, else first healthy backup
            primary = next(
                (r for r in healthy if r.replica_id == self.primary_replica),
                None
            )
            return primary or healthy[0]

        elif self.config.strategy == FailoverStrategy.ACTIVE_ACTIVE:
            # Load balance across all healthy
            return min(healthy, key=lambda r: r.consecutive_failures)

        return healthy[0]

    async def handle_failure(self, replica_id: str, error: str):
        """Handle replica failure."""
        replica = self.health_checker.replicas.get(replica_id)
        if replica:
            replica.consecutive_failures += 1
            replica.last_error = error

            if replica.consecutive_failures >= self.config.consecutive_failures_for_unhealthy:
                replica.healthy = False

        if self.circuit_breaker:
            self.circuit_breaker.record_failure(replica_id)


class DisasterRecovery:
    """
    Disaster recovery procedures for LLM serving.
    """

    def __init__(self, primary_region: str, backup_regions: list[str]):
        self.primary_region = primary_region
        self.backup_regions = backup_regions
        self.active_region = primary_region
        self.failover_in_progress = False

    async def initiate_failover(self, target_region: str):
        """Initiate failover to backup region."""
        if self.failover_in_progress:
            return

        self.failover_in_progress = True

        try:
            # 1. Verify target region is healthy
            if not await self._verify_region_health(target_region):
                raise Exception(f"Target region {target_region} is not healthy")

            # 2. Update DNS to point to new region
            await self._update_dns(target_region)

            # 3. Wait for DNS propagation
            await asyncio.sleep(60)

            # 4. Verify traffic is flowing to new region
            if not await self._verify_traffic(target_region):
                raise Exception("Traffic verification failed")

            self.active_region = target_region

        finally:
            self.failover_in_progress = False

    async def _verify_region_health(self, region: str) -> bool:
        """Verify region health."""
        # Implementation
        return True

    async def _update_dns(self, region: str):
        """Update DNS records."""
        # Implementation
        pass

    async def _verify_traffic(self, region: str) -> bool:
        """Verify traffic is flowing to region."""
        # Implementation
        return True

    async def failback(self):
        """Failback to primary region."""
        if self.active_region != self.primary_region:
            await self.initiate_failover(self.primary_region)


# Kubernetes HA configuration
K8S_HA_CONFIG = """
# PodDisruptionBudget for minimum availability
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: llm-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: llm-server
---
# Pod anti-affinity for zone spreading
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-server
spec:
  replicas: 4
  template:
    spec:
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: llm-server
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: llm-server
              topologyKey: kubernetes.io/hostname
"""
```

---

## 9.2.5 Performance Optimization

```python
"""
Performance optimization techniques for LLM serving.
"""

from dataclasses import dataclass, field
from typing import Optional, Any
import asyncio
import hashlib
import json


@dataclass
class OptimizationConfig:
    """Configuration for performance optimizations."""

    # Request batching
    enable_dynamic_batching: bool = True
    max_batch_size: int = 64
    batch_timeout_ms: int = 50

    # Caching
    enable_semantic_cache: bool = True
    cache_similarity_threshold: float = 0.95
    cache_ttl_seconds: int = 3600

    # Prefix caching
    enable_prefix_caching: bool = True
    prefix_cache_size_mb: int = 1024

    # Speculative decoding
    enable_speculative_decoding: bool = False
    draft_model_id: Optional[str] = None

    # Streaming
    enable_streaming: bool = True
    stream_chunk_size: int = 10


class DynamicBatcher:
    """
    Dynamic request batching for improved throughput.
    """

    def __init__(self, config: OptimizationConfig, model_server):
        self.config = config
        self.model_server = model_server
        self.pending_requests: list[dict] = []
        self.batch_event = asyncio.Event()
        self._running = False

    async def start(self):
        """Start the batching loop."""
        self._running = True
        asyncio.create_task(self._batch_loop())

    async def stop(self):
        """Stop the batching loop."""
        self._running = False

    async def submit(self, request: dict) -> asyncio.Future:
        """Submit a request for batching."""
        future = asyncio.get_event_loop().create_future()
        self.pending_requests.append({
            "request": request,
            "future": future
        })

        # Trigger batch if full
        if len(self.pending_requests) >= self.config.max_batch_size:
            self.batch_event.set()

        return await future

    async def _batch_loop(self):
        """Main batching loop."""
        while self._running:
            # Wait for batch timeout or batch full
            try:
                await asyncio.wait_for(
                    self.batch_event.wait(),
                    timeout=self.config.batch_timeout_ms / 1000
                )
            except asyncio.TimeoutError:
                pass

            self.batch_event.clear()

            if not self.pending_requests:
                continue

            # Extract batch
            batch = self.pending_requests[:self.config.max_batch_size]
            self.pending_requests = self.pending_requests[self.config.max_batch_size:]

            # Process batch
            try:
                results = await self._process_batch(batch)
                for item, result in zip(batch, results):
                    item["future"].set_result(result)
            except Exception as e:
                for item in batch:
                    item["future"].set_exception(e)

    async def _process_batch(self, batch: list[dict]) -> list[Any]:
        """Process a batch of requests."""
        prompts = [item["request"]["prompt"] for item in batch]
        max_tokens = max(item["request"].get("max_tokens", 100) for item in batch)

        results = await self.model_server.generate_batch(
            prompts=prompts,
            max_tokens=max_tokens
        )

        return results


class SemanticCache:
    """
    Semantic caching for similar queries.
    Uses embedding similarity to match cached responses.
    """

    def __init__(
        self,
        config: OptimizationConfig,
        embedding_model,
        cache_backend
    ):
        self.config = config
        self.embedding_model = embedding_model
        self.cache = cache_backend

    async def get(self, prompt: str) -> Optional[str]:
        """Get cached response for prompt."""
        # Generate embedding for query
        query_embedding = await self.embedding_model.embed(prompt)

        # Search cache for similar embeddings
        similar = await self.cache.similarity_search(
            embedding=query_embedding,
            threshold=self.config.cache_similarity_threshold,
            limit=1
        )

        if similar:
            return similar[0]["response"]

        return None

    async def set(self, prompt: str, response: str):
        """Cache a response."""
        embedding = await self.embedding_model.embed(prompt)

        await self.cache.store(
            key=self._hash_prompt(prompt),
            embedding=embedding,
            response=response,
            ttl=self.config.cache_ttl_seconds
        )

    def _hash_prompt(self, prompt: str) -> str:
        """Hash prompt for cache key."""
        return hashlib.sha256(prompt.encode()).hexdigest()


class ExactMatchCache:
    """
    Simple exact-match cache for repeated queries.
    """

    def __init__(self, config: OptimizationConfig, redis_client):
        self.config = config
        self.redis = redis_client

    async def get(self, prompt: str, params: dict) -> Optional[str]:
        """Get cached response."""
        key = self._make_key(prompt, params)
        cached = await self.redis.get(key)
        return cached.decode() if cached else None

    async def set(self, prompt: str, params: dict, response: str):
        """Cache a response."""
        key = self._make_key(prompt, params)
        await self.redis.setex(
            key,
            self.config.cache_ttl_seconds,
            response
        )

    def _make_key(self, prompt: str, params: dict) -> str:
        """Create cache key from prompt and params."""
        content = json.dumps({"prompt": prompt, "params": params}, sort_keys=True)
        return f"llm:cache:{hashlib.sha256(content.encode()).hexdigest()}"


class PrefixCache:
    """
    Prefix caching for shared context across requests.
    Leverages engine-level prefix caching (vLLM, SGLang).
    """

    def __init__(self, config: OptimizationConfig):
        self.config = config
        self.prefix_registry: dict[str, dict] = {}

    def register_prefix(self, prefix_id: str, prefix_text: str) -> str:
        """Register a prefix for caching."""
        self.prefix_registry[prefix_id] = {
            "text": prefix_text,
            "token_count": len(prefix_text.split()),  # Approximate
            "created_at": asyncio.get_event_loop().time()
        }
        return prefix_id

    def get_prefix(self, prefix_id: str) -> Optional[str]:
        """Get registered prefix."""
        entry = self.prefix_registry.get(prefix_id)
        return entry["text"] if entry else None

    def build_prompt_with_prefix(
        self,
        prefix_id: str,
        user_input: str
    ) -> tuple[str, Optional[str]]:
        """Build prompt with cached prefix."""
        prefix = self.get_prefix(prefix_id)
        if prefix:
            return f"{prefix}\n{user_input}", prefix_id
        return user_input, None


class StreamingOptimizer:
    """
    Optimizations for streaming responses.
    """

    def __init__(self, config: OptimizationConfig):
        self.config = config

    async def stream_with_buffering(
        self,
        generator,
        chunk_size: Optional[int] = None
    ):
        """Stream with token buffering for smoother output."""
        chunk_size = chunk_size or self.config.stream_chunk_size
        buffer = []

        async for token in generator:
            buffer.append(token)

            if len(buffer) >= chunk_size:
                yield "".join(buffer)
                buffer = []

        # Flush remaining
        if buffer:
            yield "".join(buffer)

    async def stream_with_timing(self, generator, target_tps: int = 50):
        """Stream with consistent timing for smooth UX."""
        import time

        target_interval = 1.0 / target_tps
        last_yield_time = time.time()

        async for token in generator:
            # Wait to maintain consistent rate
            elapsed = time.time() - last_yield_time
            if elapsed < target_interval:
                await asyncio.sleep(target_interval - elapsed)

            yield token
            last_yield_time = time.time()


class ContinuousBatchingOptimizer:
    """
    Optimizations for continuous batching.
    """

    def __init__(self, config: OptimizationConfig):
        self.config = config

    def calculate_optimal_batch_params(
        self,
        gpu_memory_gb: int,
        model_size_gb: float,
        avg_sequence_length: int
    ) -> dict:
        """Calculate optimal batching parameters."""

        # Available memory for KV cache
        available_memory = gpu_memory_gb - model_size_gb

        # Estimate KV cache size per token (approximate)
        kv_cache_per_token_mb = 0.5  # Varies by model

        # Calculate max tokens
        max_tokens = int(
            (available_memory * 1024) /
            kv_cache_per_token_mb
        )

        # Calculate max sequences
        max_sequences = max_tokens // avg_sequence_length

        return {
            "max_num_batched_tokens": max_tokens,
            "max_num_seqs": min(max_sequences, 256),
            "recommended_block_size": 16 if gpu_memory_gb >= 40 else 8
        }


# Prometheus metrics for optimization monitoring
OPTIMIZATION_METRICS = """
# Batching metrics
llm_batch_size_histogram:
  type: histogram
  help: Distribution of batch sizes
  buckets: [1, 4, 8, 16, 32, 64, 128]

llm_batch_wait_time_seconds:
  type: histogram
  help: Time requests wait for batching
  buckets: [0.01, 0.025, 0.05, 0.1, 0.25, 0.5]

# Cache metrics
llm_cache_hit_total:
  type: counter
  help: Total cache hits

llm_cache_miss_total:
  type: counter
  help: Total cache misses

llm_cache_hit_ratio:
  type: gauge
  help: Cache hit ratio

# Prefix cache metrics
llm_prefix_cache_hit_total:
  type: counter
  help: Prefix cache hits

llm_prefix_cache_tokens_saved_total:
  type: counter
  help: Total tokens saved by prefix caching
"""
```

---

## Appendices

### Appendix A: Architecture Diagrams

```
## Basic Architecture

┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│   Client    │────▶│    Load     │────▶│   Model     │
│             │     │  Balancer   │     │   Server    │
└─────────────┘     └─────────────┘     └─────────────┘


## Production Architecture

┌─────────────┐
│   Client    │
└──────┬──────┘
       │
       ▼
┌─────────────┐
│    Load     │
│  Balancer   │
└──────┬──────┘
       │
       ▼
┌─────────────┐
│    API      │◀───── Auth, Rate Limit
│   Gateway   │
└──────┬──────┘
       │
       ▼
┌─────────────┐
│  Request    │◀───── Model Selection
│   Router    │
└──────┬──────┘
       │
       ▼
┌─────────────┐
│   Cache     │◀───── Semantic Cache
│   Layer     │
└──────┬──────┘
       │
       ▼
┌─────────────┐
│   Queue     │◀───── Priority Queue
│  Manager    │
└──────┬──────┘
       │
       ▼
┌─────────────────────────────────────┐
│         Model Servers               │
│  ┌───────┐ ┌───────┐ ┌───────┐     │
│  │ vLLM  │ │ vLLM  │ │ vLLM  │     │
│  │ Pod 1 │ │ Pod 2 │ │ Pod 3 │     │
│  └───────┘ └───────┘ └───────┘     │
└─────────────────────────────────────┘
```

### Appendix B: Kubernetes Manifests

```yaml
# Complete production deployment
apiVersion: v1
kind: Namespace
metadata:
  name: llm-serving
  labels:
    istio-injection: enabled
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-server
  namespace: llm-serving
spec:
  replicas: 4
  selector:
    matchLabels:
      app: llm-server
  template:
    metadata:
      labels:
        app: llm-server
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8000"
    spec:
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: llm-server
      containers:
      - name: vllm
        image: vllm/vllm-openai:v0.6.0
        args:
        - "--model"
        - "meta-llama/Llama-3.1-70B-Instruct"
        - "--tensor-parallel-size"
        - "8"
        - "--gpu-memory-utilization"
        - "0.90"
        - "--enable-prefix-caching"
        ports:
        - containerPort: 8000
        resources:
          limits:
            nvidia.com/gpu: 8
            memory: 640Gi
          requests:
            nvidia.com/gpu: 8
            memory: 512Gi
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 300
          periodSeconds: 10
          failureThreshold: 3
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 300
          periodSeconds: 30
          failureThreshold: 5
        volumeMounts:
        - name: model-cache
          mountPath: /root/.cache/huggingface
      volumes:
      - name: model-cache
        persistentVolumeClaim:
          claimName: model-cache-pvc
```

### Appendix C: Load Balancer Configs

```nginx
# NGINX load balancer configuration
upstream llm_backend {
    least_conn;  # Least connections algorithm

    server llm-server-1:8000 weight=1 max_fails=3 fail_timeout=30s;
    server llm-server-2:8000 weight=1 max_fails=3 fail_timeout=30s;
    server llm-server-3:8000 weight=1 max_fails=3 fail_timeout=30s;

    keepalive 32;
}

server {
    listen 443 ssl http2;
    server_name llm-api.example.com;

    ssl_certificate /etc/nginx/ssl/server.crt;
    ssl_certificate_key /etc/nginx/ssl/server.key;

    # Timeouts for long-running LLM requests
    proxy_connect_timeout 10s;
    proxy_send_timeout 120s;
    proxy_read_timeout 120s;

    # Buffering for streaming
    proxy_buffering off;
    proxy_cache off;

    location /v1/ {
        proxy_pass http://llm_backend;
        proxy_http_version 1.1;
        proxy_set_header Connection "";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Request-ID $request_id;

        # SSE support
        proxy_set_header Accept-Encoding "";
        chunked_transfer_encoding on;
    }

    location /health {
        proxy_pass http://llm_backend;
        proxy_connect_timeout 5s;
        proxy_read_timeout 5s;
    }
}
```

### Appendix D: HA Testing Procedures

```markdown
## HA Testing Checklist

### 1. Replica Failure Test
- [ ] Kill one replica
- [ ] Verify traffic routes to remaining replicas
- [ ] Verify no requests are dropped
- [ ] Verify replacement replica starts
- [ ] Verify traffic rebalances

### 2. Zone Failure Test
- [ ] Simulate zone outage
- [ ] Verify cross-zone failover
- [ ] Measure failover latency
- [ ] Verify no data loss
- [ ] Verify recovery when zone returns

### 3. Load Balancer Failure Test
- [ ] Fail primary load balancer
- [ ] Verify failover to secondary
- [ ] Measure failover time
- [ ] Test failback

### 4. Circuit Breaker Test
- [ ] Induce errors on one replica
- [ ] Verify circuit opens
- [ ] Verify traffic routes around failed replica
- [ ] Verify circuit half-opens after timeout
- [ ] Verify circuit closes on recovery

### 5. Chaos Engineering
- [ ] Random pod kills (chaos monkey)
- [ ] Network latency injection
- [ ] CPU/memory stress tests
- [ ] DNS failure simulation
```

---

## Summary

This guide covered:

1. **Architecture Components**: Load balancers, API gateways, routers, model servers, queues, caches
2. **Serving Patterns**: Single-model, multi-model with routing, tiered cascading, federated multi-region
3. **Scaling Strategies**: Horizontal (replicas), vertical (GPU upgrade), predictive, cost-optimized
4. **High Availability**: Health checking, circuit breakers, failover strategies, disaster recovery
5. **Performance Optimization**: Dynamic batching, semantic caching, prefix caching, streaming

Key takeaways:
- Start simple (single model) and evolve architecture as needs grow
- Use tiered serving for cost optimization with quality guarantees
- Implement proper health checking and circuit breakers for reliability
- Leverage engine-level optimizations (prefix caching, continuous batching)
- Design for multi-region from the start if global availability is needed
