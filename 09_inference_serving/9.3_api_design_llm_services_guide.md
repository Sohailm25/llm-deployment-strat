# Document 9.3: API Design for LLM Services Guide

## Purpose

This guide provides comprehensive patterns for designing robust, user-friendly APIs for LLM services. It covers API paradigms (REST, GraphQL, gRPC), OpenAI API compatibility, streaming with Server-Sent Events (SSE), authentication, rate limiting, versioning, and SDK generation. Following these patterns ensures your LLM service is easy to integrate, maintain, and scale.

---

## Prerequisites

- **Infrastructure**: API gateway, load balancer, authentication service
- **Tools**: FastAPI/Flask, OpenAPI/Swagger, gRPC tools
- **Knowledge**: REST API design, authentication protocols, HTTP streaming
- **Standards**: OpenAI API specification familiarity

---

## 9.3.1 API Paradigms

```python
"""
Comparison of API paradigms for LLM services.
"""

from dataclasses import dataclass, field
from enum import Enum
from typing import Optional


class APIParadigm(Enum):
    REST = "rest"
    GRAPHQL = "graphql"
    GRPC = "grpc"
    WEBSOCKET = "websocket"


@dataclass
class ParadigmCharacteristics:
    """Characteristics of an API paradigm."""

    paradigm: APIParadigm
    transport: str
    serialization: str
    streaming_support: str
    browser_native: bool
    tooling_maturity: str
    best_for: list[str] = field(default_factory=list)


API_PARADIGMS = {
    APIParadigm.REST: ParadigmCharacteristics(
        paradigm=APIParadigm.REST,
        transport="HTTP/1.1, HTTP/2",
        serialization="JSON",
        streaming_support="SSE (Server-Sent Events)",
        browser_native=True,
        tooling_maturity="Excellent",
        best_for=[
            "Public APIs",
            "OpenAI compatibility",
            "Simple integrations",
            "Browser clients"
        ]
    ),

    APIParadigm.GRAPHQL: ParadigmCharacteristics(
        paradigm=APIParadigm.GRAPHQL,
        transport="HTTP/1.1, HTTP/2, WebSocket",
        serialization="JSON",
        streaming_support="Subscriptions (WebSocket)",
        browser_native=True,
        tooling_maturity="Good",
        best_for=[
            "Complex queries",
            "Multiple model orchestration",
            "Flexible client requirements",
            "Reducing over-fetching"
        ]
    ),

    APIParadigm.GRPC: ParadigmCharacteristics(
        paradigm=APIParadigm.GRPC,
        transport="HTTP/2",
        serialization="Protocol Buffers",
        streaming_support="Native bidirectional",
        browser_native=False,  # Requires grpc-web
        tooling_maturity="Excellent",
        best_for=[
            "Internal services",
            "High-performance requirements",
            "Microservice communication",
            "Strong typing needs"
        ]
    ),

    APIParadigm.WEBSOCKET: ParadigmCharacteristics(
        paradigm=APIParadigm.WEBSOCKET,
        transport="WebSocket",
        serialization="JSON/Binary",
        streaming_support="Native bidirectional",
        browser_native=True,
        tooling_maturity="Good",
        best_for=[
            "Real-time applications",
            "Interactive chat",
            "Bidirectional streaming",
            "Long-lived connections"
        ]
    )
}


def select_paradigm(requirements: dict) -> APIParadigm:
    """Select best paradigm based on requirements."""

    if requirements.get("openai_compatible"):
        return APIParadigm.REST

    if requirements.get("browser_client") and requirements.get("streaming"):
        return APIParadigm.REST  # SSE is simpler than WebSocket for one-way

    if requirements.get("bidirectional_streaming"):
        return APIParadigm.WEBSOCKET

    if requirements.get("internal_service") and requirements.get("high_performance"):
        return APIParadigm.GRPC

    if requirements.get("complex_queries"):
        return APIParadigm.GRAPHQL

    return APIParadigm.REST  # Default
```

### REST API Implementation

```python
"""
REST API implementation for LLM services using FastAPI.
"""

from fastapi import FastAPI, HTTPException, Depends, Header, BackgroundTasks
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from typing import Optional, List, Union
import asyncio
import json
import time
from datetime import datetime


app = FastAPI(
    title="LLM Service API",
    version="1.0.0",
    description="OpenAI-compatible LLM inference API"
)


# Request/Response Models
class Message(BaseModel):
    role: str = Field(..., description="Role: system, user, or assistant")
    content: str = Field(..., description="Message content")
    name: Optional[str] = Field(None, description="Optional name for the message author")


class ChatCompletionRequest(BaseModel):
    model: str = Field(..., description="Model ID to use")
    messages: List[Message] = Field(..., description="List of messages")
    max_tokens: Optional[int] = Field(None, description="Maximum tokens to generate")
    temperature: Optional[float] = Field(0.7, ge=0, le=2, description="Sampling temperature")
    top_p: Optional[float] = Field(1.0, ge=0, le=1, description="Nucleus sampling parameter")
    n: Optional[int] = Field(1, ge=1, le=10, description="Number of completions")
    stream: Optional[bool] = Field(False, description="Enable streaming")
    stop: Optional[Union[str, List[str]]] = Field(None, description="Stop sequences")
    presence_penalty: Optional[float] = Field(0, ge=-2, le=2)
    frequency_penalty: Optional[float] = Field(0, ge=-2, le=2)
    user: Optional[str] = Field(None, description="User identifier")


class Usage(BaseModel):
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int


class ChatCompletionChoice(BaseModel):
    index: int
    message: Message
    finish_reason: str


class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[ChatCompletionChoice]
    usage: Usage


class CompletionRequest(BaseModel):
    model: str
    prompt: Union[str, List[str]]
    max_tokens: Optional[int] = 16
    temperature: Optional[float] = 1.0
    top_p: Optional[float] = 1.0
    n: Optional[int] = 1
    stream: Optional[bool] = False
    logprobs: Optional[int] = None
    stop: Optional[Union[str, List[str]]] = None
    echo: Optional[bool] = False
    suffix: Optional[str] = None
    user: Optional[str] = None


class EmbeddingRequest(BaseModel):
    model: str
    input: Union[str, List[str]]
    encoding_format: Optional[str] = "float"
    user: Optional[str] = None


# Endpoints
@app.post("/v1/chat/completions", response_model=ChatCompletionResponse)
async def chat_completions(
    request: ChatCompletionRequest,
    authorization: str = Header(...)
):
    """
    Create a chat completion.
    OpenAI-compatible endpoint.
    """
    # Validate API key
    api_key = authorization.replace("Bearer ", "")
    await validate_api_key(api_key)

    if request.stream:
        return StreamingResponse(
            stream_chat_completion(request),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no"  # Disable nginx buffering
            }
        )

    # Non-streaming response
    result = await generate_chat_completion(request)
    return result


@app.post("/v1/completions")
async def completions(
    request: CompletionRequest,
    authorization: str = Header(...)
):
    """
    Create a completion (legacy endpoint).
    """
    api_key = authorization.replace("Bearer ", "")
    await validate_api_key(api_key)

    if request.stream:
        return StreamingResponse(
            stream_completion(request),
            media_type="text/event-stream"
        )

    result = await generate_completion(request)
    return result


@app.post("/v1/embeddings")
async def embeddings(
    request: EmbeddingRequest,
    authorization: str = Header(...)
):
    """
    Create embeddings for input text.
    """
    api_key = authorization.replace("Bearer ", "")
    await validate_api_key(api_key)

    result = await generate_embeddings(request)
    return result


@app.get("/v1/models")
async def list_models(authorization: str = Header(...)):
    """
    List available models.
    """
    return {
        "object": "list",
        "data": [
            {
                "id": "llama-3.1-8b-instruct",
                "object": "model",
                "created": 1700000000,
                "owned_by": "organization"
            },
            {
                "id": "llama-3.1-70b-instruct",
                "object": "model",
                "created": 1700000000,
                "owned_by": "organization"
            }
        ]
    }


@app.get("/health")
async def health_check():
    """Health check endpoint."""
    return {"status": "healthy", "timestamp": datetime.utcnow().isoformat()}


# Helper functions
async def validate_api_key(api_key: str):
    """Validate API key."""
    # Implementation
    pass


async def generate_chat_completion(request: ChatCompletionRequest) -> ChatCompletionResponse:
    """Generate non-streaming chat completion."""
    # Implementation
    pass


async def stream_chat_completion(request: ChatCompletionRequest):
    """Stream chat completion using SSE."""
    # SSE format: data: {json}\n\n
    request_id = f"chatcmpl-{int(time.time()*1000)}"

    async for token in model_generate(request):
        chunk = {
            "id": request_id,
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": request.model,
            "choices": [{
                "index": 0,
                "delta": {"content": token},
                "finish_reason": None
            }]
        }
        yield f"data: {json.dumps(chunk)}\n\n"

    # Final chunk with finish_reason
    final_chunk = {
        "id": request_id,
        "object": "chat.completion.chunk",
        "created": int(time.time()),
        "model": request.model,
        "choices": [{
            "index": 0,
            "delta": {},
            "finish_reason": "stop"
        }]
    }
    yield f"data: {json.dumps(final_chunk)}\n\n"
    yield "data: [DONE]\n\n"


async def model_generate(request):
    """Generator for model tokens."""
    # Placeholder - actual implementation calls model
    for word in "Hello! How can I help you today?".split():
        yield word + " "
        await asyncio.sleep(0.05)
```

### gRPC Implementation

```python
"""
gRPC API implementation for LLM services.
"""

# llm_service.proto
GRPC_PROTO = '''
syntax = "proto3";

package llm;

service LLMService {
    // Unary completion
    rpc Complete(CompletionRequest) returns (CompletionResponse);

    // Streaming completion
    rpc StreamComplete(CompletionRequest) returns (stream CompletionChunk);

    // Chat completion
    rpc ChatComplete(ChatRequest) returns (ChatResponse);

    // Streaming chat
    rpc StreamChatComplete(ChatRequest) returns (stream ChatChunk);

    // Embeddings
    rpc GetEmbeddings(EmbeddingRequest) returns (EmbeddingResponse);
}

message CompletionRequest {
    string model = 1;
    string prompt = 2;
    int32 max_tokens = 3;
    float temperature = 4;
    float top_p = 5;
    repeated string stop = 6;
}

message CompletionResponse {
    string id = 1;
    string text = 2;
    int32 prompt_tokens = 3;
    int32 completion_tokens = 4;
    string finish_reason = 5;
}

message CompletionChunk {
    string id = 1;
    string text = 2;
    bool done = 3;
    string finish_reason = 4;
}

message ChatMessage {
    string role = 1;
    string content = 2;
}

message ChatRequest {
    string model = 1;
    repeated ChatMessage messages = 2;
    int32 max_tokens = 3;
    float temperature = 4;
    float top_p = 5;
    repeated string stop = 6;
}

message ChatResponse {
    string id = 1;
    ChatMessage message = 2;
    int32 prompt_tokens = 3;
    int32 completion_tokens = 4;
    string finish_reason = 5;
}

message ChatChunk {
    string id = 1;
    string content = 2;
    bool done = 3;
    string finish_reason = 4;
}

message EmbeddingRequest {
    string model = 1;
    repeated string texts = 2;
}

message EmbeddingResponse {
    repeated Embedding embeddings = 1;
    int32 total_tokens = 2;
}

message Embedding {
    int32 index = 1;
    repeated float vector = 2;
}
'''


# Python gRPC server implementation
import grpc
from concurrent import futures
import asyncio


class LLMServiceServicer:
    """gRPC service implementation."""

    def __init__(self, model_server):
        self.model_server = model_server

    async def Complete(self, request, context):
        """Unary completion."""
        result = await self.model_server.generate(
            prompt=request.prompt,
            max_tokens=request.max_tokens,
            temperature=request.temperature
        )

        return CompletionResponse(
            id=f"cmpl-{int(time.time()*1000)}",
            text=result.text,
            prompt_tokens=result.prompt_tokens,
            completion_tokens=result.completion_tokens,
            finish_reason="stop"
        )

    async def StreamComplete(self, request, context):
        """Streaming completion."""
        request_id = f"cmpl-{int(time.time()*1000)}"

        async for token in self.model_server.stream(
            prompt=request.prompt,
            max_tokens=request.max_tokens,
            temperature=request.temperature
        ):
            yield CompletionChunk(
                id=request_id,
                text=token,
                done=False
            )

        yield CompletionChunk(
            id=request_id,
            text="",
            done=True,
            finish_reason="stop"
        )

    async def ChatComplete(self, request, context):
        """Chat completion."""
        messages = [
            {"role": msg.role, "content": msg.content}
            for msg in request.messages
        ]

        result = await self.model_server.chat(
            messages=messages,
            max_tokens=request.max_tokens,
            temperature=request.temperature
        )

        return ChatResponse(
            id=f"chat-{int(time.time()*1000)}",
            message=ChatMessage(
                role="assistant",
                content=result.content
            ),
            prompt_tokens=result.prompt_tokens,
            completion_tokens=result.completion_tokens,
            finish_reason="stop"
        )

    async def StreamChatComplete(self, request, context):
        """Streaming chat completion."""
        messages = [
            {"role": msg.role, "content": msg.content}
            for msg in request.messages
        ]

        request_id = f"chat-{int(time.time()*1000)}"

        async for token in self.model_server.stream_chat(
            messages=messages,
            max_tokens=request.max_tokens
        ):
            yield ChatChunk(
                id=request_id,
                content=token,
                done=False
            )

        yield ChatChunk(
            id=request_id,
            content="",
            done=True,
            finish_reason="stop"
        )


def serve(port: int = 50051):
    """Start gRPC server."""
    server = grpc.aio.server(
        futures.ThreadPoolExecutor(max_workers=10),
        options=[
            ('grpc.max_receive_message_length', 100 * 1024 * 1024),  # 100MB
            ('grpc.max_send_message_length', 100 * 1024 * 1024),
        ]
    )
    # Add servicer
    # llm_pb2_grpc.add_LLMServiceServicer_to_server(servicer, server)
    server.add_insecure_port(f'[::]:{port}')
    return server
```

---

## 9.3.2 OpenAI API Compatibility

```python
"""
OpenAI API compatibility layer for LLM services.
Enables drop-in replacement for OpenAI SDK.
"""

from dataclasses import dataclass, field
from typing import Optional, List, Union, Dict, Any
from enum import Enum
import json
import time


class OpenAIEndpoint(Enum):
    CHAT_COMPLETIONS = "/v1/chat/completions"
    COMPLETIONS = "/v1/completions"
    EMBEDDINGS = "/v1/embeddings"
    MODELS = "/v1/models"
    MODERATIONS = "/v1/moderations"


@dataclass
class OpenAICompatConfig:
    """Configuration for OpenAI compatibility."""

    # Model mapping
    model_aliases: Dict[str, str] = field(default_factory=dict)

    # Feature support
    support_function_calling: bool = True
    support_vision: bool = False
    support_json_mode: bool = True

    # Response format
    include_usage: bool = True
    include_system_fingerprint: bool = True


class OpenAICompatibilityLayer:
    """
    Translates between OpenAI API format and internal format.
    """

    def __init__(self, config: OpenAICompatConfig):
        self.config = config

    def translate_request(
        self,
        endpoint: OpenAIEndpoint,
        request: dict
    ) -> dict:
        """Translate OpenAI request to internal format."""

        if endpoint == OpenAIEndpoint.CHAT_COMPLETIONS:
            return self._translate_chat_request(request)
        elif endpoint == OpenAIEndpoint.COMPLETIONS:
            return self._translate_completion_request(request)
        elif endpoint == OpenAIEndpoint.EMBEDDINGS:
            return self._translate_embedding_request(request)

        return request

    def _translate_chat_request(self, request: dict) -> dict:
        """Translate chat completion request."""
        # Map model alias
        model = request.get("model", "")
        if model in self.config.model_aliases:
            request["model"] = self.config.model_aliases[model]

        # Handle function calling
        if "functions" in request or "tools" in request:
            if not self.config.support_function_calling:
                raise ValueError("Function calling not supported")
            request = self._translate_function_calling(request)

        # Handle response format
        if request.get("response_format", {}).get("type") == "json_object":
            if not self.config.support_json_mode:
                raise ValueError("JSON mode not supported")
            request["json_mode"] = True

        return request

    def _translate_completion_request(self, request: dict) -> dict:
        """Translate legacy completion request."""
        # Convert to chat format internally
        prompt = request.get("prompt", "")
        if isinstance(prompt, list):
            prompt = prompt[0]

        return {
            "messages": [{"role": "user", "content": prompt}],
            "model": self.config.model_aliases.get(
                request.get("model", ""),
                request.get("model", "")
            ),
            "max_tokens": request.get("max_tokens", 16),
            "temperature": request.get("temperature", 1.0),
            "top_p": request.get("top_p", 1.0),
            "stop": request.get("stop"),
            "echo": request.get("echo", False),
            "logprobs": request.get("logprobs"),
            "_legacy_completion": True
        }

    def _translate_embedding_request(self, request: dict) -> dict:
        """Translate embedding request."""
        inputs = request.get("input", [])
        if isinstance(inputs, str):
            inputs = [inputs]

        return {
            "texts": inputs,
            "model": self.config.model_aliases.get(
                request.get("model", ""),
                request.get("model", "")
            ),
            "encoding_format": request.get("encoding_format", "float")
        }

    def _translate_function_calling(self, request: dict) -> dict:
        """Translate function calling to tool use format."""
        # Convert functions to tools format
        functions = request.pop("functions", [])
        if functions:
            request["tools"] = [
                {"type": "function", "function": func}
                for func in functions
            ]

        function_call = request.pop("function_call", None)
        if function_call:
            if function_call == "auto":
                request["tool_choice"] = "auto"
            elif function_call == "none":
                request["tool_choice"] = "none"
            elif isinstance(function_call, dict):
                request["tool_choice"] = {
                    "type": "function",
                    "function": {"name": function_call["name"]}
                }

        return request

    def translate_response(
        self,
        endpoint: OpenAIEndpoint,
        response: dict,
        request: dict
    ) -> dict:
        """Translate internal response to OpenAI format."""

        if endpoint == OpenAIEndpoint.CHAT_COMPLETIONS:
            return self._translate_chat_response(response, request)
        elif endpoint == OpenAIEndpoint.COMPLETIONS:
            return self._translate_completion_response(response, request)
        elif endpoint == OpenAIEndpoint.EMBEDDINGS:
            return self._translate_embedding_response(response)

        return response

    def _translate_chat_response(self, response: dict, request: dict) -> dict:
        """Translate to OpenAI chat completion format."""
        return {
            "id": response.get("id", f"chatcmpl-{int(time.time()*1000)}"),
            "object": "chat.completion",
            "created": int(time.time()),
            "model": request.get("model", "unknown"),
            "choices": [
                {
                    "index": i,
                    "message": {
                        "role": "assistant",
                        "content": choice.get("content", ""),
                        **({"tool_calls": choice["tool_calls"]} if "tool_calls" in choice else {})
                    },
                    "logprobs": choice.get("logprobs"),
                    "finish_reason": choice.get("finish_reason", "stop")
                }
                for i, choice in enumerate(response.get("choices", []))
            ],
            **({"usage": response["usage"]} if self.config.include_usage and "usage" in response else {}),
            **({"system_fingerprint": response.get("system_fingerprint")}
               if self.config.include_system_fingerprint else {})
        }

    def _translate_completion_response(self, response: dict, request: dict) -> dict:
        """Translate to OpenAI completion format (legacy)."""
        return {
            "id": response.get("id", f"cmpl-{int(time.time()*1000)}"),
            "object": "text_completion",
            "created": int(time.time()),
            "model": request.get("model", "unknown"),
            "choices": [
                {
                    "index": i,
                    "text": choice.get("content", ""),
                    "logprobs": choice.get("logprobs"),
                    "finish_reason": choice.get("finish_reason", "stop")
                }
                for i, choice in enumerate(response.get("choices", []))
            ],
            "usage": response.get("usage", {})
        }

    def _translate_embedding_response(self, response: dict) -> dict:
        """Translate to OpenAI embedding format."""
        return {
            "object": "list",
            "data": [
                {
                    "object": "embedding",
                    "index": i,
                    "embedding": emb
                }
                for i, emb in enumerate(response.get("embeddings", []))
            ],
            "model": response.get("model", "unknown"),
            "usage": {
                "prompt_tokens": response.get("total_tokens", 0),
                "total_tokens": response.get("total_tokens", 0)
            }
        }


# Example: Drop-in replacement with openai SDK
OPENAI_SDK_USAGE = '''
"""
Using OpenAI SDK with custom LLM service.
"""

from openai import OpenAI

# Point to your LLM service instead of OpenAI
client = OpenAI(
    api_key="your-api-key",
    base_url="https://your-llm-service.com/v1"
)

# Works exactly like OpenAI
response = client.chat.completions.create(
    model="llama-3.1-70b-instruct",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7,
    max_tokens=100
)

print(response.choices[0].message.content)

# Streaming also works
stream = client.chat.completions.create(
    model="llama-3.1-70b-instruct",
    messages=[{"role": "user", "content": "Tell me a story"}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="")
'''
```

---

## 9.3.3 API Design Best Practices

### Request Design

```python
"""
Request design patterns for LLM APIs.
"""

from pydantic import BaseModel, Field, validator, root_validator
from typing import Optional, List, Union, Any, Literal
import tiktoken


class InputValidation(BaseModel):
    """Input validation utilities."""

    @staticmethod
    def validate_prompt_length(
        prompt: str,
        max_tokens: int,
        model_context_length: int = 8192
    ) -> tuple[bool, str]:
        """Validate prompt doesn't exceed context length."""
        try:
            enc = tiktoken.get_encoding("cl100k_base")
            prompt_tokens = len(enc.encode(prompt))

            available_tokens = model_context_length - prompt_tokens

            if available_tokens <= 0:
                return False, f"Prompt ({prompt_tokens} tokens) exceeds context length ({model_context_length})"

            if max_tokens > available_tokens:
                return False, f"max_tokens ({max_tokens}) exceeds available space ({available_tokens})"

            return True, ""
        except Exception as e:
            return False, str(e)


class ValidatedChatRequest(BaseModel):
    """Chat request with comprehensive validation."""

    model: str = Field(
        ...,
        description="Model ID",
        min_length=1,
        max_length=100
    )

    messages: List[dict] = Field(
        ...,
        description="Conversation messages",
        min_items=1,
        max_items=1000
    )

    max_tokens: Optional[int] = Field(
        None,
        description="Maximum tokens to generate",
        ge=1,
        le=32768
    )

    temperature: Optional[float] = Field(
        0.7,
        description="Sampling temperature",
        ge=0.0,
        le=2.0
    )

    top_p: Optional[float] = Field(
        1.0,
        description="Nucleus sampling",
        ge=0.0,
        le=1.0
    )

    top_k: Optional[int] = Field(
        None,
        description="Top-k sampling",
        ge=1,
        le=100
    )

    stream: Optional[bool] = Field(
        False,
        description="Enable streaming"
    )

    stop: Optional[Union[str, List[str]]] = Field(
        None,
        description="Stop sequences",
        max_items=4
    )

    presence_penalty: Optional[float] = Field(
        0.0,
        ge=-2.0,
        le=2.0
    )

    frequency_penalty: Optional[float] = Field(
        0.0,
        ge=-2.0,
        le=2.0
    )

    user: Optional[str] = Field(
        None,
        description="User identifier for tracking",
        max_length=256
    )

    response_format: Optional[dict] = Field(
        None,
        description="Response format specification"
    )

    seed: Optional[int] = Field(
        None,
        description="Random seed for reproducibility"
    )

    @validator("messages")
    def validate_messages(cls, messages):
        """Validate message format."""
        valid_roles = {"system", "user", "assistant", "tool"}

        for msg in messages:
            if "role" not in msg:
                raise ValueError("Each message must have a 'role' field")
            if msg["role"] not in valid_roles:
                raise ValueError(f"Invalid role: {msg['role']}")
            if "content" not in msg and "tool_calls" not in msg:
                raise ValueError("Each message must have 'content' or 'tool_calls'")

        return messages

    @validator("stop")
    def validate_stop(cls, stop):
        """Convert single stop to list."""
        if isinstance(stop, str):
            return [stop]
        return stop

    @root_validator
    def validate_sampling_params(cls, values):
        """Validate sampling parameter combinations."""
        temp = values.get("temperature", 0.7)
        top_p = values.get("top_p", 1.0)

        # Warn if both temperature and top_p are modified
        if temp != 0.7 and top_p != 1.0:
            # Both modified - this is fine but unusual
            pass

        return values


class TokenCounter:
    """Token counting for request validation and usage tracking."""

    def __init__(self, model: str = "cl100k_base"):
        self.encoding = tiktoken.get_encoding(model)

    def count_tokens(self, text: str) -> int:
        """Count tokens in text."""
        return len(self.encoding.encode(text))

    def count_messages(self, messages: List[dict]) -> int:
        """Count tokens in chat messages."""
        total = 0
        for msg in messages:
            # Role and message overhead
            total += 4  # <im_start>{role}\n ... <im_end>\n
            total += self.count_tokens(msg.get("content", ""))

        total += 2  # <im_start>assistant
        return total

    def estimate_cost(
        self,
        prompt_tokens: int,
        completion_tokens: int,
        model: str
    ) -> float:
        """Estimate cost based on token usage."""
        # Example pricing (per 1M tokens)
        pricing = {
            "llama-3.1-8b": {"input": 0.10, "output": 0.10},
            "llama-3.1-70b": {"input": 1.00, "output": 1.00},
            "gpt-4": {"input": 30.00, "output": 60.00}
        }

        if model not in pricing:
            model = "llama-3.1-8b"  # Default

        price = pricing[model]
        cost = (prompt_tokens * price["input"] + completion_tokens * price["output"]) / 1_000_000

        return cost


class RateLimitConfig(BaseModel):
    """Rate limiting configuration."""

    requests_per_minute: int = 60
    requests_per_day: int = 10000
    tokens_per_minute: int = 100000
    tokens_per_day: int = 1000000
    concurrent_requests: int = 10


class RateLimiter:
    """Token bucket rate limiter."""

    def __init__(self, config: RateLimitConfig):
        self.config = config
        self.request_buckets: dict[str, dict] = {}
        self.token_buckets: dict[str, dict] = {}

    async def check_limit(
        self,
        api_key: str,
        estimated_tokens: int
    ) -> tuple[bool, Optional[dict]]:
        """Check if request is within rate limits."""
        import time

        now = time.time()

        # Initialize buckets
        if api_key not in self.request_buckets:
            self.request_buckets[api_key] = {
                "minute": {"count": 0, "reset": now + 60},
                "day": {"count": 0, "reset": now + 86400}
            }
            self.token_buckets[api_key] = {
                "minute": {"count": 0, "reset": now + 60},
                "day": {"count": 0, "reset": now + 86400}
            }

        req_bucket = self.request_buckets[api_key]
        token_bucket = self.token_buckets[api_key]

        # Reset expired buckets
        for bucket in [req_bucket, token_bucket]:
            for period in ["minute", "day"]:
                if now > bucket[period]["reset"]:
                    bucket[period]["count"] = 0
                    bucket[period]["reset"] = now + (60 if period == "minute" else 86400)

        # Check limits
        if req_bucket["minute"]["count"] >= self.config.requests_per_minute:
            return False, {
                "error": "rate_limit_exceeded",
                "type": "requests",
                "period": "minute",
                "retry_after": int(req_bucket["minute"]["reset"] - now)
            }

        if token_bucket["minute"]["count"] + estimated_tokens > self.config.tokens_per_minute:
            return False, {
                "error": "rate_limit_exceeded",
                "type": "tokens",
                "period": "minute",
                "retry_after": int(token_bucket["minute"]["reset"] - now)
            }

        # Update counts
        req_bucket["minute"]["count"] += 1
        req_bucket["day"]["count"] += 1
        token_bucket["minute"]["count"] += estimated_tokens
        token_bucket["day"]["count"] += estimated_tokens

        return True, None

    def get_remaining(self, api_key: str) -> dict:
        """Get remaining quota."""
        if api_key not in self.request_buckets:
            return {
                "requests_remaining_minute": self.config.requests_per_minute,
                "tokens_remaining_minute": self.config.tokens_per_minute
            }

        req_bucket = self.request_buckets[api_key]
        token_bucket = self.token_buckets[api_key]

        return {
            "requests_remaining_minute": max(0, self.config.requests_per_minute - req_bucket["minute"]["count"]),
            "tokens_remaining_minute": max(0, self.config.tokens_per_minute - token_bucket["minute"]["count"])
        }
```

### Response Design

```python
"""
Response design patterns for LLM APIs.
"""

from fastapi import Request
from fastapi.responses import StreamingResponse, JSONResponse
from typing import AsyncGenerator, Optional
import json
import asyncio


class SSEResponse(StreamingResponse):
    """
    Server-Sent Events response for streaming.
    """

    def __init__(
        self,
        generator: AsyncGenerator,
        status_code: int = 200,
        headers: Optional[dict] = None
    ):
        headers = headers or {}
        headers.update({
            "Content-Type": "text/event-stream",
            "Cache-Control": "no-cache, no-store, must-revalidate",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # Disable nginx buffering
            "Transfer-Encoding": "chunked"
        })

        super().__init__(
            content=generator,
            status_code=status_code,
            headers=headers,
            media_type="text/event-stream"
        )


async def stream_with_sse(
    request_id: str,
    model: str,
    generator: AsyncGenerator,
    include_usage: bool = True
) -> AsyncGenerator[str, None]:
    """
    Stream LLM response using SSE format.
    Compatible with OpenAI streaming format.
    """
    import time

    prompt_tokens = 0
    completion_tokens = 0

    try:
        async for token in generator:
            completion_tokens += 1

            chunk = {
                "id": request_id,
                "object": "chat.completion.chunk",
                "created": int(time.time()),
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {"content": token},
                    "finish_reason": None
                }]
            }

            yield f"data: {json.dumps(chunk)}\n\n"

        # Final chunk with finish_reason
        final_chunk = {
            "id": request_id,
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": model,
            "choices": [{
                "index": 0,
                "delta": {},
                "finish_reason": "stop"
            }]
        }

        if include_usage:
            final_chunk["usage"] = {
                "prompt_tokens": prompt_tokens,
                "completion_tokens": completion_tokens,
                "total_tokens": prompt_tokens + completion_tokens
            }

        yield f"data: {json.dumps(final_chunk)}\n\n"
        yield "data: [DONE]\n\n"

    except Exception as e:
        error_chunk = {
            "id": request_id,
            "object": "error",
            "error": {
                "message": str(e),
                "type": "internal_error"
            }
        }
        yield f"data: {json.dumps(error_chunk)}\n\n"


class UsageMetadata:
    """Track and report usage metadata."""

    def __init__(self):
        self.prompt_tokens = 0
        self.completion_tokens = 0
        self.start_time = None
        self.end_time = None
        self.model = None

    def start(self, prompt_tokens: int, model: str):
        """Start tracking."""
        import time
        self.prompt_tokens = prompt_tokens
        self.model = model
        self.start_time = time.time()

    def add_completion_tokens(self, count: int):
        """Add completion tokens."""
        self.completion_tokens += count

    def finish(self) -> dict:
        """Finish and return usage."""
        import time
        self.end_time = time.time()

        return {
            "prompt_tokens": self.prompt_tokens,
            "completion_tokens": self.completion_tokens,
            "total_tokens": self.prompt_tokens + self.completion_tokens,
            "latency_ms": int((self.end_time - self.start_time) * 1000),
            "tokens_per_second": self.completion_tokens / (self.end_time - self.start_time)
        }


class ErrorResponse:
    """Standard error response format."""

    @staticmethod
    def create(
        status_code: int,
        error_type: str,
        message: str,
        param: Optional[str] = None,
        code: Optional[str] = None
    ) -> JSONResponse:
        """Create standardized error response."""
        return JSONResponse(
            status_code=status_code,
            content={
                "error": {
                    "message": message,
                    "type": error_type,
                    "param": param,
                    "code": code
                }
            }
        )

    @staticmethod
    def validation_error(message: str, param: str) -> JSONResponse:
        return ErrorResponse.create(400, "invalid_request_error", message, param)

    @staticmethod
    def authentication_error() -> JSONResponse:
        return ErrorResponse.create(401, "authentication_error", "Invalid API key")

    @staticmethod
    def rate_limit_error(retry_after: int) -> JSONResponse:
        response = ErrorResponse.create(
            429,
            "rate_limit_error",
            "Rate limit exceeded",
            code="rate_limit_exceeded"
        )
        response.headers["Retry-After"] = str(retry_after)
        return response

    @staticmethod
    def model_not_found(model: str) -> JSONResponse:
        return ErrorResponse.create(
            404,
            "invalid_request_error",
            f"Model '{model}' not found",
            param="model"
        )

    @staticmethod
    def context_length_exceeded(tokens: int, max_tokens: int) -> JSONResponse:
        return ErrorResponse.create(
            400,
            "invalid_request_error",
            f"This model's maximum context length is {max_tokens} tokens. "
            f"Your request has {tokens} tokens.",
            code="context_length_exceeded"
        )

    @staticmethod
    def internal_error(request_id: str) -> JSONResponse:
        return ErrorResponse.create(
            500,
            "internal_error",
            f"An internal error occurred. Request ID: {request_id}",
            code="internal_error"
        )


# Response headers middleware
async def add_rate_limit_headers(request: Request, response, rate_limiter: RateLimiter):
    """Add rate limit headers to response."""
    api_key = request.headers.get("authorization", "").replace("Bearer ", "")

    if api_key:
        remaining = rate_limiter.get_remaining(api_key)
        response.headers["X-RateLimit-Limit-Requests"] = str(rate_limiter.config.requests_per_minute)
        response.headers["X-RateLimit-Remaining-Requests"] = str(remaining["requests_remaining_minute"])
        response.headers["X-RateLimit-Limit-Tokens"] = str(rate_limiter.config.tokens_per_minute)
        response.headers["X-RateLimit-Remaining-Tokens"] = str(remaining["tokens_remaining_minute"])

    return response
```

### Authentication & Authorization

```python
"""
Authentication and authorization for LLM APIs.
"""

from fastapi import Security, HTTPException, Depends
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials, APIKeyHeader
from typing import Optional, List
from dataclasses import dataclass, field
import hashlib
import secrets
import time


security = HTTPBearer()
api_key_header = APIKeyHeader(name="X-API-Key", auto_error=False)


@dataclass
class APIKey:
    """API key representation."""

    key_id: str
    key_hash: str  # Hashed key, not plaintext
    name: str
    organization_id: str
    created_at: int
    expires_at: Optional[int] = None
    rate_limit_tier: str = "default"
    allowed_models: List[str] = field(default_factory=list)
    permissions: List[str] = field(default_factory=lambda: ["read", "write"])
    active: bool = True


class APIKeyManager:
    """Manage API keys."""

    def __init__(self, db):
        self.db = db

    def generate_key(
        self,
        name: str,
        organization_id: str,
        rate_limit_tier: str = "default",
        allowed_models: Optional[List[str]] = None
    ) -> tuple[str, APIKey]:
        """Generate new API key."""
        # Generate secure random key
        raw_key = f"sk-{secrets.token_urlsafe(32)}"
        key_id = f"key-{secrets.token_hex(8)}"
        key_hash = hashlib.sha256(raw_key.encode()).hexdigest()

        api_key = APIKey(
            key_id=key_id,
            key_hash=key_hash,
            name=name,
            organization_id=organization_id,
            created_at=int(time.time()),
            rate_limit_tier=rate_limit_tier,
            allowed_models=allowed_models or []
        )

        self.db.store_api_key(api_key)

        return raw_key, api_key

    def validate_key(self, raw_key: str) -> Optional[APIKey]:
        """Validate API key and return key info."""
        key_hash = hashlib.sha256(raw_key.encode()).hexdigest()
        api_key = self.db.get_api_key_by_hash(key_hash)

        if not api_key:
            return None

        if not api_key.active:
            return None

        if api_key.expires_at and api_key.expires_at < int(time.time()):
            return None

        return api_key

    def revoke_key(self, key_id: str):
        """Revoke an API key."""
        self.db.deactivate_api_key(key_id)


async def get_api_key(
    bearer: HTTPAuthorizationCredentials = Security(security),
    x_api_key: str = Security(api_key_header)
) -> APIKey:
    """
    Extract and validate API key from request.
    Supports both Bearer token and X-API-Key header.
    """
    raw_key = None

    if bearer:
        raw_key = bearer.credentials
    elif x_api_key:
        raw_key = x_api_key

    if not raw_key:
        raise HTTPException(
            status_code=401,
            detail={"error": {"message": "API key required", "type": "authentication_error"}}
        )

    # Validate key
    api_key = api_key_manager.validate_key(raw_key)

    if not api_key:
        raise HTTPException(
            status_code=401,
            detail={"error": {"message": "Invalid API key", "type": "authentication_error"}}
        )

    return api_key


def require_permission(permission: str):
    """Decorator to require specific permission."""

    async def check_permission(api_key: APIKey = Depends(get_api_key)):
        if permission not in api_key.permissions:
            raise HTTPException(
                status_code=403,
                detail={"error": {"message": f"Permission '{permission}' required", "type": "permission_error"}}
            )
        return api_key

    return check_permission


def require_model_access(model: str):
    """Check if API key has access to model."""

    async def check_model_access(api_key: APIKey = Depends(get_api_key)):
        if api_key.allowed_models and model not in api_key.allowed_models:
            raise HTTPException(
                status_code=403,
                detail={"error": {"message": f"Access to model '{model}' not allowed", "type": "permission_error"}}
            )
        return api_key

    return check_model_access


# OAuth2 / OIDC integration
from authlib.integrations.starlette_client import OAuth


oauth = OAuth()

# Configure OAuth provider
oauth.register(
    name='auth0',
    client_id='your-client-id',
    client_secret='your-client-secret',
    server_metadata_url='https://your-tenant.auth0.com/.well-known/openid-configuration',
    client_kwargs={'scope': 'openid email profile'}
)


async def get_current_user_oauth(request):
    """Get current user from OAuth token."""
    token = request.headers.get("authorization", "").replace("Bearer ", "")

    if not token:
        raise HTTPException(status_code=401, detail="Token required")

    # Verify JWT token
    try:
        # Decode and verify JWT
        payload = jwt.decode(token, options={"verify_signature": True})
        return payload
    except Exception as e:
        raise HTTPException(status_code=401, detail=str(e))
```

---

## 9.3.4 Versioning Strategy

```python
"""
API versioning strategies for LLM services.
"""

from fastapi import FastAPI, APIRouter, Request
from typing import Optional, Callable
from enum import Enum


class VersioningStrategy(Enum):
    URL_PATH = "url_path"       # /v1/chat/completions
    HEADER = "header"           # X-API-Version: 2024-01-01
    QUERY_PARAM = "query"       # ?version=v1
    ACCEPT_HEADER = "accept"    # Accept: application/vnd.api+json;version=1


class APIVersion:
    """Represents an API version."""

    def __init__(
        self,
        version: str,
        release_date: str,
        deprecated: bool = False,
        sunset_date: Optional[str] = None
    ):
        self.version = version
        self.release_date = release_date
        self.deprecated = deprecated
        self.sunset_date = sunset_date


# Version registry
API_VERSIONS = {
    "v1": APIVersion("v1", "2024-01-01"),
    "v2": APIVersion("v2", "2024-06-01"),
    "2024-01-01": APIVersion("2024-01-01", "2024-01-01", deprecated=True, sunset_date="2025-01-01"),
    "2024-06-01": APIVersion("2024-06-01", "2024-06-01")
}


def create_versioned_app() -> FastAPI:
    """Create FastAPI app with URL-based versioning."""

    app = FastAPI(title="LLM Service API")

    # Version 1 router
    v1_router = APIRouter(prefix="/v1", tags=["v1"])

    @v1_router.post("/chat/completions")
    async def chat_completions_v1(request: dict):
        return {"version": "v1", "response": "..."}

    # Version 2 router with new features
    v2_router = APIRouter(prefix="/v2", tags=["v2"])

    @v2_router.post("/chat/completions")
    async def chat_completions_v2(request: dict):
        # V2 might have new parameters, response format, etc.
        return {"version": "v2", "response": "..."}

    app.include_router(v1_router)
    app.include_router(v2_router)

    return app


class HeaderVersionMiddleware:
    """Middleware for header-based versioning."""

    def __init__(self, app: FastAPI, default_version: str = "v1"):
        self.app = app
        self.default_version = default_version

    async def __call__(self, scope, receive, send):
        if scope["type"] == "http":
            headers = dict(scope["headers"])
            version = headers.get(b"x-api-version", b"").decode() or self.default_version

            # Add version to request state
            scope["state"] = {"api_version": version}

            # Check if version is deprecated
            if version in API_VERSIONS and API_VERSIONS[version].deprecated:
                # Add deprecation warning header
                async def send_with_deprecation(message):
                    if message["type"] == "http.response.start":
                        headers = list(message.get("headers", []))
                        headers.append((b"deprecation", b"true"))
                        headers.append((
                            b"sunset",
                            API_VERSIONS[version].sunset_date.encode()
                        ))
                        message["headers"] = headers
                    await send(message)

                await self.app(scope, receive, send_with_deprecation)
                return

        await self.app(scope, receive, send)


class VersionedEndpoint:
    """
    Decorator for version-specific endpoint handling.
    """

    def __init__(self, handlers: dict[str, Callable]):
        self.handlers = handlers

    async def __call__(self, request: Request, **kwargs):
        version = request.state.get("api_version", "v1")

        if version not in self.handlers:
            # Fall back to latest version
            version = max(self.handlers.keys())

        handler = self.handlers[version]
        return await handler(request, **kwargs)


# Deprecation policy
DEPRECATION_POLICY = """
# API Deprecation Policy

## Timeline
- **Announcement**: 6 months before deprecation
- **Deprecation**: API version marked deprecated, warnings added
- **Sunset**: 12 months after deprecation, API version removed

## Communication
- Deprecation header added to responses
- Email notification to registered users
- Documentation updated with migration guide

## Migration Support
- Changelog documenting all breaking changes
- SDKs updated with compatibility shims
- Migration scripts provided where applicable
"""


class MigrationGuide:
    """Generate migration guide between versions."""

    def __init__(self):
        self.changes: list[dict] = []

    def add_breaking_change(
        self,
        from_version: str,
        to_version: str,
        change_type: str,
        description: str,
        before: str,
        after: str
    ):
        """Add a breaking change."""
        self.changes.append({
            "from_version": from_version,
            "to_version": to_version,
            "type": change_type,
            "description": description,
            "before": before,
            "after": after
        })

    def generate_markdown(self, from_version: str, to_version: str) -> str:
        """Generate migration guide markdown."""
        relevant = [
            c for c in self.changes
            if c["from_version"] == from_version and c["to_version"] == to_version
        ]

        md = f"# Migration Guide: {from_version}  {to_version}\n\n"

        for change in relevant:
            md += f"## {change['type']}: {change['description']}\n\n"
            md += f"**Before ({from_version}):**\n```json\n{change['before']}\n```\n\n"
            md += f"**After ({to_version}):**\n```json\n{change['after']}\n```\n\n"

        return md
```

---

## 9.3.5 Documentation

```python
"""
API documentation generation and management.
"""

from fastapi import FastAPI
from fastapi.openapi.utils import get_openapi
from typing import Optional, List, Dict, Any
import json
import yaml


def custom_openapi(app: FastAPI) -> dict:
    """Generate custom OpenAPI schema with LLM-specific extensions."""

    if app.openapi_schema:
        return app.openapi_schema

    openapi_schema = get_openapi(
        title="LLM Service API",
        version="1.0.0",
        description="""
# LLM Service API

OpenAI-compatible API for LLM inference.

## Authentication
All API requests require an API key passed via:
- `Authorization: Bearer sk-...` header
- `X-API-Key: sk-...` header

## Rate Limits
- 60 requests per minute
- 100,000 tokens per minute

## Streaming
Streaming responses use Server-Sent Events (SSE).
Set `stream: true` in your request.

## Models
Use `/v1/models` to list available models.
        """,
        routes=app.routes,
        servers=[
            {"url": "https://api.llm-service.com", "description": "Production"},
            {"url": "https://staging.llm-service.com", "description": "Staging"}
        ]
    )

    # Add security schemes
    openapi_schema["components"]["securitySchemes"] = {
        "bearerAuth": {
            "type": "http",
            "scheme": "bearer",
            "bearerFormat": "API Key"
        },
        "apiKeyAuth": {
            "type": "apiKey",
            "in": "header",
            "name": "X-API-Key"
        }
    }

    # Apply security globally
    openapi_schema["security"] = [
        {"bearerAuth": []},
        {"apiKeyAuth": []}
    ]

    # Add rate limit extension
    for path in openapi_schema["paths"].values():
        for operation in path.values():
            if isinstance(operation, dict):
                operation["x-rateLimit"] = {
                    "requests": {"limit": 60, "period": "minute"},
                    "tokens": {"limit": 100000, "period": "minute"}
                }

    app.openapi_schema = openapi_schema
    return app.openapi_schema


# OpenAPI specification template
OPENAPI_TEMPLATE = '''
openapi: 3.1.0
info:
  title: LLM Service API
  version: 1.0.0
  description: OpenAI-compatible LLM inference API
  contact:
    name: API Support
    email: support@llm-service.com

servers:
  - url: https://api.llm-service.com/v1
    description: Production

paths:
  /chat/completions:
    post:
      operationId: createChatCompletion
      summary: Create a chat completion
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/ChatCompletionRequest'
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ChatCompletionResponse'
            text/event-stream:
              schema:
                type: string
                description: SSE stream of ChatCompletionChunk objects
        '400':
          $ref: '#/components/responses/BadRequest'
        '401':
          $ref: '#/components/responses/Unauthorized'
        '429':
          $ref: '#/components/responses/RateLimitExceeded'

components:
  schemas:
    ChatCompletionRequest:
      type: object
      required:
        - model
        - messages
      properties:
        model:
          type: string
          description: Model ID to use
          example: "llama-3.1-70b-instruct"
        messages:
          type: array
          items:
            $ref: '#/components/schemas/Message'
        max_tokens:
          type: integer
          minimum: 1
          maximum: 32768
        temperature:
          type: number
          minimum: 0
          maximum: 2
          default: 0.7
        stream:
          type: boolean
          default: false

    Message:
      type: object
      required:
        - role
        - content
      properties:
        role:
          type: string
          enum: [system, user, assistant]
        content:
          type: string

    ChatCompletionResponse:
      type: object
      properties:
        id:
          type: string
        object:
          type: string
          enum: [chat.completion]
        created:
          type: integer
        model:
          type: string
        choices:
          type: array
          items:
            $ref: '#/components/schemas/Choice'
        usage:
          $ref: '#/components/schemas/Usage'

    Choice:
      type: object
      properties:
        index:
          type: integer
        message:
          $ref: '#/components/schemas/Message'
        finish_reason:
          type: string
          enum: [stop, length, tool_calls]

    Usage:
      type: object
      properties:
        prompt_tokens:
          type: integer
        completion_tokens:
          type: integer
        total_tokens:
          type: integer

  responses:
    BadRequest:
      description: Invalid request
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/Error'

    Unauthorized:
      description: Authentication failed
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/Error'

    RateLimitExceeded:
      description: Rate limit exceeded
      headers:
        Retry-After:
          schema:
            type: integer
          description: Seconds until rate limit resets
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/Error'

  securitySchemes:
    bearerAuth:
      type: http
      scheme: bearer
'''


class SDKGenerator:
    """Generate SDK code from OpenAPI spec."""

    def __init__(self, openapi_spec: dict):
        self.spec = openapi_spec

    def generate_python_sdk(self) -> str:
        """Generate Python SDK."""
        sdk_code = '''
"""
LLM Service Python SDK
Auto-generated from OpenAPI specification
"""

from typing import Optional, List, Union, AsyncIterator
import httpx
import json


class LLMClient:
    """Client for LLM Service API."""

    def __init__(
        self,
        api_key: str,
        base_url: str = "https://api.llm-service.com/v1"
    ):
        self.api_key = api_key
        self.base_url = base_url
        self._client = httpx.Client(
            base_url=base_url,
            headers={"Authorization": f"Bearer {api_key}"}
        )

    def chat_completions(
        self,
        model: str,
        messages: List[dict],
        max_tokens: Optional[int] = None,
        temperature: float = 0.7,
        stream: bool = False,
        **kwargs
    ):
        """Create a chat completion."""
        payload = {
            "model": model,
            "messages": messages,
            "temperature": temperature,
            "stream": stream,
            **kwargs
        }

        if max_tokens:
            payload["max_tokens"] = max_tokens

        if stream:
            return self._stream_chat(payload)

        response = self._client.post("/chat/completions", json=payload)
        response.raise_for_status()
        return response.json()

    def _stream_chat(self, payload: dict):
        """Stream chat completion."""
        with self._client.stream("POST", "/chat/completions", json=payload) as response:
            for line in response.iter_lines():
                if line.startswith("data: "):
                    data = line[6:]
                    if data == "[DONE]":
                        break
                    yield json.loads(data)


class AsyncLLMClient:
    """Async client for LLM Service API."""

    def __init__(
        self,
        api_key: str,
        base_url: str = "https://api.llm-service.com/v1"
    ):
        self.api_key = api_key
        self.base_url = base_url
        self._client = httpx.AsyncClient(
            base_url=base_url,
            headers={"Authorization": f"Bearer {api_key}"}
        )

    async def chat_completions(
        self,
        model: str,
        messages: List[dict],
        max_tokens: Optional[int] = None,
        temperature: float = 0.7,
        stream: bool = False,
        **kwargs
    ):
        """Create a chat completion."""
        payload = {
            "model": model,
            "messages": messages,
            "temperature": temperature,
            "stream": stream,
            **kwargs
        }

        if max_tokens:
            payload["max_tokens"] = max_tokens

        if stream:
            async for chunk in self._stream_chat(payload):
                yield chunk
        else:
            response = await self._client.post("/chat/completions", json=payload)
            response.raise_for_status()
            return response.json()

    async def _stream_chat(self, payload: dict) -> AsyncIterator[dict]:
        """Stream chat completion."""
        async with self._client.stream("POST", "/chat/completions", json=payload) as response:
            async for line in response.aiter_lines():
                if line.startswith("data: "):
                    data = line[6:]
                    if data == "[DONE]":
                        break
                    yield json.loads(data)
'''
        return sdk_code

    def generate_typescript_sdk(self) -> str:
        """Generate TypeScript SDK."""
        return '''
/**
 * LLM Service TypeScript SDK
 * Auto-generated from OpenAPI specification
 */

export interface Message {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

export interface ChatCompletionRequest {
  model: string;
  messages: Message[];
  max_tokens?: number;
  temperature?: number;
  stream?: boolean;
}

export interface ChatCompletionResponse {
  id: string;
  object: 'chat.completion';
  created: number;
  model: string;
  choices: Array<{
    index: number;
    message: Message;
    finish_reason: string;
  }>;
  usage: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}

export class LLMClient {
  private apiKey: string;
  private baseUrl: string;

  constructor(apiKey: string, baseUrl: string = 'https://api.llm-service.com/v1') {
    this.apiKey = apiKey;
    this.baseUrl = baseUrl;
  }

  async chatCompletions(request: ChatCompletionRequest): Promise<ChatCompletionResponse> {
    const response = await fetch(`${this.baseUrl}/chat/completions`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${this.apiKey}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify(request),
    });

    if (!response.ok) {
      throw new Error(`API error: ${response.status}`);
    }

    return response.json();
  }

  async *streamChatCompletions(
    request: ChatCompletionRequest
  ): AsyncGenerator<ChatCompletionChunk> {
    const response = await fetch(`${this.baseUrl}/chat/completions`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${this.apiKey}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({ ...request, stream: true }),
    });

    const reader = response.body?.getReader();
    const decoder = new TextDecoder();

    if (!reader) throw new Error('No response body');

    while (true) {
      const { done, value } = await reader.read();
      if (done) break;

      const chunk = decoder.decode(value);
      const lines = chunk.split('\\n');

      for (const line of lines) {
        if (line.startsWith('data: ')) {
          const data = line.slice(6);
          if (data === '[DONE]') return;
          yield JSON.parse(data);
        }
      }
    }
  }
}
'''
```

---

## Appendices

### Appendix A: OpenAPI Specification Template

See the `OPENAPI_TEMPLATE` in section 9.3.5 for a complete OpenAPI specification template.

### Appendix B: SDK Templates

```python
# Python SDK Template (see SDKGenerator.generate_python_sdk())
# TypeScript SDK Template (see SDKGenerator.generate_typescript_sdk())

# Additional: curl examples for quick testing
CURL_EXAMPLES = """
# Chat completion
curl https://api.llm-service.com/v1/chat/completions \\
  -H "Authorization: Bearer sk-your-api-key" \\
  -H "Content-Type: application/json" \\
  -d '{
    "model": "llama-3.1-70b-instruct",
    "messages": [{"role": "user", "content": "Hello!"}],
    "max_tokens": 100
  }'

# Streaming chat completion
curl https://api.llm-service.com/v1/chat/completions \\
  -H "Authorization: Bearer sk-your-api-key" \\
  -H "Content-Type: application/json" \\
  -N \\
  -d '{
    "model": "llama-3.1-70b-instruct",
    "messages": [{"role": "user", "content": "Tell me a story"}],
    "stream": true
  }'

# List models
curl https://api.llm-service.com/v1/models \\
  -H "Authorization: Bearer sk-your-api-key"

# Embeddings
curl https://api.llm-service.com/v1/embeddings \\
  -H "Authorization: Bearer sk-your-api-key" \\
  -H "Content-Type: application/json" \\
  -d '{
    "model": "text-embedding-ada-002",
    "input": "Hello, world!"
  }'
"""
```

### Appendix C: Rate Limiting Implementation

```python
"""
Complete rate limiting implementation with Redis.
"""

import redis
import time
from typing import Optional, Tuple


class RedisRateLimiter:
    """Production rate limiter using Redis."""

    def __init__(self, redis_url: str):
        self.redis = redis.from_url(redis_url)

    async def check_rate_limit(
        self,
        key: str,
        limit: int,
        window_seconds: int
    ) -> Tuple[bool, dict]:
        """
        Check rate limit using sliding window.
        Returns (allowed, info).
        """
        now = time.time()
        window_start = now - window_seconds

        pipe = self.redis.pipeline()

        # Remove old entries
        pipe.zremrangebyscore(key, 0, window_start)

        # Count current entries
        pipe.zcard(key)

        # Add current request if allowed
        pipe.zadd(key, {str(now): now})

        # Set expiry
        pipe.expire(key, window_seconds)

        results = pipe.execute()
        current_count = results[1]

        allowed = current_count < limit

        if not allowed:
            # Remove the request we just added
            self.redis.zrem(key, str(now))

        # Get oldest entry for reset time
        oldest = self.redis.zrange(key, 0, 0, withscores=True)
        reset_time = int(oldest[0][1] + window_seconds) if oldest else int(now + window_seconds)

        return allowed, {
            "limit": limit,
            "remaining": max(0, limit - current_count - 1),
            "reset": reset_time,
            "retry_after": reset_time - int(now) if not allowed else None
        }
```

### Appendix D: Error Handling Patterns

```python
"""
Comprehensive error handling for LLM APIs.
"""

from fastapi import Request, HTTPException
from fastapi.responses import JSONResponse
from starlette.middleware.base import BaseHTTPMiddleware
import traceback
import logging

logger = logging.getLogger(__name__)


class ErrorHandlingMiddleware(BaseHTTPMiddleware):
    """Global error handling middleware."""

    async def dispatch(self, request: Request, call_next):
        try:
            response = await call_next(request)
            return response
        except HTTPException:
            raise
        except Exception as e:
            # Log the full error
            logger.error(f"Unhandled error: {e}\n{traceback.format_exc()}")

            # Return generic error to client
            return JSONResponse(
                status_code=500,
                content={
                    "error": {
                        "message": "An internal error occurred",
                        "type": "internal_error",
                        "code": "internal_error"
                    }
                }
            )


# Error codes and messages
ERROR_CODES = {
    "invalid_api_key": ("Invalid API key provided", 401),
    "rate_limit_exceeded": ("Rate limit exceeded", 429),
    "model_not_found": ("The model does not exist", 404),
    "context_length_exceeded": ("Maximum context length exceeded", 400),
    "invalid_request": ("Invalid request parameters", 400),
    "content_filter": ("Content filtered due to policy", 400),
    "server_error": ("Internal server error", 500),
    "model_overloaded": ("Model is currently overloaded", 503),
}
```

---

## Summary

This guide covered:

1. **API Paradigms**: REST, GraphQL, gRPC, WebSocket - when to use each
2. **OpenAI Compatibility**: Drop-in replacement API design for seamless migration
3. **Request/Response Design**: Validation, token counting, rate limiting, streaming
4. **Authentication**: API keys, OAuth2, permission management
5. **Versioning**: URL path, header-based versioning with deprecation policies
6. **Documentation**: OpenAPI specs, SDK generation, interactive docs

Key takeaways:
- OpenAI-compatible REST APIs enable the widest ecosystem compatibility
- SSE streaming is essential for responsive LLM applications
- Proper rate limiting protects both your service and users
- Version your API from day one with clear deprecation policies
- Generate SDKs from OpenAPI specs to reduce integration friction
