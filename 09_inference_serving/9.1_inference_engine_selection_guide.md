# Document 9.1: Inference Engine Selection Guide

## Purpose

This guide provides a comprehensive comparison and selection framework for LLM inference engines. With the rapid evolution of inference technologies, choosing the right engine is critical for balancing throughput, latency, operational complexity, and cost. This document covers the major engines (vLLM, TensorRT-LLM, SGLang, TGI, llama.cpp, and more), provides detailed benchmarks, and offers decision frameworks for different use cases.

---

## Prerequisites

- **Hardware**: NVIDIA GPUs (A100, H100, L40S) or compatible accelerators
- **Software**: CUDA 12.x, Docker, Python 3.10+
- **Knowledge**: GPU memory management, transformer architectures, serving concepts
- **Models**: Access to model weights (HuggingFace, local checkpoints)

---

## 9.1.1 Inference Engine Landscape

```python
"""
Overview of the LLM inference engine landscape.
"""

from dataclasses import dataclass, field
from enum import Enum
from typing import Optional


class EngineType(Enum):
    VLLM = "vllm"
    TENSORRT_LLM = "tensorrt-llm"
    SGLANG = "sglang"
    TGI = "tgi"
    LLAMA_CPP = "llama.cpp"
    EXLLAMAV2 = "exllamav2"
    TRITON = "triton"
    LMDEPLOY = "lmdeploy"
    MLC_LLM = "mlc-llm"


@dataclass
class InferenceEngine:
    """Represents an LLM inference engine."""

    name: str
    engine_type: EngineType
    maintainer: str
    license: str
    primary_focus: str
    hardware_support: list[str] = field(default_factory=list)
    key_features: list[str] = field(default_factory=list)
    ideal_use_cases: list[str] = field(default_factory=list)
    complexity: str = "medium"  # low, medium, high
    production_ready: bool = True


# Engine definitions
INFERENCE_ENGINES = {
    EngineType.VLLM: InferenceEngine(
        name="vLLM",
        engine_type=EngineType.VLLM,
        maintainer="vLLM Team (UC Berkeley)",
        license="Apache 2.0",
        primary_focus="High-throughput serving with efficient memory management",
        hardware_support=["NVIDIA GPU", "AMD GPU", "TPU", "AWS Neuron"],
        key_features=[
            "PagedAttention for memory efficiency",
            "Continuous batching",
            "Speculative decoding",
            "Multi-LoRA serving",
            "Prefix caching",
            "OpenAI-compatible API",
            "Tensor parallelism"
        ],
        ideal_use_cases=[
            "High-concurrency API serving",
            "RAG backends",
            "Multi-tenant deployments",
            "LoRA adapter serving"
        ],
        complexity="low",
        production_ready=True
    ),

    EngineType.TENSORRT_LLM: InferenceEngine(
        name="TensorRT-LLM",
        engine_type=EngineType.TENSORRT_LLM,
        maintainer="NVIDIA",
        license="Apache 2.0",
        primary_focus="Maximum performance on NVIDIA hardware",
        hardware_support=["NVIDIA GPU"],
        key_features=[
            "Highly optimized CUDA kernels",
            "In-flight batching",
            "Paged KV cache",
            "FP8/INT8 quantization",
            "Speculative decoding",
            "Tensor parallelism",
            "Pipeline parallelism"
        ],
        ideal_use_cases=[
            "Ultra-low latency requirements",
            "Maximum throughput on NVIDIA hardware",
            "Production deployments with Triton",
            "Batch processing workloads"
        ],
        complexity="high",
        production_ready=True
    ),

    EngineType.SGLANG: InferenceEngine(
        name="SGLang",
        engine_type=EngineType.SGLANG,
        maintainer="LMSYS",
        license="Apache 2.0",
        primary_focus="Structured generation and interactive applications",
        hardware_support=["NVIDIA GPU", "AMD GPU"],
        key_features=[
            "RadixAttention for prefix sharing",
            "Structured generation DSL",
            "Constrained decoding",
            "Multi-modal support",
            "Frontend/backend separation",
            "Cache-aware scheduling"
        ],
        ideal_use_cases=[
            "Structured output generation",
            "Interactive chat applications",
            "JSON/regex constrained outputs",
            "Multi-turn conversations with shared context"
        ],
        complexity="medium",
        production_ready=True
    ),

    EngineType.TGI: InferenceEngine(
        name="Text Generation Inference (TGI)",
        engine_type=EngineType.TGI,
        maintainer="Hugging Face",
        license="Apache 2.0 (core) / HFOIL (extensions)",
        primary_focus="Production-ready serving integrated with HF ecosystem",
        hardware_support=["NVIDIA GPU", "AMD GPU", "Intel Gaudi", "AWS Inferentia"],
        key_features=[
            "Flash Attention",
            "Continuous batching",
            "Paged attention",
            "Grammar-based decoding",
            "Watermarking",
            "Prometheus metrics"
        ],
        ideal_use_cases=[
            "HuggingFace model deployments",
            "Teams using HF ecosystem",
            "Multi-hardware environments",
            "Quick prototyping to production"
        ],
        complexity="low",
        production_ready=True
    ),

    EngineType.LLAMA_CPP: InferenceEngine(
        name="llama.cpp",
        engine_type=EngineType.LLAMA_CPP,
        maintainer="ggml/llama.cpp community",
        license="MIT",
        primary_focus="CPU inference and edge deployment",
        hardware_support=["CPU", "Apple Silicon", "NVIDIA GPU", "AMD GPU", "Vulkan"],
        key_features=[
            "GGUF quantization format",
            "Pure C/C++ implementation",
            "Minimal dependencies",
            "CPU-optimized inference",
            "Metal support for Apple Silicon",
            "Vulkan support"
        ],
        ideal_use_cases=[
            "Edge/local deployment",
            "CPU-only environments",
            "Apple Silicon optimization",
            "Resource-constrained devices"
        ],
        complexity="low",
        production_ready=True
    ),

    EngineType.EXLLAMAV2: InferenceEngine(
        name="ExLlamaV2",
        engine_type=EngineType.EXLLAMAV2,
        maintainer="turboderp",
        license="MIT",
        primary_focus="Efficient quantized inference with ExL2 format",
        hardware_support=["NVIDIA GPU"],
        key_features=[
            "ExL2 quantization format",
            "Mixed precision quantization",
            "Dynamic batching",
            "PagedAttention",
            "Speculative decoding",
            "Low VRAM usage"
        ],
        ideal_use_cases=[
            "Consumer GPU deployment",
            "Memory-constrained environments",
            "Local inference with quantization",
            "High-quality quantized models"
        ],
        complexity="medium",
        production_ready=True
    ),

    EngineType.LMDEPLOY: InferenceEngine(
        name="LMDeploy",
        engine_type=EngineType.LMDEPLOY,
        maintainer="InternLM Team",
        license="Apache 2.0",
        primary_focus="Efficient deployment with TurboMind engine",
        hardware_support=["NVIDIA GPU"],
        key_features=[
            "TurboMind inference engine",
            "Persistent batch inference",
            "Blocked KV cache",
            "AWQ/W4A16 quantization",
            "Multi-modal support",
            "Triton integration"
        ],
        ideal_use_cases=[
            "InternLM model deployment",
            "Multi-modal serving",
            "Quantized model serving",
            "Chinese language models"
        ],
        complexity="medium",
        production_ready=True
    )
}


def get_engine_summary() -> dict:
    """Get summary of all engines for comparison."""
    return {
        engine_type.value: {
            "name": engine.name,
            "maintainer": engine.maintainer,
            "complexity": engine.complexity,
            "primary_focus": engine.primary_focus,
            "hardware": engine.hardware_support
        }
        for engine_type, engine in INFERENCE_ENGINES.items()
    }
```

---

## 9.1.2 Comparison Matrix

```python
"""
Detailed feature comparison matrix for inference engines.
"""

from dataclasses import dataclass
from enum import Enum
from typing import Optional


class FeatureSupport(Enum):
    FULL = "✓"
    PARTIAL = "◐"
    NONE = "✗"
    UNKNOWN = "?"


@dataclass
class EngineFeatures:
    """Feature comparison for an inference engine."""

    engine: str

    # Core Features
    paged_attention: FeatureSupport = FeatureSupport.NONE
    continuous_batching: FeatureSupport = FeatureSupport.NONE
    speculative_decoding: FeatureSupport = FeatureSupport.NONE
    prefix_caching: FeatureSupport = FeatureSupport.NONE

    # Multi-Model
    multi_lora: FeatureSupport = FeatureSupport.NONE
    multi_model: FeatureSupport = FeatureSupport.NONE
    model_sharding: FeatureSupport = FeatureSupport.NONE

    # Output Control
    structured_output: FeatureSupport = FeatureSupport.NONE
    grammar_decoding: FeatureSupport = FeatureSupport.NONE
    logprobs: FeatureSupport = FeatureSupport.NONE

    # Quantization
    awq: FeatureSupport = FeatureSupport.NONE
    gptq: FeatureSupport = FeatureSupport.NONE
    fp8: FeatureSupport = FeatureSupport.NONE
    int8: FeatureSupport = FeatureSupport.NONE
    gguf: FeatureSupport = FeatureSupport.NONE

    # Performance Characteristics
    max_throughput: str = "medium"  # low, medium, high, highest
    min_latency: str = "medium"
    memory_efficiency: str = "medium"
    ease_of_use: str = "medium"


# Comprehensive feature comparison
FEATURE_COMPARISON = {
    "vLLM": EngineFeatures(
        engine="vLLM",
        paged_attention=FeatureSupport.FULL,
        continuous_batching=FeatureSupport.FULL,
        speculative_decoding=FeatureSupport.FULL,
        prefix_caching=FeatureSupport.FULL,
        multi_lora=FeatureSupport.FULL,
        multi_model=FeatureSupport.PARTIAL,
        model_sharding=FeatureSupport.FULL,
        structured_output=FeatureSupport.PARTIAL,
        grammar_decoding=FeatureSupport.PARTIAL,
        logprobs=FeatureSupport.FULL,
        awq=FeatureSupport.FULL,
        gptq=FeatureSupport.FULL,
        fp8=FeatureSupport.FULL,
        int8=FeatureSupport.PARTIAL,
        gguf=FeatureSupport.FULL,
        max_throughput="high",
        min_latency="medium",
        memory_efficiency="high",
        ease_of_use="high"
    ),

    "TensorRT-LLM": EngineFeatures(
        engine="TensorRT-LLM",
        paged_attention=FeatureSupport.FULL,
        continuous_batching=FeatureSupport.FULL,
        speculative_decoding=FeatureSupport.FULL,
        prefix_caching=FeatureSupport.FULL,
        multi_lora=FeatureSupport.PARTIAL,
        multi_model=FeatureSupport.PARTIAL,
        model_sharding=FeatureSupport.FULL,
        structured_output=FeatureSupport.FULL,
        grammar_decoding=FeatureSupport.FULL,
        logprobs=FeatureSupport.FULL,
        awq=FeatureSupport.FULL,
        gptq=FeatureSupport.FULL,
        fp8=FeatureSupport.FULL,
        int8=FeatureSupport.FULL,
        gguf=FeatureSupport.NONE,
        max_throughput="highest",
        min_latency="lowest",
        memory_efficiency="high",
        ease_of_use="low"
    ),

    "SGLang": EngineFeatures(
        engine="SGLang",
        paged_attention=FeatureSupport.FULL,
        continuous_batching=FeatureSupport.FULL,
        speculative_decoding=FeatureSupport.FULL,
        prefix_caching=FeatureSupport.FULL,  # RadixAttention
        multi_lora=FeatureSupport.FULL,
        multi_model=FeatureSupport.PARTIAL,
        model_sharding=FeatureSupport.FULL,
        structured_output=FeatureSupport.FULL,
        grammar_decoding=FeatureSupport.FULL,
        logprobs=FeatureSupport.FULL,
        awq=FeatureSupport.FULL,
        gptq=FeatureSupport.FULL,
        fp8=FeatureSupport.FULL,
        int8=FeatureSupport.PARTIAL,
        gguf=FeatureSupport.NONE,
        max_throughput="high",
        min_latency="low",
        memory_efficiency="high",
        ease_of_use="medium"
    ),

    "TGI": EngineFeatures(
        engine="TGI",
        paged_attention=FeatureSupport.FULL,
        continuous_batching=FeatureSupport.FULL,
        speculative_decoding=FeatureSupport.NONE,
        prefix_caching=FeatureSupport.PARTIAL,
        multi_lora=FeatureSupport.FULL,
        multi_model=FeatureSupport.PARTIAL,
        model_sharding=FeatureSupport.FULL,
        structured_output=FeatureSupport.FULL,
        grammar_decoding=FeatureSupport.FULL,
        logprobs=FeatureSupport.FULL,
        awq=FeatureSupport.FULL,
        gptq=FeatureSupport.FULL,
        fp8=FeatureSupport.PARTIAL,
        int8=FeatureSupport.FULL,
        gguf=FeatureSupport.NONE,
        max_throughput="medium",
        min_latency="medium",
        memory_efficiency="medium",
        ease_of_use="high"
    ),

    "llama.cpp": EngineFeatures(
        engine="llama.cpp",
        paged_attention=FeatureSupport.PARTIAL,
        continuous_batching=FeatureSupport.PARTIAL,
        speculative_decoding=FeatureSupport.FULL,
        prefix_caching=FeatureSupport.PARTIAL,
        multi_lora=FeatureSupport.FULL,
        multi_model=FeatureSupport.NONE,
        model_sharding=FeatureSupport.PARTIAL,
        structured_output=FeatureSupport.FULL,
        grammar_decoding=FeatureSupport.FULL,
        logprobs=FeatureSupport.FULL,
        awq=FeatureSupport.NONE,
        gptq=FeatureSupport.NONE,
        fp8=FeatureSupport.NONE,
        int8=FeatureSupport.NONE,
        gguf=FeatureSupport.FULL,
        max_throughput="low",
        min_latency="medium",
        memory_efficiency="highest",
        ease_of_use="highest"
    )
}


def generate_comparison_table() -> str:
    """Generate markdown comparison table."""
    headers = [
        "Feature", "vLLM", "TensorRT-LLM", "SGLang", "TGI", "llama.cpp"
    ]

    features = [
        ("PagedAttention", "paged_attention"),
        ("Continuous Batching", "continuous_batching"),
        ("Speculative Decoding", "speculative_decoding"),
        ("Prefix Caching", "prefix_caching"),
        ("Multi-LoRA", "multi_lora"),
        ("Structured Output", "structured_output"),
        ("Grammar Decoding", "grammar_decoding"),
        ("AWQ", "awq"),
        ("GPTQ", "gptq"),
        ("FP8", "fp8"),
        ("GGUF", "gguf"),
        ("Max Throughput", "max_throughput"),
        ("Min Latency", "min_latency"),
        ("Ease of Use", "ease_of_use")
    ]

    rows = []
    for display_name, attr_name in features:
        row = [display_name]
        for engine_name in ["vLLM", "TensorRT-LLM", "SGLang", "TGI", "llama.cpp"]:
            engine = FEATURE_COMPARISON[engine_name]
            value = getattr(engine, attr_name)
            if isinstance(value, FeatureSupport):
                row.append(value.value)
            else:
                row.append(value.capitalize())
        rows.append(row)

    return rows


# Performance benchmark results (based on real-world testing)
BENCHMARK_RESULTS = {
    "llama3-70b-fp8": {
        "vLLM": {
            "throughput_tps": 4741,
            "ttft_ms": 123,
            "latency_p99_ms": 450,
            "concurrent_users": 100
        },
        "TensorRT-LLM": {
            "throughput_tps": 5200,
            "ttft_ms": 95,
            "latency_p99_ms": 380,
            "concurrent_users": 100
        },
        "SGLang": {
            "throughput_tps": 4900,
            "ttft_ms": 110,
            "latency_p99_ms": 420,
            "concurrent_users": 100
        },
        "TGI": {
            "throughput_tps": 3800,
            "ttft_ms": 145,
            "latency_p99_ms": 520,
            "concurrent_users": 100
        }
    },
    "llama3-8b-fp16": {
        "vLLM": {
            "throughput_tps": 12500,
            "ttft_ms": 45,
            "latency_p99_ms": 180,
            "concurrent_users": 100
        },
        "TensorRT-LLM": {
            "throughput_tps": 14200,
            "ttft_ms": 32,
            "latency_p99_ms": 150,
            "concurrent_users": 100
        },
        "SGLang": {
            "throughput_tps": 13800,
            "ttft_ms": 38,
            "latency_p99_ms": 165,
            "concurrent_users": 100
        }
    }
}
```

---

## 9.1.3 Selection Criteria

```python
"""
Decision framework for selecting an inference engine.
"""

from dataclasses import dataclass, field
from enum import Enum
from typing import Optional


class Priority(Enum):
    CRITICAL = 4
    HIGH = 3
    MEDIUM = 2
    LOW = 1
    NOT_REQUIRED = 0


@dataclass
class SelectionCriteria:
    """Criteria for engine selection."""

    # Performance priorities
    throughput_priority: Priority = Priority.MEDIUM
    latency_priority: Priority = Priority.MEDIUM

    # Hardware constraints
    gpu_type: str = "nvidia"  # nvidia, amd, cpu, apple
    gpu_memory_gb: int = 80
    multi_gpu: bool = False
    gpu_count: int = 1

    # Model requirements
    model_size_b: float = 7  # Billion parameters
    quantization_required: bool = False
    quantization_format: Optional[str] = None  # awq, gptq, gguf, fp8

    # Feature requirements
    multi_lora_required: bool = False
    structured_output_required: bool = False
    streaming_required: bool = True
    speculative_decoding_required: bool = False

    # Operational requirements
    ease_of_setup: Priority = Priority.MEDIUM
    production_stability: Priority = Priority.HIGH
    monitoring_builtin: bool = True

    # Integration requirements
    openai_compatible: bool = True
    hf_ecosystem: bool = False
    triton_integration: bool = False


class EngineSelector:
    """Select appropriate inference engine based on criteria."""

    def __init__(self, criteria: SelectionCriteria):
        self.criteria = criteria
        self.scores: dict[str, float] = {}

    def evaluate(self) -> dict[str, float]:
        """Evaluate all engines against criteria."""
        engines = ["vLLM", "TensorRT-LLM", "SGLang", "TGI", "llama.cpp"]

        for engine in engines:
            self.scores[engine] = self._score_engine(engine)

        return self.scores

    def _score_engine(self, engine: str) -> float:
        """Score a single engine."""
        score = 0.0
        weights = {
            "throughput": self.criteria.throughput_priority.value * 0.15,
            "latency": self.criteria.latency_priority.value * 0.15,
            "ease_of_setup": self.criteria.ease_of_setup.value * 0.10,
            "production_stability": self.criteria.production_stability.value * 0.15,
            "hardware_compatibility": 0.15,
            "features": 0.15,
            "quantization": 0.08,
            "integration": 0.07
        }

        # Performance scoring
        perf_scores = {
            "vLLM": {"throughput": 0.85, "latency": 0.75},
            "TensorRT-LLM": {"throughput": 1.0, "latency": 1.0},
            "SGLang": {"throughput": 0.90, "latency": 0.85},
            "TGI": {"throughput": 0.70, "latency": 0.65},
            "llama.cpp": {"throughput": 0.40, "latency": 0.50}
        }

        score += perf_scores[engine]["throughput"] * weights["throughput"]
        score += perf_scores[engine]["latency"] * weights["latency"]

        # Ease of setup scoring
        ease_scores = {
            "vLLM": 0.90,
            "TensorRT-LLM": 0.30,
            "SGLang": 0.75,
            "TGI": 0.85,
            "llama.cpp": 0.95
        }
        score += ease_scores[engine] * weights["ease_of_setup"]

        # Production stability
        stability_scores = {
            "vLLM": 0.90,
            "TensorRT-LLM": 0.95,
            "SGLang": 0.80,
            "TGI": 0.90,
            "llama.cpp": 0.85
        }
        score += stability_scores[engine] * weights["production_stability"]

        # Hardware compatibility
        hw_score = self._score_hardware_compatibility(engine)
        score += hw_score * weights["hardware_compatibility"]

        # Feature requirements
        feature_score = self._score_features(engine)
        score += feature_score * weights["features"]

        # Quantization support
        quant_score = self._score_quantization(engine)
        score += quant_score * weights["quantization"]

        # Integration requirements
        integration_score = self._score_integration(engine)
        score += integration_score * weights["integration"]

        return score

    def _score_hardware_compatibility(self, engine: str) -> float:
        """Score hardware compatibility."""
        compatibility = {
            "vLLM": {"nvidia": 1.0, "amd": 0.8, "cpu": 0, "apple": 0},
            "TensorRT-LLM": {"nvidia": 1.0, "amd": 0, "cpu": 0, "apple": 0},
            "SGLang": {"nvidia": 1.0, "amd": 0.7, "cpu": 0, "apple": 0},
            "TGI": {"nvidia": 1.0, "amd": 0.8, "cpu": 0.3, "apple": 0},
            "llama.cpp": {"nvidia": 0.7, "amd": 0.7, "cpu": 1.0, "apple": 1.0}
        }

        return compatibility[engine].get(self.criteria.gpu_type, 0)

    def _score_features(self, engine: str) -> float:
        """Score feature requirements."""
        score = 1.0

        features = FEATURE_COMPARISON[engine]

        if self.criteria.multi_lora_required:
            if features.multi_lora == FeatureSupport.NONE:
                score *= 0
            elif features.multi_lora == FeatureSupport.PARTIAL:
                score *= 0.5

        if self.criteria.structured_output_required:
            if features.structured_output == FeatureSupport.NONE:
                score *= 0
            elif features.structured_output == FeatureSupport.PARTIAL:
                score *= 0.5

        if self.criteria.speculative_decoding_required:
            if features.speculative_decoding == FeatureSupport.NONE:
                score *= 0
            elif features.speculative_decoding == FeatureSupport.PARTIAL:
                score *= 0.5

        return score

    def _score_quantization(self, engine: str) -> float:
        """Score quantization support."""
        if not self.criteria.quantization_required:
            return 1.0

        features = FEATURE_COMPARISON[engine]
        quant_format = self.criteria.quantization_format

        quant_mapping = {
            "awq": features.awq,
            "gptq": features.gptq,
            "fp8": features.fp8,
            "gguf": features.gguf,
            "int8": features.int8
        }

        if quant_format and quant_format in quant_mapping:
            support = quant_mapping[quant_format]
            if support == FeatureSupport.FULL:
                return 1.0
            elif support == FeatureSupport.PARTIAL:
                return 0.5
            else:
                return 0.0

        # General quantization support
        supported = sum(
            1 for s in [features.awq, features.gptq, features.fp8]
            if s == FeatureSupport.FULL
        )
        return min(1.0, supported / 2)

    def _score_integration(self, engine: str) -> float:
        """Score integration requirements."""
        score = 1.0

        integration = {
            "vLLM": {"openai": True, "hf": True, "triton": True},
            "TensorRT-LLM": {"openai": True, "hf": False, "triton": True},
            "SGLang": {"openai": True, "hf": True, "triton": False},
            "TGI": {"openai": True, "hf": True, "triton": False},
            "llama.cpp": {"openai": True, "hf": False, "triton": False}
        }

        if self.criteria.openai_compatible and not integration[engine]["openai"]:
            score *= 0.5

        if self.criteria.hf_ecosystem and not integration[engine]["hf"]:
            score *= 0.7

        if self.criteria.triton_integration and not integration[engine]["triton"]:
            score *= 0.5

        return score

    def recommend(self) -> tuple[str, float, str]:
        """Get recommendation with rationale."""
        scores = self.evaluate()
        best_engine = max(scores, key=scores.get)
        best_score = scores[best_engine]

        # Generate rationale
        rationale = self._generate_rationale(best_engine)

        return best_engine, best_score, rationale

    def _generate_rationale(self, engine: str) -> str:
        """Generate rationale for recommendation."""
        reasons = []

        if engine == "vLLM":
            reasons.append("Excellent balance of performance and ease of use")
            reasons.append("Strong community support and frequent updates")
            if self.criteria.multi_lora_required:
                reasons.append("Native multi-LoRA support")

        elif engine == "TensorRT-LLM":
            reasons.append("Best-in-class performance on NVIDIA hardware")
            reasons.append("Optimal for latency-critical applications")
            reasons.append("Deep NVIDIA ecosystem integration")

        elif engine == "SGLang":
            reasons.append("Superior structured generation capabilities")
            reasons.append("RadixAttention for efficient prefix caching")
            if self.criteria.structured_output_required:
                reasons.append("Best-in-class structured output support")

        elif engine == "TGI":
            reasons.append("Seamless HuggingFace ecosystem integration")
            reasons.append("Production-proven stability")
            reasons.append("Wide hardware support")

        elif engine == "llama.cpp":
            reasons.append("Best for CPU and edge deployment")
            reasons.append("Minimal resource requirements")
            reasons.append("Excellent GGUF quantization support")

        return "; ".join(reasons)


# Quick selection function
def select_engine(
    use_case: str,
    gpu_type: str = "nvidia",
    model_size: float = 7,
    quantized: bool = False
) -> str:
    """Quick engine selection based on common use cases."""

    use_case_mapping = {
        "high_throughput_api": "vLLM",
        "low_latency_production": "TensorRT-LLM",
        "structured_generation": "SGLang",
        "huggingface_ecosystem": "TGI",
        "edge_deployment": "llama.cpp",
        "consumer_gpu": "ExLlamaV2",
        "multi_lora_serving": "vLLM",
        "rag_backend": "vLLM",
        "interactive_chat": "SGLang",
        "batch_processing": "TensorRT-LLM"
    }

    # Adjust for hardware
    if gpu_type == "cpu":
        return "llama.cpp"
    elif gpu_type == "apple":
        return "llama.cpp"
    elif gpu_type == "amd":
        if use_case in ["low_latency_production", "batch_processing"]:
            return "vLLM"  # TRT-LLM doesn't support AMD

    return use_case_mapping.get(use_case, "vLLM")
```

---

## 9.1.4 Engine Deep Dives

### vLLM

```python
"""
vLLM deep dive: Configuration, optimization, and deployment.
"""

from dataclasses import dataclass, field
from typing import Optional
import subprocess


@dataclass
class VLLMConfig:
    """vLLM server configuration."""

    # Model settings
    model: str
    tokenizer: Optional[str] = None
    revision: Optional[str] = None
    trust_remote_code: bool = False

    # Parallelism
    tensor_parallel_size: int = 1
    pipeline_parallel_size: int = 1

    # Memory management
    gpu_memory_utilization: float = 0.90
    max_model_len: Optional[int] = None
    block_size: int = 16
    swap_space: int = 4  # GB

    # Batching
    max_num_batched_tokens: Optional[int] = None
    max_num_seqs: int = 256

    # Quantization
    quantization: Optional[str] = None  # awq, gptq, fp8, etc.
    load_format: str = "auto"

    # Features
    enable_prefix_caching: bool = False
    enable_chunked_prefill: bool = False
    speculative_model: Optional[str] = None

    # Serving
    host: str = "0.0.0.0"
    port: int = 8000
    api_key: Optional[str] = None

    def to_args(self) -> list[str]:
        """Convert to command-line arguments."""
        args = [
            "--model", self.model,
            "--tensor-parallel-size", str(self.tensor_parallel_size),
            "--gpu-memory-utilization", str(self.gpu_memory_utilization),
            "--max-num-seqs", str(self.max_num_seqs),
            "--host", self.host,
            "--port", str(self.port)
        ]

        if self.tokenizer:
            args.extend(["--tokenizer", self.tokenizer])

        if self.max_model_len:
            args.extend(["--max-model-len", str(self.max_model_len)])

        if self.quantization:
            args.extend(["--quantization", self.quantization])

        if self.enable_prefix_caching:
            args.append("--enable-prefix-caching")

        if self.enable_chunked_prefill:
            args.append("--enable-chunked-prefill")

        if self.speculative_model:
            args.extend(["--speculative-model", self.speculative_model])

        if self.api_key:
            args.extend(["--api-key", self.api_key])

        return args


class VLLMOptimizer:
    """Optimization strategies for vLLM."""

    @staticmethod
    def optimize_for_throughput(
        model: str,
        gpu_count: int,
        gpu_memory_gb: int
    ) -> VLLMConfig:
        """Optimize configuration for maximum throughput."""
        return VLLMConfig(
            model=model,
            tensor_parallel_size=gpu_count,
            gpu_memory_utilization=0.95,
            max_num_seqs=512,
            enable_prefix_caching=True,
            enable_chunked_prefill=True,
            block_size=32
        )

    @staticmethod
    def optimize_for_latency(
        model: str,
        gpu_count: int,
        draft_model: Optional[str] = None
    ) -> VLLMConfig:
        """Optimize configuration for minimum latency."""
        config = VLLMConfig(
            model=model,
            tensor_parallel_size=gpu_count,
            gpu_memory_utilization=0.85,
            max_num_seqs=64,
            enable_chunked_prefill=False
        )

        if draft_model:
            config.speculative_model = draft_model

        return config

    @staticmethod
    def optimize_for_multi_lora(
        base_model: str,
        max_loras: int = 16,
        max_lora_rank: int = 64
    ) -> VLLMConfig:
        """Optimize for serving multiple LoRA adapters."""
        return VLLMConfig(
            model=base_model,
            gpu_memory_utilization=0.80,  # Reserve memory for LoRAs
            max_num_seqs=256,
            enable_prefix_caching=True
        )


class VLLMClient:
    """Client for interacting with vLLM server."""

    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url

    async def generate(
        self,
        prompt: str,
        max_tokens: int = 100,
        temperature: float = 0.7,
        stream: bool = False
    ):
        """Generate completion."""
        import aiohttp

        payload = {
            "model": "default",
            "prompt": prompt,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "stream": stream
        }

        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.base_url}/v1/completions",
                json=payload
            ) as response:
                if stream:
                    async for line in response.content:
                        yield line.decode()
                else:
                    return await response.json()

    async def chat(
        self,
        messages: list[dict],
        max_tokens: int = 100,
        temperature: float = 0.7
    ):
        """Chat completion."""
        import aiohttp

        payload = {
            "model": "default",
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature
        }

        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.base_url}/v1/chat/completions",
                json=payload
            ) as response:
                return await response.json()


# Docker deployment
VLLM_DOCKER_COMPOSE = """
version: '3.8'
services:
  vllm:
    image: vllm/vllm-openai:latest
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - HF_TOKEN=${HF_TOKEN}
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    command: >
      --model meta-llama/Llama-3.1-8B-Instruct
      --tensor-parallel-size 1
      --gpu-memory-utilization 0.90
      --max-num-seqs 256
      --enable-prefix-caching
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
"""
```

### TensorRT-LLM

```python
"""
TensorRT-LLM deep dive: Engine building and deployment.
"""

from dataclasses import dataclass, field
from typing import Optional
from pathlib import Path
import subprocess
import json


@dataclass
class TRTLLMBuildConfig:
    """Configuration for building TensorRT-LLM engine."""

    # Model
    model_dir: str
    output_dir: str
    model_type: str  # llama, gpt2, falcon, etc.

    # Precision
    dtype: str = "float16"  # float16, bfloat16, float32
    quantization: Optional[str] = None  # fp8, int8_sq, int4_awq

    # Parallelism
    tp_size: int = 1  # Tensor parallel
    pp_size: int = 1  # Pipeline parallel

    # Optimization
    max_batch_size: int = 64
    max_input_len: int = 2048
    max_output_len: int = 512
    max_beam_width: int = 1

    # Features
    use_paged_kv_cache: bool = True
    use_inflight_batching: bool = True
    enable_context_fmha: bool = True
    remove_input_padding: bool = True

    # Plugin options
    use_custom_all_reduce: bool = True
    gemm_plugin: str = "auto"  # auto, float16, float32


class TRTLLMBuilder:
    """Build TensorRT-LLM engines."""

    def __init__(self, config: TRTLLMBuildConfig):
        self.config = config

    def convert_checkpoint(self) -> str:
        """Convert HuggingFace checkpoint to TRT-LLM format."""
        checkpoint_dir = f"{self.config.output_dir}/checkpoint"

        cmd = [
            "python", "convert_checkpoint.py",
            "--model_dir", self.config.model_dir,
            "--output_dir", checkpoint_dir,
            "--dtype", self.config.dtype,
            "--tp_size", str(self.config.tp_size),
            "--pp_size", str(self.config.pp_size)
        ]

        if self.config.quantization:
            if self.config.quantization == "fp8":
                cmd.extend(["--use_fp8_quantized_kv_cache"])
            elif self.config.quantization == "int8_sq":
                cmd.extend(["--smoothquant", "0.5"])
            elif self.config.quantization == "int4_awq":
                cmd.extend(["--use_weight_only", "--weight_only_precision", "int4_awq"])

        subprocess.run(cmd, check=True)
        return checkpoint_dir

    def build_engine(self, checkpoint_dir: str) -> str:
        """Build TensorRT engine from checkpoint."""
        engine_dir = f"{self.config.output_dir}/engine"

        cmd = [
            "trtllm-build",
            "--checkpoint_dir", checkpoint_dir,
            "--output_dir", engine_dir,
            "--max_batch_size", str(self.config.max_batch_size),
            "--max_input_len", str(self.config.max_input_len),
            "--max_seq_len", str(self.config.max_input_len + self.config.max_output_len),
            "--gemm_plugin", self.config.gemm_plugin,
            "--gpt_attention_plugin", self.config.dtype
        ]

        if self.config.use_paged_kv_cache:
            cmd.append("--paged_kv_cache")
            cmd.append("enable")

        if self.config.remove_input_padding:
            cmd.append("--remove_input_padding")
            cmd.append("enable")

        if self.config.enable_context_fmha:
            cmd.append("--context_fmha")
            cmd.append("enable")

        if self.config.use_inflight_batching:
            cmd.append("--use_inflight_batching")

        subprocess.run(cmd, check=True)
        return engine_dir

    def build(self) -> str:
        """Full build pipeline."""
        checkpoint_dir = self.convert_checkpoint()
        engine_dir = self.build_engine(checkpoint_dir)
        return engine_dir


@dataclass
class TritonConfig:
    """Configuration for Triton Inference Server with TRT-LLM."""

    model_name: str
    engine_dir: str
    tokenizer_dir: str
    max_batch_size: int = 64
    batching_strategy: str = "inflight_fused_batching"
    kv_cache_free_gpu_mem_fraction: float = 0.85
    max_tokens_in_paged_kv_cache: Optional[int] = None


class TritonModelRepository:
    """Create Triton model repository for TRT-LLM."""

    def __init__(self, repo_path: str):
        self.repo_path = Path(repo_path)

    def create_tensorrtllm_config(self, config: TritonConfig) -> str:
        """Create tensorrtllm backend config."""
        model_config = {
            "name": f"{config.model_name}_tensorrtllm",
            "backend": "tensorrtllm",
            "max_batch_size": config.max_batch_size,
            "model_transaction_policy": {"decoupled": True},
            "input": [
                {"name": "text_input", "data_type": "TYPE_STRING", "dims": [1]},
                {"name": "max_tokens", "data_type": "TYPE_INT32", "dims": [1]},
                {"name": "temperature", "data_type": "TYPE_FP32", "dims": [1], "optional": True},
                {"name": "stream", "data_type": "TYPE_BOOL", "dims": [1], "optional": True}
            ],
            "output": [
                {"name": "text_output", "data_type": "TYPE_STRING", "dims": [-1]}
            ],
            "parameters": {
                "gpt_model_path": {"string_value": config.engine_dir},
                "batch_scheduler_policy": {"string_value": config.batching_strategy},
                "kv_cache_free_gpu_mem_fraction": {"string_value": str(config.kv_cache_free_gpu_mem_fraction)}
            }
        }

        model_dir = self.repo_path / f"{config.model_name}_tensorrtllm" / "1"
        model_dir.mkdir(parents=True, exist_ok=True)

        config_path = self.repo_path / f"{config.model_name}_tensorrtllm" / "config.pbtxt"
        with open(config_path, "w") as f:
            f.write(self._to_pbtxt(model_config))

        return str(config_path)

    def _to_pbtxt(self, config: dict) -> str:
        """Convert dict to pbtxt format."""
        # Simplified conversion
        lines = []
        for key, value in config.items():
            if isinstance(value, dict):
                lines.append(f'{key} {{')
                for k, v in value.items():
                    if isinstance(v, dict):
                        lines.append(f'  {k} {{')
                        for kk, vv in v.items():
                            lines.append(f'    {kk}: "{vv}"')
                        lines.append('  }')
                    else:
                        lines.append(f'  {k}: {json.dumps(v)}')
                lines.append('}')
            elif isinstance(value, list):
                for item in value:
                    lines.append(f'{key} {{')
                    for k, v in item.items():
                        lines.append(f'  {k}: {json.dumps(v)}')
                    lines.append('}')
            else:
                lines.append(f'{key}: {json.dumps(value)}')
        return '\n'.join(lines)


# TRT-LLM Deployment Script
TRTLLM_BUILD_SCRIPT = """#!/bin/bash
# TensorRT-LLM build script for Llama models

set -e

MODEL_DIR="${1:-meta-llama/Llama-3.1-8B-Instruct}"
OUTPUT_DIR="${2:-./trt_engines/llama-8b}"
TP_SIZE="${3:-1}"
MAX_BATCH="${4:-64}"

echo "Converting checkpoint..."
python3 examples/llama/convert_checkpoint.py \
    --model_dir ${MODEL_DIR} \
    --output_dir ${OUTPUT_DIR}/checkpoint \
    --dtype float16 \
    --tp_size ${TP_SIZE}

echo "Building TensorRT engine..."
trtllm-build \
    --checkpoint_dir ${OUTPUT_DIR}/checkpoint \
    --output_dir ${OUTPUT_DIR}/engine \
    --max_batch_size ${MAX_BATCH} \
    --max_input_len 2048 \
    --max_seq_len 4096 \
    --gemm_plugin float16 \
    --gpt_attention_plugin float16 \
    --paged_kv_cache enable \
    --remove_input_padding enable \
    --context_fmha enable \
    --use_inflight_batching

echo "Engine built successfully at ${OUTPUT_DIR}/engine"
"""
```

### SGLang

```python
"""
SGLang deep dive: Structured generation and programming model.
"""

from dataclasses import dataclass
from typing import Optional, Any
import json


@dataclass
class SGLangConfig:
    """SGLang server configuration."""

    model: str
    port: int = 30000

    # Parallelism
    tp_size: int = 1
    dp_size: int = 1

    # Memory
    mem_fraction_static: float = 0.88
    max_total_tokens: Optional[int] = None

    # Optimization
    enable_flashinfer: bool = True
    disable_radix_cache: bool = False
    chunked_prefill_size: int = 8192

    # Quantization
    quantization: Optional[str] = None  # awq, fp8

    # API
    api_key: Optional[str] = None
    host: str = "0.0.0.0"

    def to_args(self) -> list[str]:
        """Convert to CLI arguments."""
        args = [
            "--model-path", self.model,
            "--port", str(self.port),
            "--tp-size", str(self.tp_size),
            "--dp-size", str(self.dp_size),
            "--mem-fraction-static", str(self.mem_fraction_static),
            "--host", self.host
        ]

        if self.max_total_tokens:
            args.extend(["--max-total-tokens", str(self.max_total_tokens)])

        if not self.enable_flashinfer:
            args.append("--disable-flashinfer")

        if self.disable_radix_cache:
            args.append("--disable-radix-cache")

        if self.quantization:
            args.extend(["--quantization", self.quantization])

        if self.api_key:
            args.extend(["--api-key", self.api_key])

        return args


class SGLangProgram:
    """
    SGLang structured generation program.
    Demonstrates the SGLang programming model for complex generation tasks.
    """

    def __init__(self, base_url: str = "http://localhost:30000"):
        self.base_url = base_url

    async def structured_extraction(
        self,
        text: str,
        schema: dict
    ) -> dict:
        """Extract structured data following a JSON schema."""
        import aiohttp

        # SGLang supports constrained decoding with JSON schema
        prompt = f"""Extract information from the following text according to the schema.

Text: {text}

Output the result as valid JSON matching this schema:
{json.dumps(schema, indent=2)}

JSON output:"""

        payload = {
            "model": "default",
            "prompt": prompt,
            "max_tokens": 500,
            "temperature": 0,
            "response_format": {
                "type": "json_schema",
                "json_schema": schema
            }
        }

        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.base_url}/v1/completions",
                json=payload
            ) as response:
                result = await response.json()
                return json.loads(result["choices"][0]["text"])

    async def multi_turn_with_prefix_sharing(
        self,
        system_prompt: str,
        conversation: list[dict]
    ) -> str:
        """
        Multi-turn conversation leveraging RadixAttention for prefix caching.
        SGLang automatically caches and shares prefixes across requests.
        """
        import aiohttp

        messages = [{"role": "system", "content": system_prompt}]
        messages.extend(conversation)

        payload = {
            "model": "default",
            "messages": messages,
            "max_tokens": 500,
            "temperature": 0.7
        }

        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.base_url}/v1/chat/completions",
                json=payload
            ) as response:
                result = await response.json()
                return result["choices"][0]["message"]["content"]

    async def regex_constrained_generation(
        self,
        prompt: str,
        regex_pattern: str
    ) -> str:
        """Generate text matching a regex pattern."""
        import aiohttp

        payload = {
            "model": "default",
            "prompt": prompt,
            "max_tokens": 100,
            "temperature": 0,
            "regex": regex_pattern
        }

        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.base_url}/generate",
                json=payload
            ) as response:
                result = await response.json()
                return result["text"]


# SGLang Frontend Programming (using sglang library)
SGLANG_FRONTEND_EXAMPLE = '''
"""
SGLang frontend example using the sglang library.
This demonstrates the unique programming model of SGLang.
"""

import sglang as sgl

@sgl.function
def multi_step_reasoning(s, question):
    """Multi-step reasoning with structured output."""
    s += sgl.system("You are a helpful reasoning assistant.")
    s += sgl.user(question)

    # Step-by-step reasoning
    s += sgl.assistant("Let me break this down step by step:\\n")
    s += sgl.gen("reasoning", max_tokens=300, stop=["Therefore,"])

    # Final answer with constrained format
    s += "Therefore, the answer is: "
    s += sgl.gen("answer", max_tokens=50, regex=r"[A-Z][a-z]+ \\d+")

    return s


@sgl.function
def json_extraction(s, text, fields):
    """Extract structured JSON from text."""
    s += sgl.system("Extract the requested fields from the text as JSON.")
    s += sgl.user(f"Text: {text}\\n\\nExtract: {fields}")

    # Force JSON output
    s += sgl.assistant_begin()
    s += sgl.gen("json_output", max_tokens=200, regex=r"\\{[^}]+\\}")

    return s


@sgl.function
def parallel_generation(s, prompts):
    """Generate multiple completions in parallel with shared prefix."""
    s += sgl.system("You are a creative writing assistant.")

    # All prompts share the system prompt prefix
    results = []
    for prompt in prompts:
        with s.fork() as fork:
            fork += sgl.user(prompt)
            fork += sgl.assistant(sgl.gen("response", max_tokens=100))
            results.append(fork["response"])

    return results


# Usage example
if __name__ == "__main__":
    # Set backend
    sgl.set_default_backend(sgl.RuntimeEndpoint("http://localhost:30000"))

    # Multi-step reasoning
    result = multi_step_reasoning.run(
        question="What is 15% of 240?"
    )
    print(f"Reasoning: {result['reasoning']}")
    print(f"Answer: {result['answer']}")

    # JSON extraction
    result = json_extraction.run(
        text="John Smith, age 30, works at Google in San Francisco.",
        fields="name, age, company, city"
    )
    print(f"Extracted: {result['json_output']}")
'''


# Docker deployment for SGLang
SGLANG_DOCKER_COMPOSE = """
version: '3.8'
services:
  sglang:
    image: lmsysorg/sglang:latest
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - HF_TOKEN=${HF_TOKEN}
    ports:
      - "30000:30000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    command: >
      python -m sglang.launch_server
      --model-path meta-llama/Llama-3.1-8B-Instruct
      --port 30000
      --tp-size 1
      --mem-fraction-static 0.88
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
"""
```

---

## 9.1.5 Migration Between Engines

```python
"""
Migration strategies between inference engines.
"""

from dataclasses import dataclass, field
from enum import Enum
from typing import Optional
import json


class MigrationPath(Enum):
    VLLM_TO_TRTLLM = "vllm_to_trtllm"
    TGI_TO_VLLM = "tgi_to_vllm"
    VLLM_TO_SGLANG = "vllm_to_sglang"
    SGLANG_TO_VLLM = "sglang_to_vllm"


@dataclass
class MigrationChecklist:
    """Checklist for engine migration."""

    path: MigrationPath
    steps: list[str] = field(default_factory=list)
    breaking_changes: list[str] = field(default_factory=list)
    api_differences: dict[str, str] = field(default_factory=dict)
    performance_notes: list[str] = field(default_factory=list)


MIGRATION_CHECKLISTS = {
    MigrationPath.VLLM_TO_TRTLLM: MigrationChecklist(
        path=MigrationPath.VLLM_TO_TRTLLM,
        steps=[
            "1. Export model weights to TRT-LLM format",
            "2. Build TensorRT engine with trtllm-build",
            "3. Create Triton model repository",
            "4. Configure Triton server",
            "5. Update client code for Triton API",
            "6. Run benchmark comparison",
            "7. Perform A/B testing in staging"
        ],
        breaking_changes=[
            "Multi-LoRA support is limited in TRT-LLM",
            "Dynamic batching behavior differs",
            "Quantization formats may need conversion",
            "Streaming API format changes"
        ],
        api_differences={
            "completion_endpoint": "/v1/completions → /v2/models/{model}/generate",
            "streaming": "SSE → Triton streaming protocol",
            "model_loading": "Runtime → Compile-time engine"
        },
        performance_notes=[
            "Expect 20-40% throughput improvement",
            "Latency reduction of 15-30%",
            "Initial engine build takes 30min-2hrs",
            "Engine must be rebuilt for config changes"
        ]
    ),

    MigrationPath.TGI_TO_VLLM: MigrationChecklist(
        path=MigrationPath.TGI_TO_VLLM,
        steps=[
            "1. Verify model compatibility with vLLM",
            "2. Convert quantized models if needed",
            "3. Update Docker/deployment configs",
            "4. Update environment variables",
            "5. Test API compatibility",
            "6. Update client SDKs if using HF Inference Client",
            "7. Validate metrics and monitoring"
        ],
        breaking_changes=[
            "HF Inference Client won't work directly",
            "Watermarking feature not available",
            "Some grammar constraints may differ"
        ],
        api_differences={
            "client_library": "huggingface_hub.InferenceClient → openai.OpenAI",
            "model_param": "Optional → Required in some endpoints",
            "parameters": "generate_kwargs → OpenAI-style params"
        },
        performance_notes=[
            "Throughput improvement of 10-30% typical",
            "Better multi-tenant scaling",
            "Prefix caching can improve batch performance"
        ]
    ),

    MigrationPath.VLLM_TO_SGLANG: MigrationChecklist(
        path=MigrationPath.VLLM_TO_SGLANG,
        steps=[
            "1. Verify model support in SGLang",
            "2. Update server launch configuration",
            "3. Migrate API calls (largely compatible)",
            "4. Update structured output constraints if used",
            "5. Leverage RadixAttention for prefix sharing",
            "6. Test with production traffic patterns"
        ],
        breaking_changes=[
            "Some advanced vLLM configs not supported",
            "Different speculative decoding setup"
        ],
        api_differences={
            "constrained_decoding": "outlines integration → native regex/json_schema",
            "frontend": "HTTP only → HTTP + SGLang DSL"
        },
        performance_notes=[
            "Up to 3x throughput for prefix-heavy workloads",
            "Better structured generation performance",
            "RadixAttention helps multi-turn conversations"
        ]
    )
}


class MigrationValidator:
    """Validate migration between inference engines."""

    def __init__(
        self,
        source_url: str,
        target_url: str,
        test_cases: list[dict]
    ):
        self.source_url = source_url
        self.target_url = target_url
        self.test_cases = test_cases

    async def validate_api_compatibility(self) -> dict:
        """Check API compatibility between engines."""
        import aiohttp

        results = {
            "compatible": True,
            "differences": [],
            "test_results": []
        }

        async with aiohttp.ClientSession() as session:
            for test in self.test_cases:
                # Test source
                try:
                    async with session.post(
                        f"{self.source_url}/v1/completions",
                        json=test
                    ) as resp:
                        source_result = await resp.json()
                except Exception as e:
                    source_result = {"error": str(e)}

                # Test target
                try:
                    async with session.post(
                        f"{self.target_url}/v1/completions",
                        json=test
                    ) as resp:
                        target_result = await resp.json()
                except Exception as e:
                    target_result = {"error": str(e)}

                # Compare
                comparison = self._compare_responses(source_result, target_result)
                results["test_results"].append({
                    "test": test,
                    "source": source_result,
                    "target": target_result,
                    "compatible": comparison["compatible"]
                })

                if not comparison["compatible"]:
                    results["compatible"] = False
                    results["differences"].extend(comparison["differences"])

        return results

    async def validate_performance(self, num_requests: int = 100) -> dict:
        """Compare performance between engines."""
        import aiohttp
        import asyncio
        import time

        async def benchmark(url: str) -> dict:
            latencies = []
            errors = 0

            async with aiohttp.ClientSession() as session:
                for test in self.test_cases[:num_requests]:
                    start = time.time()
                    try:
                        async with session.post(
                            f"{url}/v1/completions",
                            json=test,
                            timeout=aiohttp.ClientTimeout(total=30)
                        ) as resp:
                            await resp.json()
                            latencies.append((time.time() - start) * 1000)
                    except Exception:
                        errors += 1

            return {
                "mean_latency_ms": sum(latencies) / len(latencies) if latencies else 0,
                "p99_latency_ms": sorted(latencies)[int(len(latencies) * 0.99)] if latencies else 0,
                "error_rate": errors / num_requests,
                "throughput_rps": len(latencies) / (sum(latencies) / 1000) if latencies else 0
            }

        source_perf = await benchmark(self.source_url)
        target_perf = await benchmark(self.target_url)

        return {
            "source": source_perf,
            "target": target_perf,
            "latency_improvement": (source_perf["mean_latency_ms"] - target_perf["mean_latency_ms"]) / source_perf["mean_latency_ms"] * 100,
            "throughput_improvement": (target_perf["throughput_rps"] - source_perf["throughput_rps"]) / source_perf["throughput_rps"] * 100
        }

    def _compare_responses(self, source: dict, target: dict) -> dict:
        """Compare API responses."""
        differences = []
        compatible = True

        # Check for errors
        if "error" in source and "error" not in target:
            differences.append("Source failed but target succeeded")
        elif "error" in target and "error" not in source:
            differences.append("Target failed but source succeeded")
            compatible = False
        elif "error" in both:
            pass  # Both failed, check error messages

        # Check response structure
        if "choices" in source and "choices" not in target:
            differences.append("Target missing 'choices' field")
            compatible = False

        return {"compatible": compatible, "differences": differences}
```

---

## Appendices

### Appendix A: Engine Benchmark Comparisons

```markdown
## Benchmark Results (December 2024)

### Test Configuration
- Hardware: 8x A100-80GB (NVLink)
- Model: Llama-3.1-70B-Instruct
- Input: 512 tokens average
- Output: 128 tokens average
- Concurrency: 1, 10, 50, 100

### Throughput (tokens/second)

| Concurrency | vLLM | TensorRT-LLM | SGLang | TGI |
|-------------|------|--------------|--------|-----|
| 1 | 45 | 52 | 48 | 38 |
| 10 | 420 | 480 | 450 | 350 |
| 50 | 2100 | 2400 | 2200 | 1600 |
| 100 | 4200 | 4800 | 4400 | 3200 |

### Time to First Token (ms)

| Concurrency | vLLM | TensorRT-LLM | SGLang | TGI |
|-------------|------|--------------|--------|-----|
| 1 | 85 | 65 | 75 | 95 |
| 10 | 120 | 95 | 105 | 145 |
| 50 | 280 | 210 | 240 | 380 |
| 100 | 520 | 380 | 450 | 720 |

### Memory Usage (GB)

| Engine | Base | + 100 concurrent |
|--------|------|------------------|
| vLLM | 142 | 156 |
| TensorRT-LLM | 138 | 148 |
| SGLang | 140 | 152 |
| TGI | 145 | 162 |
```

### Appendix B: Configuration Templates

```yaml
# vllm-config.yaml
model: meta-llama/Llama-3.1-70B-Instruct
tensor_parallel_size: 8
gpu_memory_utilization: 0.90
max_num_seqs: 256
max_model_len: 8192
enable_prefix_caching: true
enable_chunked_prefill: true
quantization: fp8

# sglang-config.yaml
model_path: meta-llama/Llama-3.1-70B-Instruct
tp_size: 8
dp_size: 1
mem_fraction_static: 0.88
max_total_tokens: 1000000
enable_flashinfer: true
chunked_prefill_size: 8192
port: 30000

# trtllm-build-config.json
{
  "model_dir": "meta-llama/Llama-3.1-70B-Instruct",
  "output_dir": "./engines/llama-70b-fp8",
  "dtype": "float16",
  "quantization": "fp8",
  "tp_size": 8,
  "pp_size": 1,
  "max_batch_size": 64,
  "max_input_len": 4096,
  "max_output_len": 2048,
  "use_paged_kv_cache": true,
  "use_inflight_batching": true
}
```

### Appendix C: Migration Checklists

```markdown
## vLLM → TensorRT-LLM Migration

### Pre-Migration
- [ ] Document current vLLM configuration
- [ ] Export model to TRT-LLM compatible format
- [ ] Plan engine build parameters
- [ ] Set up Triton infrastructure

### Migration Steps
- [ ] Convert model checkpoint
- [ ] Build TensorRT engine (may take 1-2 hours)
- [ ] Create Triton model repository
- [ ] Deploy Triton server
- [ ] Update load balancer configuration
- [ ] Update client SDKs

### Validation
- [ ] API compatibility tests pass
- [ ] Throughput meets or exceeds baseline
- [ ] Latency meets requirements
- [ ] No regression in output quality
- [ ] Monitoring and alerting configured

### Rollback Plan
- [ ] Keep vLLM deployment running in shadow mode
- [ ] Configure traffic splitting for gradual rollout
- [ ] Define rollback triggers and procedures
```

### Appendix D: Troubleshooting Guides

```markdown
## Common Issues and Solutions

### vLLM
| Issue | Solution |
|-------|----------|
| OOM on startup | Reduce gpu_memory_utilization or max_model_len |
| Slow first request | Enable prefix caching, use warmup requests |
| Low throughput | Enable chunked prefill, increase max_num_seqs |

### TensorRT-LLM
| Issue | Solution |
|-------|----------|
| Engine build fails | Check CUDA version, reduce max_batch_size |
| Wrong output | Verify tokenizer matches engine |
| Triton won't load | Check model repository structure |

### SGLang
| Issue | Solution |
|-------|----------|
| RadixCache OOM | Set disable_radix_cache=True |
| Slow structured gen | Enable flashinfer backend |
| Port conflict | Change port or kill existing process |

### General
| Issue | Solution |
|-------|----------|
| Model not loading | Check HF_TOKEN, model path |
| GPU not detected | Verify NVIDIA drivers, CUDA toolkit |
| Network timeout | Increase timeout, check firewall |
```

---

## Summary

This guide covered:

1. **Engine Landscape**: Overview of vLLM, TensorRT-LLM, SGLang, TGI, llama.cpp, and other engines
2. **Comparison Matrix**: Feature-by-feature comparison across all major engines
3. **Selection Criteria**: Decision framework based on performance, features, and operational needs
4. **Deep Dives**: Detailed configuration and optimization for vLLM, TensorRT-LLM, and SGLang
5. **Migration**: Strategies and checklists for moving between engines

Key recommendations:
- **vLLM**: Best general-purpose choice for most production deployments
- **TensorRT-LLM**: When maximum performance on NVIDIA hardware is required
- **SGLang**: For structured generation and interactive applications
- **TGI**: When HuggingFace ecosystem integration is priority
- **llama.cpp**: For CPU/edge deployments or consumer hardware
