> **Navigation** | [← 6.1 Quantization](6.1_quantization_guide.md) | [6.3 Knowledge Distillation →](6.3_knowledge_distillation_guide.md)
>
> | | |
> |---|---|
> | **Prerequisites** | [2.2 Model Architecture](../02_model_training/2.2_model_architecture_selection.md) &#124; [6.1 Quantization](6.1_quantization_guide.md) |
> | **Related** | [6.1 Quantization](6.1_quantization_guide.md) &#124; [6.3 Distillation](6.3_knowledge_distillation_guide.md) |
> | **Next** | [6.3 Knowledge Distillation](6.3_knowledge_distillation_guide.md) |

# Document 6.2: Model Pruning & Sparsity Guide

## Executive Summary

This guide covers techniques for reducing LLM model size through pruning and sparse representations. It addresses unstructured pruning, structured pruning (neurons, heads, layers), hardware-friendly sparsity patterns (2:4, N:M), and practical implementation using methods like SparseGPT and Wanda. The goal is to enable practitioners to achieve meaningful compression while maintaining model quality and achieving actual inference speedups on target hardware.

## Prerequisites

- Understanding of transformer architecture (see Document 2.2)
- Familiarity with model training and fine-tuning
- Knowledge of target hardware capabilities
- Python proficiency
- Access to compute for pruning experiments

---

## 6.2.1 Pruning Fundamentals

### Types of Pruning

```python
"""
ABOUTME: Defines pruning types and sparsity patterns for LLM compression.
ABOUTME: Provides framework for understanding pruning tradeoffs.
"""

from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple
from enum import Enum
import numpy as np


class PruningType(Enum):
    """Types of neural network pruning."""
    UNSTRUCTURED = "unstructured"      # Individual weight pruning
    STRUCTURED = "structured"           # Remove entire structures
    SEMI_STRUCTURED = "semi_structured" # N:M sparsity patterns


class StructuredPruningTarget(Enum):
    """Targets for structured pruning."""
    NEURONS = "neurons"           # Output neurons of linear layers
    ATTENTION_HEADS = "heads"     # Entire attention heads
    LAYERS = "layers"             # Entire transformer layers
    FFN_INTERMEDIATE = "ffn"      # FFN intermediate dimension
    CHANNELS = "channels"         # Input/output channels


class SparsityPattern(Enum):
    """Hardware-friendly sparsity patterns."""
    UNSTRUCTURED = "unstructured"  # Random sparsity
    TWO_FOUR = "2:4"              # 2 zeros per 4 elements
    FOUR_EIGHT = "4:8"            # 4 zeros per 8 elements
    BLOCK = "block"                # Block-wise sparsity


@dataclass
class PruningConfig:
    """Configuration for pruning operation."""
    pruning_type: PruningType
    target_sparsity: float              # 0.0 to 1.0
    structured_target: Optional[StructuredPruningTarget] = None
    sparsity_pattern: SparsityPattern = SparsityPattern.UNSTRUCTURED
    gradual: bool = True                # Gradual vs one-shot
    num_pruning_steps: int = 10
    fine_tune_after: bool = True


@dataclass
class SparsitySpec:
    """Specification for a sparsity pattern."""
    pattern: SparsityPattern
    n_zeros: int
    group_size: int
    hardware_accelerated: List[str]
    typical_speedup: str
    notes: List[str]


class SparsityCatalog:
    """
    Catalog of sparsity patterns with characteristics.

    Different patterns offer different tradeoffs between
    compression, quality, and hardware acceleration.
    """

    PATTERNS = {
        SparsityPattern.UNSTRUCTURED: SparsitySpec(
            pattern=SparsityPattern.UNSTRUCTURED,
            n_zeros=0,  # Variable
            group_size=1,
            hardware_accelerated=[],
            typical_speedup="None without special hardware",
            notes=[
                "Maximum flexibility in which weights to prune",
                "Best quality retention for given sparsity",
                "No hardware acceleration on current GPUs",
                "Memory savings but no compute speedup"
            ]
        ),
        SparsityPattern.TWO_FOUR: SparsitySpec(
            pattern=SparsityPattern.TWO_FOUR,
            n_zeros=2,
            group_size=4,
            hardware_accelerated=["NVIDIA Ampere+", "NVIDIA Hopper"],
            typical_speedup="1.5-2x on supported hardware",
            notes=[
                "50% sparsity enforced",
                "Native tensor core support on Ampere+",
                "2x compute speedup theoretical maximum",
                "Good balance of quality and speed"
            ]
        ),
        SparsityPattern.FOUR_EIGHT: SparsitySpec(
            pattern=SparsityPattern.FOUR_EIGHT,
            n_zeros=4,
            group_size=8,
            hardware_accelerated=["Future hardware"],
            typical_speedup="Theoretical 2x",
            notes=[
                "50% sparsity with larger groups",
                "May offer better quality than 2:4 at same sparsity"
            ]
        ),
        SparsityPattern.BLOCK: SparsitySpec(
            pattern=SparsityPattern.BLOCK,
            n_zeros=0,  # Variable
            group_size=32,  # Typical block size
            hardware_accelerated=["Specialized accelerators"],
            typical_speedup="Depends on block size",
            notes=[
                "Removes entire blocks of weights",
                "Can achieve higher compression",
                "Quality depends heavily on block selection"
            ]
        )
    }

    def get_recommended_pattern(
        self,
        target_hardware: str,
        quality_priority: bool = True
    ) -> SparsityPattern:
        """Recommend sparsity pattern for hardware."""
        if "Ampere" in target_hardware or "Hopper" in target_hardware:
            return SparsityPattern.TWO_FOUR
        elif quality_priority:
            return SparsityPattern.UNSTRUCTURED
        else:
            return SparsityPattern.TWO_FOUR


class PruningSchedule:
    """
    Schedules for gradual pruning.

    Gradual pruning typically achieves better quality than
    one-shot pruning at the same sparsity level.
    """

    @staticmethod
    def linear_schedule(
        initial_sparsity: float,
        target_sparsity: float,
        num_steps: int
    ) -> List[float]:
        """Linear sparsity schedule."""
        return np.linspace(initial_sparsity, target_sparsity, num_steps).tolist()

    @staticmethod
    def cubic_schedule(
        initial_sparsity: float,
        target_sparsity: float,
        num_steps: int
    ) -> List[float]:
        """
        Cubic sparsity schedule.

        Slower increase at start and end, faster in middle.
        Often achieves better quality than linear.
        """
        t = np.linspace(0, 1, num_steps)
        sparsity = initial_sparsity + (target_sparsity - initial_sparsity) * (
            3 * t**2 - 2 * t**3
        )
        return sparsity.tolist()

    @staticmethod
    def exponential_schedule(
        initial_sparsity: float,
        target_sparsity: float,
        num_steps: int,
        gamma: float = 0.9
    ) -> List[float]:
        """Exponential sparsity schedule."""
        remaining = target_sparsity - initial_sparsity
        schedule = []
        current = initial_sparsity

        for _ in range(num_steps):
            schedule.append(current)
            current = current + remaining * (1 - gamma)
            remaining = target_sparsity - current

        return schedule
```

### Importance Criteria

```python
"""
ABOUTME: Weight importance criteria for pruning decisions.
ABOUTME: Implements various methods to determine which weights to prune.
"""

from abc import ABC, abstractmethod
from typing import Dict, List, Optional
import numpy as np


class ImportanceCriterion(ABC):
    """Base class for weight importance criteria."""

    @abstractmethod
    def compute_importance(
        self,
        weights: np.ndarray,
        **kwargs
    ) -> np.ndarray:
        """Compute importance score for each weight."""
        pass


class MagnitudeImportance(ImportanceCriterion):
    """
    Magnitude-based importance.

    Simple and effective: larger magnitude weights are more important.
    Works well as a baseline and for unstructured pruning.
    """

    def compute_importance(
        self,
        weights: np.ndarray,
        **kwargs
    ) -> np.ndarray:
        """Importance is absolute value of weight."""
        return np.abs(weights)


class GradientImportance(ImportanceCriterion):
    """
    Gradient-based importance.

    Weights with larger gradients during training are more important.
    Requires gradient information from training/calibration.
    """

    def compute_importance(
        self,
        weights: np.ndarray,
        gradients: np.ndarray = None,
        **kwargs
    ) -> np.ndarray:
        """
        Importance based on gradient magnitude or weight * gradient.

        weight * gradient approximates impact on loss.
        """
        if gradients is None:
            raise ValueError("Gradients required for gradient-based importance")

        # Taylor expansion approximation
        return np.abs(weights * gradients)


class ActivationImportance(ImportanceCriterion):
    """
    Activation-based importance.

    Weights connected to high-magnitude activations are important.
    Similar to AWQ's saliency concept.
    """

    def compute_importance(
        self,
        weights: np.ndarray,
        activations: np.ndarray = None,
        **kwargs
    ) -> np.ndarray:
        """
        Importance based on activation patterns.

        Combines weight magnitude with activation magnitude.
        """
        if activations is None:
            # Fall back to pure magnitude
            return np.abs(weights)

        # Activation-weighted importance
        # activations: [batch, in_features]
        # weights: [out_features, in_features]
        act_importance = np.mean(np.abs(activations), axis=0)

        # Weight importance scaled by activation importance
        return np.abs(weights) * act_importance.reshape(1, -1)


class WandaImportance(ImportanceCriterion):
    """
    Wanda (Pruning by Weights and Activations).

    Simple metric: |weight| * ||activation||_2
    Achieves competitive results with minimal calibration data.

    Reference: Sun et al., "A Simple and Effective Pruning Approach
    for Large Language Models" (2023)
    """

    def compute_importance(
        self,
        weights: np.ndarray,
        activations: np.ndarray = None,
        **kwargs
    ) -> np.ndarray:
        """
        Wanda importance metric.

        For each weight W[i,j]: importance = |W[i,j]| * ||X[:,j]||_2
        """
        if activations is None:
            return np.abs(weights)

        # L2 norm of each input feature across batch
        act_norms = np.linalg.norm(activations, axis=0)  # [in_features]

        # Scale weight magnitude by activation norm
        importance = np.abs(weights) * act_norms.reshape(1, -1)

        return importance


class HessianImportance(ImportanceCriterion):
    """
    Hessian-based importance (Optimal Brain Surgeon family).

    Uses second-order information for more accurate importance.
    More expensive but can achieve better quality.
    """

    def compute_importance(
        self,
        weights: np.ndarray,
        hessian: np.ndarray = None,
        **kwargs
    ) -> np.ndarray:
        """
        Importance based on Hessian diagonal.

        Approximates loss increase from removing each weight.
        """
        if hessian is None:
            return np.abs(weights)

        # Diagonal Hessian approximation
        # Importance = w^2 / (2 * H_ii)
        # Higher Hessian means weight is in sharp region - important

        H_diag = np.diag(hessian) if hessian.ndim == 2 else hessian
        H_diag = np.maximum(H_diag, 1e-8)  # Prevent division by zero

        # Reshape to match weights
        if weights.ndim == 2:
            H_diag = H_diag.reshape(weights.shape)

        return (weights ** 2) / (2 * H_diag)


class ImportanceFactory:
    """Factory for creating importance criteria."""

    CRITERIA = {
        "magnitude": MagnitudeImportance,
        "gradient": GradientImportance,
        "activation": ActivationImportance,
        "wanda": WandaImportance,
        "hessian": HessianImportance
    }

    @classmethod
    def create(cls, name: str) -> ImportanceCriterion:
        """Create importance criterion by name."""
        if name not in cls.CRITERIA:
            raise ValueError(f"Unknown criterion: {name}. Available: {list(cls.CRITERIA.keys())}")
        return cls.CRITERIA[name]()
```

---

## 6.2.2 Pruning Methods

### SparseGPT

```python
"""
ABOUTME: SparseGPT implementation for one-shot LLM pruning.
ABOUTME: Uses Hessian-based optimization for quality preservation.
"""

from dataclasses import dataclass
from typing import Dict, List, Tuple, Optional
import numpy as np


@dataclass
class SparseGPTConfig:
    """Configuration for SparseGPT pruning."""
    sparsity: float = 0.5
    pruning_pattern: str = "unstructured"  # or "2:4"
    blocksize: int = 128
    percdamp: float = 0.01
    min_layer_sparsity: float = 0.0  # Minimum per-layer sparsity


class SparseGPT:
    """
    SparseGPT: Accurate One-Shot Pruning for LLMs.

    Key insight: Use approximate second-order information (Hessian)
    to jointly prune and update remaining weights to minimize error.

    Reference: Frantar & Alistarh, "SparseGPT: Massive Language Models
    Can Be Accurately Pruned in One-Shot" (2023)
    """

    def __init__(self, config: SparseGPTConfig):
        self.config = config

    def prune_layer(
        self,
        weight: np.ndarray,
        activations: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Prune a single linear layer using SparseGPT.

        Args:
            weight: Weight matrix [out_features, in_features]
            activations: Calibration activations [batch, in_features]

        Returns:
            (pruned_weight, mask) where mask is 1 for kept weights
        """
        W = weight.copy()
        out_features, in_features = W.shape

        # Compute Hessian approximation
        H = self._compute_hessian(activations)

        # Add damping
        damp = self.config.percdamp * np.mean(np.diag(H))
        H += damp * np.eye(H.shape[0])

        # Cholesky for efficient operations
        try:
            H_inv = np.linalg.cholesky(np.linalg.inv(H))
        except:
            H_inv = np.linalg.inv(H + 0.1 * np.eye(H.shape[0]))

        # Track which weights to prune
        mask = np.ones_like(W)

        # Process in blocks for efficiency
        for block_start in range(0, in_features, self.config.blocksize):
            block_end = min(block_start + self.config.blocksize, in_features)
            block_size = block_end - block_start

            # Get block of weights and Hessian
            W_block = W[:, block_start:block_end].copy()
            H_block = H_inv[block_start:block_end, block_start:block_end]

            # Compute importance for this block
            importance = self._compute_block_importance(W_block, H_block)

            # Determine which weights to prune in this block
            if self.config.pruning_pattern == "2:4":
                prune_mask = self._get_2_4_mask(importance)
            else:
                prune_mask = self._get_unstructured_mask(
                    importance, self.config.sparsity
                )

            # Update weights to compensate for pruned weights
            for i in range(block_size):
                col = block_start + i
                if prune_mask[:, i].sum() > 0:
                    # Weights being pruned in this column
                    pruned_indices = np.where(prune_mask[:, i])[0]

                    # Error from pruning these weights
                    error = W[:, col].copy()
                    error[~prune_mask[:, i].astype(bool)] = 0

                    # Update remaining weights
                    h_col = H_inv[col, col]
                    if h_col > 0 and col < in_features - 1:
                        remaining_cols = np.arange(col + 1, block_end)
                        update = np.outer(error, H_inv[col, remaining_cols]) / h_col
                        W[:, remaining_cols] -= update

                    # Set pruned weights to zero
                    W[prune_mask[:, i].astype(bool), col] = 0
                    mask[prune_mask[:, i].astype(bool), col] = 0

        return W, mask

    def _compute_hessian(self, activations: np.ndarray) -> np.ndarray:
        """Compute Hessian approximation from activations."""
        # H = X^T X / n
        n_samples = activations.shape[0]
        H = activations.T @ activations / n_samples
        return H

    def _compute_block_importance(
        self,
        W_block: np.ndarray,
        H_block: np.ndarray
    ) -> np.ndarray:
        """
        Compute importance scores for a block.

        Uses Hessian information for more accurate importance.
        """
        # Importance = W^2 / H_diag
        H_diag = np.diag(H_block)
        H_diag = np.maximum(H_diag, 1e-8)

        importance = (W_block ** 2) / H_diag.reshape(1, -1)
        return importance

    def _get_unstructured_mask(
        self,
        importance: np.ndarray,
        sparsity: float
    ) -> np.ndarray:
        """Get mask for unstructured pruning."""
        threshold = np.percentile(importance, sparsity * 100)
        return importance < threshold

    def _get_2_4_mask(self, importance: np.ndarray) -> np.ndarray:
        """
        Get mask for 2:4 sparsity pattern.

        In each group of 4, keep the 2 most important weights.
        """
        out_features, in_features = importance.shape
        mask = np.zeros_like(importance, dtype=bool)

        for i in range(out_features):
            for j in range(0, in_features, 4):
                group_end = min(j + 4, in_features)
                group = importance[i, j:group_end]

                if len(group) == 4:
                    # Keep top 2, prune bottom 2
                    prune_indices = np.argsort(group)[:2]
                    mask[i, j + prune_indices[0]] = True
                    mask[i, j + prune_indices[1]] = True

        return mask


class WandaPruner:
    """
    Wanda: Pruning by Weights and Activations.

    Simple, fast one-shot pruning that achieves competitive quality.
    Key advantages: no weight updates needed, minimal calibration.

    Reference: Sun et al., "A Simple and Effective Pruning Approach
    for Large Language Models" (2023)
    """

    def __init__(
        self,
        sparsity: float = 0.5,
        pruning_pattern: str = "unstructured"
    ):
        self.sparsity = sparsity
        self.pruning_pattern = pruning_pattern

    def prune_layer(
        self,
        weight: np.ndarray,
        activations: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Prune layer using Wanda criterion.

        Much simpler than SparseGPT - no Hessian computation
        or weight updates.
        """
        W = weight.copy()

        # Compute Wanda importance: |W| * ||X||_2
        act_norms = np.linalg.norm(activations, axis=0)  # [in_features]
        importance = np.abs(W) * act_norms.reshape(1, -1)

        # Get pruning mask
        if self.pruning_pattern == "2:4":
            mask = self._get_2_4_mask(importance)
        else:
            mask = self._get_unstructured_mask(importance)

        # Apply mask
        W[mask] = 0

        return W, ~mask

    def _get_unstructured_mask(self, importance: np.ndarray) -> np.ndarray:
        """Get unstructured pruning mask."""
        threshold = np.percentile(importance, self.sparsity * 100)
        return importance < threshold

    def _get_2_4_mask(self, importance: np.ndarray) -> np.ndarray:
        """Get 2:4 sparsity mask."""
        out_features, in_features = importance.shape
        mask = np.zeros_like(importance, dtype=bool)

        for i in range(out_features):
            for j in range(0, in_features, 4):
                group_end = min(j + 4, in_features)
                group = importance[i, j:group_end]

                if len(group) == 4:
                    prune_indices = np.argsort(group)[:2]
                    mask[i, j + prune_indices[0]] = True
                    mask[i, j + prune_indices[1]] = True

        return mask
```

### Structured Pruning

```python
"""
ABOUTME: Structured pruning methods for transformer models.
ABOUTME: Implements head pruning, layer pruning, and FFN dimension reduction.
"""

from dataclasses import dataclass
from typing import Dict, List, Tuple, Optional
import numpy as np


@dataclass
class StructuredPruningConfig:
    """Configuration for structured pruning."""
    target: StructuredPruningTarget
    target_reduction: float  # Fraction to remove (0.0-1.0)
    importance_metric: str = "magnitude"
    calibration_samples: int = 128


class AttentionHeadPruner:
    """
    Prunes entire attention heads from transformer layers.

    Advantages:
    - Immediate inference speedup (no special hardware)
    - Reduces model size and compute
    - Well-studied in research

    Considerations:
    - Requires identifying least important heads
    - May need fine-tuning to recover quality
    """

    def __init__(self, config: StructuredPruningConfig):
        self.config = config

    def compute_head_importance(
        self,
        attention_weights: np.ndarray,
        head_outputs: np.ndarray
    ) -> np.ndarray:
        """
        Compute importance score for each attention head.

        Args:
            attention_weights: Attention patterns [batch, heads, seq, seq]
            head_outputs: Output of each head [batch, heads, seq, dim]

        Returns:
            Importance score for each head
        """
        n_heads = attention_weights.shape[1]
        importance = np.zeros(n_heads)

        for h in range(n_heads):
            # Metric 1: Attention entropy (heads with higher entropy may be more important)
            attn = attention_weights[:, h, :, :]
            entropy = -np.sum(attn * np.log(attn + 1e-10), axis=-1).mean()

            # Metric 2: Output magnitude
            output_norm = np.linalg.norm(head_outputs[:, h, :, :])

            # Combined importance
            importance[h] = entropy * output_norm

        return importance

    def select_heads_to_prune(
        self,
        importance_scores: Dict[int, np.ndarray],
        num_to_prune: int
    ) -> Dict[int, List[int]]:
        """
        Select which heads to prune across all layers.

        Args:
            importance_scores: layer_idx -> head importance array
            num_to_prune: Total number of heads to prune

        Returns:
            layer_idx -> list of head indices to prune
        """
        # Flatten all head scores with layer info
        all_heads = []
        for layer_idx, scores in importance_scores.items():
            for head_idx, score in enumerate(scores):
                all_heads.append((layer_idx, head_idx, score))

        # Sort by importance (ascending - prune least important)
        all_heads.sort(key=lambda x: x[2])

        # Select heads to prune
        heads_to_prune = {}
        for i in range(num_to_prune):
            layer_idx, head_idx, _ = all_heads[i]
            if layer_idx not in heads_to_prune:
                heads_to_prune[layer_idx] = []
            heads_to_prune[layer_idx].append(head_idx)

        return heads_to_prune


class LayerPruner:
    """
    Prunes entire transformer layers.

    Most aggressive form of structured pruning.
    Can achieve significant speedups but requires careful selection.
    """

    def __init__(self, target_layer_reduction: float = 0.25):
        self.target_reduction = target_layer_reduction

    def compute_layer_importance(
        self,
        layer_inputs: List[np.ndarray],
        layer_outputs: List[np.ndarray]
    ) -> np.ndarray:
        """
        Compute importance for each layer.

        Uses similarity between input and output as proxy for importance.
        Layers with output similar to input contribute less.
        """
        n_layers = len(layer_inputs)
        importance = np.zeros(n_layers)

        for i in range(n_layers):
            inp = layer_inputs[i].flatten()
            out = layer_outputs[i].flatten()

            # Cosine similarity - higher means layer changes input more
            similarity = np.dot(inp, out) / (
                np.linalg.norm(inp) * np.linalg.norm(out) + 1e-8
            )

            # Layers that change input more are more important
            importance[i] = 1 - similarity

        return importance

    def select_layers_to_prune(
        self,
        importance: np.ndarray,
        n_layers_total: int
    ) -> List[int]:
        """
        Select layers to prune.

        Args:
            importance: Importance score per layer
            n_layers_total: Total number of layers

        Returns:
            Indices of layers to prune
        """
        n_to_prune = int(n_layers_total * self.target_reduction)

        # Don't prune first or last few layers (often critical)
        protected_start = 2
        protected_end = 2

        prunable_layers = list(range(protected_start, n_layers_total - protected_end))
        prunable_importance = importance[protected_start:n_layers_total - protected_end]

        # Select least important layers
        sorted_indices = np.argsort(prunable_importance)
        layers_to_prune = [prunable_layers[i] for i in sorted_indices[:n_to_prune]]

        return sorted(layers_to_prune)


class FFNPruner:
    """
    Prunes FFN intermediate dimension (reduces hidden dimension).

    Effectively reduces the width of MLP layers.
    Can be combined with head pruning for comprehensive structured pruning.
    """

    def __init__(self, target_reduction: float = 0.25):
        self.target_reduction = target_reduction

    def compute_neuron_importance(
        self,
        weight_up: np.ndarray,
        weight_down: np.ndarray,
        activations: np.ndarray
    ) -> np.ndarray:
        """
        Compute importance of each intermediate neuron.

        Args:
            weight_up: Gate/up projection [intermediate, hidden]
            weight_down: Down projection [hidden, intermediate]
            activations: Intermediate activations [batch, seq, intermediate]

        Returns:
            Importance score per intermediate neuron
        """
        intermediate_dim = weight_up.shape[0]

        # Metric 1: Weight magnitude
        weight_importance = np.linalg.norm(weight_up, axis=1) * np.linalg.norm(weight_down, axis=0)

        # Metric 2: Activation magnitude
        act_importance = np.mean(np.abs(activations), axis=(0, 1))

        # Combined importance
        importance = weight_importance * act_importance

        return importance

    def get_neurons_to_keep(
        self,
        importance: np.ndarray
    ) -> np.ndarray:
        """Get indices of neurons to keep."""
        n_keep = int(len(importance) * (1 - self.target_reduction))
        indices = np.argsort(importance)[-n_keep:]  # Keep most important
        return np.sort(indices)
```

---

## 6.2.3 Hardware Support for Sparsity

### NVIDIA 2:4 Sparsity

```python
"""
ABOUTME: Hardware acceleration for sparse operations on NVIDIA GPUs.
ABOUTME: Implements 2:4 sparsity conversion and validation.
"""

from dataclasses import dataclass
from typing import Dict, Tuple, Optional
import numpy as np


@dataclass
class SparseHardwareConfig:
    """Configuration for hardware-accelerated sparsity."""
    pattern: str = "2:4"
    hardware: str = "ampere"
    validate_pattern: bool = True


class NVIDIASparseConverter:
    """
    Converts dense weights to NVIDIA sparse format.

    NVIDIA Ampere+ GPUs support 2:4 structured sparsity
    with native tensor core acceleration.
    """

    def convert_to_2_4_sparse(
        self,
        weight: np.ndarray,
        importance_metric: str = "magnitude"
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Convert weight matrix to 2:4 sparse format.

        In each group of 4 consecutive elements, exactly 2 are zero.

        Args:
            weight: Dense weight matrix
            importance_metric: How to select which weights to keep

        Returns:
            (compressed_weight, indices) for sparse representation
        """
        if weight.ndim != 2:
            raise ValueError("Expected 2D weight matrix")

        out_features, in_features = weight.shape

        # Ensure in_features is divisible by 4
        if in_features % 4 != 0:
            # Pad to nearest multiple of 4
            pad_size = 4 - (in_features % 4)
            weight = np.pad(weight, ((0, 0), (0, pad_size)), mode='constant')
            in_features = weight.shape[1]

        # Compute importance
        if importance_metric == "magnitude":
            importance = np.abs(weight)
        else:
            importance = np.abs(weight)

        # Select top 2 in each group of 4
        compressed = np.zeros((out_features, in_features // 2))
        indices = np.zeros((out_features, in_features // 4), dtype=np.uint8)

        for i in range(out_features):
            for j in range(0, in_features, 4):
                group = weight[i, j:j+4]
                group_importance = importance[i, j:j+4]

                # Get indices of top 2
                top_2_local = np.argsort(group_importance)[-2:]
                top_2_local = np.sort(top_2_local)

                # Store compressed values and indices
                compressed_idx = j // 2
                compressed[i, compressed_idx] = group[top_2_local[0]]
                compressed[i, compressed_idx + 1] = group[top_2_local[1]]

                # Encode indices as 2-bit per position (4 possibilities each)
                # Index encoding: which 2 of 4 positions are non-zero
                index_code = self._encode_2_4_indices(top_2_local)
                indices[i, j // 4] = index_code

        return compressed, indices

    def _encode_2_4_indices(self, positions: np.ndarray) -> int:
        """
        Encode which 2 of 4 positions are non-zero.

        There are C(4,2) = 6 possibilities:
        [0,1], [0,2], [0,3], [1,2], [1,3], [2,3]
        """
        encoding_map = {
            (0, 1): 0,
            (0, 2): 1,
            (0, 3): 2,
            (1, 2): 3,
            (1, 3): 4,
            (2, 3): 5
        }
        return encoding_map.get(tuple(sorted(positions)), 0)

    def decompress_2_4_sparse(
        self,
        compressed: np.ndarray,
        indices: np.ndarray,
        original_shape: Tuple[int, int]
    ) -> np.ndarray:
        """Decompress 2:4 sparse format back to dense."""
        out_features, in_features = original_shape
        dense = np.zeros((out_features, in_features))

        decoding_map = {
            0: (0, 1),
            1: (0, 2),
            2: (0, 3),
            3: (1, 2),
            4: (1, 3),
            5: (2, 3)
        }

        for i in range(out_features):
            for j in range(0, in_features, 4):
                compressed_idx = j // 2
                index_code = indices[i, j // 4]

                positions = decoding_map[index_code]
                dense[i, j + positions[0]] = compressed[i, compressed_idx]
                dense[i, j + positions[1]] = compressed[i, compressed_idx + 1]

        return dense

    def validate_2_4_pattern(self, weight: np.ndarray) -> bool:
        """Validate that weight matrix follows 2:4 pattern."""
        out_features, in_features = weight.shape

        for i in range(out_features):
            for j in range(0, in_features, 4):
                group = weight[i, j:min(j+4, in_features)]
                if len(group) == 4:
                    n_zeros = np.sum(group == 0)
                    if n_zeros != 2:
                        return False

        return True

    def get_theoretical_speedup(self) -> Dict[str, float]:
        """Get theoretical speedup from 2:4 sparsity."""
        return {
            "compute_speedup": 2.0,      # 2x FLOPS reduction
            "memory_reduction": 1.5,      # Compressed format overhead
            "practical_speedup": "1.5-1.8x",  # Real-world (memory bandwidth bound)
            "notes": [
                "Speedup depends on operation being compute-bound",
                "Memory-bound operations see less benefit",
                "Sparse tensor cores handle 2:4 natively"
            ]
        }
```

---

## 6.2.4 Sparsity in Practice

### Quality-Sparsity Tradeoffs

```python
"""
ABOUTME: Analysis and prediction of quality degradation from sparsity.
ABOUTME: Provides guidance on achievable sparsity levels.
"""

from dataclasses import dataclass
from typing import Dict, List, Tuple
import numpy as np


@dataclass
class SparsityQualityProfile:
    """Profile of quality vs sparsity relationship."""
    model_family: str
    size_b: float
    sparsity_levels: List[float]
    perplexity_increase: List[float]
    benchmark_retention: Dict[str, List[float]]


class SparsityAnalyzer:
    """
    Analyzes quality-sparsity tradeoffs for LLMs.

    Based on empirical data from pruning studies.
    """

    # Empirical data from various pruning studies
    QUALITY_PROFILES = {
        "llama-7b": SparsityQualityProfile(
            model_family="llama",
            size_b=7.0,
            sparsity_levels=[0.0, 0.25, 0.50, 0.60, 0.70],
            perplexity_increase=[0, 0.1, 0.3, 0.8, 2.5],
            benchmark_retention={
                "mmlu": [1.0, 0.98, 0.95, 0.90, 0.80],
                "hellaswag": [1.0, 0.99, 0.97, 0.93, 0.85],
                "gsm8k": [1.0, 0.95, 0.85, 0.70, 0.50]
            }
        ),
        "llama-70b": SparsityQualityProfile(
            model_family="llama",
            size_b=70.0,
            sparsity_levels=[0.0, 0.25, 0.50, 0.60, 0.70],
            perplexity_increase=[0, 0.05, 0.15, 0.4, 1.5],
            benchmark_retention={
                "mmlu": [1.0, 0.99, 0.97, 0.94, 0.88],
                "hellaswag": [1.0, 0.995, 0.98, 0.96, 0.90],
                "gsm8k": [1.0, 0.98, 0.92, 0.82, 0.65]
            }
        )
    }

    def estimate_quality_at_sparsity(
        self,
        model_size_b: float,
        target_sparsity: float,
        method: str = "wanda"
    ) -> Dict[str, float]:
        """
        Estimate quality retention at given sparsity.

        Args:
            model_size_b: Model size in billions
            target_sparsity: Target sparsity (0.0-1.0)
            method: Pruning method
        """
        # Find closest model profile
        profile = self._get_closest_profile(model_size_b)

        # Interpolate quality metrics
        perplexity_increase = np.interp(
            target_sparsity,
            profile.sparsity_levels,
            profile.perplexity_increase
        )

        benchmark_retention = {}
        for benchmark, retentions in profile.benchmark_retention.items():
            retention = np.interp(
                target_sparsity,
                profile.sparsity_levels,
                retentions
            )
            benchmark_retention[benchmark] = float(retention)

        # Adjust for method (SparseGPT typically better than Wanda at high sparsity)
        method_adjustment = {
            "sparsegpt": 0.95,  # 5% less degradation
            "wanda": 1.0,
            "magnitude": 1.2   # 20% more degradation
        }
        adjustment = method_adjustment.get(method, 1.0)

        perplexity_increase *= adjustment

        return {
            "estimated_perplexity_increase": float(perplexity_increase),
            "benchmark_retention": benchmark_retention,
            "method": method,
            "notes": self._get_quality_notes(target_sparsity, model_size_b)
        }

    def recommend_sparsity(
        self,
        model_size_b: float,
        max_quality_loss_pct: float = 5.0,
        hardware: str = "generic"
    ) -> Dict[str, any]:
        """
        Recommend sparsity level based on quality constraints.

        Args:
            model_size_b: Model size
            max_quality_loss_pct: Maximum acceptable quality loss
            hardware: Target hardware for pattern selection
        """
        # Search for maximum sparsity within quality budget
        profile = self._get_closest_profile(model_size_b)

        recommended_sparsity = 0.0
        for sparsity, retentions in zip(
            profile.sparsity_levels,
            zip(*profile.benchmark_retention.values())
        ):
            avg_retention = np.mean(retentions)
            quality_loss = (1 - avg_retention) * 100

            if quality_loss <= max_quality_loss_pct:
                recommended_sparsity = sparsity

        # Adjust for hardware
        pattern = "unstructured"
        if hardware in ["ampere", "hopper", "A100", "H100"]:
            # Use 2:4 if close to 50%
            if 0.4 <= recommended_sparsity <= 0.6:
                recommended_sparsity = 0.5
                pattern = "2:4"

        return {
            "recommended_sparsity": recommended_sparsity,
            "pattern": pattern,
            "expected_quality_loss_pct": max_quality_loss_pct,
            "expected_speedup": self._estimate_speedup(recommended_sparsity, pattern),
            "method_recommendation": "sparsegpt" if recommended_sparsity > 0.5 else "wanda"
        }

    def _get_closest_profile(self, model_size_b: float) -> SparsityQualityProfile:
        """Get profile for closest model size."""
        # Simple heuristic - use smaller model profile if no exact match
        if model_size_b <= 13:
            return self.QUALITY_PROFILES["llama-7b"]
        else:
            return self.QUALITY_PROFILES["llama-70b"]

    def _get_quality_notes(
        self,
        sparsity: float,
        model_size: float
    ) -> List[str]:
        """Get notes about quality at given sparsity."""
        notes = []

        if sparsity > 0.6:
            notes.append("High sparsity - consider fine-tuning after pruning")

        if sparsity > 0.5 and model_size < 13:
            notes.append("Smaller models more sensitive to high sparsity")

        if sparsity <= 0.5:
            notes.append("Moderate sparsity - should retain most quality")

        return notes

    def _estimate_speedup(self, sparsity: float, pattern: str) -> str:
        """Estimate speedup from sparsity."""
        if pattern == "2:4":
            return "1.5-2.0x on Ampere+"
        elif sparsity > 0:
            return f"Memory reduction only ({1/(1-sparsity):.1f}x smaller)"
        else:
            return "No speedup"
```

---

## 6.2.5 Layer-wise Pruning

### Sensitivity Analysis

```python
"""
ABOUTME: Layer-wise sensitivity analysis for non-uniform pruning.
ABOUTME: Enables targeted pruning based on layer importance.
"""

from dataclasses import dataclass
from typing import Dict, List, Tuple
import numpy as np


@dataclass
class LayerSensitivity:
    """Sensitivity analysis results for a layer."""
    layer_name: str
    layer_idx: int
    baseline_loss: float
    pruned_losses: Dict[float, float]  # sparsity -> loss
    recommended_sparsity: float
    sensitivity_score: float


class LayerSensitivityAnalyzer:
    """
    Analyzes per-layer sensitivity to pruning.

    Enables non-uniform sparsity allocation where less
    sensitive layers get higher sparsity.
    """

    def analyze_layer_sensitivity(
        self,
        model,
        eval_data,
        layer_names: List[str],
        sparsity_levels: List[float] = [0.25, 0.50, 0.75]
    ) -> Dict[str, LayerSensitivity]:
        """
        Analyze sensitivity of each layer to pruning.

        Temporarily prunes each layer and measures loss increase.
        """
        results = {}

        # Get baseline loss
        baseline_loss = self._compute_loss(model, eval_data)

        for idx, layer_name in enumerate(layer_names):
            layer_results = {}

            for sparsity in sparsity_levels:
                # Temporarily prune this layer
                original_weights = self._get_layer_weights(model, layer_name)
                self._apply_sparsity(model, layer_name, sparsity)

                # Compute loss with pruned layer
                pruned_loss = self._compute_loss(model, eval_data)
                layer_results[sparsity] = pruned_loss

                # Restore original weights
                self._set_layer_weights(model, layer_name, original_weights)

            # Compute sensitivity score (loss increase per unit sparsity)
            sensitivity_score = self._compute_sensitivity_score(
                baseline_loss, layer_results
            )

            # Recommend sparsity based on sensitivity
            recommended = self._recommend_layer_sparsity(
                sensitivity_score, layer_results
            )

            results[layer_name] = LayerSensitivity(
                layer_name=layer_name,
                layer_idx=idx,
                baseline_loss=baseline_loss,
                pruned_losses=layer_results,
                recommended_sparsity=recommended,
                sensitivity_score=sensitivity_score
            )

        return results

    def compute_non_uniform_sparsity(
        self,
        sensitivity_results: Dict[str, LayerSensitivity],
        target_overall_sparsity: float
    ) -> Dict[str, float]:
        """
        Compute non-uniform sparsity allocation.

        Less sensitive layers get higher sparsity,
        while sensitive layers are preserved.
        """
        # Normalize sensitivity scores
        scores = {
            name: result.sensitivity_score
            for name, result in sensitivity_results.items()
        }

        total_score = sum(scores.values())
        normalized = {name: s / total_score for name, s in scores.items()}

        # Invert: less sensitive = higher sparsity
        inverted = {name: 1 - s for name, s in normalized.items()}
        inv_total = sum(inverted.values())
        inv_normalized = {name: s / inv_total for name, s in inverted.items()}

        # Scale to target overall sparsity
        # Each layer gets sparsity proportional to (1 - normalized_sensitivity)
        n_layers = len(scores)
        sparsity_allocation = {}

        for name in scores:
            # Base sparsity scaled by inverse sensitivity
            layer_sparsity = target_overall_sparsity * inv_normalized[name] * n_layers

            # Clip to reasonable range
            layer_sparsity = np.clip(layer_sparsity, 0.1, 0.9)
            sparsity_allocation[name] = float(layer_sparsity)

        return sparsity_allocation

    def _compute_sensitivity_score(
        self,
        baseline: float,
        pruned_losses: Dict[float, float]
    ) -> float:
        """
        Compute sensitivity score from pruned losses.

        Score = slope of loss increase vs sparsity.
        """
        sparsities = sorted(pruned_losses.keys())
        losses = [pruned_losses[s] - baseline for s in sparsities]

        # Linear regression for slope
        if len(sparsities) > 1:
            slope = np.polyfit(sparsities, losses, 1)[0]
            return max(0, slope)
        return 0

    def _recommend_layer_sparsity(
        self,
        sensitivity: float,
        pruned_losses: Dict[float, float]
    ) -> float:
        """Recommend sparsity for layer based on sensitivity."""
        # Higher sensitivity = lower recommended sparsity
        if sensitivity > 1.0:
            return 0.25
        elif sensitivity > 0.5:
            return 0.50
        else:
            return 0.75

    def _compute_loss(self, model, data) -> float:
        """Compute loss on evaluation data."""
        # Placeholder - would compute actual loss
        return np.random.uniform(1, 3)

    def _get_layer_weights(self, model, layer_name) -> np.ndarray:
        """Get weights for a layer."""
        # Placeholder
        return np.random.randn(1000, 1000)

    def _set_layer_weights(self, model, layer_name, weights: np.ndarray):
        """Set weights for a layer."""
        pass

    def _apply_sparsity(self, model, layer_name: str, sparsity: float):
        """Apply sparsity to a layer."""
        pass
```

---

## Appendix A: SparseGPT Implementation Example

```python
# Example usage of SparseGPT with transformers
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

def sparsify_model_sparsegpt(
    model_name: str,
    sparsity: float = 0.5,
    calibration_data: list = None
):
    """
    Apply SparseGPT to a HuggingFace model.

    Simplified example - use actual SparseGPT library for production.
    """
    # Load model
    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # Prepare calibration data
    if calibration_data is None:
        calibration_data = ["The quick brown fox"] * 128

    # Tokenize
    inputs = tokenizer(
        calibration_data,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=2048
    )

    # Collect activations and apply SparseGPT per layer
    # (Simplified - actual implementation is more complex)

    for name, module in model.named_modules():
        if isinstance(module, torch.nn.Linear):
            # Apply SparseGPT to this layer
            pass

    return model
```

---

## Appendix B: Wanda Pruning Script

```python
# Quick Wanda pruning implementation
import torch
import torch.nn as nn

def wanda_prune_layer(
    weight: torch.Tensor,
    activations: torch.Tensor,
    sparsity: float,
    pattern: str = "unstructured"
) -> torch.Tensor:
    """
    Prune layer using Wanda criterion.

    Args:
        weight: [out_features, in_features]
        activations: [batch, seq, in_features]
        sparsity: Target sparsity (0.0-1.0)
        pattern: "unstructured" or "2:4"

    Returns:
        Pruned weight tensor
    """
    # Flatten activations
    act = activations.view(-1, activations.shape[-1])

    # Compute L2 norm per input feature
    act_norms = torch.norm(act, p=2, dim=0)

    # Wanda importance: |W| * ||X||_2
    importance = weight.abs() * act_norms.unsqueeze(0)

    if pattern == "unstructured":
        # Global threshold
        threshold = torch.quantile(importance.flatten(), sparsity)
        mask = importance >= threshold
    elif pattern == "2:4":
        # Per-group selection
        mask = torch.ones_like(weight, dtype=torch.bool)
        for i in range(weight.shape[0]):
            for j in range(0, weight.shape[1], 4):
                group = importance[i, j:j+4]
                if len(group) == 4:
                    prune_idx = torch.topk(group, k=2, largest=False).indices
                    mask[i, j + prune_idx] = False
    else:
        mask = torch.ones_like(weight, dtype=torch.bool)

    # Apply mask
    return weight * mask.float()
```

---

## Troubleshooting

**Issue: Large quality drop after pruning**
```
Diagnosis:
1. Check if target sparsity is too aggressive
2. Verify calibration data quality
3. Check layer-wise sparsity distribution

Solutions:
- Reduce sparsity level
- Use better calibration data (domain-specific)
- Apply non-uniform sparsity (less on sensitive layers)
- Fine-tune after pruning
```

**Issue: No speedup despite high sparsity**
```
Diagnosis:
1. Check sparsity pattern (unstructured won't help)
2. Verify hardware supports sparse operations
3. Check if inference framework uses sparse kernels

Solutions:
- Use 2:4 structured sparsity on Ampere+
- Use TensorRT-LLM or other optimized framework
- Accept memory-only benefits for unstructured
```

**Issue: 2:4 pattern causing quality issues**
```
Diagnosis:
1. 50% sparsity may be too aggressive for model
2. Per-group constraint is limiting

Solutions:
- Compare with unstructured at 50%
- Use SparseGPT with 2:4 pattern (better weight updates)
- Fine-tune with 2:4 constraint
```

---

## Glossary

| Term | Definition |
|------|------------|
| **Unstructured Pruning** | Removing individual weights regardless of position |
| **Structured Pruning** | Removing entire neurons, heads, or layers |
| **2:4 Sparsity** | Pattern where 2 of every 4 weights are zero |
| **N:M Sparsity** | N zeros in every M consecutive weights |
| **Magnitude Pruning** | Pruning weights with smallest absolute values |
| **SparseGPT** | One-shot pruning using Hessian information |
| **Wanda** | Simple pruning using weight × activation norm |
| **Sensitivity** | How much layer quality degrades when pruned |

---

## References

1. Frantar, E., & Alistarh, D. (2023). "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot."
2. Sun, M., et al. (2023). "A Simple and Effective Pruning Approach for Large Language Models." [Wanda]
3. Pool, J., & Yu, J. (2021). "Accelerating Sparse Deep Neural Networks."
4. Michel, P., et al. (2019). "Are Sixteen Heads Really Better than One?"
5. Ma, X., et al. (2023). "LLM-Pruner: On the Structural Pruning of Large Language Models."
6. NVIDIA. (2023). "Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture."

---

> **Navigation**
> [← 6.1 Quantization](6.1_quantization_guide.md) | **[Index](../README.md#15-repository-structure)** | [6.3 Distillation →](6.3_knowledge_distillation_guide.md)
