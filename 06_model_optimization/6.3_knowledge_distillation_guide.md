> **Navigation** | [← 6.2 Pruning](6.2_pruning_sparsity_guide.md) | [6.4 Speculative Decoding →](6.4_speculative_decoding_guide.md)
>
> | | |
> |---|---|
> | **Prerequisites** | [6.1 Quantization](6.1_quantization_guide.md) &#124; [6.2 Pruning](6.2_pruning_sparsity_guide.md) |
> | **Related** | [3.1 SFT](../03_fine_tuning/3.1_supervised_fine_tuning.md) &#124; [2.2 Model Architecture](../02_model_training/2.2_model_architecture_selection.md) |
> | **Next** | [6.4 Speculative Decoding](6.4_speculative_decoding_guide.md) |

# Document 6.3: Knowledge Distillation Guide

## Executive Summary

Knowledge distillation is a model compression technique that transfers knowledge from a large, capable teacher model to a smaller, efficient student model. This guide provides comprehensive coverage of distillation fundamentals, methods (logit-based, feature-based, attention transfer, chain-of-thought), practical implementation strategies, student model design principles, and quality-size tradeoff analysis. Proper distillation can achieve 60-97% of teacher performance with models 4-10x smaller.

## Prerequisites

- Understanding of neural network architectures (transformers, attention mechanisms)
- Familiarity with model training and optimization
- Experience with PyTorch or similar deep learning frameworks
- Knowledge of loss functions (cross-entropy, KL divergence)
- Completion of documents 6.1 (Quantization) and 6.2 (Pruning) recommended

---

## 6.3.1 Distillation Fundamentals

### Teacher-Student Paradigm

Knowledge distillation operates on the principle that a large, well-trained teacher model contains "dark knowledge" in its output distributions beyond just the correct labels.

```python
"""
ABOUTME: Core knowledge distillation implementation with teacher-student framework.
ABOUTME: Provides foundational classes for all distillation approaches.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple, Any, Callable
from dataclasses import dataclass, field
from enum import Enum
import numpy as np
from abc import ABC, abstractmethod


class DistillationType(Enum):
    """Types of knowledge distillation."""
    LOGIT = "logit"              # Output probability distillation
    FEATURE = "feature"          # Intermediate representation distillation
    ATTENTION = "attention"      # Attention pattern distillation
    CHAIN_OF_THOUGHT = "cot"     # Reasoning chain distillation
    DATA = "data"                # Synthetic data generation
    HYBRID = "hybrid"            # Combined approaches


@dataclass
class DistillationConfig:
    """Configuration for knowledge distillation."""
    # Temperature scaling
    temperature: float = 4.0          # Softens probability distribution
    temperature_schedule: str = "constant"  # constant, linear_decay, cosine

    # Loss weighting
    alpha: float = 0.5                # Weight for distillation loss
    beta: float = 0.5                 # Weight for hard label loss
    alpha_schedule: str = "constant"  # constant, linear_decay, warmup_decay

    # Feature distillation
    feature_layers: List[int] = field(default_factory=list)
    feature_weight: float = 0.1

    # Attention distillation
    attention_layers: List[int] = field(default_factory=list)
    attention_weight: float = 0.1

    # Training settings
    max_steps: int = 100000
    warmup_steps: int = 1000
    eval_steps: int = 500


class TemperatureScheduler:
    """
    Manages temperature scheduling during distillation.

    Higher temperatures produce softer probability distributions,
    revealing more information about teacher's uncertainty.
    """

    def __init__(
        self,
        initial_temp: float = 4.0,
        final_temp: float = 1.0,
        schedule: str = "constant",
        total_steps: int = 100000,
        warmup_steps: int = 1000
    ):
        self.initial_temp = initial_temp
        self.final_temp = final_temp
        self.schedule = schedule
        self.total_steps = total_steps
        self.warmup_steps = warmup_steps

    def get_temperature(self, step: int) -> float:
        """Get temperature for current training step."""
        if self.schedule == "constant":
            return self.initial_temp

        if step < self.warmup_steps:
            # Linear warmup to initial temperature
            return 1.0 + (self.initial_temp - 1.0) * (step / self.warmup_steps)

        # Progress after warmup
        progress = (step - self.warmup_steps) / (self.total_steps - self.warmup_steps)
        progress = min(1.0, max(0.0, progress))

        if self.schedule == "linear_decay":
            return self.initial_temp - (self.initial_temp - self.final_temp) * progress

        elif self.schedule == "cosine":
            return self.final_temp + (self.initial_temp - self.final_temp) * \
                   (1 + np.cos(np.pi * progress)) / 2

        elif self.schedule == "exponential_decay":
            decay_rate = np.log(self.final_temp / self.initial_temp)
            return self.initial_temp * np.exp(decay_rate * progress)

        return self.initial_temp


class AlphaScheduler:
    """
    Manages loss weight scheduling during distillation.

    Alpha controls balance between distillation loss (soft labels)
    and classification loss (hard labels).
    """

    def __init__(
        self,
        initial_alpha: float = 0.9,
        final_alpha: float = 0.1,
        schedule: str = "linear_decay",
        total_steps: int = 100000,
        warmup_steps: int = 5000
    ):
        self.initial_alpha = initial_alpha
        self.final_alpha = final_alpha
        self.schedule = schedule
        self.total_steps = total_steps
        self.warmup_steps = warmup_steps

    def get_alpha(self, step: int) -> Tuple[float, float]:
        """
        Get alpha (distillation weight) and beta (hard label weight).

        Returns:
            Tuple of (alpha, beta) where alpha + beta = 1
        """
        if self.schedule == "constant":
            alpha = self.initial_alpha
        else:
            if step < self.warmup_steps:
                # Start with high distillation weight
                alpha = self.initial_alpha
            else:
                progress = (step - self.warmup_steps) / (self.total_steps - self.warmup_steps)
                progress = min(1.0, max(0.0, progress))

                if self.schedule == "linear_decay":
                    alpha = self.initial_alpha - (self.initial_alpha - self.final_alpha) * progress
                elif self.schedule == "cosine":
                    alpha = self.final_alpha + (self.initial_alpha - self.final_alpha) * \
                           (1 + np.cos(np.pi * progress)) / 2
                else:
                    alpha = self.initial_alpha

        return alpha, 1.0 - alpha


class SoftTargetDistillation(nn.Module):
    """
    Standard soft target knowledge distillation (Hinton et al., 2015).

    The teacher's soft probability distribution provides richer
    supervision than hard labels alone, encoding relationships
    between classes.
    """

    def __init__(self, temperature: float = 4.0):
        super().__init__()
        self.temperature = temperature

    def forward(
        self,
        student_logits: torch.Tensor,
        teacher_logits: torch.Tensor,
        labels: Optional[torch.Tensor] = None,
        alpha: float = 0.5
    ) -> Dict[str, torch.Tensor]:
        """
        Compute distillation loss.

        Args:
            student_logits: Student model output logits [batch, vocab]
            teacher_logits: Teacher model output logits [batch, vocab]
            labels: Hard labels for classification loss
            alpha: Weight for distillation loss

        Returns:
            Dictionary with total loss and components
        """
        # Soft targets with temperature scaling
        soft_teacher = F.softmax(teacher_logits / self.temperature, dim=-1)
        soft_student = F.log_softmax(student_logits / self.temperature, dim=-1)

        # KL divergence loss (scaled by T^2 as per Hinton et al.)
        distillation_loss = F.kl_div(
            soft_student,
            soft_teacher,
            reduction='batchmean'
        ) * (self.temperature ** 2)

        result = {
            'distillation_loss': distillation_loss,
            'total_loss': distillation_loss
        }

        # Add hard label loss if labels provided
        if labels is not None:
            hard_loss = F.cross_entropy(student_logits, labels)
            result['hard_loss'] = hard_loss
            result['total_loss'] = alpha * distillation_loss + (1 - alpha) * hard_loss

        return result


class SequenceDistillation(nn.Module):
    """
    Knowledge distillation for sequence generation (LLMs).

    Handles token-by-token distillation for autoregressive models.
    """

    def __init__(
        self,
        temperature: float = 2.0,
        use_reverse_kl: bool = True,  # MiniLLM finding
        top_k_distill: Optional[int] = None
    ):
        super().__init__()
        self.temperature = temperature
        self.use_reverse_kl = use_reverse_kl
        self.top_k_distill = top_k_distill

    def forward(
        self,
        student_logits: torch.Tensor,
        teacher_logits: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        labels: Optional[torch.Tensor] = None,
        alpha: float = 0.5
    ) -> Dict[str, torch.Tensor]:
        """
        Compute sequence-level distillation loss.

        Args:
            student_logits: [batch, seq_len, vocab_size]
            teacher_logits: [batch, seq_len, vocab_size]
            attention_mask: [batch, seq_len] - 1 for valid positions
            labels: [batch, seq_len] - hard labels
            alpha: distillation weight

        Returns:
            Loss dictionary
        """
        batch_size, seq_len, vocab_size = student_logits.shape

        # Flatten for efficient computation
        student_flat = student_logits.view(-1, vocab_size)
        teacher_flat = teacher_logits.view(-1, vocab_size)

        if self.top_k_distill is not None:
            # Only distill from top-k teacher probabilities
            teacher_probs = F.softmax(teacher_flat / self.temperature, dim=-1)
            top_k_mask = torch.zeros_like(teacher_probs)
            top_k_indices = teacher_probs.topk(self.top_k_distill, dim=-1).indices
            top_k_mask.scatter_(-1, top_k_indices, 1.0)
            teacher_flat = teacher_flat.masked_fill(~top_k_mask.bool(), float('-inf'))

        # Temperature-scaled distributions
        teacher_probs = F.softmax(teacher_flat / self.temperature, dim=-1)

        if self.use_reverse_kl:
            # Reverse KL: prevents student from overestimating low-prob regions
            student_probs = F.softmax(student_flat / self.temperature, dim=-1)
            # KL(student || teacher) instead of KL(teacher || student)
            kl_loss = F.kl_div(
                teacher_probs.log(),
                student_probs,
                reduction='none'
            ).sum(dim=-1)
        else:
            # Forward KL: standard approach
            student_log_probs = F.log_softmax(student_flat / self.temperature, dim=-1)
            kl_loss = F.kl_div(
                student_log_probs,
                teacher_probs,
                reduction='none'
            ).sum(dim=-1)

        # Apply temperature scaling
        kl_loss = kl_loss * (self.temperature ** 2)

        # Reshape and apply mask
        kl_loss = kl_loss.view(batch_size, seq_len)

        if attention_mask is not None:
            kl_loss = kl_loss * attention_mask
            distillation_loss = kl_loss.sum() / attention_mask.sum()
        else:
            distillation_loss = kl_loss.mean()

        result = {
            'distillation_loss': distillation_loss,
            'total_loss': distillation_loss
        }

        # Hard label loss
        if labels is not None:
            labels_flat = labels.view(-1)
            hard_loss = F.cross_entropy(
                student_flat,
                labels_flat,
                ignore_index=-100,
                reduction='mean'
            )
            result['hard_loss'] = hard_loss
            result['total_loss'] = alpha * distillation_loss + (1 - alpha) * hard_loss

        return result


class TeacherStudentFramework:
    """
    Complete framework for teacher-student distillation.

    Manages teacher model, student training, and loss computation.
    """

    def __init__(
        self,
        teacher_model: nn.Module,
        student_model: nn.Module,
        config: DistillationConfig
    ):
        self.teacher = teacher_model
        self.student = student_model
        self.config = config

        # Freeze teacher
        self.teacher.eval()
        for param in self.teacher.parameters():
            param.requires_grad = False

        # Initialize schedulers
        self.temp_scheduler = TemperatureScheduler(
            initial_temp=config.temperature,
            schedule=config.temperature_schedule,
            total_steps=config.max_steps
        )

        self.alpha_scheduler = AlphaScheduler(
            initial_alpha=config.alpha,
            final_alpha=0.1,
            schedule=config.alpha_schedule,
            total_steps=config.max_steps
        )

        # Initialize distillation module
        self.distillation = SequenceDistillation(
            temperature=config.temperature
        )

        self.current_step = 0

    @torch.no_grad()
    def get_teacher_outputs(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None
    ) -> Dict[str, torch.Tensor]:
        """Get teacher model outputs for distillation."""
        outputs = self.teacher(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True,
            output_attentions=True
        )

        return {
            'logits': outputs.logits,
            'hidden_states': outputs.hidden_states,
            'attentions': outputs.attentions
        }

    def compute_loss(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        labels: Optional[torch.Tensor] = None
    ) -> Dict[str, torch.Tensor]:
        """
        Compute combined distillation loss.

        Returns:
            Dictionary with all loss components
        """
        # Get current hyperparameters
        temperature = self.temp_scheduler.get_temperature(self.current_step)
        alpha, beta = self.alpha_scheduler.get_alpha(self.current_step)

        # Teacher forward pass
        teacher_outputs = self.get_teacher_outputs(input_ids, attention_mask)

        # Student forward pass
        student_outputs = self.student(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True,
            output_attentions=True
        )

        # Update distillation temperature
        self.distillation.temperature = temperature

        # Compute logit distillation loss
        loss_dict = self.distillation(
            student_logits=student_outputs.logits,
            teacher_logits=teacher_outputs['logits'],
            attention_mask=attention_mask,
            labels=labels,
            alpha=alpha
        )

        loss_dict['temperature'] = temperature
        loss_dict['alpha'] = alpha

        self.current_step += 1

        return loss_dict
```

### Temperature Scaling Deep Dive

Temperature scaling is the key mechanism that makes soft targets informative:

```python
class TemperatureAnalyzer:
    """
    Analyze the effect of temperature on teacher distributions.

    Higher temperatures reveal more about class relationships
    but may introduce noise for very small students.
    """

    def __init__(self, vocab_size: int = 32000):
        self.vocab_size = vocab_size

    def analyze_temperature_effect(
        self,
        logits: torch.Tensor,
        temperatures: List[float] = [1.0, 2.0, 4.0, 8.0, 20.0]
    ) -> Dict[str, Any]:
        """
        Analyze how temperature affects probability distribution.

        Args:
            logits: Raw model logits [batch, vocab_size]
            temperatures: Temperature values to analyze

        Returns:
            Analysis results including entropy, effective support
        """
        results = {}

        for temp in temperatures:
            probs = F.softmax(logits / temp, dim=-1)

            # Entropy (measure of uncertainty/information)
            entropy = -torch.sum(probs * torch.log(probs + 1e-10), dim=-1)

            # Effective support (number of significant probabilities)
            # Using threshold of 1/vocab_size
            threshold = 1.0 / self.vocab_size
            effective_support = (probs > threshold).float().sum(dim=-1)

            # Top-k concentration
            top_1 = probs.max(dim=-1).values
            top_5 = probs.topk(5, dim=-1).values.sum(dim=-1)
            top_10 = probs.topk(10, dim=-1).values.sum(dim=-1)

            results[f'temp_{temp}'] = {
                'entropy': entropy.mean().item(),
                'effective_support': effective_support.mean().item(),
                'top_1_prob': top_1.mean().item(),
                'top_5_prob': top_5.mean().item(),
                'top_10_prob': top_10.mean().item()
            }

        return results

    def recommend_temperature(
        self,
        teacher_size: int,
        student_size: int,
        task_type: str = "general"
    ) -> Tuple[float, str]:
        """
        Recommend temperature based on model size ratio and task.

        Guidelines from research:
        - Larger size gaps benefit from lower temperatures
        - Complex tasks may need higher temperatures
        - T=2-5 works well for most cases
        """
        size_ratio = teacher_size / student_size

        recommendations = {
            'general': {
                'small_gap': (4.0, "Standard T=4 for similar sizes"),
                'medium_gap': (3.0, "Moderate T=3 for 4-10x compression"),
                'large_gap': (2.0, "Lower T=2 for >10x compression")
            },
            'reasoning': {
                'small_gap': (5.0, "Higher T=5 captures reasoning nuances"),
                'medium_gap': (4.0, "T=4 balances detail and capacity"),
                'large_gap': (2.5, "T=2.5 prevents information overload")
            },
            'classification': {
                'small_gap': (3.0, "T=3 for classification tasks"),
                'medium_gap': (2.5, "T=2.5 for moderate compression"),
                'large_gap': (2.0, "T=2 preserves decision boundaries")
            }
        }

        task_recs = recommendations.get(task_type, recommendations['general'])

        if size_ratio < 4:
            return task_recs['small_gap']
        elif size_ratio < 10:
            return task_recs['medium_gap']
        else:
            return task_recs['large_gap']
```

---

## 6.3.2 Distillation Methods

### Logit-Based Distillation

The most common and effective approach for LLM distillation:

```python
"""
ABOUTME: Advanced logit-based distillation methods for LLMs.
ABOUTME: Includes standard KD, MiniLLM, and cross-tokenizer approaches.
"""

class LogitDistillationMethod(Enum):
    """Available logit distillation methods."""
    STANDARD_KD = "standard"       # Hinton et al. 2015
    REVERSE_KL = "reverse_kl"      # MiniLLM approach
    BIDIRECTIONAL = "bidir"        # BiLD: Bi-directional
    UNIVERSAL = "universal"        # Cross-tokenizer
    SELECTIVE = "selective"        # Top-k token focused


class StandardLogitDistillation(nn.Module):
    """
    Standard logit distillation (Hinton et al., 2015).

    Minimizes KL(teacher || student) with temperature scaling.
    """

    def __init__(self, temperature: float = 4.0):
        super().__init__()
        self.temperature = temperature

    def forward(
        self,
        student_logits: torch.Tensor,
        teacher_logits: torch.Tensor
    ) -> torch.Tensor:
        """
        Compute forward KL divergence loss.

        Forward KL encourages student to cover all modes of teacher,
        which can lead to oversmoothing.
        """
        teacher_probs = F.softmax(teacher_logits / self.temperature, dim=-1)
        student_log_probs = F.log_softmax(student_logits / self.temperature, dim=-1)

        loss = F.kl_div(
            student_log_probs,
            teacher_probs,
            reduction='batchmean'
        ) * (self.temperature ** 2)

        return loss


class MiniLLMDistillation(nn.Module):
    """
    MiniLLM: Reverse KL distillation for language models.

    Uses KL(student || teacher) instead of KL(teacher || student).
    This prevents student from assigning probability mass to
    low-probability regions of teacher distribution.

    Reference: Gu et al., "MiniLLM: Knowledge Distillation of Large Language Models"
    """

    def __init__(
        self,
        temperature: float = 1.0,
        use_on_policy: bool = True,
        policy_gradient_steps: int = 1
    ):
        super().__init__()
        self.temperature = temperature
        self.use_on_policy = use_on_policy
        self.policy_gradient_steps = policy_gradient_steps

    def forward(
        self,
        student_logits: torch.Tensor,
        teacher_logits: torch.Tensor,
        student_samples: Optional[torch.Tensor] = None
    ) -> Dict[str, torch.Tensor]:
        """
        Compute reverse KL divergence loss.

        Reverse KL is mode-seeking rather than mode-covering,
        better suited for generative models.
        """
        teacher_probs = F.softmax(teacher_logits / self.temperature, dim=-1)
        student_probs = F.softmax(student_logits / self.temperature, dim=-1)

        # Reverse KL: KL(P_student || P_teacher)
        # = sum(P_student * log(P_student / P_teacher))
        reverse_kl = F.kl_div(
            (teacher_probs + 1e-10).log(),  # target in log space
            student_probs,                   # input as probabilities
            reduction='batchmean'
        )

        result = {
            'reverse_kl_loss': reverse_kl,
            'total_loss': reverse_kl
        }

        return result

    def on_policy_gradient(
        self,
        student_model: nn.Module,
        teacher_model: nn.Module,
        input_ids: torch.Tensor,
        max_length: int = 128
    ) -> torch.Tensor:
        """
        On-policy gradient estimation for reverse KL.

        Generates sequences from student, evaluates under both models,
        then uses policy gradient to minimize reverse KL.
        """
        # Generate from student (on-policy)
        with torch.no_grad():
            student_samples = student_model.generate(
                input_ids,
                max_length=max_length,
                do_sample=True,
                temperature=self.temperature
            )

        # Get log probabilities
        student_logits = student_model(student_samples).logits
        with torch.no_grad():
            teacher_logits = teacher_model(student_samples).logits

        # Compute per-token log probs
        student_log_probs = F.log_softmax(student_logits[:, :-1], dim=-1)
        teacher_log_probs = F.log_softmax(teacher_logits[:, :-1], dim=-1)

        targets = student_samples[:, 1:]

        # Gather log probs for generated tokens
        student_token_log_probs = student_log_probs.gather(
            -1, targets.unsqueeze(-1)
        ).squeeze(-1)
        teacher_token_log_probs = teacher_log_probs.gather(
            -1, targets.unsqueeze(-1)
        ).squeeze(-1)

        # Policy gradient objective
        # Reward: log P_teacher(token) - log P_student(token)
        rewards = teacher_token_log_probs - student_token_log_probs.detach()

        # REINFORCE with baseline
        policy_loss = -(rewards * student_token_log_probs).mean()

        return policy_loss


class BidirectionalLogitDistillation(nn.Module):
    """
    BiLD: Bi-directional Logits Difference Loss.

    Combines forward and reverse KL for balanced distillation.
    """

    def __init__(
        self,
        temperature: float = 2.0,
        forward_weight: float = 0.5,
        reverse_weight: float = 0.5
    ):
        super().__init__()
        self.temperature = temperature
        self.forward_weight = forward_weight
        self.reverse_weight = reverse_weight

    def forward(
        self,
        student_logits: torch.Tensor,
        teacher_logits: torch.Tensor
    ) -> Dict[str, torch.Tensor]:
        """Compute bidirectional KL loss."""
        teacher_probs = F.softmax(teacher_logits / self.temperature, dim=-1)
        student_probs = F.softmax(student_logits / self.temperature, dim=-1)
        student_log_probs = F.log_softmax(student_logits / self.temperature, dim=-1)
        teacher_log_probs = F.log_softmax(teacher_logits / self.temperature, dim=-1)

        # Forward KL: KL(teacher || student)
        forward_kl = F.kl_div(
            student_log_probs,
            teacher_probs,
            reduction='batchmean'
        )

        # Reverse KL: KL(student || teacher)
        reverse_kl = F.kl_div(
            teacher_log_probs,
            student_probs,
            reduction='batchmean'
        )

        total_loss = (
            self.forward_weight * forward_kl +
            self.reverse_weight * reverse_kl
        ) * (self.temperature ** 2)

        return {
            'forward_kl': forward_kl,
            'reverse_kl': reverse_kl,
            'total_loss': total_loss
        }


class SelectiveTokenDistillation(nn.Module):
    """
    Selective distillation focusing on important tokens.

    Not all tokens are equally important for distillation.
    Focus on tokens where teacher is confident or uncertain.
    """

    def __init__(
        self,
        temperature: float = 2.0,
        top_k: int = 100,
        entropy_threshold: float = 0.5
    ):
        super().__init__()
        self.temperature = temperature
        self.top_k = top_k
        self.entropy_threshold = entropy_threshold

    def forward(
        self,
        student_logits: torch.Tensor,
        teacher_logits: torch.Tensor
    ) -> Dict[str, torch.Tensor]:
        """
        Compute selective distillation loss.

        Focuses on top-k tokens and high-entropy positions.
        """
        batch_size, seq_len, vocab_size = student_logits.shape

        teacher_probs = F.softmax(teacher_logits / self.temperature, dim=-1)

        # Compute teacher entropy per position
        entropy = -torch.sum(
            teacher_probs * torch.log(teacher_probs + 1e-10),
            dim=-1
        )  # [batch, seq_len]

        # Mask for high-entropy positions (uncertain, more informative)
        entropy_mask = (entropy > self.entropy_threshold).float()

        # Top-k teacher probabilities only
        top_k_probs, top_k_indices = teacher_probs.topk(self.top_k, dim=-1)

        # Create sparse student distribution over top-k
        student_top_k_logits = torch.gather(
            student_logits,
            -1,
            top_k_indices
        )
        student_top_k_probs = F.log_softmax(
            student_top_k_logits / self.temperature,
            dim=-1
        )

        # Normalize teacher top-k
        top_k_probs = top_k_probs / top_k_probs.sum(dim=-1, keepdim=True)

        # KL divergence over top-k only
        kl_loss = F.kl_div(
            student_top_k_probs,
            top_k_probs,
            reduction='none'
        ).sum(dim=-1)  # [batch, seq_len]

        # Weight by entropy mask
        weighted_loss = (kl_loss * entropy_mask).sum() / (entropy_mask.sum() + 1e-10)

        return {
            'selective_loss': weighted_loss * (self.temperature ** 2),
            'mean_entropy': entropy.mean(),
            'high_entropy_ratio': entropy_mask.mean()
        }
```

### Feature-Based Distillation

Transfer knowledge from intermediate representations:

```python
"""
ABOUTME: Feature-based distillation for transferring intermediate representations.
ABOUTME: Includes FitNets-style hints and flexible layer mapping.
"""

class FeatureDistillation(nn.Module):
    """
    Feature-based knowledge distillation (FitNets style).

    Transfers knowledge from teacher's intermediate layers to student.
    Requires mapping between teacher and student layers when
    architectures differ.
    """

    def __init__(
        self,
        teacher_hidden_size: int,
        student_hidden_size: int,
        layer_mapping: Dict[int, int],  # student_layer -> teacher_layer
        projection_type: str = "linear"  # linear, mlp, or none
    ):
        super().__init__()
        self.layer_mapping = layer_mapping

        # Create projection layers if dimensions differ
        self.projections = nn.ModuleDict()

        if teacher_hidden_size != student_hidden_size:
            for student_layer in layer_mapping:
                if projection_type == "linear":
                    self.projections[str(student_layer)] = nn.Linear(
                        student_hidden_size,
                        teacher_hidden_size
                    )
                elif projection_type == "mlp":
                    self.projections[str(student_layer)] = nn.Sequential(
                        nn.Linear(student_hidden_size, teacher_hidden_size),
                        nn.ReLU(),
                        nn.Linear(teacher_hidden_size, teacher_hidden_size)
                    )

    def forward(
        self,
        student_hidden_states: Tuple[torch.Tensor, ...],
        teacher_hidden_states: Tuple[torch.Tensor, ...]
    ) -> Dict[str, torch.Tensor]:
        """
        Compute feature distillation loss.

        Args:
            student_hidden_states: Tuple of [batch, seq, hidden] per layer
            teacher_hidden_states: Tuple of [batch, seq, hidden] per layer

        Returns:
            Feature alignment losses
        """
        total_loss = 0.0
        layer_losses = {}

        for student_layer, teacher_layer in self.layer_mapping.items():
            student_feat = student_hidden_states[student_layer]
            teacher_feat = teacher_hidden_states[teacher_layer]

            # Project student features if needed
            if str(student_layer) in self.projections:
                student_feat = self.projections[str(student_layer)](student_feat)

            # L2 loss (MSE)
            loss = F.mse_loss(student_feat, teacher_feat)

            layer_losses[f'layer_{student_layer}_to_{teacher_layer}'] = loss
            total_loss += loss

        total_loss /= len(self.layer_mapping)

        return {
            'feature_loss': total_loss,
            'layer_losses': layer_losses
        }


class FlexibleFeatureDistillation(nn.Module):
    """
    Flexible feature distillation handling different hidden sizes.

    Uses learned transformations to align features across architectures.
    Based on Flex-KD approach.
    """

    def __init__(
        self,
        teacher_config: Dict[str, int],
        student_config: Dict[str, int],
        alignment_method: str = "learned"  # learned, cka, pooling
    ):
        super().__init__()
        self.alignment_method = alignment_method

        teacher_hidden = teacher_config['hidden_size']
        student_hidden = student_config['hidden_size']
        teacher_layers = teacher_config['num_layers']
        student_layers = student_config['num_layers']

        # Automatic layer mapping (uniform sampling)
        self.layer_mapping = self._compute_layer_mapping(
            teacher_layers, student_layers
        )

        # Alignment networks
        if alignment_method == "learned":
            self.student_projection = nn.Linear(student_hidden, teacher_hidden)
            self.alignment_head = nn.Sequential(
                nn.Linear(teacher_hidden * 2, teacher_hidden),
                nn.ReLU(),
                nn.Linear(teacher_hidden, 1)
            )

    def _compute_layer_mapping(
        self,
        teacher_layers: int,
        student_layers: int
    ) -> Dict[int, int]:
        """
        Compute optimal layer mapping using uniform sampling.

        Maps student layers to uniformly spaced teacher layers.
        """
        mapping = {}
        for s_layer in range(student_layers):
            # Map to corresponding position in teacher
            t_layer = int(s_layer * (teacher_layers - 1) / (student_layers - 1))
            mapping[s_layer] = t_layer
        return mapping

    def compute_cka_similarity(
        self,
        x: torch.Tensor,
        y: torch.Tensor
    ) -> torch.Tensor:
        """
        Compute Centered Kernel Alignment (CKA) similarity.

        CKA is invariant to orthogonal transformations and isotropic scaling,
        making it suitable for comparing representations of different sizes.
        """
        # Flatten to [batch*seq, hidden]
        x_flat = x.reshape(-1, x.shape[-1])
        y_flat = y.reshape(-1, y.shape[-1])

        # Center the features
        x_centered = x_flat - x_flat.mean(dim=0)
        y_centered = y_flat - y_flat.mean(dim=0)

        # Compute similarity matrices
        x_sim = x_centered @ x_centered.t()
        y_sim = y_centered @ y_centered.t()

        # CKA formula
        hsic_xy = (x_sim * y_sim).sum()
        hsic_xx = (x_sim * x_sim).sum()
        hsic_yy = (y_sim * y_sim).sum()

        cka = hsic_xy / (torch.sqrt(hsic_xx * hsic_yy) + 1e-10)

        return cka

    def forward(
        self,
        student_hidden_states: Tuple[torch.Tensor, ...],
        teacher_hidden_states: Tuple[torch.Tensor, ...]
    ) -> Dict[str, torch.Tensor]:
        """Compute flexible feature distillation loss."""

        if self.alignment_method == "cka":
            # Maximize CKA similarity
            total_cka = 0.0
            for s_layer, t_layer in self.layer_mapping.items():
                cka = self.compute_cka_similarity(
                    student_hidden_states[s_layer],
                    teacher_hidden_states[t_layer]
                )
                total_cka += cka

            # Minimize negative CKA (maximize CKA)
            loss = -total_cka / len(self.layer_mapping)
            return {'feature_loss': loss, 'mean_cka': -loss}

        elif self.alignment_method == "learned":
            # Learned projection approach
            total_loss = 0.0
            for s_layer, t_layer in self.layer_mapping.items():
                student_feat = self.student_projection(
                    student_hidden_states[s_layer]
                )
                teacher_feat = teacher_hidden_states[t_layer]

                loss = F.mse_loss(student_feat, teacher_feat.detach())
                total_loss += loss

            return {'feature_loss': total_loss / len(self.layer_mapping)}

        else:
            raise ValueError(f"Unknown alignment method: {self.alignment_method}")
```

### Attention Transfer Distillation

Transfer attention patterns from teacher to student:

```python
"""
ABOUTME: Attention-based distillation for transferring attention patterns.
ABOUTME: Captures how teacher model focuses on different parts of input.
"""

class AttentionDistillation(nn.Module):
    """
    Attention transfer distillation (Zagoruyko & Komodakis, 2017).

    Transfers attention maps from teacher to student, helping
    student learn what parts of input to focus on.
    """

    def __init__(
        self,
        layer_mapping: Dict[int, int],
        head_mapping: Optional[Dict[int, int]] = None,
        normalize: bool = True
    ):
        super().__init__()
        self.layer_mapping = layer_mapping
        self.head_mapping = head_mapping
        self.normalize = normalize

    def forward(
        self,
        student_attentions: Tuple[torch.Tensor, ...],
        teacher_attentions: Tuple[torch.Tensor, ...]
    ) -> Dict[str, torch.Tensor]:
        """
        Compute attention distillation loss.

        Args:
            student_attentions: Tuple of [batch, heads, seq, seq] per layer
            teacher_attentions: Tuple of [batch, heads, seq, seq] per layer

        Returns:
            Attention alignment losses
        """
        total_loss = 0.0
        layer_losses = {}

        for student_layer, teacher_layer in self.layer_mapping.items():
            student_attn = student_attentions[student_layer]
            teacher_attn = teacher_attentions[teacher_layer]

            # Handle different number of attention heads
            if student_attn.shape[1] != teacher_attn.shape[1]:
                # Average teacher heads to match student
                if student_attn.shape[1] < teacher_attn.shape[1]:
                    # Group teacher heads
                    ratio = teacher_attn.shape[1] // student_attn.shape[1]
                    teacher_attn = teacher_attn.reshape(
                        teacher_attn.shape[0],
                        student_attn.shape[1],
                        ratio,
                        teacher_attn.shape[2],
                        teacher_attn.shape[3]
                    ).mean(dim=2)
                else:
                    # Repeat teacher heads
                    ratio = student_attn.shape[1] // teacher_attn.shape[1]
                    teacher_attn = teacher_attn.repeat_interleave(ratio, dim=1)

            if self.normalize:
                # Normalize attention maps
                student_attn = F.normalize(student_attn, p=2, dim=-1)
                teacher_attn = F.normalize(teacher_attn, p=2, dim=-1)

            # MSE loss on attention patterns
            loss = F.mse_loss(student_attn, teacher_attn)

            layer_losses[f'layer_{student_layer}'] = loss
            total_loss += loss

        return {
            'attention_loss': total_loss / len(self.layer_mapping),
            'layer_losses': layer_losses
        }


class MultiHeadAttentionDistillation(nn.Module):
    """
    Advanced attention distillation with head-level granularity.

    Different attention heads capture different aspects;
    selective transfer may be more effective.
    """

    def __init__(
        self,
        teacher_heads: int,
        student_heads: int,
        layer_mapping: Dict[int, int],
        importance_weighted: bool = True
    ):
        super().__init__()
        self.teacher_heads = teacher_heads
        self.student_heads = student_heads
        self.layer_mapping = layer_mapping
        self.importance_weighted = importance_weighted

        # Learnable head importance weights
        if importance_weighted:
            self.head_importance = nn.ParameterDict({
                str(layer): nn.Parameter(torch.ones(teacher_heads))
                for layer in layer_mapping.values()
            })

    def compute_head_importance(
        self,
        attention_maps: torch.Tensor,
        method: str = "entropy"
    ) -> torch.Tensor:
        """
        Compute importance scores for each attention head.

        Args:
            attention_maps: [batch, heads, seq, seq]
            method: "entropy" or "variance"

        Returns:
            Importance scores [heads]
        """
        if method == "entropy":
            # Lower entropy = more focused = more important
            entropy = -torch.sum(
                attention_maps * torch.log(attention_maps + 1e-10),
                dim=-1
            ).mean(dim=(0, 2))
            importance = 1.0 / (entropy + 1.0)

        elif method == "variance":
            # Higher variance = more discriminative
            importance = attention_maps.var(dim=-1).mean(dim=(0, 2))

        # Normalize
        importance = F.softmax(importance, dim=0)

        return importance

    def forward(
        self,
        student_attentions: Tuple[torch.Tensor, ...],
        teacher_attentions: Tuple[torch.Tensor, ...]
    ) -> Dict[str, torch.Tensor]:
        """Compute importance-weighted attention distillation."""
        total_loss = 0.0

        for student_layer, teacher_layer in self.layer_mapping.items():
            student_attn = student_attentions[student_layer]
            teacher_attn = teacher_attentions[teacher_layer]

            # Get importance weights
            if self.importance_weighted:
                weights = F.softmax(
                    self.head_importance[str(teacher_layer)],
                    dim=0
                )
            else:
                weights = self.compute_head_importance(teacher_attn)

            # Compute weighted loss per head
            head_losses = []
            for h in range(min(student_attn.shape[1], teacher_attn.shape[1])):
                s_head = student_attn[:, h]
                t_head = teacher_attn[:, h % teacher_attn.shape[1]]

                head_loss = F.mse_loss(s_head, t_head, reduction='none').mean()
                head_losses.append(head_loss * weights[h % len(weights)])

            total_loss += torch.stack(head_losses).sum()

        return {
            'attention_loss': total_loss / len(self.layer_mapping),
            'head_importance': {
                k: v.detach().cpu().numpy()
                for k, v in self.head_importance.items()
            } if self.importance_weighted else {}
        }
```

### Chain-of-Thought Distillation

Transfer reasoning capabilities to smaller models:

```python
"""
ABOUTME: Chain-of-thought distillation for reasoning capability transfer.
ABOUTME: Transfers step-by-step reasoning from teacher to student.
"""

from typing import List, Dict, Any, Optional
import json
import re


@dataclass
class ReasoningExample:
    """A single reasoning example with chain-of-thought."""
    question: str
    reasoning_steps: List[str]
    answer: str
    metadata: Dict[str, Any] = field(default_factory=dict)


class ChainOfThoughtDistillation:
    """
    Chain-of-Thought distillation for reasoning transfer.

    Generates reasoning chains from teacher and trains student
    to reproduce both reasoning and answers.
    """

    def __init__(
        self,
        teacher_model: Any,
        tokenizer: Any,
        reasoning_template: str = "step_by_step"
    ):
        self.teacher = teacher_model
        self.tokenizer = tokenizer
        self.reasoning_template = reasoning_template

        self.templates = {
            "step_by_step": (
                "Solve this problem step by step:\n{question}\n\n"
                "Let's think through this carefully:\n"
            ),
            "structured": (
                "Problem: {question}\n\n"
                "Analysis:\n"
                "Step 1: [identify the key information]\n"
                "Step 2: [determine the approach]\n"
                "Step 3: [execute the solution]\n"
                "Conclusion:\n"
            ),
            "socratic": (
                "{question}\n\n"
                "To solve this, let me ask myself:\n"
                "- What do I know?\n"
                "- What do I need to find?\n"
                "- How can I get there?\n"
            )
        }

    def generate_reasoning_chain(
        self,
        question: str,
        max_length: int = 512,
        temperature: float = 0.7
    ) -> ReasoningExample:
        """
        Generate reasoning chain from teacher model.

        Args:
            question: Input question/problem
            max_length: Maximum generation length
            temperature: Sampling temperature

        Returns:
            ReasoningExample with steps and answer
        """
        template = self.templates.get(
            self.reasoning_template,
            self.templates["step_by_step"]
        )
        prompt = template.format(question=question)

        inputs = self.tokenizer(prompt, return_tensors="pt")

        with torch.no_grad():
            outputs = self.teacher.generate(
                **inputs,
                max_length=max_length,
                temperature=temperature,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id
            )

        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Parse reasoning steps and answer
        steps, answer = self._parse_reasoning(response)

        return ReasoningExample(
            question=question,
            reasoning_steps=steps,
            answer=answer,
            metadata={
                'template': self.reasoning_template,
                'temperature': temperature
            }
        )

    def _parse_reasoning(
        self,
        response: str
    ) -> Tuple[List[str], str]:
        """Parse reasoning response into steps and final answer."""
        # Split by common step indicators
        step_patterns = [
            r'Step \d+[:.]\s*',
            r'\d+\.\s*',
            r'•\s*',
            r'-\s*'
        ]

        lines = response.split('\n')
        steps = []
        answer = ""

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # Check for answer indicators
            if any(ind in line.lower() for ind in
                   ['therefore', 'answer:', 'conclusion:', 'finally,']):
                answer = line
            else:
                # Clean step indicators
                for pattern in step_patterns:
                    line = re.sub(pattern, '', line)
                if line:
                    steps.append(line)

        return steps, answer

    def create_training_data(
        self,
        questions: List[str],
        include_reasoning: bool = True,
        parallel: bool = True
    ) -> List[Dict[str, str]]:
        """
        Create training dataset with reasoning chains.

        Args:
            questions: List of questions/problems
            include_reasoning: Include reasoning in training targets
            parallel: Generate in parallel (requires batch support)

        Returns:
            List of training examples
        """
        training_data = []

        for question in questions:
            example = self.generate_reasoning_chain(question)

            if include_reasoning:
                # Train on full reasoning chain
                target = "\n".join(example.reasoning_steps)
                if example.answer:
                    target += f"\n\nAnswer: {example.answer}"
            else:
                # Train on answer only (but reasoning helps)
                target = example.answer

            training_data.append({
                'input': question,
                'target': target,
                'reasoning_steps': example.reasoning_steps
            })

        return training_data


class ReasoningDistillationLoss(nn.Module):
    """
    Loss function for reasoning distillation.

    Combines standard distillation with reasoning structure.
    """

    def __init__(
        self,
        temperature: float = 2.0,
        reasoning_weight: float = 0.3,
        answer_weight: float = 0.7
    ):
        super().__init__()
        self.temperature = temperature
        self.reasoning_weight = reasoning_weight
        self.answer_weight = answer_weight

    def forward(
        self,
        student_logits: torch.Tensor,
        teacher_logits: torch.Tensor,
        labels: torch.Tensor,
        reasoning_mask: Optional[torch.Tensor] = None,
        answer_mask: Optional[torch.Tensor] = None
    ) -> Dict[str, torch.Tensor]:
        """
        Compute reasoning-aware distillation loss.

        Args:
            student_logits: [batch, seq, vocab]
            teacher_logits: [batch, seq, vocab]
            labels: [batch, seq] - token IDs
            reasoning_mask: [batch, seq] - 1 for reasoning tokens
            answer_mask: [batch, seq] - 1 for answer tokens
        """
        batch_size, seq_len, vocab_size = student_logits.shape

        # Temperature-scaled distributions
        teacher_probs = F.softmax(teacher_logits / self.temperature, dim=-1)
        student_log_probs = F.log_softmax(student_logits / self.temperature, dim=-1)

        # KL divergence per position
        kl_div = F.kl_div(
            student_log_probs,
            teacher_probs,
            reduction='none'
        ).sum(dim=-1) * (self.temperature ** 2)

        # Apply different weights to reasoning vs answer
        if reasoning_mask is not None and answer_mask is not None:
            reasoning_loss = (kl_div * reasoning_mask).sum() / (reasoning_mask.sum() + 1e-10)
            answer_loss = (kl_div * answer_mask).sum() / (answer_mask.sum() + 1e-10)

            distillation_loss = (
                self.reasoning_weight * reasoning_loss +
                self.answer_weight * answer_loss
            )
        else:
            distillation_loss = kl_div.mean()

        # Hard label loss
        hard_loss = F.cross_entropy(
            student_logits.view(-1, vocab_size),
            labels.view(-1),
            ignore_index=-100
        )

        return {
            'distillation_loss': distillation_loss,
            'hard_loss': hard_loss,
            'total_loss': 0.5 * distillation_loss + 0.5 * hard_loss
        }


class MixedDistillation:
    """
    Mixed Distillation (MD) combining multiple reasoning approaches.

    Combines Chain-of-Thought and Program-of-Thought distillation
    for enhanced mathematical reasoning transfer.
    """

    def __init__(
        self,
        teacher_model: Any,
        tokenizer: Any,
        use_cot: bool = True,
        use_pot: bool = True
    ):
        self.teacher = teacher_model
        self.tokenizer = tokenizer
        self.use_cot = use_cot
        self.use_pot = use_pot

    def generate_cot_solution(
        self,
        problem: str
    ) -> str:
        """Generate Chain-of-Thought solution."""
        prompt = f"""Solve this problem step by step using natural language reasoning:

Problem: {problem}

Solution:
Let me work through this step by step.
"""
        return self._generate(prompt)

    def generate_pot_solution(
        self,
        problem: str
    ) -> str:
        """Generate Program-of-Thought solution with code."""
        prompt = f"""Solve this problem by writing Python code:

Problem: {problem}

```python
# Solution code
"""
        return self._generate(prompt)

    def _generate(self, prompt: str) -> str:
        """Generate solution from teacher."""
        inputs = self.tokenizer(prompt, return_tensors="pt")

        with torch.no_grad():
            outputs = self.teacher.generate(
                **inputs,
                max_length=512,
                temperature=0.7,
                do_sample=True
            )

        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)

    def create_mixed_dataset(
        self,
        problems: List[str]
    ) -> List[Dict[str, Any]]:
        """
        Create dataset with both CoT and PoT solutions.

        Student learns from both approaches, selecting the best
        method for each problem type.
        """
        dataset = []

        for problem in problems:
            example = {'problem': problem, 'solutions': []}

            if self.use_cot:
                cot_solution = self.generate_cot_solution(problem)
                example['solutions'].append({
                    'type': 'cot',
                    'solution': cot_solution
                })

            if self.use_pot:
                pot_solution = self.generate_pot_solution(problem)
                example['solutions'].append({
                    'type': 'pot',
                    'solution': pot_solution
                })

            dataset.append(example)

        return dataset
```

---

## 6.3.3 Distillation Strategies

### Online vs Offline Distillation

```python
"""
ABOUTME: Distillation strategies including online, offline, and self-distillation.
ABOUTME: Provides framework selection and implementation guidance.
"""

class DistillationStrategy(Enum):
    """Types of distillation strategies."""
    OFFLINE = "offline"       # Pre-computed teacher outputs
    ONLINE = "online"         # Teacher runs during training
    SELF = "self"            # Model distills itself
    MULTI_TEACHER = "multi"   # Multiple teacher ensemble


class OfflineDistillation:
    """
    Offline distillation with pre-computed teacher outputs.

    Teacher outputs are computed once and stored, making training
    more efficient but requiring more storage.
    """

    def __init__(
        self,
        teacher_model: nn.Module,
        cache_dir: str,
        batch_size: int = 32
    ):
        self.teacher = teacher_model
        self.cache_dir = cache_dir
        self.batch_size = batch_size

        self.teacher.eval()
        for param in self.teacher.parameters():
            param.requires_grad = False

    @torch.no_grad()
    def precompute_teacher_outputs(
        self,
        dataloader: Any,
        save_hidden: bool = False,
        save_attention: bool = False
    ) -> str:
        """
        Pre-compute and cache teacher outputs.

        Args:
            dataloader: Data loader for input data
            save_hidden: Also cache hidden states
            save_attention: Also cache attention maps

        Returns:
            Path to cache directory
        """
        import os
        os.makedirs(self.cache_dir, exist_ok=True)

        all_logits = []
        all_hidden = [] if save_hidden else None
        all_attention = [] if save_attention else None

        for batch_idx, batch in enumerate(dataloader):
            outputs = self.teacher(
                input_ids=batch['input_ids'].cuda(),
                attention_mask=batch.get('attention_mask'),
                output_hidden_states=save_hidden,
                output_attentions=save_attention
            )

            all_logits.append(outputs.logits.cpu())

            if save_hidden and outputs.hidden_states:
                # Save only selected layers to reduce storage
                selected_hidden = [
                    outputs.hidden_states[i].cpu()
                    for i in [0, len(outputs.hidden_states)//2, -1]
                ]
                all_hidden.append(selected_hidden)

            if save_attention and outputs.attentions:
                all_attention.append([a.cpu() for a in outputs.attentions])

        # Save to disk
        torch.save(torch.cat(all_logits, dim=0),
                   f"{self.cache_dir}/logits.pt")

        if all_hidden:
            torch.save(all_hidden, f"{self.cache_dir}/hidden.pt")

        if all_attention:
            torch.save(all_attention, f"{self.cache_dir}/attention.pt")

        return self.cache_dir

    def load_cached_outputs(
        self,
        batch_indices: torch.Tensor
    ) -> Dict[str, torch.Tensor]:
        """Load cached teacher outputs for given batch."""
        logits = torch.load(f"{self.cache_dir}/logits.pt")
        return {
            'logits': logits[batch_indices]
        }


class OnlineDistillation(nn.Module):
    """
    Online distillation with live teacher computation.

    Teacher model runs alongside student during training.
    More flexible but requires more compute per step.
    """

    def __init__(
        self,
        teacher_model: nn.Module,
        student_model: nn.Module,
        config: DistillationConfig
    ):
        super().__init__()
        self.teacher = teacher_model
        self.student = student_model
        self.config = config

        # Freeze teacher
        self.teacher.eval()
        for param in self.teacher.parameters():
            param.requires_grad = False

        # Distillation modules
        self.logit_distillation = SequenceDistillation(
            temperature=config.temperature
        )

        if config.feature_layers:
            self.feature_distillation = FeatureDistillation(
                teacher_hidden_size=teacher_model.config.hidden_size,
                student_hidden_size=student_model.config.hidden_size,
                layer_mapping={i: i*2 for i in config.feature_layers}
            )

        if config.attention_layers:
            self.attention_distillation = AttentionDistillation(
                layer_mapping={i: i*2 for i in config.attention_layers}
            )

    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        labels: Optional[torch.Tensor] = None
    ) -> Dict[str, torch.Tensor]:
        """
        Compute all distillation losses in online mode.
        """
        # Teacher forward (no grad)
        with torch.no_grad():
            teacher_outputs = self.teacher(
                input_ids=input_ids,
                attention_mask=attention_mask,
                output_hidden_states=bool(self.config.feature_layers),
                output_attentions=bool(self.config.attention_layers)
            )

        # Student forward
        student_outputs = self.student(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=bool(self.config.feature_layers),
            output_attentions=bool(self.config.attention_layers)
        )

        # Logit distillation
        loss_dict = self.logit_distillation(
            student_logits=student_outputs.logits,
            teacher_logits=teacher_outputs.logits,
            attention_mask=attention_mask,
            labels=labels,
            alpha=self.config.alpha
        )

        total_loss = loss_dict['total_loss']

        # Feature distillation
        if hasattr(self, 'feature_distillation'):
            feature_loss = self.feature_distillation(
                student_hidden_states=student_outputs.hidden_states,
                teacher_hidden_states=teacher_outputs.hidden_states
            )
            total_loss += self.config.feature_weight * feature_loss['feature_loss']
            loss_dict['feature_loss'] = feature_loss['feature_loss']

        # Attention distillation
        if hasattr(self, 'attention_distillation'):
            attention_loss = self.attention_distillation(
                student_attentions=student_outputs.attentions,
                teacher_attentions=teacher_outputs.attentions
            )
            total_loss += self.config.attention_weight * attention_loss['attention_loss']
            loss_dict['attention_loss'] = attention_loss['attention_loss']

        loss_dict['total_loss'] = total_loss

        return loss_dict


class SelfDistillation(nn.Module):
    """
    Self-distillation where model teaches itself.

    Earlier checkpoints or ensemble of predictions serve as teacher.
    Useful when no larger model is available.
    """

    def __init__(
        self,
        model: nn.Module,
        ema_decay: float = 0.999,
        use_deep_supervision: bool = True
    ):
        super().__init__()
        self.model = model
        self.ema_decay = ema_decay
        self.use_deep_supervision = use_deep_supervision

        # Create EMA teacher
        self.ema_model = self._create_ema_model()

    def _create_ema_model(self) -> nn.Module:
        """Create exponential moving average model as teacher."""
        import copy
        ema_model = copy.deepcopy(self.model)
        for param in ema_model.parameters():
            param.requires_grad = False
        return ema_model

    @torch.no_grad()
    def update_ema(self):
        """Update EMA model parameters."""
        for ema_param, model_param in zip(
            self.ema_model.parameters(),
            self.model.parameters()
        ):
            ema_param.data.mul_(self.ema_decay).add_(
                model_param.data, alpha=1 - self.ema_decay
            )

    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        labels: Optional[torch.Tensor] = None
    ) -> Dict[str, torch.Tensor]:
        """Compute self-distillation loss."""
        # Teacher (EMA) forward
        with torch.no_grad():
            teacher_outputs = self.ema_model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                output_hidden_states=self.use_deep_supervision
            )

        # Student forward
        student_outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=self.use_deep_supervision
        )

        # Logit distillation
        teacher_probs = F.softmax(teacher_outputs.logits / 2.0, dim=-1)
        student_log_probs = F.log_softmax(student_outputs.logits / 2.0, dim=-1)

        distillation_loss = F.kl_div(
            student_log_probs,
            teacher_probs,
            reduction='batchmean'
        ) * 4.0  # T^2

        loss_dict = {'distillation_loss': distillation_loss}

        # Deep supervision: intermediate layer matching
        if self.use_deep_supervision:
            deep_loss = 0.0
            teacher_hidden = teacher_outputs.hidden_states
            student_hidden = student_outputs.hidden_states

            # Match every other layer
            for i in range(0, len(student_hidden), 2):
                deep_loss += F.mse_loss(
                    student_hidden[i],
                    teacher_hidden[i].detach()
                )

            deep_loss /= (len(student_hidden) // 2)
            loss_dict['deep_supervision_loss'] = deep_loss
            distillation_loss += 0.1 * deep_loss

        # Hard label loss
        if labels is not None:
            hard_loss = F.cross_entropy(
                student_outputs.logits.view(-1, student_outputs.logits.size(-1)),
                labels.view(-1),
                ignore_index=-100
            )
            loss_dict['hard_loss'] = hard_loss
            total_loss = 0.5 * distillation_loss + 0.5 * hard_loss
        else:
            total_loss = distillation_loss

        loss_dict['total_loss'] = total_loss

        return loss_dict


class MultiTeacherDistillation(nn.Module):
    """
    Multi-teacher distillation from ensemble of teachers.

    Different teachers capture different aspects of knowledge.
    Combined distillation can outperform single teacher.
    """

    def __init__(
        self,
        teacher_models: List[nn.Module],
        student_model: nn.Module,
        aggregation: str = "average",  # average, weighted, adaptive
        temperature: float = 4.0
    ):
        super().__init__()
        self.teachers = nn.ModuleList(teacher_models)
        self.student = student_model
        self.aggregation = aggregation
        self.temperature = temperature

        # Freeze teachers
        for teacher in self.teachers:
            teacher.eval()
            for param in teacher.parameters():
                param.requires_grad = False

        # Learnable teacher weights for adaptive aggregation
        if aggregation == "adaptive":
            self.teacher_weights = nn.Parameter(
                torch.ones(len(teacher_models)) / len(teacher_models)
            )

    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        labels: Optional[torch.Tensor] = None
    ) -> Dict[str, torch.Tensor]:
        """Compute multi-teacher distillation loss."""
        # Get all teacher outputs
        teacher_logits = []
        with torch.no_grad():
            for teacher in self.teachers:
                outputs = teacher(
                    input_ids=input_ids,
                    attention_mask=attention_mask
                )
                teacher_logits.append(outputs.logits)

        # Aggregate teacher predictions
        if self.aggregation == "average":
            # Simple average of softmax probabilities
            teacher_probs = torch.stack([
                F.softmax(logits / self.temperature, dim=-1)
                for logits in teacher_logits
            ]).mean(dim=0)

        elif self.aggregation == "weighted":
            # Pre-defined weights based on teacher quality
            weights = torch.softmax(self.teacher_weights, dim=0)
            teacher_probs = sum(
                w * F.softmax(logits / self.temperature, dim=-1)
                for w, logits in zip(weights, teacher_logits)
            )

        elif self.aggregation == "adaptive":
            # Learn weights during training
            weights = F.softmax(self.teacher_weights, dim=0)
            teacher_probs = sum(
                w * F.softmax(logits / self.temperature, dim=-1)
                for w, logits in zip(weights, teacher_logits)
            )

        # Student forward
        student_outputs = self.student(
            input_ids=input_ids,
            attention_mask=attention_mask
        )

        student_log_probs = F.log_softmax(
            student_outputs.logits / self.temperature,
            dim=-1
        )

        # KL divergence
        distillation_loss = F.kl_div(
            student_log_probs,
            teacher_probs,
            reduction='batchmean'
        ) * (self.temperature ** 2)

        loss_dict = {
            'distillation_loss': distillation_loss,
            'total_loss': distillation_loss
        }

        if self.aggregation in ["weighted", "adaptive"]:
            loss_dict['teacher_weights'] = F.softmax(
                self.teacher_weights, dim=0
            ).detach().cpu().numpy()

        # Hard label loss
        if labels is not None:
            hard_loss = F.cross_entropy(
                student_outputs.logits.view(-1, student_outputs.logits.size(-1)),
                labels.view(-1),
                ignore_index=-100
            )
            loss_dict['hard_loss'] = hard_loss
            loss_dict['total_loss'] = 0.5 * distillation_loss + 0.5 * hard_loss

        return loss_dict
```

### Data Distillation

Generate synthetic training data from teacher:

```python
"""
ABOUTME: Data distillation through synthetic data generation.
ABOUTME: Creates training data from teacher model outputs.
"""

class DataDistillation:
    """
    Data distillation: generate synthetic training data from teacher.

    Useful when:
    - Original training data unavailable
    - Want to augment existing data
    - Need task-specific fine-tuning data
    """

    def __init__(
        self,
        teacher_model: Any,
        tokenizer: Any,
        generation_config: Optional[Dict] = None
    ):
        self.teacher = teacher_model
        self.tokenizer = tokenizer

        self.default_generation_config = {
            'max_length': 512,
            'temperature': 0.8,
            'top_p': 0.95,
            'top_k': 50,
            'do_sample': True,
            'num_return_sequences': 1
        }

        if generation_config:
            self.default_generation_config.update(generation_config)

    def generate_synthetic_data(
        self,
        prompts: List[str],
        num_samples_per_prompt: int = 5,
        diversity_penalty: float = 0.5
    ) -> List[Dict[str, str]]:
        """
        Generate diverse synthetic responses for training.

        Args:
            prompts: Seed prompts for generation
            num_samples_per_prompt: Number of responses per prompt
            diversity_penalty: Penalty for repetitive generations

        Returns:
            List of (prompt, response) pairs
        """
        synthetic_data = []

        for prompt in prompts:
            inputs = self.tokenizer(prompt, return_tensors="pt")

            # Generate multiple diverse samples
            outputs = self.teacher.generate(
                **inputs,
                num_return_sequences=num_samples_per_prompt,
                diversity_penalty=diversity_penalty,
                num_beam_groups=num_samples_per_prompt,
                num_beams=num_samples_per_prompt * 2,
                **self.default_generation_config
            )

            for output in outputs:
                response = self.tokenizer.decode(
                    output, skip_special_tokens=True
                )
                # Remove prompt from response if present
                if response.startswith(prompt):
                    response = response[len(prompt):].strip()

                synthetic_data.append({
                    'prompt': prompt,
                    'response': response
                })

        return synthetic_data

    def generate_instruction_data(
        self,
        topics: List[str],
        task_types: List[str] = ['explain', 'summarize', 'analyze', 'compare'],
        num_per_combination: int = 3
    ) -> List[Dict[str, str]]:
        """
        Generate instruction-following training data.

        Creates diverse instructions and responses covering
        various topics and task types.
        """
        instruction_data = []

        instruction_templates = {
            'explain': "Explain {topic} in simple terms.",
            'summarize': "Provide a brief summary of {topic}.",
            'analyze': "Analyze the key aspects of {topic}.",
            'compare': "Compare and contrast different approaches to {topic}.",
            'list': "List the main components of {topic}.",
            'evaluate': "Evaluate the strengths and weaknesses of {topic}."
        }

        for topic in topics:
            for task_type in task_types:
                template = instruction_templates.get(
                    task_type,
                    instruction_templates['explain']
                )
                instruction = template.format(topic=topic)

                # Generate responses
                samples = self.generate_synthetic_data(
                    [instruction],
                    num_samples_per_prompt=num_per_combination
                )

                for sample in samples:
                    instruction_data.append({
                        'instruction': instruction,
                        'input': '',
                        'output': sample['response'],
                        'topic': topic,
                        'task_type': task_type
                    })

        return instruction_data

    def filter_quality(
        self,
        data: List[Dict[str, str]],
        min_length: int = 50,
        max_length: int = 2000,
        deduplicate: bool = True
    ) -> List[Dict[str, str]]:
        """
        Filter generated data for quality.

        Removes low-quality or duplicate generations.
        """
        filtered = []
        seen_responses = set()

        for item in data:
            response = item.get('response') or item.get('output', '')

            # Length filter
            if len(response) < min_length or len(response) > max_length:
                continue

            # Deduplication
            if deduplicate:
                response_hash = hash(response[:100])  # Compare first 100 chars
                if response_hash in seen_responses:
                    continue
                seen_responses.add(response_hash)

            # Quality heuristics
            # Check for repetition
            words = response.split()
            if len(words) > 10:
                unique_ratio = len(set(words)) / len(words)
                if unique_ratio < 0.3:  # Too repetitive
                    continue

            filtered.append(item)

        return filtered
```

---

## 6.3.4 Student Model Design

### Architecture Considerations

```python
"""
ABOUTME: Student model architecture design for efficient distillation.
ABOUTME: Covers architecture selection, initialization, and capacity planning.
"""

@dataclass
class StudentConfig:
    """Configuration for student model architecture."""
    hidden_size: int
    intermediate_size: int
    num_hidden_layers: int
    num_attention_heads: int
    num_key_value_heads: int  # For GQA
    vocab_size: int
    max_position_embeddings: int
    rope_theta: float = 10000.0
    tie_word_embeddings: bool = True

    @property
    def num_parameters(self) -> int:
        """Estimate number of parameters."""
        embedding_params = self.vocab_size * self.hidden_size
        if not self.tie_word_embeddings:
            embedding_params *= 2

        # Per-layer params
        attention_params = (
            3 * self.hidden_size * self.hidden_size +  # Q, K, V
            self.hidden_size * self.hidden_size         # Output projection
        )
        ffn_params = (
            2 * self.hidden_size * self.intermediate_size +  # Up and gate
            self.intermediate_size * self.hidden_size         # Down
        )
        layer_norm_params = 2 * self.hidden_size * 2  # Two norms per layer

        per_layer = attention_params + ffn_params + layer_norm_params
        total_layers = per_layer * self.num_hidden_layers

        return embedding_params + total_layers


class StudentArchitectureDesigner:
    """
    Design optimal student architecture based on constraints.
    """

    # Reference architectures
    REFERENCE_CONFIGS = {
        'tinyllama-1.1b': StudentConfig(
            hidden_size=2048,
            intermediate_size=5632,
            num_hidden_layers=22,
            num_attention_heads=32,
            num_key_value_heads=4,
            vocab_size=32000,
            max_position_embeddings=2048
        ),
        'distilbert': StudentConfig(
            hidden_size=768,
            intermediate_size=3072,
            num_hidden_layers=6,
            num_attention_heads=12,
            num_key_value_heads=12,
            vocab_size=30522,
            max_position_embeddings=512
        ),
        'small-1b': StudentConfig(
            hidden_size=2048,
            intermediate_size=5504,
            num_hidden_layers=24,
            num_attention_heads=16,
            num_key_value_heads=8,
            vocab_size=32000,
            max_position_embeddings=4096
        ),
        'tiny-500m': StudentConfig(
            hidden_size=1024,
            intermediate_size=2816,
            num_hidden_layers=24,
            num_attention_heads=16,
            num_key_value_heads=4,
            vocab_size=32000,
            max_position_embeddings=2048
        )
    }

    def __init__(self, teacher_config: Dict[str, Any]):
        self.teacher_config = teacher_config

    def design_student(
        self,
        target_params: Optional[int] = None,
        compression_ratio: Optional[float] = None,
        preserve_depth: bool = True,
        preserve_width: bool = False
    ) -> StudentConfig:
        """
        Design student architecture meeting constraints.

        Args:
            target_params: Target parameter count
            compression_ratio: Teacher/student size ratio
            preserve_depth: Prioritize keeping layers
            preserve_width: Prioritize keeping hidden size

        Returns:
            Student configuration
        """
        teacher_hidden = self.teacher_config.get('hidden_size', 4096)
        teacher_layers = self.teacher_config.get('num_hidden_layers', 32)
        teacher_intermediate = self.teacher_config.get('intermediate_size', 11008)
        teacher_heads = self.teacher_config.get('num_attention_heads', 32)

        if target_params:
            ratio = target_params / self._estimate_teacher_params()
        elif compression_ratio:
            ratio = 1.0 / compression_ratio
        else:
            ratio = 0.1  # Default 10x compression

        if preserve_depth:
            # Keep layer count, reduce width
            student_layers = teacher_layers
            width_ratio = np.sqrt(ratio)  # Sqrt because params ~ width^2
            student_hidden = int(teacher_hidden * width_ratio)
            student_intermediate = int(teacher_intermediate * width_ratio)
            student_heads = max(4, int(teacher_heads * width_ratio))

        elif preserve_width:
            # Keep width, reduce layers
            student_hidden = teacher_hidden
            student_intermediate = teacher_intermediate
            student_heads = teacher_heads
            student_layers = max(4, int(teacher_layers * ratio))

        else:
            # Balanced reduction
            layer_ratio = ratio ** 0.4
            width_ratio = ratio ** 0.3

            student_layers = max(4, int(teacher_layers * layer_ratio))
            student_hidden = int(teacher_hidden * width_ratio)
            student_intermediate = int(teacher_intermediate * width_ratio)
            student_heads = max(4, int(teacher_heads * width_ratio))

        # Ensure hidden_size is divisible by num_heads
        student_hidden = (student_hidden // student_heads) * student_heads

        # GQA: fewer KV heads than Q heads
        student_kv_heads = max(1, student_heads // 4)

        return StudentConfig(
            hidden_size=student_hidden,
            intermediate_size=student_intermediate,
            num_hidden_layers=student_layers,
            num_attention_heads=student_heads,
            num_key_value_heads=student_kv_heads,
            vocab_size=self.teacher_config.get('vocab_size', 32000),
            max_position_embeddings=self.teacher_config.get(
                'max_position_embeddings', 4096
            )
        )

    def _estimate_teacher_params(self) -> int:
        """Estimate teacher model parameters."""
        h = self.teacher_config.get('hidden_size', 4096)
        l = self.teacher_config.get('num_hidden_layers', 32)
        i = self.teacher_config.get('intermediate_size', 11008)
        v = self.teacher_config.get('vocab_size', 32000)

        embedding = v * h
        per_layer = 4 * h * h + 2 * h * i + 2 * h  # Simplified

        return embedding + per_layer * l


class StudentInitializer:
    """
    Initialize student model from teacher weights.

    Proper initialization significantly improves distillation.
    """

    @staticmethod
    def initialize_from_teacher(
        student_model: nn.Module,
        teacher_model: nn.Module,
        layer_mapping: Optional[Dict[int, int]] = None,
        method: str = "selective"  # selective, truncate, interpolate
    ) -> nn.Module:
        """
        Initialize student weights from teacher.

        Args:
            student_model: Student model to initialize
            teacher_model: Teacher model to copy from
            layer_mapping: Student->teacher layer mapping
            method: Initialization method

        Returns:
            Initialized student model
        """
        student_state = student_model.state_dict()
        teacher_state = teacher_model.state_dict()

        # Determine layer mapping if not provided
        if layer_mapping is None:
            student_layers = sum(
                1 for k in student_state.keys() if 'layers.' in k
            ) // len([k for k in student_state.keys() if 'layers.0.' in k])
            teacher_layers = sum(
                1 for k in teacher_state.keys() if 'layers.' in k
            ) // len([k for k in teacher_state.keys() if 'layers.0.' in k])

            # Uniform mapping
            layer_mapping = {
                i: int(i * teacher_layers / student_layers)
                for i in range(student_layers)
            }

        initialized_keys = []

        for student_key, student_param in student_state.items():
            # Direct copy for embedding and output layers
            if 'embed' in student_key or 'lm_head' in student_key:
                if student_key in teacher_state:
                    teacher_param = teacher_state[student_key]
                    if student_param.shape == teacher_param.shape:
                        student_state[student_key] = teacher_param.clone()
                        initialized_keys.append(student_key)
                continue

            # Layer-wise initialization
            if 'layers.' in student_key:
                # Extract layer number
                import re
                match = re.search(r'layers\.(\d+)', student_key)
                if match:
                    student_layer = int(match.group(1))
                    teacher_layer = layer_mapping.get(student_layer, student_layer)

                    teacher_key = student_key.replace(
                        f'layers.{student_layer}',
                        f'layers.{teacher_layer}'
                    )

                    if teacher_key in teacher_state:
                        teacher_param = teacher_state[teacher_key]

                        if student_param.shape == teacher_param.shape:
                            student_state[student_key] = teacher_param.clone()
                        elif method == "truncate":
                            # Truncate teacher weights to student size
                            slices = tuple(
                                slice(0, min(s, t))
                                for s, t in zip(student_param.shape, teacher_param.shape)
                            )
                            student_state[student_key] = teacher_param[slices].clone()
                        elif method == "interpolate":
                            # Interpolate teacher weights
                            student_state[student_key] = F.interpolate(
                                teacher_param.unsqueeze(0).unsqueeze(0).float(),
                                size=student_param.shape,
                                mode='nearest'
                            ).squeeze().to(teacher_param.dtype)

                        initialized_keys.append(student_key)

        student_model.load_state_dict(student_state)

        print(f"Initialized {len(initialized_keys)}/{len(student_state)} parameters from teacher")

        return student_model


class CapacityEstimator:
    """
    Estimate student model capacity requirements.

    Helps determine minimum student size for target performance.
    """

    @staticmethod
    def estimate_capacity_requirement(
        task_complexity: str,
        target_performance_ratio: float = 0.95,
        teacher_params: int = 7_000_000_000
    ) -> Dict[str, Any]:
        """
        Estimate minimum student capacity for target performance.

        Args:
            task_complexity: "simple", "medium", or "complex"
            target_performance_ratio: Desired student/teacher performance
            teacher_params: Teacher model parameters

        Returns:
            Capacity estimates and recommendations
        """
        # Empirical compression ratios from research
        complexity_factors = {
            'simple': {  # Classification, sentiment
                0.95: 10,   # Can achieve 95% performance with 10x compression
                0.90: 20,
                0.80: 50
            },
            'medium': {  # QA, summarization
                0.95: 5,
                0.90: 10,
                0.80: 20
            },
            'complex': {  # Reasoning, math, code
                0.95: 2,
                0.90: 4,
                0.80: 8
            }
        }

        factors = complexity_factors.get(task_complexity, complexity_factors['medium'])

        # Find appropriate compression ratio
        compression_ratio = factors.get(target_performance_ratio, 5)

        min_params = teacher_params / compression_ratio

        return {
            'task_complexity': task_complexity,
            'target_performance': target_performance_ratio,
            'recommended_compression': compression_ratio,
            'minimum_parameters': int(min_params),
            'minimum_parameters_human': f"{min_params/1e9:.1f}B",
            'notes': [
                f"For {task_complexity} tasks at {target_performance_ratio:.0%} performance",
                f"Recommend {compression_ratio}x compression",
                "Actual results depend on training data quality and distillation method"
            ]
        }
```

---

## 6.3.5 Quality-Size Tradeoffs

### Comprehensive Tradeoff Analysis

```python
"""
ABOUTME: Quality vs size tradeoff analysis for distillation decisions.
ABOUTME: Provides frameworks for evaluating compression impact.
"""

@dataclass
class DistillationResult:
    """Results from a distillation experiment."""
    teacher_params: int
    student_params: int
    teacher_performance: Dict[str, float]
    student_performance: Dict[str, float]
    training_compute: float  # FLOPs
    inference_speedup: float
    memory_reduction: float

    @property
    def compression_ratio(self) -> float:
        return self.teacher_params / self.student_params

    @property
    def performance_retention(self) -> Dict[str, float]:
        return {
            k: self.student_performance.get(k, 0) / v
            for k, v in self.teacher_performance.items()
            if v > 0
        }


class QualitySizeAnalyzer:
    """
    Analyze quality-size tradeoffs in distillation.
    """

    # Reference results from literature
    REFERENCE_RESULTS = {
        'distilbert': DistillationResult(
            teacher_params=110_000_000,
            student_params=66_000_000,
            teacher_performance={'glue_avg': 79.0},
            student_performance={'glue_avg': 77.0},
            training_compute=1e18,
            inference_speedup=1.6,
            memory_reduction=0.4
        ),
        'tinyllama': DistillationResult(
            teacher_params=7_000_000_000,
            student_params=1_100_000_000,
            teacher_performance={'mmlu': 35.0, 'hellaswag': 60.0},
            student_performance={'mmlu': 25.0, 'hellaswag': 53.0},
            training_compute=3e21,
            inference_speedup=4.5,
            memory_reduction=0.84
        ),
        'llama3.2-1b': DistillationResult(
            teacher_params=8_000_000_000,
            student_params=1_000_000_000,
            teacher_performance={'mmlu': 66.0, 'gsm8k': 76.0},
            student_performance={'mmlu': 49.0, 'gsm8k': 44.0},
            training_compute=5e21,
            inference_speedup=5.0,
            memory_reduction=0.87
        )
    }

    def analyze_tradeoff(
        self,
        results: List[DistillationResult]
    ) -> Dict[str, Any]:
        """
        Analyze tradeoffs across multiple distillation results.

        Returns:
            Analysis including Pareto frontier and recommendations
        """
        analysis = {
            'summary': [],
            'pareto_optimal': [],
            'recommendations': []
        }

        # Compute aggregate metrics
        for result in results:
            avg_retention = np.mean(list(result.performance_retention.values()))
            efficiency = avg_retention * result.compression_ratio

            analysis['summary'].append({
                'compression_ratio': result.compression_ratio,
                'avg_performance_retention': avg_retention,
                'inference_speedup': result.inference_speedup,
                'memory_reduction': result.memory_reduction,
                'efficiency_score': efficiency
            })

        # Find Pareto optimal points
        for i, result_i in enumerate(results):
            is_pareto = True
            ret_i = np.mean(list(result_i.performance_retention.values()))
            comp_i = result_i.compression_ratio

            for j, result_j in enumerate(results):
                if i == j:
                    continue
                ret_j = np.mean(list(result_j.performance_retention.values()))
                comp_j = result_j.compression_ratio

                # Check if j dominates i
                if ret_j >= ret_i and comp_j >= comp_i:
                    if ret_j > ret_i or comp_j > comp_i:
                        is_pareto = False
                        break

            if is_pareto:
                analysis['pareto_optimal'].append(i)

        return analysis

    def predict_performance(
        self,
        teacher_performance: float,
        compression_ratio: float,
        task_type: str = "general"
    ) -> Tuple[float, float]:
        """
        Predict student performance based on compression ratio.

        Uses power law relationship observed in scaling research.

        Args:
            teacher_performance: Teacher performance score
            compression_ratio: Target compression ratio
            task_type: Type of task

        Returns:
            Tuple of (predicted_performance, confidence_interval)
        """
        # Empirical exponents from research
        # Performance ~ params^α
        task_exponents = {
            'general': 0.1,      # General language understanding
            'reasoning': 0.15,   # Math, logic (more sensitive to size)
            'classification': 0.05,  # Less sensitive
            'generation': 0.08
        }

        alpha = task_exponents.get(task_type, 0.1)

        # Predicted performance drop
        performance_ratio = compression_ratio ** (-alpha)
        predicted = teacher_performance * performance_ratio

        # Confidence interval (rough estimate)
        ci_width = teacher_performance * 0.1 * np.log(compression_ratio)

        return predicted, ci_width

    def recommend_compression(
        self,
        constraints: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Recommend compression based on constraints.

        Args:
            constraints: Dict with keys like:
                - max_latency_ms: Maximum inference latency
                - max_memory_gb: Maximum memory
                - min_performance: Minimum acceptable performance
                - deployment_target: edge, server, mobile

        Returns:
            Recommendation with configuration
        """
        target = constraints.get('deployment_target', 'server')

        deployment_profiles = {
            'edge': {
                'max_params': 500_000_000,
                'recommended_compression': 10,
                'quantization': 'INT4',
                'notes': 'Consider pruning + quantization + distillation'
            },
            'mobile': {
                'max_params': 1_000_000_000,
                'recommended_compression': 7,
                'quantization': 'INT8',
                'notes': 'Balance size and quality for mobile inference'
            },
            'server': {
                'max_params': 3_000_000_000,
                'recommended_compression': 3,
                'quantization': 'FP16',
                'notes': 'Higher quality acceptable with server resources'
            },
            'browser': {
                'max_params': 200_000_000,
                'recommended_compression': 20,
                'quantization': 'INT4',
                'notes': 'Aggressive compression for WebGPU deployment'
            }
        }

        profile = deployment_profiles.get(target, deployment_profiles['server'])

        # Adjust based on other constraints
        if 'min_performance' in constraints:
            min_perf = constraints['min_performance']
            if min_perf > 0.9:
                profile['recommended_compression'] = min(
                    profile['recommended_compression'], 3
                )
            elif min_perf > 0.8:
                profile['recommended_compression'] = min(
                    profile['recommended_compression'], 5
                )

        return {
            'deployment_target': target,
            'profile': profile,
            'distillation_config': {
                'method': 'hybrid' if profile['recommended_compression'] > 5 else 'logit',
                'temperature': 4.0 if profile['recommended_compression'] <= 5 else 2.0,
                'use_chain_of_thought': profile['recommended_compression'] <= 3
            }
        }


class DistillationBenchmark:
    """
    Benchmark distilled models comprehensively.
    """

    def __init__(
        self,
        teacher_model: nn.Module,
        student_model: nn.Module,
        tokenizer: Any
    ):
        self.teacher = teacher_model
        self.student = student_model
        self.tokenizer = tokenizer

    def measure_quality(
        self,
        eval_data: List[Dict],
        metrics: List[str] = ['accuracy', 'perplexity', 'kl_divergence']
    ) -> Dict[str, float]:
        """
        Measure student quality against teacher.
        """
        results = {}

        self.teacher.eval()
        self.student.eval()

        with torch.no_grad():
            all_teacher_logits = []
            all_student_logits = []
            correct = 0
            total = 0

            for item in eval_data:
                inputs = self.tokenizer(
                    item['input'],
                    return_tensors='pt',
                    truncation=True
                )

                teacher_outputs = self.teacher(**inputs)
                student_outputs = self.student(**inputs)

                all_teacher_logits.append(teacher_outputs.logits)
                all_student_logits.append(student_outputs.logits)

                # Check prediction match
                teacher_pred = teacher_outputs.logits.argmax(dim=-1)
                student_pred = student_outputs.logits.argmax(dim=-1)
                correct += (teacher_pred == student_pred).sum().item()
                total += teacher_pred.numel()

            # Compute metrics
            if 'accuracy' in metrics:
                results['teacher_match_accuracy'] = correct / total

            if 'kl_divergence' in metrics:
                teacher_logits = torch.cat(all_teacher_logits, dim=0)
                student_logits = torch.cat(all_student_logits, dim=0)

                teacher_probs = F.softmax(teacher_logits, dim=-1)
                student_log_probs = F.log_softmax(student_logits, dim=-1)

                kl = F.kl_div(
                    student_log_probs,
                    teacher_probs,
                    reduction='batchmean'
                )
                results['kl_divergence'] = kl.item()

        return results

    def measure_efficiency(
        self,
        batch_sizes: List[int] = [1, 8, 32],
        seq_lengths: List[int] = [128, 512, 2048]
    ) -> Dict[str, Any]:
        """
        Measure efficiency improvements from distillation.
        """
        import time

        results = {
            'teacher': {},
            'student': {},
            'speedup': {}
        }

        self.teacher.eval()
        self.student.eval()

        for bs in batch_sizes:
            for seq_len in seq_lengths:
                key = f"bs{bs}_seq{seq_len}"

                # Create dummy input
                input_ids = torch.randint(
                    0, 32000, (bs, seq_len),
                    device=next(self.teacher.parameters()).device
                )

                # Warm up
                with torch.no_grad():
                    self.teacher(input_ids)
                    self.student(input_ids)

                # Teacher timing
                torch.cuda.synchronize()
                start = time.perf_counter()

                with torch.no_grad():
                    for _ in range(10):
                        self.teacher(input_ids)

                torch.cuda.synchronize()
                teacher_time = (time.perf_counter() - start) / 10

                # Student timing
                torch.cuda.synchronize()
                start = time.perf_counter()

                with torch.no_grad():
                    for _ in range(10):
                        self.student(input_ids)

                torch.cuda.synchronize()
                student_time = (time.perf_counter() - start) / 10

                results['teacher'][key] = teacher_time
                results['student'][key] = student_time
                results['speedup'][key] = teacher_time / student_time

        return results
```

---

## Appendix A: Complete Distillation Pipeline

```python
"""
ABOUTME: End-to-end distillation pipeline implementation.
ABOUTME: Orchestrates the complete distillation workflow.
"""

from transformers import AutoModelForCausalLM, AutoTokenizer
from torch.utils.data import DataLoader
import wandb


class DistillationPipeline:
    """
    Complete pipeline for knowledge distillation.
    """

    def __init__(
        self,
        teacher_name_or_path: str,
        student_config: StudentConfig,
        distillation_config: DistillationConfig,
        output_dir: str
    ):
        self.teacher_path = teacher_name_or_path
        self.student_config = student_config
        self.distillation_config = distillation_config
        self.output_dir = output_dir

        self.teacher = None
        self.student = None
        self.tokenizer = None

    def setup(self):
        """Initialize models and tokenizer."""
        print("Loading teacher model...")
        self.tokenizer = AutoTokenizer.from_pretrained(self.teacher_path)
        self.teacher = AutoModelForCausalLM.from_pretrained(
            self.teacher_path,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        self.teacher.eval()

        print("Creating student model...")
        from transformers import LlamaConfig, LlamaForCausalLM

        student_hf_config = LlamaConfig(
            hidden_size=self.student_config.hidden_size,
            intermediate_size=self.student_config.intermediate_size,
            num_hidden_layers=self.student_config.num_hidden_layers,
            num_attention_heads=self.student_config.num_attention_heads,
            num_key_value_heads=self.student_config.num_key_value_heads,
            vocab_size=self.student_config.vocab_size,
            max_position_embeddings=self.student_config.max_position_embeddings,
            rope_theta=self.student_config.rope_theta,
            tie_word_embeddings=self.student_config.tie_word_embeddings
        )

        self.student = LlamaForCausalLM(student_hf_config)

        # Initialize from teacher
        self.student = StudentInitializer.initialize_from_teacher(
            self.student,
            self.teacher,
            method="selective"
        )

        self.student = self.student.to(torch.float16)

        print(f"Teacher params: {sum(p.numel() for p in self.teacher.parameters()):,}")
        print(f"Student params: {sum(p.numel() for p in self.student.parameters()):,}")

    def train(
        self,
        train_dataloader: DataLoader,
        eval_dataloader: Optional[DataLoader] = None,
        num_epochs: int = 3,
        learning_rate: float = 1e-4,
        gradient_accumulation_steps: int = 4,
        use_wandb: bool = True
    ):
        """
        Train student model with knowledge distillation.
        """
        if use_wandb:
            wandb.init(
                project="knowledge-distillation",
                config={
                    'teacher': self.teacher_path,
                    'student_config': self.student_config.__dict__,
                    'distillation_config': self.distillation_config.__dict__,
                    'learning_rate': learning_rate
                }
            )

        # Setup optimizer
        optimizer = torch.optim.AdamW(
            self.student.parameters(),
            lr=learning_rate,
            weight_decay=0.01
        )

        # Setup distillation module
        distillation = OnlineDistillation(
            self.teacher,
            self.student,
            self.distillation_config
        )

        # Training loop
        global_step = 0
        self.student.train()

        for epoch in range(num_epochs):
            epoch_loss = 0.0

            for batch_idx, batch in enumerate(train_dataloader):
                # Move to device
                input_ids = batch['input_ids'].cuda()
                attention_mask = batch.get('attention_mask')
                if attention_mask is not None:
                    attention_mask = attention_mask.cuda()
                labels = batch.get('labels')
                if labels is not None:
                    labels = labels.cuda()

                # Forward pass
                loss_dict = distillation(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )

                loss = loss_dict['total_loss'] / gradient_accumulation_steps
                loss.backward()

                if (batch_idx + 1) % gradient_accumulation_steps == 0:
                    torch.nn.utils.clip_grad_norm_(
                        self.student.parameters(),
                        max_norm=1.0
                    )
                    optimizer.step()
                    optimizer.zero_grad()
                    global_step += 1

                    if use_wandb:
                        wandb.log({
                            'loss': loss.item() * gradient_accumulation_steps,
                            'distillation_loss': loss_dict['distillation_loss'].item(),
                            'temperature': loss_dict.get('temperature', self.distillation_config.temperature),
                            'alpha': loss_dict.get('alpha', self.distillation_config.alpha),
                            'step': global_step
                        })

                epoch_loss += loss.item() * gradient_accumulation_steps

                if batch_idx % 100 == 0:
                    print(f"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item() * gradient_accumulation_steps:.4f}")

            # Evaluation
            if eval_dataloader is not None:
                eval_results = self.evaluate(eval_dataloader)
                print(f"Epoch {epoch} - Eval: {eval_results}")

                if use_wandb:
                    wandb.log({
                        f'eval_{k}': v for k, v in eval_results.items()
                    })

            # Save checkpoint
            self.save_checkpoint(f"checkpoint-epoch-{epoch}")

        # Save final model
        self.save_checkpoint("final")

        if use_wandb:
            wandb.finish()

    @torch.no_grad()
    def evaluate(
        self,
        eval_dataloader: DataLoader
    ) -> Dict[str, float]:
        """Evaluate student model."""
        self.student.eval()

        total_loss = 0.0
        total_tokens = 0

        for batch in eval_dataloader:
            input_ids = batch['input_ids'].cuda()
            labels = batch.get('labels', input_ids).cuda()

            outputs = self.student(
                input_ids=input_ids,
                labels=labels
            )

            total_loss += outputs.loss.item() * input_ids.numel()
            total_tokens += input_ids.numel()

        self.student.train()

        perplexity = np.exp(total_loss / total_tokens)

        return {
            'perplexity': perplexity,
            'loss': total_loss / total_tokens
        }

    def save_checkpoint(self, name: str):
        """Save model checkpoint."""
        import os

        save_path = os.path.join(self.output_dir, name)
        os.makedirs(save_path, exist_ok=True)

        self.student.save_pretrained(save_path)
        self.tokenizer.save_pretrained(save_path)

        print(f"Saved checkpoint to {save_path}")


# Example usage
def main():
    # Design student architecture
    teacher_config = {
        'hidden_size': 4096,
        'num_hidden_layers': 32,
        'intermediate_size': 11008,
        'num_attention_heads': 32,
        'vocab_size': 32000
    }

    designer = StudentArchitectureDesigner(teacher_config)
    student_config = designer.design_student(
        compression_ratio=7,
        preserve_depth=False
    )

    print(f"Student config: {student_config}")
    print(f"Estimated params: {student_config.num_parameters:,}")

    # Setup distillation config
    distillation_config = DistillationConfig(
        temperature=4.0,
        temperature_schedule="linear_decay",
        alpha=0.7,
        alpha_schedule="linear_decay",
        feature_layers=[4, 8, 12],
        feature_weight=0.1,
        max_steps=100000
    )

    # Create pipeline
    pipeline = DistillationPipeline(
        teacher_name_or_path="meta-llama/Llama-2-7b-hf",
        student_config=student_config,
        distillation_config=distillation_config,
        output_dir="./distilled_model"
    )

    pipeline.setup()

    # Train with your data
    # pipeline.train(train_dataloader, eval_dataloader)


if __name__ == "__main__":
    main()
```

---

## Appendix B: Troubleshooting Guide

### Common Issues

| Issue | Cause | Solution |
|-------|-------|----------|
| Student performance plateaus | Temperature too high | Reduce temperature (try T=2) |
| Training instability | Learning rate too high | Use 1e-5 to 1e-4 range |
| Poor initialization | Random init hurts | Always initialize from teacher |
| Feature mismatch | Different hidden sizes | Use projection layers |
| Attention NaN | Numerical instability | Use float32 for attention |
| Memory OOM | Teacher too large | Use gradient checkpointing |
| Slow convergence | Alpha too low | Start with alpha=0.9, decay to 0.1 |
| Student overfits | Insufficient data | Use data augmentation or synthetic data |

### Debugging Checklist

```python
class DistillationDebugger:
    """Debug distillation training issues."""

    @staticmethod
    def check_gradient_flow(model: nn.Module) -> Dict[str, Any]:
        """Check gradient statistics."""
        grad_stats = {}

        for name, param in model.named_parameters():
            if param.grad is not None:
                grad_stats[name] = {
                    'mean': param.grad.mean().item(),
                    'std': param.grad.std().item(),
                    'max': param.grad.max().item(),
                    'min': param.grad.min().item(),
                    'has_nan': torch.isnan(param.grad).any().item()
                }

        return grad_stats

    @staticmethod
    def compare_distributions(
        student_logits: torch.Tensor,
        teacher_logits: torch.Tensor,
        temperature: float = 1.0
    ) -> Dict[str, float]:
        """Compare student and teacher distributions."""
        student_probs = F.softmax(student_logits / temperature, dim=-1)
        teacher_probs = F.softmax(teacher_logits / temperature, dim=-1)

        # KL divergence
        kl = F.kl_div(
            student_probs.log(),
            teacher_probs,
            reduction='batchmean'
        )

        # JS divergence
        m = 0.5 * (student_probs + teacher_probs)
        js = 0.5 * (
            F.kl_div(student_probs.log(), m, reduction='batchmean') +
            F.kl_div(teacher_probs.log(), m, reduction='batchmean')
        )

        # Top-k agreement
        student_top = student_logits.topk(10, dim=-1).indices
        teacher_top = teacher_logits.topk(10, dim=-1).indices
        agreement = (
            (student_top.unsqueeze(-1) == teacher_top.unsqueeze(-2))
            .any(dim=-1)
            .float()
            .mean()
        )

        return {
            'kl_divergence': kl.item(),
            'js_divergence': js.item(),
            'top10_agreement': agreement.item()
        }
```

---

## Appendix C: Glossary

| Term | Definition |
|------|------------|
| **Knowledge Distillation** | Transferring knowledge from large teacher to smaller student |
| **Soft Targets** | Temperature-scaled probability distributions from teacher |
| **Temperature** | Hyperparameter controlling distribution softness |
| **Dark Knowledge** | Information in teacher's output beyond correct labels |
| **Forward KL** | KL(teacher || student) - mode covering |
| **Reverse KL** | KL(student || teacher) - mode seeking |
| **Feature Distillation** | Matching intermediate layer representations |
| **Attention Transfer** | Matching attention patterns between models |
| **Chain-of-Thought** | Step-by-step reasoning distillation |
| **Online Distillation** | Teacher runs during training |
| **Offline Distillation** | Pre-computed teacher outputs |
| **Self-Distillation** | Model teaches itself (EMA or deep supervision) |

---

## References

1. Hinton, G., Vinyals, O., & Dean, J. (2015). Distilling the Knowledge in a Neural Network
2. Romero, A., et al. (2015). FitNets: Hints for Thin Deep Nets
3. Zagoruyko, S. & Komodakis, N. (2017). Paying More Attention to Attention
4. Gu, Y., et al. (2023). MiniLLM: Knowledge Distillation of Large Language Models
5. [LLM Distillation Explained - Adaline Labs](https://labs.adaline.ai/p/llm-distillation-explained)
6. [Survey on Knowledge Distillation for LLMs](https://arxiv.org/html/2407.01885)
7. [Knowledge Distillation Guide - Hugging Face](https://huggingface.co/blog/Kseniase/kd)
8. [DistilBERT Paper](https://arxiv.org/abs/1910.01108)
9. [TinyLlama Project](https://github.com/jzhang38/TinyLlama)
10. [Keras Knowledge Distillation](https://keras.io/examples/vision/knowledge_distillation/)
