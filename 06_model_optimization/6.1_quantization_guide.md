# Document 6.1: Quantization Guide

## Executive Summary

This guide provides comprehensive coverage of model quantization techniques for efficient LLM deployment. It covers precision formats (FP32 through INT4/FP4), post-training quantization methods (GPTQ, AWQ, GGML/GGUF), quantization-aware training, hardware-specific optimizations, and quality validation strategies. The goal is to enable practitioners to reduce model memory footprint and increase inference speed while maintaining acceptable quality for their use cases.

## Prerequisites

- Understanding of neural network fundamentals
- Familiarity with model inference pipelines
- Access to GPU hardware for testing (NVIDIA recommended)
- Python proficiency
- Understanding of target deployment constraints

---

## 6.1.1 Quantization Fundamentals

### Precision Formats

```python
"""
ABOUTME: Defines precision formats and their characteristics for LLM quantization.
ABOUTME: Provides utilities for understanding precision tradeoffs.
"""

from dataclasses import dataclass
from typing import Dict, List, Tuple, Optional
from enum import Enum
import numpy as np


class PrecisionFormat(Enum):
    """Supported precision formats for quantization."""
    FP32 = "fp32"       # Full precision (baseline)
    FP16 = "fp16"       # Half precision
    BF16 = "bf16"       # Brain floating point
    FP8_E4M3 = "fp8_e4m3"  # 8-bit float (4 exp, 3 mantissa)
    FP8_E5M2 = "fp8_e5m2"  # 8-bit float (5 exp, 2 mantissa)
    INT8 = "int8"       # 8-bit integer
    INT4 = "int4"       # 4-bit integer
    FP4 = "fp4"         # 4-bit float (NVFP4)
    NF4 = "nf4"         # 4-bit NormalFloat


@dataclass
class PrecisionSpec:
    """Specification for a precision format."""
    format: PrecisionFormat
    bits: int
    exponent_bits: int
    mantissa_bits: int
    sign_bit: bool
    dynamic_range: float  # Approximate
    memory_reduction: float  # vs FP32
    typical_accuracy_loss: str
    hardware_support: List[str]


class PrecisionCatalog:
    """
    Catalog of precision formats with characteristics.

    Provides information for selecting appropriate formats
    based on hardware and accuracy requirements.
    """

    FORMATS = {
        PrecisionFormat.FP32: PrecisionSpec(
            format=PrecisionFormat.FP32,
            bits=32,
            exponent_bits=8,
            mantissa_bits=23,
            sign_bit=True,
            dynamic_range=1e38,
            memory_reduction=1.0,
            typical_accuracy_loss="baseline",
            hardware_support=["all"]
        ),
        PrecisionFormat.FP16: PrecisionSpec(
            format=PrecisionFormat.FP16,
            bits=16,
            exponent_bits=5,
            mantissa_bits=10,
            sign_bit=True,
            dynamic_range=6e4,
            memory_reduction=2.0,
            typical_accuracy_loss="<1%",
            hardware_support=["NVIDIA Volta+", "AMD MI200+", "Apple M1+"]
        ),
        PrecisionFormat.BF16: PrecisionSpec(
            format=PrecisionFormat.BF16,
            bits=16,
            exponent_bits=8,
            mantissa_bits=7,
            sign_bit=True,
            dynamic_range=1e38,
            memory_reduction=2.0,
            typical_accuracy_loss="<1%",
            hardware_support=["NVIDIA Ampere+", "AMD MI200+", "Intel Sapphire Rapids+"]
        ),
        PrecisionFormat.FP8_E4M3: PrecisionSpec(
            format=PrecisionFormat.FP8_E4M3,
            bits=8,
            exponent_bits=4,
            mantissa_bits=3,
            sign_bit=True,
            dynamic_range=448,
            memory_reduction=4.0,
            typical_accuracy_loss="1-3%",
            hardware_support=["NVIDIA Hopper+", "NVIDIA Ada Lovelace"]
        ),
        PrecisionFormat.INT8: PrecisionSpec(
            format=PrecisionFormat.INT8,
            bits=8,
            exponent_bits=0,
            mantissa_bits=7,
            sign_bit=True,
            dynamic_range=256,
            memory_reduction=4.0,
            typical_accuracy_loss="1-5%",
            hardware_support=["NVIDIA Turing+", "AMD MI200+", "Intel", "Apple"]
        ),
        PrecisionFormat.INT4: PrecisionSpec(
            format=PrecisionFormat.INT4,
            bits=4,
            exponent_bits=0,
            mantissa_bits=3,
            sign_bit=True,
            dynamic_range=16,
            memory_reduction=8.0,
            typical_accuracy_loss="3-10%",
            hardware_support=["NVIDIA Ampere+ (weight-only)", "CPU with unpacking"]
        ),
        PrecisionFormat.NF4: PrecisionSpec(
            format=PrecisionFormat.NF4,
            bits=4,
            exponent_bits=0,
            mantissa_bits=4,  # Non-uniform
            sign_bit=False,
            dynamic_range=16,
            memory_reduction=8.0,
            typical_accuracy_loss="2-8%",
            hardware_support=["CPU (bitsandbytes)", "CUDA with dequant"]
        ),
        PrecisionFormat.FP4: PrecisionSpec(
            format=PrecisionFormat.FP4,
            bits=4,
            exponent_bits=2,
            mantissa_bits=1,
            sign_bit=True,
            dynamic_range=6,
            memory_reduction=8.0,
            typical_accuracy_loss="5-15%",
            hardware_support=["NVIDIA Blackwell"]
        )
    }

    def get_recommended_format(
        self,
        model_size_b: float,
        target_hardware: str,
        max_accuracy_loss_pct: float = 5.0
    ) -> PrecisionFormat:
        """
        Recommend precision format based on constraints.

        Args:
            model_size_b: Model size in billions of parameters
            target_hardware: Target hardware (e.g., "H100", "A100")
            max_accuracy_loss_pct: Maximum acceptable accuracy loss
        """
        # Hardware capabilities
        hardware_support = {
            "H100": [PrecisionFormat.FP8_E4M3, PrecisionFormat.INT8, PrecisionFormat.INT4],
            "A100": [PrecisionFormat.INT8, PrecisionFormat.INT4],
            "B200": [PrecisionFormat.FP4, PrecisionFormat.FP8_E4M3, PrecisionFormat.INT4],
            "RTX4090": [PrecisionFormat.FP8_E4M3, PrecisionFormat.INT8, PrecisionFormat.INT4],
            "CPU": [PrecisionFormat.INT8, PrecisionFormat.INT4]
        }

        available = hardware_support.get(target_hardware, [PrecisionFormat.INT8])

        # Filter by accuracy tolerance
        candidates = []
        for fmt in available:
            spec = self.FORMATS[fmt]
            # Parse typical accuracy loss
            if "%" in spec.typical_accuracy_loss:
                loss_str = spec.typical_accuracy_loss.replace("<", "").replace("%", "")
                if "-" in loss_str:
                    max_loss = float(loss_str.split("-")[1])
                else:
                    max_loss = float(loss_str)

                if max_loss <= max_accuracy_loss_pct:
                    candidates.append((fmt, spec.memory_reduction))

        if not candidates:
            return PrecisionFormat.INT8  # Safe default

        # Return format with best memory reduction
        candidates.sort(key=lambda x: x[1], reverse=True)
        return candidates[0][0]


class QuantizationScheme(Enum):
    """Quantization scheme types."""
    SYMMETRIC = "symmetric"      # Zero point at 0
    ASYMMETRIC = "asymmetric"    # Zero point can shift


class QuantizationGranularity(Enum):
    """Granularity of quantization parameters."""
    PER_TENSOR = "per_tensor"    # One scale per tensor
    PER_CHANNEL = "per_channel"  # One scale per output channel
    PER_GROUP = "per_group"      # One scale per group of weights
    PER_TOKEN = "per_token"      # One scale per token (activations)


@dataclass
class QuantizationConfig:
    """Configuration for quantization operation."""
    format: PrecisionFormat
    scheme: QuantizationScheme
    granularity: QuantizationGranularity
    group_size: int = 128  # For per-group quantization
    calibration_method: str = "minmax"
    symmetric: bool = True


class QuantizationMath:
    """
    Mathematical operations for quantization.

    Implements core quantization and dequantization functions.
    """

    @staticmethod
    def compute_scale_zp_symmetric(
        tensor: np.ndarray,
        num_bits: int
    ) -> Tuple[float, int]:
        """
        Compute scale for symmetric quantization.

        Args:
            tensor: Tensor to quantize
            num_bits: Target bit width

        Returns:
            (scale, zero_point) where zero_point is always 0 for symmetric
        """
        qmax = (1 << (num_bits - 1)) - 1
        qmin = -(1 << (num_bits - 1))

        abs_max = np.max(np.abs(tensor))
        scale = abs_max / qmax if abs_max > 0 else 1.0

        return scale, 0

    @staticmethod
    def compute_scale_zp_asymmetric(
        tensor: np.ndarray,
        num_bits: int
    ) -> Tuple[float, int]:
        """
        Compute scale and zero point for asymmetric quantization.

        Allows full range utilization for non-symmetric distributions.
        """
        qmax = (1 << num_bits) - 1
        qmin = 0

        min_val = np.min(tensor)
        max_val = np.max(tensor)

        scale = (max_val - min_val) / (qmax - qmin) if max_val > min_val else 1.0
        zero_point = int(round(qmin - min_val / scale))
        zero_point = np.clip(zero_point, qmin, qmax)

        return scale, zero_point

    @staticmethod
    def quantize(
        tensor: np.ndarray,
        scale: float,
        zero_point: int,
        num_bits: int,
        symmetric: bool = True
    ) -> np.ndarray:
        """Quantize tensor to integer representation."""
        if symmetric:
            qmax = (1 << (num_bits - 1)) - 1
            qmin = -(1 << (num_bits - 1))
        else:
            qmax = (1 << num_bits) - 1
            qmin = 0

        quantized = np.round(tensor / scale) + zero_point
        quantized = np.clip(quantized, qmin, qmax).astype(np.int8)

        return quantized

    @staticmethod
    def dequantize(
        quantized: np.ndarray,
        scale: float,
        zero_point: int
    ) -> np.ndarray:
        """Dequantize integer tensor back to floating point."""
        return (quantized.astype(np.float32) - zero_point) * scale

    @staticmethod
    def compute_quantization_error(
        original: np.ndarray,
        quantized: np.ndarray,
        scale: float,
        zero_point: int
    ) -> Dict[str, float]:
        """Compute quantization error metrics."""
        dequantized = QuantizationMath.dequantize(quantized, scale, zero_point)

        mse = np.mean((original - dequantized) ** 2)
        mae = np.mean(np.abs(original - dequantized))
        max_error = np.max(np.abs(original - dequantized))

        # Signal-to-quantization-noise ratio
        signal_power = np.mean(original ** 2)
        sqnr = 10 * np.log10(signal_power / mse) if mse > 0 else float('inf')

        return {
            "mse": float(mse),
            "mae": float(mae),
            "max_error": float(max_error),
            "sqnr_db": float(sqnr)
        }
```

---

## 6.1.2 Post-Training Quantization (PTQ)

### Weight-Only Quantization Methods

```python
"""
ABOUTME: Implementation of post-training quantization methods for LLMs.
ABOUTME: Covers GPTQ, AWQ, and other weight-only quantization approaches.
"""

from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple, Any
import numpy as np
from abc import ABC, abstractmethod


@dataclass
class CalibrationConfig:
    """Configuration for PTQ calibration."""
    num_samples: int = 128
    sequence_length: int = 2048
    dataset: str = "wikitext"  # or "c4", "pile", "openplatypus"
    batch_size: int = 1


class PTQMethod(ABC):
    """Base class for post-training quantization methods."""

    @abstractmethod
    def quantize_layer(
        self,
        weight: np.ndarray,
        activations: Optional[np.ndarray] = None
    ) -> Tuple[np.ndarray, Dict]:
        """Quantize a single layer."""
        pass


class GPTQQuantizer(PTQMethod):
    """
    GPTQ (Generative Pre-trained Transformer Quantization).

    Uses approximate second-order information (Hessian) to minimize
    quantization error layer by layer. Works column-by-column with
    error compensation.

    Reference: Frantar et al., "GPTQ: Accurate Post-Training Quantization
    for Generative Pre-trained Transformers" (2023)
    """

    def __init__(
        self,
        bits: int = 4,
        group_size: int = 128,
        act_order: bool = True,
        damp_percent: float = 0.01,
        sym: bool = True
    ):
        self.bits = bits
        self.group_size = group_size
        self.act_order = act_order  # Order columns by activation magnitude
        self.damp_percent = damp_percent
        self.sym = sym

    def quantize_layer(
        self,
        weight: np.ndarray,
        activations: np.ndarray
    ) -> Tuple[np.ndarray, Dict]:
        """
        Quantize a weight matrix using GPTQ algorithm.

        Args:
            weight: Weight matrix [out_features, in_features]
            activations: Calibration activations [num_samples, in_features]

        Returns:
            (quantized_weights, quantization_metadata)
        """
        out_features, in_features = weight.shape

        # Compute Hessian approximation: H = X^T X
        H = activations.T @ activations
        H /= activations.shape[0]

        # Add damping for numerical stability
        damp = self.damp_percent * np.mean(np.diag(H))
        H += damp * np.eye(H.shape[0])

        # Cholesky decomposition for efficient updates
        try:
            H_inv = np.linalg.cholesky(np.linalg.inv(H))
        except np.linalg.LinAlgError:
            # Fallback to direct inversion
            H_inv = np.linalg.inv(H + 0.1 * np.eye(H.shape[0]))

        # Column ordering (act_order optimization)
        if self.act_order:
            col_order = np.argsort(np.diag(H))[::-1]  # Highest activation first
        else:
            col_order = np.arange(in_features)

        # Quantize column by column
        W = weight.copy()
        Q = np.zeros_like(weight)
        scales = []
        zeros = []

        for group_start in range(0, in_features, self.group_size):
            group_end = min(group_start + self.group_size, in_features)
            group_cols = col_order[group_start:group_end]

            # Compute group scale
            group_weights = W[:, group_cols]
            scale, zero = self._compute_group_scale(group_weights)
            scales.append(scale)
            zeros.append(zero)

            # Quantize each column in group
            for i, col in enumerate(group_cols):
                # Quantize current column
                q_col = self._quantize_column(W[:, col], scale, zero)
                Q[:, col] = q_col

                # Compute quantization error
                error = W[:, col] - self._dequantize_column(q_col, scale, zero)

                # Update remaining columns to compensate for error
                if col < in_features - 1:
                    remaining_cols = col_order[group_start + i + 1:]
                    if len(remaining_cols) > 0:
                        # Error compensation using Hessian
                        h_col = H_inv[col, col]
                        if h_col > 0:
                            update = np.outer(error, H_inv[col, remaining_cols]) / h_col
                            W[:, remaining_cols] -= update

        metadata = {
            "method": "gptq",
            "bits": self.bits,
            "group_size": self.group_size,
            "scales": np.array(scales),
            "zeros": np.array(zeros),
            "act_order": self.act_order,
            "col_order": col_order if self.act_order else None
        }

        return Q.astype(np.int8), metadata

    def _compute_group_scale(self, weights: np.ndarray) -> Tuple[float, float]:
        """Compute scale for a group of weights."""
        qmax = (1 << (self.bits - 1)) - 1

        if self.sym:
            abs_max = np.max(np.abs(weights))
            scale = abs_max / qmax if abs_max > 0 else 1.0
            zero = 0.0
        else:
            min_val = np.min(weights)
            max_val = np.max(weights)
            scale = (max_val - min_val) / (2 * qmax) if max_val > min_val else 1.0
            zero = -min_val / scale

        return scale, zero

    def _quantize_column(
        self,
        column: np.ndarray,
        scale: float,
        zero: float
    ) -> np.ndarray:
        """Quantize a single column."""
        qmax = (1 << (self.bits - 1)) - 1
        qmin = -qmax - 1 if self.sym else 0

        quantized = np.round(column / scale + zero)
        return np.clip(quantized, qmin, qmax)

    def _dequantize_column(
        self,
        quantized: np.ndarray,
        scale: float,
        zero: float
    ) -> np.ndarray:
        """Dequantize a column."""
        return (quantized - zero) * scale


class AWQQuantizer(PTQMethod):
    """
    AWQ (Activation-aware Weight Quantization).

    Identifies salient weights based on activation patterns and
    applies scaling to protect them during quantization. Requires
    much less calibration data than GPTQ (128-1024 tokens).

    Reference: Lin et al., "AWQ: Activation-aware Weight Quantization
    for LLM Compression and Acceleration" (2023)
    """

    def __init__(
        self,
        bits: int = 4,
        group_size: int = 128,
        zero_point: bool = True,
        version: str = "gemm"  # gemm or gemv
    ):
        self.bits = bits
        self.group_size = group_size
        self.zero_point = zero_point
        self.version = version

    def quantize_layer(
        self,
        weight: np.ndarray,
        activations: np.ndarray
    ) -> Tuple[np.ndarray, Dict]:
        """
        Quantize weight matrix using AWQ.

        Key insight: Scale salient channels to reduce their
        quantization error, then compensate in subsequent layers.
        """
        out_features, in_features = weight.shape

        # Step 1: Compute activation-based saliency
        # Salient weights are those connected to high-magnitude activations
        act_scales = np.mean(np.abs(activations), axis=0)

        # Step 2: Search for optimal per-channel scaling
        scales = self._search_best_scales(weight, act_scales)

        # Step 3: Apply scaling to weights
        scaled_weight = weight * scales.reshape(1, -1)

        # Step 4: Quantize scaled weights
        Q = np.zeros_like(weight, dtype=np.int8)
        group_scales = []
        group_zeros = []

        for g in range(0, in_features, self.group_size):
            g_end = min(g + self.group_size, in_features)
            group = scaled_weight[:, g:g_end]

            # Per-group quantization
            g_scale, g_zero = self._compute_group_params(group)
            group_scales.append(g_scale)
            group_zeros.append(g_zero)

            Q[:, g:g_end] = self._quantize_group(group, g_scale, g_zero)

        metadata = {
            "method": "awq",
            "bits": self.bits,
            "group_size": self.group_size,
            "scales": scales,  # Per-channel scaling factors
            "group_scales": np.array(group_scales),
            "group_zeros": np.array(group_zeros),
            "version": self.version
        }

        return Q, metadata

    def _search_best_scales(
        self,
        weight: np.ndarray,
        act_scales: np.ndarray,
        n_grid: int = 20
    ) -> np.ndarray:
        """
        Search for optimal scaling factors per channel.

        Uses grid search to find scales that minimize quantization error.
        """
        out_features, in_features = weight.shape

        # Search over scaling factors
        best_scales = np.ones(in_features)
        best_error = float('inf')

        for alpha in np.linspace(0.1, 1.0, n_grid):
            # Scale based on activation magnitude raised to power alpha
            scales = act_scales ** alpha

            # Prevent extreme scales
            scales = np.clip(scales, 0.1, 10.0)

            # Simulate quantization error
            scaled = weight * scales.reshape(1, -1)
            error = self._estimate_quantization_error(scaled)

            if error < best_error:
                best_error = error
                best_scales = scales.copy()

        return best_scales

    def _estimate_quantization_error(self, weight: np.ndarray) -> float:
        """Estimate quantization error for given weights."""
        # Quick approximation using round-to-nearest
        qmax = (1 << (self.bits - 1)) - 1

        abs_max = np.max(np.abs(weight), axis=0, keepdims=True)
        scale = abs_max / qmax

        quantized = np.round(weight / scale) * scale
        error = np.mean((weight - quantized) ** 2)

        return error

    def _compute_group_params(
        self,
        group: np.ndarray
    ) -> Tuple[float, float]:
        """Compute scale and zero point for a group."""
        qmax = (1 << (self.bits - 1)) - 1

        min_val = np.min(group)
        max_val = np.max(group)

        if self.zero_point:
            scale = (max_val - min_val) / (2 * qmax + 1)
            zero = -min_val / scale if scale > 0 else 0
        else:
            scale = max(abs(min_val), abs(max_val)) / qmax
            zero = 0

        return max(scale, 1e-8), zero

    def _quantize_group(
        self,
        group: np.ndarray,
        scale: float,
        zero: float
    ) -> np.ndarray:
        """Quantize a group of weights."""
        qmax = (1 << (self.bits - 1)) - 1
        qmin = -qmax - 1 if not self.zero_point else 0

        quantized = np.round(group / scale + zero)
        return np.clip(quantized, qmin, qmax).astype(np.int8)


class SmoothQuantQuantizer(PTQMethod):
    """
    SmoothQuant: Smoothing activations for W8A8 quantization.

    Migrates quantization difficulty from activations to weights
    by mathematically equivalent scaling transformation.

    Reference: Xiao et al., "SmoothQuant: Accurate and Efficient
    Post-Training Quantization for Large Language Models" (2023)
    """

    def __init__(
        self,
        alpha: float = 0.5,
        bits_weight: int = 8,
        bits_activation: int = 8
    ):
        self.alpha = alpha  # Migration strength (0 = all to weights, 1 = all to activations)
        self.bits_w = bits_weight
        self.bits_a = bits_activation

    def compute_smoothing_scales(
        self,
        weight: np.ndarray,
        activation_stats: Dict[str, np.ndarray]
    ) -> np.ndarray:
        """
        Compute per-channel smoothing scales.

        Scale = (max_act / max_weight) ^ alpha
        """
        act_max = activation_stats.get("abs_max", np.ones(weight.shape[1]))
        weight_max = np.max(np.abs(weight), axis=0)

        # Prevent division by zero
        weight_max = np.maximum(weight_max, 1e-8)

        scales = (act_max / weight_max) ** self.alpha

        return scales

    def quantize_layer(
        self,
        weight: np.ndarray,
        activations: np.ndarray
    ) -> Tuple[np.ndarray, Dict]:
        """Apply SmoothQuant to weight matrix."""
        # Compute smoothing scales
        act_stats = {"abs_max": np.max(np.abs(activations), axis=0)}
        scales = self.compute_smoothing_scales(weight, act_stats)

        # Apply smoothing to weights (multiply)
        smoothed_weight = weight * scales.reshape(1, -1)

        # Quantize smoothed weights
        w_scale, w_zp = QuantizationMath.compute_scale_zp_symmetric(
            smoothed_weight, self.bits_w
        )
        Q = QuantizationMath.quantize(
            smoothed_weight, w_scale, w_zp, self.bits_w
        )

        metadata = {
            "method": "smoothquant",
            "bits_weight": self.bits_w,
            "bits_activation": self.bits_a,
            "alpha": self.alpha,
            "smoothing_scales": scales,
            "weight_scale": w_scale,
            "weight_zero_point": w_zp
        }

        return Q, metadata
```

### Calibration Dataset Selection

```python
"""
ABOUTME: Calibration dataset management for post-training quantization.
ABOUTME: Implements dataset loading and sample selection strategies.
"""

from dataclasses import dataclass
from typing import List, Dict, Optional, Iterator
import numpy as np


@dataclass
class CalibrationSample:
    """Single calibration sample."""
    input_ids: List[int]
    attention_mask: List[int]


class CalibrationDataManager:
    """
    Manages calibration data for PTQ.

    Different methods require different amounts of data:
    - AWQ: 128-1024 tokens
    - GPTQ: 128-1024 samples (more is better)
    - SmoothQuant: Statistics from ~512 samples
    """

    DATASET_CONFIGS = {
        "wikitext": {
            "path": "wikitext",
            "name": "wikitext-2-raw-v1",
            "split": "train",
            "text_column": "text"
        },
        "c4": {
            "path": "allenai/c4",
            "name": "en",
            "split": "train",
            "text_column": "text"
        },
        "openplatypus": {
            "path": "garage-bAInd/Open-Platypus",
            "split": "train",
            "text_column": "instruction"
        },
        "pile": {
            "path": "EleutherAI/pile",
            "split": "train",
            "text_column": "text"
        }
    }

    def __init__(
        self,
        tokenizer,
        dataset_name: str = "wikitext",
        seq_length: int = 2048
    ):
        self.tokenizer = tokenizer
        self.dataset_name = dataset_name
        self.seq_length = seq_length
        self.config = self.DATASET_CONFIGS.get(dataset_name, self.DATASET_CONFIGS["wikitext"])

    def get_calibration_samples(
        self,
        num_samples: int = 128,
        seed: int = 42
    ) -> List[CalibrationSample]:
        """
        Get calibration samples from dataset.

        Args:
            num_samples: Number of samples to return
            seed: Random seed for reproducibility
        """
        # In practice, this would load from HuggingFace datasets
        # Here we show the structure
        samples = []

        # Placeholder for actual dataset loading
        # dataset = load_dataset(self.config["path"], self.config["name"])

        np.random.seed(seed)

        for i in range(num_samples):
            # Generate sample (in practice, would be from dataset)
            sample = CalibrationSample(
                input_ids=[1] + [np.random.randint(100, 30000) for _ in range(self.seq_length - 1)],
                attention_mask=[1] * self.seq_length
            )
            samples.append(sample)

        return samples

    def get_recommended_config(
        self,
        method: str
    ) -> Dict[str, any]:
        """Get recommended calibration config for method."""
        configs = {
            "gptq": {
                "dataset": "openplatypus",  # Quality data works better
                "num_samples": 128,
                "seq_length": 2048
            },
            "awq": {
                "dataset": "wikitext",
                "num_samples": 128,  # AWQ needs less data
                "seq_length": 512
            },
            "smoothquant": {
                "dataset": "pile",
                "num_samples": 512,
                "seq_length": 2048
            }
        }
        return configs.get(method, configs["gptq"])
```

---

## 6.1.3 Quantization-Aware Training (QAT)

### When QAT is Necessary

```python
"""
ABOUTME: Quantization-aware training for situations where PTQ is insufficient.
ABOUTME: Implements QAT training loop with straight-through estimators.
"""

from dataclasses import dataclass
from typing import Dict, List, Optional, Callable
import numpy as np


@dataclass
class QATDecisionCriteria:
    """Criteria for deciding between PTQ and QAT."""
    target_bits: int
    model_size_b: float
    accuracy_requirement: str  # "strict", "moderate", "relaxed"
    ptq_quality_loss_pct: Optional[float] = None


class QATDecisionMaker:
    """
    Decides whether QAT is necessary based on PTQ results.

    Generally, QAT is needed when:
    - Target precision is very low (INT4 or below)
    - PTQ causes unacceptable accuracy loss
    - Task requires very tight accuracy tolerances
    """

    def should_use_qat(self, criteria: QATDecisionCriteria) -> Dict[str, any]:
        """
        Determine if QAT is recommended.

        Returns recommendation with reasoning.
        """
        reasons = []
        recommendation = "ptq"  # Default to PTQ

        # Check bit width
        if criteria.target_bits <= 4:
            reasons.append(
                f"Low bit width ({criteria.target_bits} bits) - QAT often improves quality"
            )
            if criteria.accuracy_requirement == "strict":
                recommendation = "qat"

        # Check if PTQ quality loss is known
        if criteria.ptq_quality_loss_pct is not None:
            if criteria.ptq_quality_loss_pct > 5.0 and criteria.accuracy_requirement == "strict":
                reasons.append(
                    f"PTQ quality loss ({criteria.ptq_quality_loss_pct:.1f}%) exceeds tolerance"
                )
                recommendation = "qat"
            elif criteria.ptq_quality_loss_pct > 10.0:
                reasons.append(
                    f"High PTQ quality loss ({criteria.ptq_quality_loss_pct:.1f}%) - QAT recommended"
                )
                recommendation = "qat"

        # Consider model size (QAT is expensive for large models)
        if criteria.model_size_b > 70:
            reasons.append(
                f"Large model ({criteria.model_size_b}B) - QAT is very expensive, consider PTQ first"
            )
            if recommendation == "qat":
                recommendation = "qat_if_ptq_fails"

        return {
            "recommendation": recommendation,
            "reasons": reasons,
            "estimated_qat_time_hours": self._estimate_qat_time(criteria)
        }

    def _estimate_qat_time(self, criteria: QATDecisionCriteria) -> float:
        """Estimate QAT training time in hours."""
        # Rough estimate: 1 hour per billion parameters on 8xA100
        base_hours = criteria.model_size_b * 1.0

        # Lower bits need more training
        if criteria.target_bits <= 4:
            base_hours *= 1.5

        return base_hours


class StraightThroughEstimator:
    """
    Straight-Through Estimator for quantization gradients.

    In forward pass: quantize weights
    In backward pass: pass gradients through as if no quantization
    """

    def __init__(
        self,
        bits: int = 4,
        symmetric: bool = True
    ):
        self.bits = bits
        self.symmetric = symmetric

    def forward(
        self,
        weight: np.ndarray,
        scale: float,
        zero_point: float = 0
    ) -> np.ndarray:
        """
        Forward pass with quantization.

        Returns quantized-dequantized weights.
        """
        qmax = (1 << (self.bits - 1)) - 1 if self.symmetric else (1 << self.bits) - 1
        qmin = -qmax if self.symmetric else 0

        # Quantize
        quantized = np.round(weight / scale + zero_point)
        quantized = np.clip(quantized, qmin, qmax)

        # Dequantize
        dequantized = (quantized - zero_point) * scale

        return dequantized

    def backward(
        self,
        grad_output: np.ndarray,
        weight: np.ndarray,
        scale: float
    ) -> np.ndarray:
        """
        Backward pass using straight-through estimator.

        Passes gradient through as if no quantization occurred,
        with optional clipping for out-of-range values.
        """
        # Straight-through: gradient passes through unchanged
        # Optionally clip gradients for weights outside quantization range
        qmax = (1 << (self.bits - 1)) - 1 if self.symmetric else (1 << self.bits) - 1
        qmin = -qmax if self.symmetric else 0

        # Mask for in-range weights
        quantized = weight / scale
        in_range = (quantized >= qmin) & (quantized <= qmax)

        # Zero gradient for out-of-range weights
        grad_input = grad_output * in_range.astype(float)

        return grad_input


@dataclass
class QATConfig:
    """Configuration for quantization-aware training."""
    bits: int = 4
    group_size: int = 128
    symmetric: bool = True
    learning_rate: float = 1e-5
    num_epochs: int = 1
    warmup_steps: int = 100
    gradient_accumulation: int = 8
    max_grad_norm: float = 1.0


class QATTrainer:
    """
    Trainer for quantization-aware training.

    Wraps standard training with quantization simulation.
    """

    def __init__(
        self,
        model,
        config: QATConfig,
        tokenizer
    ):
        self.model = model
        self.config = config
        self.tokenizer = tokenizer
        self.ste = StraightThroughEstimator(bits=config.bits, symmetric=config.symmetric)

    def prepare_model_for_qat(self) -> None:
        """
        Prepare model for QAT.

        Inserts fake quantization operations and initializes scales.
        """
        # In practice, this would:
        # 1. Replace linear layers with quantized versions
        # 2. Initialize per-group scales
        # 3. Set up scale learning if dynamic

        pass

    def train_step(
        self,
        batch: Dict
    ) -> Dict[str, float]:
        """
        Single QAT training step.

        Returns loss and metrics.
        """
        # Forward pass with quantization simulation
        # Loss computation
        # Backward pass with STE

        # Placeholder implementation
        return {
            "loss": 0.0,
            "perplexity": 0.0
        }

    def save_quantized_model(self, output_path: str) -> None:
        """Save the quantized model weights."""
        pass
```

---

## 6.1.4 Hardware-Specific Quantization

### Hardware Capabilities

```python
"""
ABOUTME: Hardware-specific quantization configurations and optimizations.
ABOUTME: Maps hardware capabilities to optimal quantization strategies.
"""

from dataclasses import dataclass
from typing import Dict, List, Optional
from enum import Enum


class HardwareFamily(Enum):
    """Hardware families for inference."""
    NVIDIA_HOPPER = "nvidia_hopper"       # H100, H200
    NVIDIA_BLACKWELL = "nvidia_blackwell"  # B100, B200
    NVIDIA_AMPERE = "nvidia_ampere"        # A100, A10, A6000
    NVIDIA_ADA = "nvidia_ada"              # RTX 4090, L40
    AMD_MI300 = "amd_mi300"
    AMD_MI250 = "amd_mi250"
    INTEL_GAUDI = "intel_gaudi"
    APPLE_SILICON = "apple_silicon"
    CPU_X86 = "cpu_x86"
    CPU_ARM = "cpu_arm"


@dataclass
class HardwareQuantCapabilities:
    """Quantization capabilities for a hardware platform."""
    family: HardwareFamily
    supported_formats: List[PrecisionFormat]
    native_tensor_core_formats: List[PrecisionFormat]
    memory_gb: float
    bandwidth_gbps: float
    recommended_format: PrecisionFormat
    notes: List[str]


class HardwareQuantizationGuide:
    """
    Guide for hardware-specific quantization.

    Maps hardware to optimal quantization configurations.
    """

    HARDWARE_PROFILES = {
        HardwareFamily.NVIDIA_HOPPER: HardwareQuantCapabilities(
            family=HardwareFamily.NVIDIA_HOPPER,
            supported_formats=[
                PrecisionFormat.FP8_E4M3,
                PrecisionFormat.FP8_E5M2,
                PrecisionFormat.INT8,
                PrecisionFormat.INT4,
                PrecisionFormat.FP16,
                PrecisionFormat.BF16
            ],
            native_tensor_core_formats=[
                PrecisionFormat.FP8_E4M3,
                PrecisionFormat.FP8_E5M2,
                PrecisionFormat.INT8
            ],
            memory_gb=80,  # H100 80GB
            bandwidth_gbps=3350,
            recommended_format=PrecisionFormat.FP8_E4M3,
            notes=[
                "Transformer Engine enables automatic FP8 with master weights in higher precision",
                "FP8 achieves near-lossless quality with 2x memory savings",
                "INT4 weight-only available via TensorRT-LLM"
            ]
        ),
        HardwareFamily.NVIDIA_BLACKWELL: HardwareQuantCapabilities(
            family=HardwareFamily.NVIDIA_BLACKWELL,
            supported_formats=[
                PrecisionFormat.FP4,
                PrecisionFormat.FP8_E4M3,
                PrecisionFormat.FP8_E5M2,
                PrecisionFormat.INT8,
                PrecisionFormat.INT4,
                PrecisionFormat.FP16,
                PrecisionFormat.BF16
            ],
            native_tensor_core_formats=[
                PrecisionFormat.FP4,
                PrecisionFormat.FP8_E4M3,
                PrecisionFormat.INT8
            ],
            memory_gb=192,  # B200
            bandwidth_gbps=8000,
            recommended_format=PrecisionFormat.FP8_E4M3,  # FP4 for extreme compression
            notes=[
                "Native FP4 (NVFP4) support enables 8x compression",
                "FP4 best for inference-heavy workloads",
                "TensorRT Model Optimizer provides FP4 quantization"
            ]
        ),
        HardwareFamily.NVIDIA_AMPERE: HardwareQuantCapabilities(
            family=HardwareFamily.NVIDIA_AMPERE,
            supported_formats=[
                PrecisionFormat.INT8,
                PrecisionFormat.INT4,
                PrecisionFormat.FP16,
                PrecisionFormat.BF16
            ],
            native_tensor_core_formats=[
                PrecisionFormat.INT8
            ],
            memory_gb=80,  # A100 80GB
            bandwidth_gbps=2039,
            recommended_format=PrecisionFormat.INT8,
            notes=[
                "INT8 via TensorRT provides best throughput",
                "Weight-only INT4 (GPTQ/AWQ) works but requires dequantization",
                "2:4 structured sparsity supported"
            ]
        ),
        HardwareFamily.AMD_MI300: HardwareQuantCapabilities(
            family=HardwareFamily.AMD_MI300,
            supported_formats=[
                PrecisionFormat.FP8_E4M3,
                PrecisionFormat.INT8,
                PrecisionFormat.FP16,
                PrecisionFormat.BF16
            ],
            native_tensor_core_formats=[
                PrecisionFormat.FP8_E4M3,
                PrecisionFormat.INT8
            ],
            memory_gb=192,  # MI300X
            bandwidth_gbps=5300,
            recommended_format=PrecisionFormat.FP8_E4M3,
            notes=[
                "ROCm supports FP8 via hipBLASLt",
                "Large unified memory enables larger batch sizes",
                "vLLM supports AMD FP8 inference"
            ]
        ),
        HardwareFamily.APPLE_SILICON: HardwareQuantCapabilities(
            family=HardwareFamily.APPLE_SILICON,
            supported_formats=[
                PrecisionFormat.INT8,
                PrecisionFormat.INT4,
                PrecisionFormat.FP16
            ],
            native_tensor_core_formats=[
                PrecisionFormat.INT8
            ],
            memory_gb=128,  # M2 Ultra
            bandwidth_gbps=800,
            recommended_format=PrecisionFormat.INT4,
            notes=[
                "Unified memory enables larger models",
                "CoreML provides INT8 and INT4 quantization",
                "MLX supports efficient INT4 inference"
            ]
        ),
        HardwareFamily.CPU_X86: HardwareQuantCapabilities(
            family=HardwareFamily.CPU_X86,
            supported_formats=[
                PrecisionFormat.INT8,
                PrecisionFormat.INT4
            ],
            native_tensor_core_formats=[],
            memory_gb=512,  # Server RAM
            bandwidth_gbps=200,
            recommended_format=PrecisionFormat.INT4,
            notes=[
                "ONNX Runtime provides optimized CPU INT8/INT4",
                "Intel Neural Compressor for Intel CPUs",
                "llama.cpp GGUF format optimized for CPU"
            ]
        )
    }

    def get_optimal_config(
        self,
        hardware: HardwareFamily,
        model_size_b: float,
        latency_priority: bool = False
    ) -> Dict:
        """
        Get optimal quantization configuration for hardware.

        Args:
            hardware: Target hardware family
            model_size_b: Model size in billions
            latency_priority: Prioritize latency over throughput
        """
        profile = self.HARDWARE_PROFILES.get(hardware)
        if not profile:
            return {"error": f"Unknown hardware: {hardware}"}

        # Check if model fits in memory
        # Rough estimate: 2 bytes per param for FP16
        fp16_memory_gb = model_size_b * 2

        config = {
            "hardware": hardware.value,
            "model_size_b": model_size_b,
            "fp16_memory_gb": fp16_memory_gb
        }

        # Select format based on memory constraints
        if fp16_memory_gb > profile.memory_gb * 0.8:
            # Need aggressive quantization
            if PrecisionFormat.INT4 in profile.supported_formats:
                config["recommended_format"] = "int4"
                config["method"] = "awq" if latency_priority else "gptq"
                config["memory_estimate_gb"] = fp16_memory_gb / 4
            elif PrecisionFormat.FP8_E4M3 in profile.supported_formats:
                config["recommended_format"] = "fp8"
                config["memory_estimate_gb"] = fp16_memory_gb / 2
            else:
                config["error"] = "Model too large for hardware"
        elif fp16_memory_gb > profile.memory_gb * 0.5:
            # Moderate quantization beneficial
            if PrecisionFormat.FP8_E4M3 in profile.native_tensor_core_formats:
                config["recommended_format"] = "fp8"
                config["method"] = "ptq"
                config["memory_estimate_gb"] = fp16_memory_gb / 2
            else:
                config["recommended_format"] = "int8"
                config["method"] = "smoothquant"
                config["memory_estimate_gb"] = fp16_memory_gb / 2
        else:
            # Model fits comfortably
            config["recommended_format"] = profile.recommended_format.value
            config["memory_estimate_gb"] = fp16_memory_gb

        config["hardware_notes"] = profile.notes

        return config
```

---

## 6.1.5 Quality Validation

### Validation Framework

```python
"""
ABOUTME: Quality validation framework for quantized models.
ABOUTME: Implements automated testing and regression detection.
"""

from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple
import numpy as np


@dataclass
class ValidationResult:
    """Result from quantization validation."""
    benchmark: str
    original_score: float
    quantized_score: float
    degradation_pct: float
    passed: bool
    details: Dict


class QuantizationValidator:
    """
    Validates quantized model quality.

    Runs standardized benchmarks and compares against
    original model to detect quality regressions.
    """

    DEFAULT_BENCHMARKS = [
        "mmlu",
        "hellaswag",
        "gsm8k",
        "humaneval",
        "ifeval"
    ]

    DEFAULT_THRESHOLDS = {
        "mmlu": 0.03,           # Max 3% degradation
        "hellaswag": 0.02,      # Max 2% degradation
        "gsm8k": 0.05,          # Max 5% degradation (more sensitive)
        "humaneval": 0.05,
        "ifeval": 0.10          # IFEval is very sensitive to quantization
    }

    def __init__(
        self,
        benchmarks: Optional[List[str]] = None,
        thresholds: Optional[Dict[str, float]] = None
    ):
        self.benchmarks = benchmarks or self.DEFAULT_BENCHMARKS
        self.thresholds = thresholds or self.DEFAULT_THRESHOLDS

    def validate(
        self,
        original_results: Dict[str, float],
        quantized_results: Dict[str, float]
    ) -> Dict[str, ValidationResult]:
        """
        Validate quantized model against original.

        Args:
            original_results: Benchmark scores for original model
            quantized_results: Benchmark scores for quantized model

        Returns:
            Validation results per benchmark
        """
        results = {}

        for benchmark in self.benchmarks:
            if benchmark not in original_results or benchmark not in quantized_results:
                continue

            original = original_results[benchmark]
            quantized = quantized_results[benchmark]

            degradation = (original - quantized) / original if original > 0 else 0
            threshold = self.thresholds.get(benchmark, 0.05)

            results[benchmark] = ValidationResult(
                benchmark=benchmark,
                original_score=original,
                quantized_score=quantized,
                degradation_pct=degradation * 100,
                passed=degradation <= threshold,
                details={
                    "threshold_pct": threshold * 100,
                    "margin_pct": (threshold - degradation) * 100
                }
            )

        return results

    def generate_report(
        self,
        results: Dict[str, ValidationResult],
        quantization_config: Dict
    ) -> str:
        """Generate validation report."""
        lines = [
            "# Quantization Validation Report",
            "",
            "## Configuration",
            f"- Method: {quantization_config.get('method', 'unknown')}",
            f"- Bits: {quantization_config.get('bits', 'unknown')}",
            f"- Group Size: {quantization_config.get('group_size', 'unknown')}",
            "",
            "## Results",
            "",
            "| Benchmark | Original | Quantized | Degradation | Threshold | Status |",
            "|-----------|----------|-----------|-------------|-----------|--------|"
        ]

        all_passed = True
        for benchmark, result in results.items():
            status = "PASS" if result.passed else "FAIL"
            if not result.passed:
                all_passed = False

            lines.append(
                f"| {benchmark} | {result.original_score:.4f} | "
                f"{result.quantized_score:.4f} | {result.degradation_pct:.2f}% | "
                f"{result.details['threshold_pct']:.1f}% | {status} |"
            )

        lines.extend([
            "",
            f"## Overall: {'PASSED' if all_passed else 'FAILED'}",
            "",
            "## Recommendations"
        ])

        if not all_passed:
            lines.extend([
                "- Consider using higher precision (INT8 instead of INT4)",
                "- Try different quantization method (AWQ vs GPTQ)",
                "- Use task-specific calibration data",
                "- Consider QAT for critical tasks"
            ])
        else:
            lines.append("- Quantized model is ready for deployment")

        return "\n".join(lines)


class PerplexityValidator:
    """
    Fast perplexity-based validation for quantization.

    Perplexity is a good proxy for overall model quality
    and can be computed quickly.
    """

    def __init__(
        self,
        max_perplexity_increase_pct: float = 5.0
    ):
        self.max_increase = max_perplexity_increase_pct

    def compute_perplexity(
        self,
        model,
        tokenizer,
        texts: List[str],
        batch_size: int = 4
    ) -> float:
        """Compute perplexity on text samples."""
        # Simplified implementation
        # In practice, would compute log-likelihood and exponentiate

        total_loss = 0
        total_tokens = 0

        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]

            # Tokenize
            # Forward pass
            # Accumulate loss

            # Placeholder
            total_loss += np.random.uniform(2, 4) * len(batch_texts)
            total_tokens += sum(len(t.split()) for t in batch_texts)

        return np.exp(total_loss / total_tokens)

    def validate(
        self,
        original_ppl: float,
        quantized_ppl: float
    ) -> Dict:
        """Validate perplexity degradation."""
        increase_pct = ((quantized_ppl - original_ppl) / original_ppl) * 100

        return {
            "original_perplexity": original_ppl,
            "quantized_perplexity": quantized_ppl,
            "increase_pct": increase_pct,
            "passed": increase_pct <= self.max_increase,
            "threshold_pct": self.max_increase
        }
```

---

## Appendix A: Quantization Method Comparison

| Method | Bits | Calibration Data | Speed | Quality | Best For |
|--------|------|------------------|-------|---------|----------|
| **FP8 (E4M3)** | 8 | Minimal | 2x | Excellent | Hopper/Ada |
| **INT8 SmoothQuant** | 8 | 512 samples | 2x | Very Good | W8A8 |
| **GPTQ** | 4 | 128+ samples | 4x | Good | Large models |
| **AWQ** | 4 | 128 samples | 4x | Good | Fast deployment |
| **GGUF Q4_K_M** | 4 | None | 4x | Good | CPU inference |
| **NF4 (QLoRA)** | 4 | None | 4x | Moderate | Fine-tuning |

---

## Appendix B: Quick Start Commands

### GPTQ with AutoGPTQ

```python
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig

# Configure quantization
quantize_config = BaseQuantizeConfig(
    bits=4,
    group_size=128,
    damp_percent=0.01,
    desc_act=True,  # Act order optimization
    sym=True
)

# Load and quantize
model = AutoGPTQForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    quantize_config=quantize_config
)

# Quantize with calibration data
model.quantize(calibration_data)

# Save
model.save_quantized("llama-2-7b-gptq")
```

### AWQ with AutoAWQ

```python
from awq import AutoAWQForCausalLM

# Load model
model = AutoAWQForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")

# Quantize
model.quantize(
    tokenizer,
    quant_config={
        "w_bit": 4,
        "q_group_size": 128,
        "zero_point": True,
        "version": "GEMM"
    }
)

# Save
model.save_quantized("llama-2-7b-awq")
```

---

## Troubleshooting

**Issue: High accuracy loss with INT4**
```
Diagnosis:
1. Check if model is sensitive to quantization (small models often are)
2. Verify calibration data quality

Solutions:
- Use quality calibration data (OpenPlatypus instead of random)
- Increase group size from 128 to 256
- Try AWQ instead of GPTQ
- Fall back to INT8 or FP8
```

**Issue: Slow inference despite quantization**
```
Diagnosis:
1. Check if hardware supports quantized format natively
2. Verify inference framework is using optimized kernels

Solutions:
- Use TensorRT-LLM or vLLM for optimized inference
- Ensure CUDA compute capability matches quantization format
- Check for dequantization overhead
```

**Issue: Model generates gibberish after quantization**
```
Diagnosis:
1. Calibration data may be corrupted
2. Scale factors may be incorrect

Solutions:
- Re-run calibration with fresh data
- Check for NaN/Inf in quantized weights
- Try lower compression (more bits)
```

---

## Glossary

| Term | Definition |
|------|------------|
| **PTQ** | Post-Training Quantization - quantize after training |
| **QAT** | Quantization-Aware Training - train with simulated quantization |
| **Scale** | Multiplier to convert between quantized and float values |
| **Zero Point** | Offset for asymmetric quantization |
| **Group Size** | Number of weights sharing quantization parameters |
| **Act Order** | GPTQ optimization ordering columns by activation |
| **STE** | Straight-Through Estimator for gradient flow |
| **Calibration** | Process of determining optimal quantization parameters |

---

## References

1. Frantar, E., et al. (2023). "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers."
2. Lin, J., et al. (2023). "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration."
3. Xiao, G., et al. (2023). "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models."
4. Dettmers, T., et al. (2023). "QLoRA: Efficient Finetuning of Quantized LLMs."
5. NVIDIA. (2024). "TensorRT-LLM Quantization Guide."
6. Huang, W., et al. (2024). "A Comprehensive Evaluation of Quantized Instruction-Tuned Large Language Models."
