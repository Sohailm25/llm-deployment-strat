> **Navigation** | [← 6.3 Distillation](6.3_knowledge_distillation_guide.md) | [7.1 Vector Database →](../07_rag_pipeline/7.1_vector_database_guide.md)
>
> | | |
> |---|---|
> | **Prerequisites** | [6.1-6.3 Optimization](6.1_quantization_guide.md) &#124; KV caching |
> | **Related** | [9.1 Inference Engines](../09_inference_serving/9.1_inference_engine_selection_guide.md) &#124; [9.2 Serving Architecture](../09_inference_serving/9.2_serving_architecture_patterns_guide.md) |
> | **Next** | [7.1 Vector Database](../07_rag_pipeline/7.1_vector_database_guide.md) |

# Document 6.4: Speculative Decoding Guide

## Executive Summary

Speculative decoding is an inference optimization technique that accelerates LLM text generation by 2-3x without degrading output quality. It works by using a small, fast draft model to propose multiple tokens that a larger target model verifies in parallel. This guide covers core algorithms, draft model selection, tree-based approaches (Medusa, EAGLE), implementation patterns, and production deployment considerations.

## Prerequisites

- Understanding of autoregressive language model inference
- Familiarity with transformer architectures and attention mechanisms
- Experience with PyTorch and inference optimization
- Knowledge of KV caching and batched inference
- Completion of documents 6.1-6.3 (Quantization, Pruning, Distillation) recommended

---

## 6.4.1 Speculative Decoding Fundamentals

### Core Algorithm

Speculative decoding parallelizes the inherently sequential autoregressive generation process by speculating on future tokens:

```python
"""
ABOUTME: Core speculative decoding implementation with draft-verify pattern.
ABOUTME: Provides foundational algorithms for accelerated LLM inference.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Tuple, Optional, Dict, Any
from dataclasses import dataclass
import numpy as np
from abc import ABC, abstractmethod


@dataclass
class SpeculativeConfig:
    """Configuration for speculative decoding."""
    # Draft settings
    num_speculative_tokens: int = 5  # K: tokens to draft per step
    draft_temperature: float = 1.0

    # Target settings
    target_temperature: float = 1.0

    # Acceptance settings
    use_typical_acceptance: bool = False
    typical_mass: float = 0.9

    # Verification method
    verification_method: str = "standard"  # standard, tree, blockwise

    # Performance tuning
    max_batch_size: int = 64
    enable_kv_cache: bool = True


class SpeculativeDecoder:
    """
    Standard speculative decoding implementation.

    Algorithm:
    1. Draft model generates K tokens autoregressively
    2. Target model processes all K tokens in parallel
    3. Verify each token using rejection sampling
    4. Accept longest prefix of matching tokens
    5. Target model samples (accepted + 1)th token
    6. Repeat until done

    Reference: Leviathan et al., "Fast Inference from Transformers
               via Speculative Decoding" (2022)
    """

    def __init__(
        self,
        target_model: nn.Module,
        draft_model: nn.Module,
        config: SpeculativeConfig
    ):
        self.target = target_model
        self.draft = draft_model
        self.config = config

        # Ensure models are in eval mode
        self.target.eval()
        self.draft.eval()

        # KV caches
        self.target_kv_cache = None
        self.draft_kv_cache = None

    @torch.no_grad()
    def generate(
        self,
        input_ids: torch.Tensor,
        max_new_tokens: int = 100,
        stopping_criteria: Optional[Any] = None
    ) -> Tuple[torch.Tensor, Dict[str, Any]]:
        """
        Generate tokens using speculative decoding.

        Args:
            input_ids: Initial token IDs [batch_size, seq_len]
            max_new_tokens: Maximum tokens to generate
            stopping_criteria: Optional early stopping

        Returns:
            Generated token IDs and statistics
        """
        batch_size, seq_len = input_ids.shape
        device = input_ids.device

        # Initialize output
        generated = input_ids.clone()
        num_generated = 0

        # Statistics tracking
        stats = {
            'total_draft_tokens': 0,
            'accepted_tokens': 0,
            'target_forward_passes': 0,
            'draft_forward_passes': 0
        }

        # Initialize KV caches with prompt
        self._init_caches(input_ids)

        while num_generated < max_new_tokens:
            # Step 1: Draft K tokens
            draft_tokens, draft_probs = self._draft_tokens(
                generated,
                self.config.num_speculative_tokens
            )
            stats['draft_forward_passes'] += self.config.num_speculative_tokens

            # Step 2: Target model forward pass (parallel verification)
            target_probs = self._get_target_probs(
                generated,
                draft_tokens
            )
            stats['target_forward_passes'] += 1

            # Step 3: Verify and accept tokens
            accepted_tokens, next_token = self._verify_and_sample(
                draft_tokens,
                draft_probs,
                target_probs
            )

            stats['total_draft_tokens'] += len(draft_tokens[0])
            stats['accepted_tokens'] += len(accepted_tokens[0])

            # Step 4: Update generated sequence
            all_new_tokens = torch.cat([accepted_tokens, next_token], dim=1)
            generated = torch.cat([generated, all_new_tokens], dim=1)
            num_generated += all_new_tokens.shape[1]

            # Check stopping criteria
            if stopping_criteria is not None:
                if stopping_criteria(generated, None):
                    break

            # Update KV caches
            self._update_caches(all_new_tokens)

        # Compute acceptance rate
        stats['acceptance_rate'] = (
            stats['accepted_tokens'] / stats['total_draft_tokens']
            if stats['total_draft_tokens'] > 0 else 0
        )
        stats['speedup_factor'] = self._estimate_speedup(stats)

        return generated, stats

    def _draft_tokens(
        self,
        context: torch.Tensor,
        num_tokens: int
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Generate draft tokens using the small model.

        Returns:
            draft_tokens: [batch, num_tokens]
            draft_probs: [batch, num_tokens, vocab_size]
        """
        batch_size = context.shape[0]
        device = context.device

        draft_tokens = []
        draft_probs = []

        current_context = context

        for _ in range(num_tokens):
            # Forward pass through draft model
            outputs = self.draft(
                current_context,
                past_key_values=self.draft_kv_cache,
                use_cache=self.config.enable_kv_cache
            )

            logits = outputs.logits[:, -1, :]  # [batch, vocab]

            # Apply temperature
            if self.config.draft_temperature != 1.0:
                logits = logits / self.config.draft_temperature

            probs = F.softmax(logits, dim=-1)

            # Sample token
            token = torch.multinomial(probs, num_samples=1)  # [batch, 1]

            draft_tokens.append(token)
            draft_probs.append(probs.unsqueeze(1))

            # Update context for next iteration
            current_context = token

            # Update draft KV cache
            if self.config.enable_kv_cache:
                self.draft_kv_cache = outputs.past_key_values

        draft_tokens = torch.cat(draft_tokens, dim=1)  # [batch, num_tokens]
        draft_probs = torch.cat(draft_probs, dim=1)    # [batch, num_tokens, vocab]

        return draft_tokens, draft_probs

    def _get_target_probs(
        self,
        context: torch.Tensor,
        draft_tokens: torch.Tensor
    ) -> torch.Tensor:
        """
        Get target model probabilities for all positions in parallel.

        The key insight: target model processes context + all draft tokens
        in a single forward pass, giving us probabilities at each position.
        """
        # Concatenate context and draft tokens
        full_sequence = torch.cat([context, draft_tokens], dim=1)

        # Single forward pass through target
        outputs = self.target(
            full_sequence,
            past_key_values=self.target_kv_cache,
            use_cache=self.config.enable_kv_cache
        )

        # Get logits for positions where draft tokens are
        # We need probs at positions: [-K-1, -K, ..., -1]
        K = draft_tokens.shape[1]
        logits = outputs.logits[:, -(K+1):, :]  # [batch, K+1, vocab]

        if self.config.target_temperature != 1.0:
            logits = logits / self.config.target_temperature

        probs = F.softmax(logits, dim=-1)

        return probs

    def _verify_and_sample(
        self,
        draft_tokens: torch.Tensor,
        draft_probs: torch.Tensor,
        target_probs: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Verify draft tokens using rejection sampling.

        For each position i:
        - If target_prob[token_i] >= draft_prob[token_i]: accept
        - Else: accept with probability target_prob / draft_prob
        - If rejected: sample from adjusted distribution

        This guarantees output distribution matches target model exactly.
        """
        batch_size, K, vocab_size = draft_probs.shape
        device = draft_tokens.device

        accepted_tokens_list = []

        for b in range(batch_size):
            accepted = []

            for i in range(K):
                token = draft_tokens[b, i].item()
                p_draft = draft_probs[b, i, token].item()
                p_target = target_probs[b, i, token].item()

                # Rejection sampling
                if p_draft > 0:
                    acceptance_prob = min(1.0, p_target / p_draft)
                else:
                    acceptance_prob = 1.0 if p_target > 0 else 0.0

                if torch.rand(1).item() < acceptance_prob:
                    # Accept this token
                    accepted.append(token)
                else:
                    # Reject: sample from adjusted distribution
                    # p_adjusted = max(0, p_target - p_draft)
                    adjusted = torch.clamp(
                        target_probs[b, i] - draft_probs[b, i],
                        min=0
                    )
                    adjusted = adjusted / (adjusted.sum() + 1e-10)

                    new_token = torch.multinomial(adjusted, num_samples=1)
                    accepted.append(new_token.item())
                    break  # Stop at first rejection

            accepted_tokens_list.append(
                torch.tensor(accepted, device=device)
            )

        # Pad accepted tokens to same length
        max_accepted = max(len(a) for a in accepted_tokens_list)
        accepted_padded = torch.zeros(
            batch_size, max_accepted,
            dtype=torch.long, device=device
        )
        for b, acc in enumerate(accepted_tokens_list):
            accepted_padded[b, :len(acc)] = acc

        # Sample next token from target at position after last accepted
        # This is the (accepted + 1)th token
        next_token_probs = target_probs[:, max_accepted, :]
        next_token = torch.multinomial(next_token_probs, num_samples=1)

        return accepted_padded, next_token

    def _init_caches(self, input_ids: torch.Tensor):
        """Initialize KV caches with prompt."""
        if not self.config.enable_kv_cache:
            return

        # Process prompt through both models
        with torch.no_grad():
            target_out = self.target(input_ids, use_cache=True)
            self.target_kv_cache = target_out.past_key_values

            draft_out = self.draft(input_ids, use_cache=True)
            self.draft_kv_cache = draft_out.past_key_values

    def _update_caches(self, new_tokens: torch.Tensor):
        """Update KV caches with newly accepted tokens."""
        # In practice, this requires careful cache management
        # to avoid recomputing the entire sequence
        pass

    def _estimate_speedup(self, stats: Dict) -> float:
        """Estimate speedup factor from statistics."""
        if stats['target_forward_passes'] == 0:
            return 1.0

        # Average tokens per target forward pass
        tokens_per_pass = (
            stats['accepted_tokens'] + stats['target_forward_passes']
        ) / stats['target_forward_passes']

        # Assume draft model is ~10x faster than target
        draft_cost_ratio = 0.1
        effective_cost = 1.0 + draft_cost_ratio * self.config.num_speculative_tokens

        return tokens_per_pass / effective_cost
```

### Mathematical Foundation

The key insight is that rejection sampling ensures the output distribution exactly matches the target model:

```python
class RejectionSamplingAnalysis:
    """
    Analysis of the rejection sampling mechanism.

    The acceptance probability p(accept) = sum_x min(p_target(x), p_draft(x))
    For optimal efficiency, draft distribution should closely match target.
    """

    @staticmethod
    def compute_acceptance_rate(
        target_probs: torch.Tensor,
        draft_probs: torch.Tensor
    ) -> float:
        """
        Compute theoretical acceptance rate.

        The expected acceptance rate is:
        alpha = sum_x min(p_target(x), p_draft(x))

        Higher alpha = more tokens accepted = more speedup.
        """
        min_probs = torch.min(target_probs, draft_probs)
        return min_probs.sum(dim=-1).mean().item()

    @staticmethod
    def compute_expected_speedup(
        acceptance_rate: float,
        K: int,
        draft_cost_ratio: float = 0.1
    ) -> float:
        """
        Compute expected speedup from acceptance rate.

        Expected tokens per speculation round:
        E[tokens] = sum_{i=1}^{K} alpha^(i-1) + (1-alpha^K)

        The +1 accounts for the target-sampled token at the end.
        """
        # Expected accepted tokens: geometric series
        if acceptance_rate >= 1.0:
            expected_accepted = K
        else:
            expected_accepted = (
                (1 - acceptance_rate ** K) / (1 - acceptance_rate)
            )

        # Total tokens = accepted + 1 (target samples one more)
        expected_tokens = expected_accepted + 1

        # Cost = 1 target pass + K draft passes
        cost = 1.0 + K * draft_cost_ratio

        return expected_tokens / cost

    @staticmethod
    def optimal_K(
        acceptance_rate: float,
        draft_cost_ratio: float = 0.1
    ) -> int:
        """
        Find optimal number of speculative tokens K.

        Balancing: more K = more potential accepted tokens
                   but also more wasted draft computation if rejected
        """
        best_K = 1
        best_speedup = 0

        for K in range(1, 20):
            speedup = RejectionSamplingAnalysis.compute_expected_speedup(
                acceptance_rate, K, draft_cost_ratio
            )
            if speedup > best_speedup:
                best_speedup = speedup
                best_K = K
            elif speedup < best_speedup * 0.95:
                # Diminishing returns
                break

        return best_K
```

---

## 6.4.2 Draft Model Selection

### Criteria and Trade-offs

```python
"""
ABOUTME: Draft model selection strategies for speculative decoding.
ABOUTME: Covers model pairing, evaluation, and optimization.
"""

@dataclass
class DraftModelProfile:
    """Profile of a draft model's characteristics."""
    name: str
    parameters: int
    vocab_size: int
    latency_ms: float           # Per-token latency
    memory_gb: float            # GPU memory usage
    acceptance_rate: float      # Measured vs target
    speedup_achieved: float     # Actual measured speedup


class DraftModelSelector:
    """
    Select optimal draft model for a given target.

    Key criteria:
    1. Same vocabulary (required for verification)
    2. Fast inference (<<< target latency)
    3. High acceptance rate (close distribution to target)
    4. Fits in memory alongside target
    """

    # Common draft-target pairings
    KNOWN_PAIRINGS = {
        'llama-70b': [
            DraftModelProfile(
                name='llama-7b',
                parameters=7_000_000_000,
                vocab_size=32000,
                latency_ms=5.0,
                memory_gb=14.0,
                acceptance_rate=0.75,
                speedup_achieved=2.1
            ),
            DraftModelProfile(
                name='tinyllama-1.1b',
                parameters=1_100_000_000,
                vocab_size=32000,
                latency_ms=1.5,
                memory_gb=2.2,
                acceptance_rate=0.55,
                speedup_achieved=1.8
            )
        ],
        'llama-13b': [
            DraftModelProfile(
                name='llama-7b',
                parameters=7_000_000_000,
                vocab_size=32000,
                latency_ms=5.0,
                memory_gb=14.0,
                acceptance_rate=0.85,
                speedup_achieved=1.7
            )
        ],
        'codellama-34b': [
            DraftModelProfile(
                name='codellama-7b',
                parameters=7_000_000_000,
                vocab_size=32016,
                latency_ms=4.5,
                memory_gb=14.0,
                acceptance_rate=0.80,
                speedup_achieved=2.3
            )
        ]
    }

    def __init__(self, target_model_name: str):
        self.target_name = target_model_name

    def get_recommended_drafts(self) -> List[DraftModelProfile]:
        """Get recommended draft models for target."""
        return self.KNOWN_PAIRINGS.get(self.target_name, [])

    def evaluate_candidate(
        self,
        draft_model: nn.Module,
        target_model: nn.Module,
        eval_prompts: List[str],
        tokenizer: Any
    ) -> DraftModelProfile:
        """
        Evaluate a candidate draft model.

        Measures:
        - Latency ratio (draft vs target)
        - Acceptance rate
        - Actual speedup
        """
        import time

        # Measure latencies
        draft_latencies = []
        target_latencies = []
        acceptance_rates = []

        for prompt in eval_prompts[:10]:  # Sample evaluation
            input_ids = tokenizer(prompt, return_tensors='pt').input_ids.cuda()

            # Draft model latency
            torch.cuda.synchronize()
            start = time.perf_counter()
            with torch.no_grad():
                draft_out = draft_model(input_ids)
            torch.cuda.synchronize()
            draft_latencies.append(time.perf_counter() - start)

            # Target model latency
            torch.cuda.synchronize()
            start = time.perf_counter()
            with torch.no_grad():
                target_out = target_model(input_ids)
            torch.cuda.synchronize()
            target_latencies.append(time.perf_counter() - start)

            # Compute acceptance rate at next position
            draft_probs = F.softmax(draft_out.logits[:, -1], dim=-1)
            target_probs = F.softmax(target_out.logits[:, -1], dim=-1)

            acceptance = RejectionSamplingAnalysis.compute_acceptance_rate(
                target_probs, draft_probs
            )
            acceptance_rates.append(acceptance)

        avg_draft_latency = np.mean(draft_latencies) * 1000  # ms
        avg_target_latency = np.mean(target_latencies) * 1000
        avg_acceptance = np.mean(acceptance_rates)

        # Estimate speedup
        cost_ratio = avg_draft_latency / avg_target_latency
        expected_speedup = RejectionSamplingAnalysis.compute_expected_speedup(
            avg_acceptance, K=5, draft_cost_ratio=cost_ratio
        )

        return DraftModelProfile(
            name="candidate",
            parameters=sum(p.numel() for p in draft_model.parameters()),
            vocab_size=draft_out.logits.shape[-1],
            latency_ms=avg_draft_latency,
            memory_gb=torch.cuda.max_memory_allocated() / 1e9,
            acceptance_rate=avg_acceptance,
            speedup_achieved=expected_speedup
        )

    def select_best_draft(
        self,
        candidates: List[nn.Module],
        target_model: nn.Module,
        memory_budget_gb: float,
        eval_prompts: List[str],
        tokenizer: Any
    ) -> Tuple[nn.Module, DraftModelProfile]:
        """
        Select best draft model within memory budget.
        """
        best_draft = None
        best_profile = None
        best_speedup = 0

        for candidate in candidates:
            profile = self.evaluate_candidate(
                candidate, target_model, eval_prompts, tokenizer
            )

            if profile.memory_gb <= memory_budget_gb:
                if profile.speedup_achieved > best_speedup:
                    best_speedup = profile.speedup_achieved
                    best_draft = candidate
                    best_profile = profile

        return best_draft, best_profile


class SelfDraftModel(nn.Module):
    """
    Use early layers of target model as draft.

    Instead of separate draft model, use first N layers
    of the target model for drafting. Reduces memory overhead
    but may have lower acceptance rate.
    """

    def __init__(
        self,
        target_model: nn.Module,
        num_draft_layers: int
    ):
        super().__init__()
        self.target = target_model
        self.num_draft_layers = num_draft_layers

        # Create draft model from early layers
        self._build_draft_model()

    def _build_draft_model(self):
        """Extract early layers for draft model."""
        # This depends on model architecture
        # Example for LLaMA-style models:
        self.embed_tokens = self.target.model.embed_tokens
        self.draft_layers = nn.ModuleList([
            self.target.model.layers[i]
            for i in range(self.num_draft_layers)
        ])
        self.norm = self.target.model.norm
        self.lm_head = self.target.lm_head

    def forward(
        self,
        input_ids: torch.Tensor,
        **kwargs
    ):
        """Forward pass through draft layers only."""
        hidden_states = self.embed_tokens(input_ids)

        for layer in self.draft_layers:
            hidden_states = layer(hidden_states)[0]

        hidden_states = self.norm(hidden_states)
        logits = self.lm_head(hidden_states)

        return type('Output', (), {'logits': logits})()
```

---

## 6.4.3 Tree-Based Speculative Decoding

### Medusa and EAGLE Approaches

```python
"""
ABOUTME: Tree-based speculative decoding with Medusa and EAGLE.
ABOUTME: Generates multiple candidate continuations in parallel.
"""

class MedusaDecoder(nn.Module):
    """
    Medusa: Multiple speculation heads for parallel token prediction.

    Instead of single autoregressive draft, Medusa adds extra "heads"
    to predict multiple future tokens simultaneously.

    Each head predicts token at position +1, +2, +3, etc. from current.

    Reference: Cai et al., "Medusa: Simple LLM Inference Acceleration
               Framework with Multiple Decoding Heads" (2024)
    """

    def __init__(
        self,
        base_model: nn.Module,
        num_heads: int = 4,
        hidden_size: int = 4096,
        vocab_size: int = 32000
    ):
        super().__init__()
        self.base_model = base_model
        self.num_heads = num_heads

        # Freeze base model
        for param in self.base_model.parameters():
            param.requires_grad = False

        # Add Medusa heads
        # Each head predicts token at different future position
        self.medusa_heads = nn.ModuleList([
            nn.Sequential(
                nn.Linear(hidden_size, hidden_size),
                nn.SiLU(),
                nn.Linear(hidden_size, vocab_size)
            )
            for _ in range(num_heads)
        ])

    def forward(
        self,
        input_ids: torch.Tensor,
        **kwargs
    ) -> Dict[str, torch.Tensor]:
        """
        Forward pass with multiple prediction heads.

        Returns:
            base_logits: Standard next-token logits
            medusa_logits: List of future position logits
        """
        # Get hidden states from base model
        outputs = self.base_model(
            input_ids,
            output_hidden_states=True,
            **kwargs
        )

        last_hidden = outputs.hidden_states[-1]  # [batch, seq, hidden]
        base_logits = outputs.logits  # [batch, seq, vocab]

        # Get predictions from each Medusa head
        medusa_logits = [
            head(last_hidden) for head in self.medusa_heads
        ]  # List of [batch, seq, vocab]

        return {
            'base_logits': base_logits,
            'medusa_logits': medusa_logits,
            'hidden_states': last_hidden
        }

    def train_heads(
        self,
        train_dataloader: Any,
        optimizer: torch.optim.Optimizer,
        num_epochs: int = 1
    ):
        """
        Train Medusa heads to predict future tokens.

        Training objective: Each head learns to predict
        the token at its designated future position.
        """
        self.train()

        for epoch in range(num_epochs):
            total_loss = 0

            for batch in train_dataloader:
                input_ids = batch['input_ids'].cuda()
                labels = input_ids.clone()

                optimizer.zero_grad()

                outputs = self.forward(input_ids)

                # Loss for each head
                loss = 0
                for i, head_logits in enumerate(outputs['medusa_logits']):
                    # Head i predicts token at position +i+1
                    shift_logits = head_logits[:, :-(i+2), :]
                    shift_labels = labels[:, (i+2):]

                    head_loss = F.cross_entropy(
                        shift_logits.reshape(-1, shift_logits.size(-1)),
                        shift_labels.reshape(-1),
                        ignore_index=-100
                    )
                    loss += head_loss

                loss.backward()
                optimizer.step()

                total_loss += loss.item()

            print(f"Epoch {epoch}: Loss = {total_loss / len(train_dataloader):.4f}")


class MedusaTreeDecoder:
    """
    Tree-structured verification for Medusa.

    Instead of single linear sequence, build tree of possible
    continuations and verify entire tree in one forward pass.
    """

    def __init__(
        self,
        medusa_model: MedusaDecoder,
        tree_config: Dict[str, Any] = None
    ):
        self.model = medusa_model

        # Default tree configuration
        # Specifies branching factor at each depth
        self.tree_config = tree_config or {
            'depth': 4,
            'branches': [3, 2, 2, 1],  # branches at each level
            'top_k': 10  # top-k candidates per head
        }

    @torch.no_grad()
    def generate_tree(
        self,
        context: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Generate tree of candidate sequences.

        Returns:
            candidates: [num_candidates, seq_len] token sequences
            candidate_probs: [num_candidates] cumulative probabilities
        """
        outputs = self.model(context)

        # Get top-k from base model (position 0)
        base_probs = F.softmax(outputs['base_logits'][:, -1], dim=-1)
        top_k = self.tree_config['top_k']

        base_topk = base_probs.topk(min(top_k, self.tree_config['branches'][0]))
        current_tokens = base_topk.indices  # [batch, k]
        current_probs = base_topk.values    # [batch, k]

        candidates = [current_tokens]
        candidate_probs = [current_probs]

        # Expand tree using Medusa heads
        for depth, (head_logits, branch_factor) in enumerate(
            zip(outputs['medusa_logits'], self.tree_config['branches'][1:])
        ):
            head_probs = F.softmax(head_logits[:, -1], dim=-1)
            head_topk = head_probs.topk(branch_factor)

            # Expand each current candidate
            new_candidates = []
            new_probs = []

            for c_idx in range(current_tokens.shape[1]):
                for b_idx in range(branch_factor):
                    token = head_topk.indices[:, b_idx:b_idx+1]
                    prob = head_topk.values[:, b_idx]

                    new_candidates.append(
                        torch.cat([candidates[-1][:, c_idx:c_idx+1], token], dim=1)
                    )
                    new_probs.append(candidate_probs[-1][:, c_idx] * prob)

            candidates.append(torch.cat(new_candidates, dim=1))
            candidate_probs.append(torch.stack(new_probs, dim=1))
            current_tokens = candidates[-1]

        # Flatten tree to list of candidates
        final_candidates = candidates[-1]
        final_probs = candidate_probs[-1]

        return final_candidates, final_probs

    @torch.no_grad()
    def verify_tree(
        self,
        context: torch.Tensor,
        candidates: torch.Tensor
    ) -> Tuple[torch.Tensor, int]:
        """
        Verify candidate sequences with target model.

        Uses tree attention to verify all candidates in one pass.
        """
        # Concatenate context with all candidates
        batch_size, num_candidates, candidate_len = candidates.shape

        # Process each candidate (can be batched more efficiently)
        best_candidate = None
        best_length = 0

        for c_idx in range(num_candidates):
            candidate = candidates[:, c_idx, :]
            full_seq = torch.cat([context, candidate], dim=1)

            # Verify with base model
            outputs = self.model.base_model(full_seq)
            target_probs = F.softmax(outputs.logits, dim=-1)

            # Check how many tokens match
            accepted = 0
            for pos in range(candidate_len):
                ctx_pos = context.shape[1] + pos - 1
                token = candidate[:, pos].item()

                # Simple acceptance check (can use rejection sampling)
                top_token = target_probs[:, ctx_pos, :].argmax().item()
                if token == top_token:
                    accepted += 1
                else:
                    break

            if accepted > best_length:
                best_length = accepted
                best_candidate = candidate[:, :accepted]

        return best_candidate, best_length


class EAGLEDecoder:
    """
    EAGLE: Efficient Autoregressive Language model through
    Gradual Layer Extrapolation.

    Uses lightweight autoregressive head attached to target's
    internal layers for speculation. No separate draft model needed.

    Reference: Li et al., "EAGLE: Speculative Sampling Requires
               Rethinking Feature Uncertainty" (2024)
    """

    def __init__(
        self,
        target_model: nn.Module,
        feature_layer: int = -2,  # Which layer to tap
        head_hidden_size: int = 512
    ):
        self.target = target_model
        self.feature_layer = feature_layer

        # Get target hidden size
        target_hidden = target_model.config.hidden_size
        vocab_size = target_model.config.vocab_size

        # Lightweight prediction head
        self.eagle_head = nn.Sequential(
            nn.Linear(target_hidden, head_hidden_size),
            nn.SiLU(),
            nn.Linear(head_hidden_size, head_hidden_size),
            nn.SiLU(),
            nn.Linear(head_hidden_size, vocab_size)
        ).cuda()

    @torch.no_grad()
    def speculate(
        self,
        context: torch.Tensor,
        num_tokens: int = 5
    ) -> torch.Tensor:
        """
        Generate speculative tokens using EAGLE head.
        """
        # Get features from target model's intermediate layer
        outputs = self.target(
            context,
            output_hidden_states=True
        )

        features = outputs.hidden_states[self.feature_layer][:, -1:]

        # Generate tokens autoregressively with EAGLE head
        speculative_tokens = []

        for _ in range(num_tokens):
            logits = self.eagle_head(features)
            probs = F.softmax(logits[:, -1], dim=-1)
            token = torch.multinomial(probs, num_samples=1)

            speculative_tokens.append(token)

            # Get next features (simplified - full impl needs caching)
            # In practice, EAGLE uses feature interpolation
            features = self._interpolate_features(features, token)

        return torch.cat(speculative_tokens, dim=1)

    def _interpolate_features(
        self,
        current_features: torch.Tensor,
        new_token: torch.Tensor
    ) -> torch.Tensor:
        """
        Interpolate features for next position.

        EAGLE's key insight: intermediate features are more predictable
        than output tokens, enabling efficient speculation.
        """
        # Simplified version - real EAGLE uses learned interpolation
        # This would require the target model's embedding
        return current_features  # Placeholder
```

---

## 6.4.4 Production Implementation

### vLLM Integration

```python
"""
ABOUTME: Production-ready speculative decoding with vLLM and optimizations.
ABOUTME: Covers batching, memory management, and deployment patterns.
"""

class vLLMSpeculativeConfig:
    """
    Configuration for vLLM speculative decoding.

    vLLM supports speculative decoding out of the box.
    """

    @staticmethod
    def get_config(
        target_model: str,
        draft_model: str,
        num_speculative_tokens: int = 5
    ) -> Dict[str, Any]:
        """
        Get vLLM configuration for speculative decoding.
        """
        return {
            'model': target_model,
            'speculative_model': draft_model,
            'num_speculative_tokens': num_speculative_tokens,
            'speculative_max_model_len': 2048,
            'use_v2_block_manager': True,
            'gpu_memory_utilization': 0.9,
            # Disable for non-speculative fallback if OOM
            'speculative_disable_by_batch_size': 8
        }


class BatchedSpeculativeDecoder:
    """
    Batched speculative decoding for production throughput.

    Handles multiple concurrent requests with shared KV cache.
    """

    def __init__(
        self,
        target_model: nn.Module,
        draft_model: nn.Module,
        config: SpeculativeConfig,
        max_batch_size: int = 32
    ):
        self.target = target_model
        self.draft = draft_model
        self.config = config
        self.max_batch_size = max_batch_size

        # Request queue
        self.pending_requests = []
        self.active_batches = {}

    def add_request(
        self,
        request_id: str,
        input_ids: torch.Tensor,
        max_new_tokens: int
    ):
        """Add generation request to queue."""
        self.pending_requests.append({
            'id': request_id,
            'input_ids': input_ids,
            'max_new_tokens': max_new_tokens,
            'generated': 0,
            'output_ids': input_ids.clone()
        })

    def step(self) -> Dict[str, torch.Tensor]:
        """
        Execute one speculation-verification step for all requests.

        Returns completed requests.
        """
        if not self.pending_requests:
            return {}

        # Batch requests
        batch_size = min(len(self.pending_requests), self.max_batch_size)
        batch = self.pending_requests[:batch_size]

        # Pad sequences to same length
        max_len = max(r['output_ids'].shape[1] for r in batch)
        padded_inputs = []

        for req in batch:
            seq = req['output_ids']
            if seq.shape[1] < max_len:
                pad = torch.zeros(
                    1, max_len - seq.shape[1],
                    dtype=seq.dtype, device=seq.device
                )
                seq = torch.cat([seq, pad], dim=1)
            padded_inputs.append(seq)

        batched_input = torch.cat(padded_inputs, dim=0)

        # Speculative generation step
        draft_tokens = self._batch_draft(batched_input)
        accepted_tokens = self._batch_verify(batched_input, draft_tokens)

        # Update requests
        completed = {}
        remaining = []

        for i, req in enumerate(batch):
            accepted = accepted_tokens[i]
            req['output_ids'] = torch.cat([
                req['output_ids'],
                accepted.unsqueeze(0)
            ], dim=1)
            req['generated'] += accepted.shape[0]

            if req['generated'] >= req['max_new_tokens']:
                completed[req['id']] = req['output_ids']
            else:
                remaining.append(req)

        self.pending_requests = remaining + self.pending_requests[batch_size:]

        return completed

    def _batch_draft(
        self,
        batched_input: torch.Tensor
    ) -> torch.Tensor:
        """Generate draft tokens for batch."""
        draft_tokens = []

        with torch.no_grad():
            current = batched_input

            for _ in range(self.config.num_speculative_tokens):
                outputs = self.draft(current)
                logits = outputs.logits[:, -1, :]
                probs = F.softmax(logits / self.config.draft_temperature, dim=-1)
                tokens = torch.multinomial(probs, num_samples=1)
                draft_tokens.append(tokens)
                current = tokens

        return torch.cat(draft_tokens, dim=1)

    def _batch_verify(
        self,
        batched_input: torch.Tensor,
        draft_tokens: torch.Tensor
    ) -> List[torch.Tensor]:
        """Verify draft tokens for batch."""
        full_seq = torch.cat([batched_input, draft_tokens], dim=1)

        with torch.no_grad():
            outputs = self.target(full_seq)

        # Simplified batch verification
        K = draft_tokens.shape[1]
        target_probs = F.softmax(outputs.logits[:, -K-1:, :], dim=-1)

        accepted_list = []

        for b in range(batched_input.shape[0]):
            accepted = []
            for i in range(K):
                token = draft_tokens[b, i].item()
                if target_probs[b, i, token] > 0.1:  # Simplified
                    accepted.append(token)
                else:
                    break

            # Add one more token from target
            if len(accepted) < K:
                next_token = torch.multinomial(
                    target_probs[b, len(accepted)], num_samples=1
                )
                accepted.append(next_token.item())

            accepted_list.append(torch.tensor(accepted, device=draft_tokens.device))

        return accepted_list


class SpeculativeDecodingBenchmark:
    """
    Benchmark speculative decoding performance.
    """

    def __init__(
        self,
        target_model: nn.Module,
        draft_model: nn.Module,
        tokenizer: Any
    ):
        self.target = target_model
        self.draft = draft_model
        self.tokenizer = tokenizer

    def benchmark(
        self,
        prompts: List[str],
        max_new_tokens: int = 100,
        num_speculative_tokens_range: List[int] = [1, 3, 5, 7, 10]
    ) -> Dict[str, Any]:
        """
        Benchmark different speculation depths.
        """
        import time

        results = {
            'baseline': self._benchmark_baseline(prompts, max_new_tokens),
            'speculative': {}
        }

        for K in num_speculative_tokens_range:
            config = SpeculativeConfig(num_speculative_tokens=K)
            decoder = SpeculativeDecoder(self.target, self.draft, config)

            latencies = []
            acceptance_rates = []

            for prompt in prompts:
                input_ids = self.tokenizer(prompt, return_tensors='pt').input_ids.cuda()

                torch.cuda.synchronize()
                start = time.perf_counter()

                output, stats = decoder.generate(input_ids, max_new_tokens)

                torch.cuda.synchronize()
                latency = time.perf_counter() - start

                latencies.append(latency)
                acceptance_rates.append(stats['acceptance_rate'])

            results['speculative'][K] = {
                'avg_latency': np.mean(latencies),
                'avg_acceptance_rate': np.mean(acceptance_rates),
                'speedup': results['baseline']['avg_latency'] / np.mean(latencies),
                'tokens_per_second': max_new_tokens / np.mean(latencies)
            }

        return results

    def _benchmark_baseline(
        self,
        prompts: List[str],
        max_new_tokens: int
    ) -> Dict[str, float]:
        """Benchmark standard autoregressive decoding."""
        import time

        latencies = []

        for prompt in prompts:
            input_ids = self.tokenizer(prompt, return_tensors='pt').input_ids.cuda()

            torch.cuda.synchronize()
            start = time.perf_counter()

            with torch.no_grad():
                output = self.target.generate(
                    input_ids,
                    max_new_tokens=max_new_tokens,
                    do_sample=False
                )

            torch.cuda.synchronize()
            latencies.append(time.perf_counter() - start)

        return {
            'avg_latency': np.mean(latencies),
            'tokens_per_second': max_new_tokens / np.mean(latencies)
        }
```

---

## 6.4.5 Advanced Techniques

### Prompt Lookup and N-gram Decoding

```python
"""
ABOUTME: Advanced speculative techniques without separate draft models.
ABOUTME: Includes prompt lookup, n-gram, and lookahead decoding.
"""

class PromptLookupDecoder:
    """
    Prompt Lookup Decoding: Use input prompt for speculation.

    For tasks like summarization, translation, or code completion,
    output often contains substrings from input. Use n-gram matching
    on prompt to speculate tokens.

    Can achieve 2-3x speedup without any draft model.
    """

    def __init__(
        self,
        target_model: nn.Module,
        tokenizer: Any,
        ngram_size: int = 3,
        num_speculative_tokens: int = 10
    ):
        self.target = target_model
        self.tokenizer = tokenizer
        self.ngram_size = ngram_size
        self.num_speculative = num_speculative_tokens

    @torch.no_grad()
    def generate(
        self,
        input_ids: torch.Tensor,
        max_new_tokens: int = 100
    ) -> torch.Tensor:
        """Generate with prompt lookup speculation."""
        generated = input_ids.clone()
        prompt_tokens = input_ids[0].tolist()  # Original prompt

        # Build n-gram index from prompt
        ngram_index = self._build_ngram_index(prompt_tokens)

        num_generated = 0

        while num_generated < max_new_tokens:
            # Get recent tokens for lookup
            recent = generated[0, -self.ngram_size:].tolist()
            recent_key = tuple(recent)

            # Look up potential continuation in prompt
            if recent_key in ngram_index:
                # Speculate based on prompt
                continuation_start = ngram_index[recent_key]
                speculation = prompt_tokens[
                    continuation_start:continuation_start + self.num_speculative
                ]
            else:
                # No match, just generate one token
                speculation = []

            if speculation:
                # Verify speculation
                spec_tensor = torch.tensor(
                    [speculation], device=generated.device
                )
                full_seq = torch.cat([generated, spec_tensor], dim=1)

                outputs = self.target(full_seq)
                logits = outputs.logits[0]

                # Check acceptance
                accepted = []
                for i, token in enumerate(speculation):
                    pos = generated.shape[1] + i - 1
                    predicted = logits[pos].argmax().item()

                    if predicted == token:
                        accepted.append(token)
                    else:
                        # Sample from actual distribution
                        probs = F.softmax(logits[pos], dim=-1)
                        new_token = torch.multinomial(probs, num_samples=1)
                        accepted.append(new_token.item())
                        break

                new_tokens = torch.tensor(
                    [accepted], device=generated.device
                )
            else:
                # Standard generation
                outputs = self.target(generated)
                logits = outputs.logits[0, -1]
                probs = F.softmax(logits, dim=-1)
                new_tokens = torch.multinomial(probs, num_samples=1).unsqueeze(0)

            generated = torch.cat([generated, new_tokens], dim=1)
            num_generated += new_tokens.shape[1]

            # Check for EOS
            if new_tokens[0, -1].item() == self.tokenizer.eos_token_id:
                break

        return generated

    def _build_ngram_index(
        self,
        tokens: List[int]
    ) -> Dict[tuple, int]:
        """Build n-gram to position index."""
        index = {}

        for i in range(len(tokens) - self.ngram_size):
            ngram = tuple(tokens[i:i + self.ngram_size])
            if ngram not in index:
                index[ngram] = i + self.ngram_size

        return index


class LookaheadDecoder:
    """
    Lookahead Decoding: Parallel n-gram generation.

    Generate multiple n-grams in parallel, then verify which
    one continues the sequence correctly.
    """

    def __init__(
        self,
        model: nn.Module,
        window_size: int = 4,
        ngram_size: int = 3
    ):
        self.model = model
        self.window_size = window_size
        self.ngram_size = ngram_size

    @torch.no_grad()
    def generate(
        self,
        input_ids: torch.Tensor,
        max_new_tokens: int = 100
    ) -> torch.Tensor:
        """
        Generate with lookahead decoding.

        Maintains a "lookahead branch" that generates ahead,
        and a "verification branch" that confirms tokens.
        """
        generated = input_ids.clone()
        lookahead_cache = []  # Cache of speculated n-grams

        num_generated = 0

        while num_generated < max_new_tokens:
            # Generate lookahead n-grams
            if len(lookahead_cache) < self.window_size:
                # Parallel generation of multiple branches
                branches = self._generate_branches(generated)
                lookahead_cache.extend(branches)

            # Verify first n-gram in cache
            if lookahead_cache:
                candidate = lookahead_cache.pop(0)
                verified, new_tokens = self._verify_ngram(generated, candidate)

                if verified:
                    generated = torch.cat([generated, new_tokens], dim=1)
                    num_generated += new_tokens.shape[1]
                else:
                    # Verification failed, clear cache and generate normally
                    lookahead_cache.clear()
                    outputs = self.model(generated)
                    logits = outputs.logits[0, -1]
                    new_token = torch.multinomial(
                        F.softmax(logits, dim=-1),
                        num_samples=1
                    ).unsqueeze(0)
                    generated = torch.cat([generated, new_token], dim=1)
                    num_generated += 1
            else:
                # No cache, normal generation
                outputs = self.model(generated)
                logits = outputs.logits[0, -1]
                new_token = torch.multinomial(
                    F.softmax(logits, dim=-1),
                    num_samples=1
                ).unsqueeze(0)
                generated = torch.cat([generated, new_token], dim=1)
                num_generated += 1

        return generated

    def _generate_branches(
        self,
        context: torch.Tensor
    ) -> List[torch.Tensor]:
        """Generate multiple n-gram branches."""
        branches = []

        # Get top-k tokens at current position
        outputs = self.model(context)
        logits = outputs.logits[0, -1]
        probs = F.softmax(logits, dim=-1)
        top_k = probs.topk(self.window_size)

        for token in top_k.indices:
            # Generate n-gram starting with this token
            branch = [token.item()]
            current = torch.cat([
                context,
                token.view(1, 1)
            ], dim=1)

            for _ in range(self.ngram_size - 1):
                out = self.model(current)
                next_token = out.logits[0, -1].argmax()
                branch.append(next_token.item())
                current = torch.cat([
                    current,
                    next_token.view(1, 1)
                ], dim=1)

            branches.append(torch.tensor(branch, device=context.device))

        return branches

    def _verify_ngram(
        self,
        context: torch.Tensor,
        ngram: torch.Tensor
    ) -> Tuple[bool, torch.Tensor]:
        """Verify n-gram against model."""
        full_seq = torch.cat([context, ngram.unsqueeze(0)], dim=1)
        outputs = self.model(full_seq)

        # Check if model would generate same tokens
        verified_tokens = []

        for i in range(len(ngram)):
            pos = context.shape[1] + i - 1
            predicted = outputs.logits[0, pos].argmax().item()
            actual = ngram[i].item()

            if predicted == actual:
                verified_tokens.append(actual)
            else:
                break

        if len(verified_tokens) == len(ngram):
            return True, ngram.unsqueeze(0)
        elif verified_tokens:
            return True, torch.tensor(
                [verified_tokens], device=context.device
            )
        else:
            return False, None
```

---

## Appendix A: Troubleshooting Guide

### Common Issues

| Issue | Cause | Solution |
|-------|-------|----------|
| Low acceptance rate | Draft too different from target | Use distilled or same-family draft |
| No speedup | Draft model too slow | Use smaller draft (1B or less) |
| Memory OOM | Both models don't fit | Use self-draft or quantized models |
| Quality degradation | Implementation bug | Verify rejection sampling is correct |
| Batched slowdown | KV cache overhead | Tune batch size, disable for large batches |
| Variable speedup | Content-dependent | Normal; code/repetitive text benefits more |

### Debugging Checklist

```python
class SpeculativeDebugger:
    """Debug speculative decoding issues."""

    @staticmethod
    def verify_output_equivalence(
        target_model: nn.Module,
        spec_decoder: SpeculativeDecoder,
        test_prompts: List[str],
        tokenizer: Any,
        num_samples: int = 100
    ) -> Dict[str, float]:
        """
        Verify speculative output matches target distribution.

        If rejection sampling is correct, outputs should be
        statistically indistinguishable.
        """
        target_samples = []
        spec_samples = []

        for prompt in test_prompts[:num_samples]:
            input_ids = tokenizer(prompt, return_tensors='pt').input_ids.cuda()

            # Target generation
            with torch.no_grad():
                target_out = target_model.generate(
                    input_ids, max_new_tokens=20, do_sample=True
                )
            target_samples.append(
                tokenizer.decode(target_out[0], skip_special_tokens=True)
            )

            # Speculative generation
            spec_out, _ = spec_decoder.generate(input_ids, max_new_tokens=20)
            spec_samples.append(
                tokenizer.decode(spec_out[0], skip_special_tokens=True)
            )

        # Compare distributions (simplified)
        # In practice, use proper statistical tests
        return {
            'num_samples': num_samples,
            'samples_match': sum(t == s for t, s in zip(target_samples, spec_samples))
        }

    @staticmethod
    def diagnose_low_acceptance(
        target_model: nn.Module,
        draft_model: nn.Module,
        test_input: torch.Tensor
    ) -> Dict[str, Any]:
        """Diagnose why acceptance rate is low."""
        with torch.no_grad():
            target_out = target_model(test_input)
            draft_out = draft_model(test_input)

        target_probs = F.softmax(target_out.logits[:, -1], dim=-1)
        draft_probs = F.softmax(draft_out.logits[:, -1], dim=-1)

        # Compute various divergence measures
        kl_div = F.kl_div(
            draft_probs.log(),
            target_probs,
            reduction='batchmean'
        ).item()

        # Top-1 agreement
        top1_agreement = (
            target_probs.argmax() == draft_probs.argmax()
        ).float().item()

        # Top-5 overlap
        target_top5 = set(target_probs.topk(5).indices[0].tolist())
        draft_top5 = set(draft_probs.topk(5).indices[0].tolist())
        top5_overlap = len(target_top5 & draft_top5) / 5

        # Theoretical acceptance rate
        acceptance = RejectionSamplingAnalysis.compute_acceptance_rate(
            target_probs, draft_probs
        )

        return {
            'kl_divergence': kl_div,
            'top1_agreement': top1_agreement,
            'top5_overlap': top5_overlap,
            'theoretical_acceptance': acceptance,
            'diagnosis': (
                "Good" if acceptance > 0.7 else
                "Moderate" if acceptance > 0.5 else
                "Poor - consider different draft model"
            )
        }
```

---

## Appendix B: Glossary

| Term | Definition |
|------|------------|
| **Draft Model** | Small, fast model that proposes speculative tokens |
| **Target Model** | Large model that verifies and produces final output |
| **Speculative Tokens** | Tokens proposed by draft model before verification |
| **Acceptance Rate** | Fraction of draft tokens accepted by target |
| **Rejection Sampling** | Statistical technique ensuring output matches target |
| **Medusa** | Multi-head speculation without separate draft model |
| **EAGLE** | Feature-based speculation using target's layers |
| **Tree Decoding** | Verify multiple candidate sequences in parallel |
| **Prompt Lookup** | Use input prompt for speculation (no draft model) |

---

## References

1. Leviathan et al. (2022). Fast Inference from Transformers via Speculative Decoding
2. Cai et al. (2024). Medusa: Simple LLM Inference Acceleration with Multiple Decoding Heads
3. Li et al. (2024). EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty
4. [Google Research Blog: Looking back at speculative decoding](https://research.google/blog/looking-back-at-speculative-decoding/)
5. [NVIDIA: Introduction to Speculative Decoding](https://developer.nvidia.com/blog/an-introduction-to-speculative-decoding-for-reducing-latency-in-ai-inference/)
6. [vLLM Speculative Decoding Blog](https://blog.vllm.ai/2024/10/17/spec-decode.html)
7. [BentoML: 3x Faster with Speculative Decoding](https://www.bentoml.com/blog/3x-faster-llm-inference-with-speculative-decoding)
8. [SpecExec: Parallel Speculative Decoding](https://www.together.ai/blog/specexec)

---

> **Navigation**
> [← 6.3 Distillation](6.3_knowledge_distillation_guide.md) | **[Index](../README.md#15-repository-structure)** | [7.1 Vector Database →](../07_rag_pipeline/7.1_vector_database_guide.md)
