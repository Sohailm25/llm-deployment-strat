> **Navigation** | [← 9.3 API Design](../09_inference_serving/9.3_api_design_llm_services_guide.md) | [10.2 Logging & Tracing →](10.2_llm_logging_tracing_guide.md)
>
> | | |
> |---|---|
> | **Prerequisites** | Prometheus &#124; Grafana &#124; OpenTelemetry |
> | **Related** | [10.2 Logging](10.2_llm_logging_tracing_guide.md) &#124; [10.3 Model Quality](10.3_model_quality_monitoring_guide.md) &#124; [13.1 Incident Response](../13_operations_reliability/13.1_incident_response_guide.md) |
> | **Next** | [10.2 LLM Logging & Tracing](10.2_llm_logging_tracing_guide.md) |

# Document 10.1: LLM Monitoring Strategy Guide

## Purpose

This guide provides a comprehensive monitoring strategy for LLM systems in production. LLMs present unique monitoring challenges: GPU-intensive workloads, variable latency, non-deterministic outputs, and complex quality metrics. This document covers the four key monitoring dimensions (system, application, model, business), essential metrics, monitoring stack implementation, dashboard design, and alerting strategies.

---

## Prerequisites

- **Infrastructure**: Prometheus server, Grafana, GPU-enabled Kubernetes cluster
- **Tools**: vLLM/TensorRT-LLM with metrics enabled, OpenTelemetry collector
- **Knowledge**: Prometheus query language (PromQL), Grafana dashboard building
- **Access**: Monitoring system credentials, Slack/PagerDuty for alerts

---

## 10.1.1 Monitoring Dimensions

```python
"""
Four dimensions of LLM monitoring.
"""

from dataclasses import dataclass, field
from enum import Enum
from typing import Optional


class MonitoringDimension(Enum):
    SYSTEM = "system"
    APPLICATION = "application"
    MODEL = "model"
    BUSINESS = "business"


@dataclass
class MetricDefinition:
    """Definition of a monitoring metric."""

    name: str
    dimension: MonitoringDimension
    description: str
    unit: str
    prometheus_name: str
    metric_type: str  # gauge, counter, histogram, summary
    labels: list[str] = field(default_factory=list)
    alert_thresholds: dict = field(default_factory=dict)


# System metrics - Infrastructure level
SYSTEM_METRICS = [
    MetricDefinition(
        name="GPU Utilization",
        dimension=MonitoringDimension.SYSTEM,
        description="Percentage of GPU compute capacity in use",
        unit="percent",
        prometheus_name="nvidia_gpu_utilization",
        metric_type="gauge",
        labels=["gpu", "instance"],
        alert_thresholds={"warning": 85, "critical": 95}
    ),
    MetricDefinition(
        name="GPU Memory Used",
        dimension=MonitoringDimension.SYSTEM,
        description="GPU memory currently in use",
        unit="bytes",
        prometheus_name="nvidia_gpu_memory_used_bytes",
        metric_type="gauge",
        labels=["gpu", "instance"],
        alert_thresholds={"warning": 0.85, "critical": 0.95}  # As fraction
    ),
    MetricDefinition(
        name="GPU Temperature",
        dimension=MonitoringDimension.SYSTEM,
        description="GPU temperature",
        unit="celsius",
        prometheus_name="nvidia_gpu_temperature_celsius",
        metric_type="gauge",
        labels=["gpu", "instance"],
        alert_thresholds={"warning": 75, "critical": 85}
    ),
    MetricDefinition(
        name="GPU Power Draw",
        dimension=MonitoringDimension.SYSTEM,
        description="GPU power consumption",
        unit="watts",
        prometheus_name="nvidia_gpu_power_watts",
        metric_type="gauge",
        labels=["gpu", "instance"]
    ),
    MetricDefinition(
        name="System CPU Usage",
        dimension=MonitoringDimension.SYSTEM,
        description="CPU utilization percentage",
        unit="percent",
        prometheus_name="node_cpu_usage_percent",
        metric_type="gauge",
        labels=["instance", "cpu"]
    ),
    MetricDefinition(
        name="System Memory",
        dimension=MonitoringDimension.SYSTEM,
        description="System RAM usage",
        unit="bytes",
        prometheus_name="node_memory_MemTotal_bytes",
        metric_type="gauge",
        labels=["instance"]
    ),
    MetricDefinition(
        name="Network RX/TX",
        dimension=MonitoringDimension.SYSTEM,
        description="Network traffic in/out",
        unit="bytes",
        prometheus_name="node_network_receive_bytes_total",
        metric_type="counter",
        labels=["instance", "device"]
    )
]

# Application metrics - Inference level
APPLICATION_METRICS = [
    MetricDefinition(
        name="Time to First Token (TTFT)",
        dimension=MonitoringDimension.APPLICATION,
        description="Time from request to first token generation",
        unit="seconds",
        prometheus_name="vllm_time_to_first_token_seconds",
        metric_type="histogram",
        labels=["model", "instance"],
        alert_thresholds={"warning": 2.0, "critical": 5.0}
    ),
    MetricDefinition(
        name="Time Per Output Token (TPOT)",
        dimension=MonitoringDimension.APPLICATION,
        description="Time to generate each subsequent token",
        unit="seconds",
        prometheus_name="vllm_inter_token_latency_seconds",
        metric_type="histogram",
        labels=["model", "instance"],
        alert_thresholds={"warning": 0.1, "critical": 0.2}
    ),
    MetricDefinition(
        name="End-to-End Latency",
        dimension=MonitoringDimension.APPLICATION,
        description="Total request latency",
        unit="seconds",
        prometheus_name="vllm_e2e_request_latency_seconds",
        metric_type="histogram",
        labels=["model", "instance"],
        alert_thresholds={"warning": 10.0, "critical": 30.0}
    ),
    MetricDefinition(
        name="Request Throughput",
        dimension=MonitoringDimension.APPLICATION,
        description="Requests processed per second",
        unit="requests/second",
        prometheus_name="vllm_requests_total",
        metric_type="counter",
        labels=["model", "instance", "status"]
    ),
    MetricDefinition(
        name="Token Throughput",
        dimension=MonitoringDimension.APPLICATION,
        description="Tokens generated per second",
        unit="tokens/second",
        prometheus_name="vllm_tokens_total",
        metric_type="counter",
        labels=["model", "instance", "type"]
    ),
    MetricDefinition(
        name="Queue Depth",
        dimension=MonitoringDimension.APPLICATION,
        description="Number of requests waiting in queue",
        unit="requests",
        prometheus_name="vllm_pending_requests",
        metric_type="gauge",
        labels=["model", "instance"],
        alert_thresholds={"warning": 50, "critical": 100}
    ),
    MetricDefinition(
        name="KV Cache Usage",
        dimension=MonitoringDimension.APPLICATION,
        description="Percentage of KV cache blocks in use",
        unit="percent",
        prometheus_name="vllm_gpu_cache_usage_perc",
        metric_type="gauge",
        labels=["model", "instance"],
        alert_thresholds={"warning": 85, "critical": 95}
    ),
    MetricDefinition(
        name="Batch Size",
        dimension=MonitoringDimension.APPLICATION,
        description="Number of requests in current batch",
        unit="requests",
        prometheus_name="vllm_batch_size",
        metric_type="histogram",
        labels=["model", "instance"]
    )
]

# Model metrics - Quality level
MODEL_METRICS = [
    MetricDefinition(
        name="Output Length Distribution",
        dimension=MonitoringDimension.MODEL,
        description="Distribution of generated token counts",
        unit="tokens",
        prometheus_name="llm_output_tokens",
        metric_type="histogram",
        labels=["model"]
    ),
    MetricDefinition(
        name="Refusal Rate",
        dimension=MonitoringDimension.MODEL,
        description="Percentage of requests refused by model",
        unit="percent",
        prometheus_name="llm_refusal_total",
        metric_type="counter",
        labels=["model", "reason"]
    ),
    MetricDefinition(
        name="Safety Trigger Rate",
        dimension=MonitoringDimension.MODEL,
        description="Rate of safety filter activations",
        unit="percent",
        prometheus_name="llm_safety_trigger_total",
        metric_type="counter",
        labels=["model", "category"],
        alert_thresholds={"warning": 0.01, "critical": 0.05}
    ),
    MetricDefinition(
        name="Quality Score",
        dimension=MonitoringDimension.MODEL,
        description="Sampled output quality score",
        unit="score",
        prometheus_name="llm_quality_score",
        metric_type="gauge",
        labels=["model", "evaluator"],
        alert_thresholds={"warning": 0.7, "critical": 0.5}
    )
]

# Business metrics
BUSINESS_METRICS = [
    MetricDefinition(
        name="Requests by Customer",
        dimension=MonitoringDimension.BUSINESS,
        description="Request count per customer/tenant",
        unit="requests",
        prometheus_name="llm_requests_by_customer_total",
        metric_type="counter",
        labels=["customer_id", "model"]
    ),
    MetricDefinition(
        name="Tokens by Customer",
        dimension=MonitoringDimension.BUSINESS,
        description="Tokens consumed per customer",
        unit="tokens",
        prometheus_name="llm_tokens_by_customer_total",
        metric_type="counter",
        labels=["customer_id", "model", "type"]
    ),
    MetricDefinition(
        name="Cost per Request",
        dimension=MonitoringDimension.BUSINESS,
        description="Estimated cost per request",
        unit="dollars",
        prometheus_name="llm_cost_per_request",
        metric_type="histogram",
        labels=["model", "customer_id"]
    ),
    MetricDefinition(
        name="User Satisfaction",
        dimension=MonitoringDimension.BUSINESS,
        description="User feedback score",
        unit="score",
        prometheus_name="llm_user_satisfaction",
        metric_type="gauge",
        labels=["model", "segment"]
    )
]
```

---

## 10.1.2 Key Metrics

```python
"""
Implementation of key LLM monitoring metrics.
"""

from prometheus_client import Counter, Histogram, Gauge, CollectorRegistry
import time
from typing import Optional
from dataclasses import dataclass


class LLMMetricsCollector:
    """
    Collects and exposes LLM inference metrics.
    Compatible with Prometheus.
    """

    def __init__(self, registry: Optional[CollectorRegistry] = None):
        self.registry = registry or CollectorRegistry()
        self._setup_metrics()

    def _setup_metrics(self):
        """Initialize all metrics."""
        # Infrastructure metrics
        self.gpu_utilization = Gauge(
            'llm_gpu_utilization_percent',
            'GPU utilization percentage',
            ['gpu_id', 'instance'],
            registry=self.registry
        )

        self.gpu_memory_used = Gauge(
            'llm_gpu_memory_used_bytes',
            'GPU memory in use',
            ['gpu_id', 'instance'],
            registry=self.registry
        )

        self.gpu_memory_total = Gauge(
            'llm_gpu_memory_total_bytes',
            'Total GPU memory',
            ['gpu_id', 'instance'],
            registry=self.registry
        )

        # Latency metrics
        self.ttft = Histogram(
            'llm_time_to_first_token_seconds',
            'Time to first token',
            ['model'],
            buckets=[0.1, 0.25, 0.5, 0.75, 1.0, 2.0, 5.0, 10.0],
            registry=self.registry
        )

        self.tpot = Histogram(
            'llm_inter_token_latency_seconds',
            'Inter-token latency',
            ['model'],
            buckets=[0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],
            registry=self.registry
        )

        self.e2e_latency = Histogram(
            'llm_request_latency_seconds',
            'End-to-end request latency',
            ['model', 'status'],
            buckets=[0.5, 1.0, 2.0, 5.0, 10.0, 20.0, 30.0, 60.0],
            registry=self.registry
        )

        # Throughput metrics
        self.requests_total = Counter(
            'llm_requests_total',
            'Total requests',
            ['model', 'status'],
            registry=self.registry
        )

        self.tokens_total = Counter(
            'llm_tokens_total',
            'Total tokens processed',
            ['model', 'type'],  # type: prompt or completion
            registry=self.registry
        )

        # Queue metrics
        self.queue_depth = Gauge(
            'llm_queue_depth',
            'Current queue depth',
            ['model'],
            registry=self.registry
        )

        self.active_requests = Gauge(
            'llm_active_requests',
            'Currently processing requests',
            ['model'],
            registry=self.registry
        )

        # Cache metrics
        self.kv_cache_usage = Gauge(
            'llm_kv_cache_usage_percent',
            'KV cache usage percentage',
            ['model'],
            registry=self.registry
        )

        self.prefix_cache_hits = Counter(
            'llm_prefix_cache_hits_total',
            'Prefix cache hits',
            ['model'],
            registry=self.registry
        )

        self.prefix_cache_misses = Counter(
            'llm_prefix_cache_misses_total',
            'Prefix cache misses',
            ['model'],
            registry=self.registry
        )

        # Quality metrics
        self.quality_score = Gauge(
            'llm_quality_score',
            'Sampled quality score',
            ['model', 'evaluator'],
            registry=self.registry
        )

        self.refusals = Counter(
            'llm_refusals_total',
            'Refusal count',
            ['model', 'reason'],
            registry=self.registry
        )

        self.safety_triggers = Counter(
            'llm_safety_triggers_total',
            'Safety filter triggers',
            ['model', 'category'],
            registry=self.registry
        )

        # Business metrics
        self.customer_requests = Counter(
            'llm_customer_requests_total',
            'Requests by customer',
            ['customer_id', 'model'],
            registry=self.registry
        )

        self.customer_tokens = Counter(
            'llm_customer_tokens_total',
            'Tokens by customer',
            ['customer_id', 'model', 'type'],
            registry=self.registry
        )


@dataclass
class RequestMetrics:
    """Metrics for a single request."""

    request_id: str
    model: str
    start_time: float
    first_token_time: Optional[float] = None
    end_time: Optional[float] = None
    prompt_tokens: int = 0
    completion_tokens: int = 0
    status: str = "pending"
    customer_id: Optional[str] = None

    def record_first_token(self):
        """Record time of first token."""
        self.first_token_time = time.time()

    def record_completion(self, status: str = "success"):
        """Record request completion."""
        self.end_time = time.time()
        self.status = status

    @property
    def ttft(self) -> Optional[float]:
        """Time to first token."""
        if self.first_token_time and self.start_time:
            return self.first_token_time - self.start_time
        return None

    @property
    def total_latency(self) -> Optional[float]:
        """Total request latency."""
        if self.end_time and self.start_time:
            return self.end_time - self.start_time
        return None

    @property
    def tpot(self) -> Optional[float]:
        """Average time per output token."""
        if self.first_token_time and self.end_time and self.completion_tokens > 1:
            decode_time = self.end_time - self.first_token_time
            return decode_time / (self.completion_tokens - 1)
        return None


class MetricsMiddleware:
    """
    Middleware to collect metrics for each request.
    """

    def __init__(self, collector: LLMMetricsCollector):
        self.collector = collector
        self.active_requests: dict[str, RequestMetrics] = {}

    def start_request(
        self,
        request_id: str,
        model: str,
        customer_id: Optional[str] = None
    ) -> RequestMetrics:
        """Start tracking a request."""
        metrics = RequestMetrics(
            request_id=request_id,
            model=model,
            start_time=time.time(),
            customer_id=customer_id
        )
        self.active_requests[request_id] = metrics
        self.collector.active_requests.labels(model=model).inc()
        return metrics

    def record_first_token(self, request_id: str):
        """Record first token time."""
        if request_id in self.active_requests:
            metrics = self.active_requests[request_id]
            metrics.record_first_token()

            if metrics.ttft:
                self.collector.ttft.labels(model=metrics.model).observe(metrics.ttft)

    def finish_request(
        self,
        request_id: str,
        prompt_tokens: int,
        completion_tokens: int,
        status: str = "success"
    ):
        """Finish tracking a request."""
        if request_id not in self.active_requests:
            return

        metrics = self.active_requests.pop(request_id)
        metrics.prompt_tokens = prompt_tokens
        metrics.completion_tokens = completion_tokens
        metrics.record_completion(status)

        # Record latency
        if metrics.total_latency:
            self.collector.e2e_latency.labels(
                model=metrics.model,
                status=status
            ).observe(metrics.total_latency)

        # Record TPOT
        if metrics.tpot:
            self.collector.tpot.labels(model=metrics.model).observe(metrics.tpot)

        # Record throughput
        self.collector.requests_total.labels(
            model=metrics.model,
            status=status
        ).inc()

        self.collector.tokens_total.labels(
            model=metrics.model,
            type="prompt"
        ).inc(prompt_tokens)

        self.collector.tokens_total.labels(
            model=metrics.model,
            type="completion"
        ).inc(completion_tokens)

        # Record customer metrics
        if metrics.customer_id:
            self.collector.customer_requests.labels(
                customer_id=metrics.customer_id,
                model=metrics.model
            ).inc()

            self.collector.customer_tokens.labels(
                customer_id=metrics.customer_id,
                model=metrics.model,
                type="prompt"
            ).inc(prompt_tokens)

            self.collector.customer_tokens.labels(
                customer_id=metrics.customer_id,
                model=metrics.model,
                type="completion"
            ).inc(completion_tokens)

        # Decrement active requests
        self.collector.active_requests.labels(model=metrics.model).dec()


class GPUMetricsCollector:
    """
    Collects GPU metrics using NVIDIA SMI or NVML.
    """

    def __init__(self, collector: LLMMetricsCollector, instance: str = "default"):
        self.collector = collector
        self.instance = instance

    def collect(self):
        """Collect GPU metrics."""
        try:
            import pynvml
            pynvml.nvmlInit()

            device_count = pynvml.nvmlDeviceGetCount()

            for i in range(device_count):
                handle = pynvml.nvmlDeviceGetHandleByIndex(i)

                # Utilization
                util = pynvml.nvmlDeviceGetUtilizationRates(handle)
                self.collector.gpu_utilization.labels(
                    gpu_id=str(i),
                    instance=self.instance
                ).set(util.gpu)

                # Memory
                mem = pynvml.nvmlDeviceGetMemoryInfo(handle)
                self.collector.gpu_memory_used.labels(
                    gpu_id=str(i),
                    instance=self.instance
                ).set(mem.used)

                self.collector.gpu_memory_total.labels(
                    gpu_id=str(i),
                    instance=self.instance
                ).set(mem.total)

            pynvml.nvmlShutdown()
        except Exception as e:
            print(f"Error collecting GPU metrics: {e}")
```

---

## 10.1.3 Monitoring Stack

```python
"""
Monitoring stack setup: Prometheus, Grafana, OpenTelemetry.
"""

from dataclasses import dataclass
from typing import Optional
import yaml


@dataclass
class MonitoringStackConfig:
    """Configuration for the monitoring stack."""

    # Prometheus
    prometheus_url: str = "http://prometheus:9090"
    scrape_interval: str = "15s"
    evaluation_interval: str = "15s"

    # Grafana
    grafana_url: str = "http://grafana:3000"
    grafana_admin_password: str = "admin"

    # OpenTelemetry
    otel_collector_endpoint: str = "otel-collector:4317"
    enable_tracing: bool = True
    enable_metrics: bool = True

    # Retention
    metrics_retention_days: int = 15
    traces_retention_days: int = 7


# Prometheus configuration
PROMETHEUS_CONFIG = """
global:
  scrape_interval: 15s
  evaluation_interval: 15s

alerting:
  alertmanagers:
    - static_configs:
        - targets: ['alertmanager:9093']

rule_files:
  - '/etc/prometheus/rules/*.yml'

scrape_configs:
  # LLM service metrics
  - job_name: 'vllm'
    static_configs:
      - targets: ['vllm:8000']
    metrics_path: /metrics
    scrape_interval: 5s

  # NVIDIA GPU metrics
  - job_name: 'nvidia-dcgm'
    static_configs:
      - targets: ['dcgm-exporter:9400']

  # Kubernetes pods with prometheus.io annotations
  - job_name: 'kubernetes-pods'
    kubernetes_sd_configs:
      - role: pod
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__

  # Node exporter for system metrics
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']
"""

# OpenTelemetry Collector configuration
OTEL_COLLECTOR_CONFIG = """
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

  prometheus:
    config:
      scrape_configs:
        - job_name: 'otel-collector'
          static_configs:
            - targets: ['localhost:8888']

processors:
  batch:
    timeout: 10s
    send_batch_size: 1000

  memory_limiter:
    check_interval: 1s
    limit_mib: 1000

  attributes:
    actions:
      - key: environment
        value: production
        action: insert

exporters:
  prometheus:
    endpoint: 0.0.0.0:8889
    namespace: llm

  jaeger:
    endpoint: jaeger:14250
    tls:
      insecure: true

  logging:
    loglevel: debug

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [memory_limiter, batch]
      exporters: [jaeger, logging]

    metrics:
      receivers: [otlp, prometheus]
      processors: [memory_limiter, batch]
      exporters: [prometheus, logging]
"""

# Docker Compose for monitoring stack
MONITORING_DOCKER_COMPOSE = """
version: '3.8'

services:
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - ./rules:/etc/prometheus/rules
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=15d'
      - '--web.enable-lifecycle'

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    volumes:
      - ./grafana/provisioning:/etc/grafana/provisioning
      - grafana_data:/var/lib/grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_INSTALL_PLUGINS=grafana-clock-panel

  alertmanager:
    image: prom/alertmanager:latest
    ports:
      - "9093:9093"
    volumes:
      - ./alertmanager.yml:/etc/alertmanager/alertmanager.yml

  otel-collector:
    image: otel/opentelemetry-collector-contrib:latest
    ports:
      - "4317:4317"   # OTLP gRPC
      - "4318:4318"   # OTLP HTTP
      - "8889:8889"   # Prometheus exporter
    volumes:
      - ./otel-collector.yml:/etc/otel-collector.yml
    command: ["--config=/etc/otel-collector.yml"]

  jaeger:
    image: jaegertracing/all-in-one:latest
    ports:
      - "16686:16686"  # UI
      - "14250:14250"  # gRPC

  dcgm-exporter:
    image: nvidia/dcgm-exporter:latest
    runtime: nvidia
    ports:
      - "9400:9400"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all

volumes:
  prometheus_data:
  grafana_data:
"""

# Kubernetes monitoring deployment
K8S_MONITORING = """
# ServiceMonitor for vLLM
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: vllm-monitor
  labels:
    release: prometheus
spec:
  selector:
    matchLabels:
      app: vllm
  endpoints:
  - port: metrics
    interval: 15s
    path: /metrics
---
# PodMonitor for GPU metrics
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: gpu-metrics
  labels:
    release: prometheus
spec:
  selector:
    matchLabels:
      app.kubernetes.io/component: dcgm-exporter
  podMetricsEndpoints:
  - port: metrics
    interval: 15s
"""
```

---

## 10.1.4 Dashboard Design

```python
"""
Grafana dashboard configurations for LLM monitoring.
"""

import json
from typing import List, Dict, Any


class GrafanaDashboardBuilder:
    """Build Grafana dashboards programmatically."""

    def __init__(self, title: str, uid: str):
        self.dashboard = {
            "title": title,
            "uid": uid,
            "tags": ["llm", "monitoring"],
            "timezone": "browser",
            "schemaVersion": 38,
            "version": 1,
            "refresh": "30s",
            "panels": [],
            "templating": {
                "list": []
            }
        }
        self.panel_id = 1
        self.y_position = 0

    def add_variable(self, name: str, query: str, datasource: str = "Prometheus"):
        """Add a template variable."""
        self.dashboard["templating"]["list"].append({
            "name": name,
            "type": "query",
            "datasource": datasource,
            "query": query,
            "refresh": 1,
            "includeAll": True,
            "multi": True
        })

    def add_row(self, title: str):
        """Add a row panel."""
        self.dashboard["panels"].append({
            "type": "row",
            "title": title,
            "gridPos": {"h": 1, "w": 24, "x": 0, "y": self.y_position}
        })
        self.y_position += 1

    def add_stat_panel(
        self,
        title: str,
        query: str,
        x: int,
        width: int = 4,
        unit: str = "short",
        thresholds: List[Dict] = None
    ):
        """Add a stat panel."""
        panel = {
            "id": self.panel_id,
            "type": "stat",
            "title": title,
            "gridPos": {"h": 4, "w": width, "x": x, "y": self.y_position},
            "datasource": "Prometheus",
            "targets": [{
                "expr": query,
                "refId": "A"
            }],
            "options": {
                "reduceOptions": {
                    "values": False,
                    "calcs": ["lastNotNull"],
                    "fields": ""
                },
                "orientation": "auto",
                "textMode": "auto",
                "colorMode": "value"
            },
            "fieldConfig": {
                "defaults": {
                    "unit": unit,
                    "thresholds": {
                        "mode": "absolute",
                        "steps": thresholds or [
                            {"color": "green", "value": None}
                        ]
                    }
                }
            }
        }
        self.dashboard["panels"].append(panel)
        self.panel_id += 1

    def add_graph_panel(
        self,
        title: str,
        queries: List[Dict[str, str]],
        x: int,
        width: int = 12,
        height: int = 8,
        unit: str = "short",
        legend_mode: str = "table"
    ):
        """Add a time series graph panel."""
        targets = [
            {
                "expr": q["query"],
                "legendFormat": q.get("legend", ""),
                "refId": chr(65 + i)  # A, B, C, ...
            }
            for i, q in enumerate(queries)
        ]

        panel = {
            "id": self.panel_id,
            "type": "timeseries",
            "title": title,
            "gridPos": {"h": height, "w": width, "x": x, "y": self.y_position},
            "datasource": "Prometheus",
            "targets": targets,
            "options": {
                "legend": {
                    "displayMode": legend_mode,
                    "placement": "bottom"
                }
            },
            "fieldConfig": {
                "defaults": {
                    "unit": unit,
                    "custom": {
                        "lineWidth": 1,
                        "fillOpacity": 10,
                        "gradientMode": "none"
                    }
                }
            }
        }
        self.dashboard["panels"].append(panel)
        self.panel_id += 1

    def add_heatmap_panel(
        self,
        title: str,
        query: str,
        x: int,
        width: int = 12,
        height: int = 8
    ):
        """Add a heatmap panel for histograms."""
        panel = {
            "id": self.panel_id,
            "type": "heatmap",
            "title": title,
            "gridPos": {"h": height, "w": width, "x": x, "y": self.y_position},
            "datasource": "Prometheus",
            "targets": [{
                "expr": query,
                "format": "heatmap",
                "refId": "A"
            }],
            "options": {
                "calculate": False,
                "yAxis": {
                    "axisPlacement": "left",
                    "unit": "s"
                }
            }
        }
        self.dashboard["panels"].append(panel)
        self.panel_id += 1

    def next_row(self, height: int = 8):
        """Move to next row."""
        self.y_position += height

    def build(self) -> dict:
        """Return the dashboard JSON."""
        return self.dashboard


def create_operations_dashboard() -> dict:
    """Create operations dashboard for LLM monitoring."""
    builder = GrafanaDashboardBuilder(
        title="LLM Operations Dashboard",
        uid="llm-ops"
    )

    # Variables
    builder.add_variable("model", 'label_values(vllm_requests_total, model)')
    builder.add_variable("instance", 'label_values(vllm_requests_total, instance)')

    # Row 1: Key Stats
    builder.add_row("Overview")

    builder.add_stat_panel(
        title="Requests/sec",
        query='sum(rate(vllm_requests_total{model=~"$model"}[5m]))',
        x=0, width=4, unit="reqps"
    )
    builder.add_stat_panel(
        title="Tokens/sec",
        query='sum(rate(vllm_tokens_total{model=~"$model",type="completion"}[5m]))',
        x=4, width=4, unit="short"
    )
    builder.add_stat_panel(
        title="P50 Latency",
        query='histogram_quantile(0.5, sum(rate(vllm_e2e_request_latency_seconds_bucket{model=~"$model"}[5m])) by (le))',
        x=8, width=4, unit="s"
    )
    builder.add_stat_panel(
        title="P99 Latency",
        query='histogram_quantile(0.99, sum(rate(vllm_e2e_request_latency_seconds_bucket{model=~"$model"}[5m])) by (le))',
        x=12, width=4, unit="s"
    )
    builder.add_stat_panel(
        title="Error Rate",
        query='sum(rate(vllm_requests_total{model=~"$model",status="error"}[5m])) / sum(rate(vllm_requests_total{model=~"$model"}[5m]))',
        x=16, width=4, unit="percentunit",
        thresholds=[
            {"color": "green", "value": None},
            {"color": "yellow", "value": 0.01},
            {"color": "red", "value": 0.05}
        ]
    )
    builder.add_stat_panel(
        title="Queue Depth",
        query='sum(vllm_pending_requests{model=~"$model"})',
        x=20, width=4, unit="short",
        thresholds=[
            {"color": "green", "value": None},
            {"color": "yellow", "value": 50},
            {"color": "red", "value": 100}
        ]
    )

    builder.next_row(4)

    # Row 2: Latency Trends
    builder.add_row("Latency")

    builder.add_graph_panel(
        title="Request Latency",
        queries=[
            {"query": 'histogram_quantile(0.5, sum(rate(vllm_e2e_request_latency_seconds_bucket{model=~"$model"}[5m])) by (le))', "legend": "P50"},
            {"query": 'histogram_quantile(0.90, sum(rate(vllm_e2e_request_latency_seconds_bucket{model=~"$model"}[5m])) by (le))', "legend": "P90"},
            {"query": 'histogram_quantile(0.99, sum(rate(vllm_e2e_request_latency_seconds_bucket{model=~"$model"}[5m])) by (le))', "legend": "P99"}
        ],
        x=0, width=12, unit="s"
    )

    builder.add_graph_panel(
        title="TTFT (Time to First Token)",
        queries=[
            {"query": 'histogram_quantile(0.5, sum(rate(vllm_time_to_first_token_seconds_bucket{model=~"$model"}[5m])) by (le))', "legend": "P50"},
            {"query": 'histogram_quantile(0.99, sum(rate(vllm_time_to_first_token_seconds_bucket{model=~"$model"}[5m])) by (le))', "legend": "P99"}
        ],
        x=12, width=12, unit="s"
    )

    builder.next_row()

    # Row 3: Throughput
    builder.add_row("Throughput")

    builder.add_graph_panel(
        title="Request Throughput",
        queries=[
            {"query": 'sum(rate(vllm_requests_total{model=~"$model"}[5m])) by (model)', "legend": "{{model}}"}
        ],
        x=0, width=12, unit="reqps"
    )

    builder.add_graph_panel(
        title="Token Throughput",
        queries=[
            {"query": 'sum(rate(vllm_tokens_total{model=~"$model",type="prompt"}[5m]))', "legend": "Prompt"},
            {"query": 'sum(rate(vllm_tokens_total{model=~"$model",type="completion"}[5m]))', "legend": "Completion"}
        ],
        x=12, width=12, unit="short"
    )

    builder.next_row()

    # Row 4: GPU Metrics
    builder.add_row("GPU Resources")

    builder.add_graph_panel(
        title="GPU Utilization",
        queries=[
            {"query": 'nvidia_gpu_utilization{instance=~"$instance"}', "legend": "GPU {{gpu}}"}
        ],
        x=0, width=8, unit="percent"
    )

    builder.add_graph_panel(
        title="GPU Memory Usage",
        queries=[
            {"query": 'nvidia_gpu_memory_used_bytes{instance=~"$instance"} / nvidia_gpu_memory_total_bytes{instance=~"$instance"} * 100', "legend": "GPU {{gpu}}"}
        ],
        x=8, width=8, unit="percent"
    )

    builder.add_graph_panel(
        title="KV Cache Usage",
        queries=[
            {"query": 'vllm_gpu_cache_usage_perc{model=~"$model"}', "legend": "{{model}}"}
        ],
        x=16, width=8, unit="percent"
    )

    return builder.build()


def create_executive_dashboard() -> dict:
    """Create executive summary dashboard."""
    builder = GrafanaDashboardBuilder(
        title="LLM Executive Dashboard",
        uid="llm-exec"
    )

    builder.add_row("Key Metrics")

    builder.add_stat_panel(
        title="Total Requests (24h)",
        query='sum(increase(vllm_requests_total[24h]))',
        x=0, width=6, unit="short"
    )
    builder.add_stat_panel(
        title="Total Tokens (24h)",
        query='sum(increase(vllm_tokens_total[24h]))',
        x=6, width=6, unit="short"
    )
    builder.add_stat_panel(
        title="Availability",
        query='1 - (sum(rate(vllm_requests_total{status="error"}[24h])) / sum(rate(vllm_requests_total[24h])))',
        x=12, width=6, unit="percentunit"
    )
    builder.add_stat_panel(
        title="Avg Latency (24h)",
        query='sum(rate(vllm_e2e_request_latency_seconds_sum[24h])) / sum(rate(vllm_e2e_request_latency_seconds_count[24h]))',
        x=18, width=6, unit="s"
    )

    return builder.build()


# Export dashboards
OPERATIONS_DASHBOARD_JSON = json.dumps(create_operations_dashboard(), indent=2)
EXECUTIVE_DASHBOARD_JSON = json.dumps(create_executive_dashboard(), indent=2)
```

---

## 10.1.5 Alerting Strategy

```python
"""
Alerting strategy and rules for LLM systems.
"""

from dataclasses import dataclass, field
from enum import Enum
from typing import List, Optional


class AlertSeverity(Enum):
    INFO = "info"
    WARNING = "warning"
    CRITICAL = "critical"
    PAGE = "page"


@dataclass
class AlertRule:
    """Definition of an alert rule."""

    name: str
    severity: AlertSeverity
    expr: str
    for_duration: str
    summary: str
    description: str
    runbook_url: Optional[str] = None
    labels: dict = field(default_factory=dict)
    annotations: dict = field(default_factory=dict)


# Alert rules for LLM systems
LLM_ALERT_RULES = [
    # Latency alerts
    AlertRule(
        name="LLMHighLatencyP99",
        severity=AlertSeverity.WARNING,
        expr='histogram_quantile(0.99, sum(rate(vllm_e2e_request_latency_seconds_bucket[5m])) by (le, model)) > 10',
        for_duration="5m",
        summary="High P99 latency for {{ $labels.model }}",
        description="P99 latency is {{ $value }}s, above 10s threshold",
        runbook_url="https://runbooks.example.com/llm/high-latency"
    ),
    AlertRule(
        name="LLMHighLatencyP99Critical",
        severity=AlertSeverity.CRITICAL,
        expr='histogram_quantile(0.99, sum(rate(vllm_e2e_request_latency_seconds_bucket[5m])) by (le, model)) > 30',
        for_duration="5m",
        summary="Critical P99 latency for {{ $labels.model }}",
        description="P99 latency is {{ $value }}s, above 30s critical threshold",
        runbook_url="https://runbooks.example.com/llm/high-latency"
    ),

    # Error rate alerts
    AlertRule(
        name="LLMHighErrorRate",
        severity=AlertSeverity.WARNING,
        expr='sum(rate(vllm_requests_total{status="error"}[5m])) by (model) / sum(rate(vllm_requests_total[5m])) by (model) > 0.01',
        for_duration="5m",
        summary="High error rate for {{ $labels.model }}",
        description="Error rate is {{ $value | humanizePercentage }}, above 1% threshold"
    ),
    AlertRule(
        name="LLMHighErrorRateCritical",
        severity=AlertSeverity.PAGE,
        expr='sum(rate(vllm_requests_total{status="error"}[5m])) by (model) / sum(rate(vllm_requests_total[5m])) by (model) > 0.05',
        for_duration="2m",
        summary="Critical error rate for {{ $labels.model }}",
        description="Error rate is {{ $value | humanizePercentage }}, above 5% critical threshold"
    ),

    # GPU alerts
    AlertRule(
        name="GPUHighUtilization",
        severity=AlertSeverity.INFO,
        expr='nvidia_gpu_utilization > 90',
        for_duration="15m",
        summary="GPU {{ $labels.gpu }} utilization above 90%",
        description="GPU utilization is {{ $value }}%, consider scaling"
    ),
    AlertRule(
        name="GPUMemoryNearFull",
        severity=AlertSeverity.WARNING,
        expr='nvidia_gpu_memory_used_bytes / nvidia_gpu_memory_total_bytes > 0.95',
        for_duration="5m",
        summary="GPU {{ $labels.gpu }} memory nearly full",
        description="GPU memory usage is {{ $value | humanizePercentage }}"
    ),
    AlertRule(
        name="GPUHighTemperature",
        severity=AlertSeverity.WARNING,
        expr='nvidia_gpu_temperature_celsius > 80',
        for_duration="5m",
        summary="GPU {{ $labels.gpu }} temperature high",
        description="GPU temperature is {{ $value }}°C"
    ),

    # Queue alerts
    AlertRule(
        name="LLMHighQueueDepth",
        severity=AlertSeverity.WARNING,
        expr='vllm_pending_requests > 100',
        for_duration="5m",
        summary="High queue depth for {{ $labels.model }}",
        description="Queue depth is {{ $value }}, requests may be delayed"
    ),

    # KV Cache alerts
    AlertRule(
        name="LLMKVCacheNearFull",
        severity=AlertSeverity.WARNING,
        expr='vllm_gpu_cache_usage_perc > 90',
        for_duration="5m",
        summary="KV cache nearly full for {{ $labels.model }}",
        description="KV cache usage is {{ $value }}%"
    ),

    # Availability
    AlertRule(
        name="LLMServiceDown",
        severity=AlertSeverity.PAGE,
        expr='up{job="vllm"} == 0',
        for_duration="1m",
        summary="LLM service is down",
        description="LLM service on {{ $labels.instance }} is not responding"
    ),

    # Safety alerts
    AlertRule(
        name="LLMHighSafetyTriggerRate",
        severity=AlertSeverity.WARNING,
        expr='sum(rate(llm_safety_triggers_total[1h])) / sum(rate(vllm_requests_total[1h])) > 0.01',
        for_duration="15m",
        summary="Elevated safety trigger rate",
        description="Safety triggers at {{ $value | humanizePercentage }} of requests"
    )
]


def generate_prometheus_rules(rules: List[AlertRule]) -> str:
    """Generate Prometheus alerting rules YAML."""
    groups = {
        "llm-latency": [],
        "llm-errors": [],
        "llm-resources": [],
        "llm-safety": []
    }

    for rule in rules:
        rule_dict = {
            "alert": rule.name,
            "expr": rule.expr,
            "for": rule.for_duration,
            "labels": {
                "severity": rule.severity.value,
                **rule.labels
            },
            "annotations": {
                "summary": rule.summary,
                "description": rule.description,
                **rule.annotations
            }
        }

        if rule.runbook_url:
            rule_dict["annotations"]["runbook_url"] = rule.runbook_url

        # Categorize rules
        if "Latency" in rule.name or "TTFT" in rule.name:
            groups["llm-latency"].append(rule_dict)
        elif "Error" in rule.name or "Down" in rule.name:
            groups["llm-errors"].append(rule_dict)
        elif "GPU" in rule.name or "Queue" in rule.name or "Cache" in rule.name:
            groups["llm-resources"].append(rule_dict)
        else:
            groups["llm-safety"].append(rule_dict)

    import yaml
    output = {"groups": []}
    for group_name, rules_list in groups.items():
        if rules_list:
            output["groups"].append({
                "name": group_name,
                "rules": rules_list
            })

    return yaml.dump(output, default_flow_style=False)


# Alertmanager configuration
ALERTMANAGER_CONFIG = """
global:
  resolve_timeout: 5m
  slack_api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'

route:
  group_by: ['alertname', 'severity', 'model']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 4h
  receiver: 'default'

  routes:
    - match:
        severity: page
      receiver: 'pagerduty'
      continue: true

    - match:
        severity: critical
      receiver: 'slack-critical'
      continue: true

    - match:
        severity: warning
      receiver: 'slack-warnings'

receivers:
  - name: 'default'
    slack_configs:
      - channel: '#llm-alerts'
        send_resolved: true

  - name: 'slack-critical'
    slack_configs:
      - channel: '#llm-critical'
        send_resolved: true
        title: '{{ .Status | toUpper }}: {{ .CommonAnnotations.summary }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

  - name: 'slack-warnings'
    slack_configs:
      - channel: '#llm-warnings'
        send_resolved: true

  - name: 'pagerduty'
    pagerduty_configs:
      - service_key: 'YOUR-PAGERDUTY-KEY'
        send_resolved: true

inhibit_rules:
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'model']
"""


class AlertFatiguePreventor:
    """
    Strategies to prevent alert fatigue.
    """

    @staticmethod
    def recommendations() -> List[str]:
        return [
            "Set appropriate thresholds based on baseline measurements",
            "Use inhibition rules to suppress redundant alerts",
            "Group related alerts to reduce notification volume",
            "Use escalation policies with increasing severity",
            "Implement alert snoozing for known issues",
            "Regularly review and tune alert thresholds",
            "Create dashboards for context before paging",
            "Define clear ownership for each alert category",
            "Use maintenance windows during planned outages",
            "Implement automatic incident creation for P1 alerts"
        ]
```

---

## Appendices

### Appendix A: Prometheus Metrics Reference

```markdown
## vLLM Prometheus Metrics

| Metric | Type | Description |
|--------|------|-------------|
| vllm_time_to_first_token_seconds | histogram | Time to generate first token |
| vllm_inter_token_latency_seconds | histogram | Time between tokens |
| vllm_e2e_request_latency_seconds | histogram | Total request latency |
| vllm_requests_total | counter | Total requests by status |
| vllm_tokens_total | counter | Total tokens by type |
| vllm_pending_requests | gauge | Requests in queue |
| vllm_running_requests | gauge | Currently processing |
| vllm_gpu_cache_usage_perc | gauge | KV cache utilization |
| vllm_prefix_cache_hit_rate | gauge | Prefix cache hit ratio |

## NVIDIA DCGM Metrics

| Metric | Type | Description |
|--------|------|-------------|
| DCGM_FI_DEV_GPU_UTIL | gauge | GPU utilization % |
| DCGM_FI_DEV_MEM_COPY_UTIL | gauge | Memory copy utilization |
| DCGM_FI_DEV_FB_USED | gauge | Framebuffer memory used |
| DCGM_FI_DEV_FB_FREE | gauge | Framebuffer memory free |
| DCGM_FI_DEV_GPU_TEMP | gauge | GPU temperature |
| DCGM_FI_DEV_POWER_USAGE | gauge | Power consumption |
| DCGM_FI_DEV_SM_CLOCK | gauge | SM clock speed |
| DCGM_FI_DEV_MEM_CLOCK | gauge | Memory clock speed |
```

### Appendix B: Grafana Dashboard JSON

See the `OPERATIONS_DASHBOARD_JSON` and `EXECUTIVE_DASHBOARD_JSON` in section 10.1.4.

### Appendix C: Alert Rule Templates

```yaml
# prometheus-rules.yml
groups:
  - name: llm-slo
    rules:
      - alert: LLMSLOLatencyBreach
        expr: |
          (
            sum(rate(vllm_e2e_request_latency_seconds_bucket{le="5"}[5m]))
            /
            sum(rate(vllm_e2e_request_latency_seconds_count[5m]))
          ) < 0.95
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: "SLO breach: <95% requests under 5s"

      - alert: LLMSLOAvailabilityBreach
        expr: |
          (
            sum(rate(vllm_requests_total{status="success"}[1h]))
            /
            sum(rate(vllm_requests_total[1h]))
          ) < 0.999
        for: 30m
        labels:
          severity: critical
        annotations:
          summary: "SLO breach: Availability below 99.9%"
```

### Appendix D: On-Call Playbooks

```markdown
# On-Call Playbook: High Latency Alert

## Symptoms
- P99 latency exceeds 10s threshold
- Users experiencing slow responses

## Investigation Steps

1. **Check queue depth**
   ```promql
   vllm_pending_requests
   ```
   If high (>50): Consider scaling up replicas

2. **Check GPU utilization**
   ```promql
   nvidia_gpu_utilization
   ```
   If >95%: GPU is bottleneck, scale up

3. **Check KV cache usage**
   ```promql
   vllm_gpu_cache_usage_perc
   ```
   If >90%: Memory pressure, reduce max_num_seqs

4. **Check for batch size issues**
   ```promql
   histogram_quantile(0.99, vllm_batch_size_bucket)
   ```
   If very high: May need to tune batching params

## Mitigation Actions

- **Immediate**: Scale up model replicas
- **Short-term**: Adjust KV cache configuration
- **Long-term**: Review capacity planning

## Escalation
If unresolved after 15 minutes, escalate to platform team lead.
```

---

## Summary

This guide covered:

1. **Monitoring Dimensions**: System, application, model, and business metrics
2. **Key Metrics**: TTFT, TPOT, latency, throughput, GPU utilization, queue depth
3. **Monitoring Stack**: Prometheus, Grafana, OpenTelemetry, DCGM
4. **Dashboard Design**: Operations, executive, model performance dashboards
5. **Alerting Strategy**: Alert rules, severity levels, fatigue prevention

Key takeaways:
- Monitor all four dimensions for complete observability
- TTFT and TPOT are critical LLM-specific metrics beyond standard latency
- GPU metrics are essential for capacity planning and cost optimization
- Tiered alerting prevents fatigue while ensuring critical issues get attention
- Dashboard design should match audience (operations vs executive)

---

> **Navigation**
> [← 9.3 API Design](../09_inference_serving/9.3_api_design_llm_services_guide.md) | **[Index](../README.md#15-repository-structure)** | [10.2 Logging & Tracing →](10.2_llm_logging_tracing_guide.md)
