# Document 10.3: Model Quality Monitoring Guide

## Purpose

This guide provides comprehensive strategies for continuous monitoring of LLM model quality in production. It covers quality dimensions, online evaluation, drift detection, feedback integration, and alerting systems to ensure models maintain expected performance over time.

## Prerequisites

- Understanding of LLM evaluation metrics (Document 5.1)
- Familiarity with monitoring systems (Document 10.1)
- Knowledge of statistical concepts (distributions, hypothesis testing)
- Experience with feedback collection systems

## 10.3.1 Quality Monitoring Dimensions

### Multi-Dimensional Quality Framework

```python
"""
Comprehensive quality monitoring framework for LLM systems.
"""

from dataclasses import dataclass, field
from typing import Dict, Any, List, Optional, Callable
from enum import Enum
from datetime import datetime, timedelta
import numpy as np
from abc import ABC, abstractmethod


class QualityDimension(Enum):
    """Quality dimensions for LLM monitoring."""
    OUTPUT_QUALITY = "output_quality"       # Response quality and relevance
    SAFETY = "safety"                       # Harmful content, jailbreaks
    CONSISTENCY = "consistency"             # Reproducibility, coherence
    USER_SATISFACTION = "user_satisfaction" # User-reported satisfaction
    FACTUAL_ACCURACY = "factual_accuracy"   # Correctness of information
    INSTRUCTION_FOLLOWING = "instruction_following"  # Task completion
    LATENCY_QUALITY = "latency_quality"     # Response time impact on UX


@dataclass
class QualityMetric:
    """Definition of a quality metric."""
    name: str
    dimension: QualityDimension
    description: str
    target: float
    warning_threshold: float
    critical_threshold: float
    higher_is_better: bool = True
    aggregation: str = "mean"  # mean, median, p95, etc.


@dataclass
class QualityScore:
    """Quality score for a single evaluation."""
    metric_name: str
    value: float
    timestamp: datetime
    request_id: str
    model_id: str
    metadata: Dict[str, Any] = field(default_factory=dict)


class QualityMetricRegistry:
    """
    Registry of quality metrics for LLM monitoring.
    """

    # Standard metrics by dimension
    STANDARD_METRICS = {
        QualityDimension.OUTPUT_QUALITY: [
            QualityMetric(
                name="relevance_score",
                dimension=QualityDimension.OUTPUT_QUALITY,
                description="How relevant the response is to the query",
                target=0.85,
                warning_threshold=0.75,
                critical_threshold=0.65
            ),
            QualityMetric(
                name="coherence_score",
                dimension=QualityDimension.OUTPUT_QUALITY,
                description="Logical coherence of the response",
                target=0.90,
                warning_threshold=0.80,
                critical_threshold=0.70
            ),
            QualityMetric(
                name="completeness_score",
                dimension=QualityDimension.OUTPUT_QUALITY,
                description="How complete the response addresses the query",
                target=0.85,
                warning_threshold=0.75,
                critical_threshold=0.65
            ),
        ],
        QualityDimension.SAFETY: [
            QualityMetric(
                name="toxicity_score",
                dimension=QualityDimension.SAFETY,
                description="Absence of toxic content (1 = safe)",
                target=0.99,
                warning_threshold=0.97,
                critical_threshold=0.95
            ),
            QualityMetric(
                name="jailbreak_resistance",
                dimension=QualityDimension.SAFETY,
                description="Resistance to jailbreak attempts",
                target=0.99,
                warning_threshold=0.98,
                critical_threshold=0.95
            ),
            QualityMetric(
                name="pii_leakage_score",
                dimension=QualityDimension.SAFETY,
                description="Absence of PII in responses (1 = no PII)",
                target=1.0,
                warning_threshold=0.99,
                critical_threshold=0.98
            ),
        ],
        QualityDimension.CONSISTENCY: [
            QualityMetric(
                name="self_consistency",
                dimension=QualityDimension.CONSISTENCY,
                description="Consistency across multiple generations",
                target=0.90,
                warning_threshold=0.80,
                critical_threshold=0.70
            ),
            QualityMetric(
                name="format_consistency",
                dimension=QualityDimension.CONSISTENCY,
                description="Consistency of output format",
                target=0.95,
                warning_threshold=0.90,
                critical_threshold=0.85
            ),
        ],
        QualityDimension.USER_SATISFACTION: [
            QualityMetric(
                name="thumbs_up_rate",
                dimension=QualityDimension.USER_SATISFACTION,
                description="Percentage of positive user ratings",
                target=0.85,
                warning_threshold=0.75,
                critical_threshold=0.65
            ),
            QualityMetric(
                name="regeneration_rate",
                dimension=QualityDimension.USER_SATISFACTION,
                description="Rate of users requesting regeneration (lower is better)",
                target=0.05,
                warning_threshold=0.10,
                critical_threshold=0.15,
                higher_is_better=False
            ),
        ],
        QualityDimension.FACTUAL_ACCURACY: [
            QualityMetric(
                name="factual_accuracy",
                dimension=QualityDimension.FACTUAL_ACCURACY,
                description="Accuracy of factual claims",
                target=0.95,
                warning_threshold=0.90,
                critical_threshold=0.85
            ),
            QualityMetric(
                name="hallucination_rate",
                dimension=QualityDimension.FACTUAL_ACCURACY,
                description="Rate of hallucinated content (lower is better)",
                target=0.02,
                warning_threshold=0.05,
                critical_threshold=0.10,
                higher_is_better=False
            ),
        ],
        QualityDimension.INSTRUCTION_FOLLOWING: [
            QualityMetric(
                name="instruction_compliance",
                dimension=QualityDimension.INSTRUCTION_FOLLOWING,
                description="Rate of following user instructions",
                target=0.95,
                warning_threshold=0.90,
                critical_threshold=0.85
            ),
            QualityMetric(
                name="task_completion_rate",
                dimension=QualityDimension.INSTRUCTION_FOLLOWING,
                description="Rate of successfully completed tasks",
                target=0.90,
                warning_threshold=0.85,
                critical_threshold=0.75
            ),
        ],
    }

    def __init__(self):
        self.metrics: Dict[str, QualityMetric] = {}
        self._register_standard_metrics()

    def _register_standard_metrics(self):
        """Register all standard metrics."""
        for dimension, metrics in self.STANDARD_METRICS.items():
            for metric in metrics:
                self.metrics[metric.name] = metric

    def register_metric(self, metric: QualityMetric):
        """Register a custom metric."""
        self.metrics[metric.name] = metric

    def get_metric(self, name: str) -> Optional[QualityMetric]:
        """Get metric by name."""
        return self.metrics.get(name)

    def get_metrics_by_dimension(self, dimension: QualityDimension) -> List[QualityMetric]:
        """Get all metrics for a dimension."""
        return [m for m in self.metrics.values() if m.dimension == dimension]

    def evaluate_against_threshold(
        self,
        metric_name: str,
        value: float
    ) -> str:
        """Evaluate metric value against thresholds."""
        metric = self.get_metric(metric_name)
        if not metric:
            return "unknown"

        if metric.higher_is_better:
            if value >= metric.target:
                return "healthy"
            elif value >= metric.warning_threshold:
                return "warning"
            elif value >= metric.critical_threshold:
                return "critical"
            else:
                return "severe"
        else:
            if value <= metric.target:
                return "healthy"
            elif value <= metric.warning_threshold:
                return "warning"
            elif value <= metric.critical_threshold:
                return "critical"
            else:
                return "severe"


class QualityMonitor:
    """
    Central quality monitoring system for LLM services.
    """

    def __init__(
        self,
        registry: QualityMetricRegistry = None,
        storage_backend: 'QualityStorageBackend' = None
    ):
        self.registry = registry or QualityMetricRegistry()
        self.storage = storage_backend
        self.evaluators: Dict[str, 'QualityEvaluator'] = {}

    def register_evaluator(self, metric_name: str, evaluator: 'QualityEvaluator'):
        """Register an evaluator for a metric."""
        self.evaluators[metric_name] = evaluator

    async def evaluate_response(
        self,
        request_id: str,
        model_id: str,
        prompt: str,
        response: str,
        metadata: Dict[str, Any] = None
    ) -> Dict[str, QualityScore]:
        """
        Evaluate a response across all registered metrics.
        """
        scores = {}
        timestamp = datetime.utcnow()

        for metric_name, evaluator in self.evaluators.items():
            try:
                value = await evaluator.evaluate(prompt, response, metadata)
                score = QualityScore(
                    metric_name=metric_name,
                    value=value,
                    timestamp=timestamp,
                    request_id=request_id,
                    model_id=model_id,
                    metadata=metadata or {}
                )
                scores[metric_name] = score

                # Store score
                if self.storage:
                    await self.storage.store_score(score)

            except Exception as e:
                # Log error but continue with other evaluators
                print(f"Error evaluating {metric_name}: {e}")

        return scores

    def get_quality_summary(
        self,
        model_id: str,
        time_range: tuple[datetime, datetime]
    ) -> Dict[str, Any]:
        """
        Get quality summary for a model over time range.
        """
        summary = {
            "model_id": model_id,
            "time_range": {
                "start": time_range[0].isoformat(),
                "end": time_range[1].isoformat()
            },
            "dimensions": {},
            "overall_health": "healthy"
        }

        worst_status = "healthy"
        status_order = {"healthy": 0, "warning": 1, "critical": 2, "severe": 3}

        for dimension in QualityDimension:
            metrics = self.registry.get_metrics_by_dimension(dimension)
            dimension_scores = {}

            for metric in metrics:
                if self.storage:
                    values = self.storage.get_metric_values(
                        metric.name, model_id, time_range
                    )
                    if values:
                        avg_value = np.mean(values)
                        status = self.registry.evaluate_against_threshold(
                            metric.name, avg_value
                        )
                        dimension_scores[metric.name] = {
                            "value": avg_value,
                            "status": status,
                            "target": metric.target,
                            "sample_count": len(values)
                        }

                        if status_order.get(status, 0) > status_order.get(worst_status, 0):
                            worst_status = status

            summary["dimensions"][dimension.value] = dimension_scores

        summary["overall_health"] = worst_status
        return summary
```

### Output Quality Metrics

```python
"""
Output quality evaluation metrics for LLM responses.
"""

from abc import ABC, abstractmethod
from typing import Dict, Any, Optional, List
import re
import asyncio


class QualityEvaluator(ABC):
    """Base class for quality evaluators."""

    @abstractmethod
    async def evaluate(
        self,
        prompt: str,
        response: str,
        metadata: Optional[Dict[str, Any]] = None
    ) -> float:
        """
        Evaluate quality and return score between 0 and 1.
        """
        pass


class RelevanceEvaluator(QualityEvaluator):
    """
    Evaluates relevance of response to the prompt.
    """

    def __init__(
        self,
        judge_model: str = "gpt-4",
        api_client: Any = None
    ):
        self.judge_model = judge_model
        self.api_client = api_client

    async def evaluate(
        self,
        prompt: str,
        response: str,
        metadata: Optional[Dict[str, Any]] = None
    ) -> float:
        """Evaluate relevance using LLM-as-judge."""
        judge_prompt = f"""
        Rate the relevance of the following response to the user's query.

        User Query: {prompt}

        Response: {response}

        Rate the relevance on a scale of 1-5:
        1 = Completely irrelevant
        2 = Mostly irrelevant with some tangential connection
        3 = Partially relevant but missing key aspects
        4 = Mostly relevant with minor gaps
        5 = Fully relevant and addresses the query

        Respond with only a number (1-5).
        """

        if self.api_client:
            result = await self.api_client.chat(
                model=self.judge_model,
                messages=[{"role": "user", "content": judge_prompt}],
                max_tokens=10,
                temperature=0
            )
            try:
                score = int(result.content.strip())
                return (score - 1) / 4  # Normalize to 0-1
            except ValueError:
                return 0.5  # Default if parsing fails

        return 0.5  # Default if no API client


class CoherenceEvaluator(QualityEvaluator):
    """
    Evaluates logical coherence of responses.
    """

    def __init__(self, judge_model: str = "gpt-4", api_client: Any = None):
        self.judge_model = judge_model
        self.api_client = api_client

    async def evaluate(
        self,
        prompt: str,
        response: str,
        metadata: Optional[Dict[str, Any]] = None
    ) -> float:
        """Evaluate coherence using LLM-as-judge."""
        judge_prompt = f"""
        Evaluate the logical coherence of this response.

        Response: {response}

        Consider:
        - Does the response have a clear structure?
        - Do the ideas flow logically?
        - Are there any contradictions?
        - Is the reasoning sound?

        Rate coherence on a scale of 1-5:
        1 = Incoherent, contradictory, or illogical
        2 = Poor coherence with significant issues
        3 = Acceptable coherence with some gaps
        4 = Good coherence with minor issues
        5 = Excellent coherence, clear and logical

        Respond with only a number (1-5).
        """

        if self.api_client:
            result = await self.api_client.chat(
                model=self.judge_model,
                messages=[{"role": "user", "content": judge_prompt}],
                max_tokens=10,
                temperature=0
            )
            try:
                score = int(result.content.strip())
                return (score - 1) / 4
            except ValueError:
                return 0.5

        return 0.5


class FastQualityMetrics:
    """
    Fast quality metrics that don't require LLM evaluation.
    """

    @staticmethod
    def response_length_score(response: str, expected_range: tuple[int, int]) -> float:
        """
        Score based on response length being in expected range.
        """
        length = len(response)
        min_len, max_len = expected_range

        if min_len <= length <= max_len:
            return 1.0
        elif length < min_len:
            return max(0, length / min_len)
        else:
            return max(0, 1 - (length - max_len) / max_len)

    @staticmethod
    def format_compliance_score(response: str, expected_format: str) -> float:
        """
        Check if response matches expected format.

        expected_format can be: "json", "markdown", "code", "plain"
        """
        if expected_format == "json":
            try:
                import json
                json.loads(response)
                return 1.0
            except json.JSONDecodeError:
                return 0.0

        elif expected_format == "markdown":
            # Check for markdown elements
            markdown_patterns = [
                r'^#{1,6}\s',  # Headers
                r'\*\*.*\*\*',  # Bold
                r'\*.*\*',  # Italic
                r'^\s*[-*]\s',  # Lists
                r'```',  # Code blocks
            ]
            matches = sum(1 for p in markdown_patterns if re.search(p, response, re.MULTILINE))
            return min(1.0, matches / 3)  # Expect at least 3 markdown elements

        elif expected_format == "code":
            # Check for code patterns
            code_indicators = [
                r'def\s+\w+\s*\(',  # Python function
                r'function\s+\w+\s*\(',  # JS function
                r'class\s+\w+',  # Class definition
                r'import\s+',  # Import statement
                r'{\s*\n',  # Code blocks
            ]
            matches = sum(1 for p in code_indicators if re.search(p, response))
            return min(1.0, matches / 2)

        return 1.0  # Plain text always passes

    @staticmethod
    def repetition_score(response: str, ngram_size: int = 3) -> float:
        """
        Detect repetitive content (lower repetition = higher score).
        """
        words = response.lower().split()
        if len(words) < ngram_size:
            return 1.0

        ngrams = []
        for i in range(len(words) - ngram_size + 1):
            ngrams.append(tuple(words[i:i + ngram_size]))

        unique_ratio = len(set(ngrams)) / len(ngrams)
        return unique_ratio

    @staticmethod
    def language_score(response: str, expected_language: str = "en") -> float:
        """
        Check if response is in expected language.
        """
        try:
            from langdetect import detect
            detected = detect(response)
            return 1.0 if detected == expected_language else 0.0
        except Exception:
            return 1.0  # Assume correct if detection fails
```

## 10.3.2 Online Evaluation

### Real-Time Quality Scoring

```python
"""
Real-time quality scoring for production LLM systems.
"""

from dataclasses import dataclass
from typing import Dict, Any, List, Optional, Callable
from datetime import datetime
import asyncio
import random
from collections import deque
import numpy as np


@dataclass
class OnlineEvaluationConfig:
    """Configuration for online evaluation."""
    sampling_rate: float = 0.1  # Evaluate 10% of requests
    async_evaluation: bool = True  # Run evaluation asynchronously
    max_concurrent_evals: int = 10
    evaluation_timeout: float = 30.0  # seconds
    priority_sampling: bool = True  # Prioritize certain request types


class SamplingStrategy:
    """
    Sampling strategies for online evaluation.
    """

    @staticmethod
    def uniform_sampling(rate: float) -> bool:
        """Simple uniform random sampling."""
        return random.random() < rate

    @staticmethod
    def stratified_sampling(
        request: Dict[str, Any],
        strata_rates: Dict[str, float]
    ) -> bool:
        """
        Sample at different rates for different request types.
        """
        # Determine stratum
        model = request.get("model", "default")
        user_tier = request.get("user_tier", "standard")
        request_type = request.get("type", "default")

        # Get sampling rate for this stratum
        stratum_key = f"{model}:{user_tier}:{request_type}"
        rate = strata_rates.get(stratum_key, strata_rates.get("default", 0.1))

        return random.random() < rate

    @staticmethod
    def importance_sampling(
        request: Dict[str, Any],
        base_rate: float = 0.1
    ) -> tuple[bool, float]:
        """
        Importance sampling with higher rates for important requests.
        Returns (should_sample, weight).
        """
        importance_score = 1.0

        # Higher importance for enterprise users
        if request.get("user_tier") == "enterprise":
            importance_score *= 2.0

        # Higher importance for long prompts
        prompt_tokens = request.get("prompt_tokens", 0)
        if prompt_tokens > 1000:
            importance_score *= 1.5

        # Higher importance for certain use cases
        if request.get("use_case") in ["medical", "legal", "financial"]:
            importance_score *= 3.0

        adjusted_rate = min(1.0, base_rate * importance_score)
        should_sample = random.random() < adjusted_rate

        # Weight for unbiased estimation
        weight = 1.0 / adjusted_rate if should_sample else 0.0

        return should_sample, weight


class OnlineEvaluator:
    """
    Real-time quality evaluation system.
    """

    def __init__(
        self,
        config: OnlineEvaluationConfig,
        evaluators: Dict[str, QualityEvaluator],
        storage: 'QualityStorageBackend' = None
    ):
        self.config = config
        self.evaluators = evaluators
        self.storage = storage
        self._semaphore = asyncio.Semaphore(config.max_concurrent_evals)
        self._pending_evals: deque = deque(maxlen=10000)
        self._running = False

    async def start(self):
        """Start the background evaluation worker."""
        self._running = True
        asyncio.create_task(self._evaluation_worker())

    async def stop(self):
        """Stop the evaluation worker."""
        self._running = False

    async def maybe_evaluate(
        self,
        request_id: str,
        model_id: str,
        prompt: str,
        response: str,
        metadata: Dict[str, Any] = None
    ) -> Optional[str]:
        """
        Conditionally queue response for evaluation based on sampling.
        Returns evaluation ID if queued, None otherwise.
        """
        # Apply sampling
        request_data = {"model": model_id, **(metadata or {})}

        if self.config.priority_sampling:
            should_sample, weight = SamplingStrategy.importance_sampling(
                request_data, self.config.sampling_rate
            )
        else:
            should_sample = SamplingStrategy.uniform_sampling(self.config.sampling_rate)
            weight = 1.0 / self.config.sampling_rate

        if not should_sample:
            return None

        # Queue evaluation
        eval_task = {
            "eval_id": f"eval_{request_id}",
            "request_id": request_id,
            "model_id": model_id,
            "prompt": prompt,
            "response": response,
            "metadata": metadata or {},
            "weight": weight,
            "queued_at": datetime.utcnow()
        }

        if self.config.async_evaluation:
            self._pending_evals.append(eval_task)
            return eval_task["eval_id"]
        else:
            # Synchronous evaluation
            await self._run_evaluation(eval_task)
            return eval_task["eval_id"]

    async def _evaluation_worker(self):
        """Background worker for processing evaluation queue."""
        while self._running:
            if self._pending_evals:
                eval_task = self._pending_evals.popleft()
                asyncio.create_task(self._run_evaluation(eval_task))

            await asyncio.sleep(0.1)

    async def _run_evaluation(self, eval_task: Dict[str, Any]):
        """Run evaluation for a single task."""
        async with self._semaphore:
            try:
                results = {}

                for metric_name, evaluator in self.evaluators.items():
                    try:
                        score = await asyncio.wait_for(
                            evaluator.evaluate(
                                eval_task["prompt"],
                                eval_task["response"],
                                eval_task["metadata"]
                            ),
                            timeout=self.config.evaluation_timeout
                        )

                        results[metric_name] = {
                            "score": score,
                            "weight": eval_task["weight"]
                        }

                    except asyncio.TimeoutError:
                        results[metric_name] = {"error": "timeout"}
                    except Exception as e:
                        results[metric_name] = {"error": str(e)}

                # Store results
                if self.storage:
                    await self.storage.store_evaluation(
                        eval_id=eval_task["eval_id"],
                        request_id=eval_task["request_id"],
                        model_id=eval_task["model_id"],
                        results=results,
                        timestamp=datetime.utcnow()
                    )

            except Exception as e:
                print(f"Evaluation error: {e}")


class LLMJudgeOnline:
    """
    LLM-as-judge for online quality evaluation.
    """

    def __init__(
        self,
        judge_model: str = "gpt-4-turbo",
        api_client: Any = None,
        cache: 'EvaluationCache' = None
    ):
        self.judge_model = judge_model
        self.api_client = api_client
        self.cache = cache

    async def evaluate_multiple(
        self,
        prompt: str,
        response: str,
        criteria: List[str]
    ) -> Dict[str, float]:
        """
        Evaluate response on multiple criteria in a single call.
        """
        # Check cache
        cache_key = self._cache_key(prompt, response, criteria)
        if self.cache:
            cached = await self.cache.get(cache_key)
            if cached:
                return cached

        # Build evaluation prompt
        criteria_list = "\n".join(f"- {c}" for c in criteria)
        judge_prompt = f"""
        Evaluate the following response on multiple criteria.

        User Query: {prompt[:500]}...

        Response: {response[:1000]}...

        Criteria to evaluate:
        {criteria_list}

        For each criterion, provide a score from 1-5 where:
        1 = Very poor
        2 = Poor
        3 = Acceptable
        4 = Good
        5 = Excellent

        Respond in JSON format:
        {{"criterion_name": score, ...}}
        """

        if self.api_client:
            result = await self.api_client.chat(
                model=self.judge_model,
                messages=[{"role": "user", "content": judge_prompt}],
                max_tokens=200,
                temperature=0
            )

            try:
                import json
                scores_raw = json.loads(result.content)
                # Normalize to 0-1
                scores = {k: (v - 1) / 4 for k, v in scores_raw.items()}

                if self.cache:
                    await self.cache.set(cache_key, scores)

                return scores
            except (json.JSONDecodeError, KeyError):
                return {c: 0.5 for c in criteria}

        return {c: 0.5 for c in criteria}

    def _cache_key(self, prompt: str, response: str, criteria: List[str]) -> str:
        """Generate cache key for evaluation."""
        import hashlib
        content = f"{prompt[:200]}|{response[:200]}|{','.join(sorted(criteria))}"
        return hashlib.sha256(content.encode()).hexdigest()
```

### Fast Metrics Implementation

```python
"""
Fast metrics that can be computed without LLM evaluation.
"""

from typing import Dict, Any, List, Optional
import re
import json
from dataclasses import dataclass


@dataclass
class FastMetricResult:
    """Result from a fast metric evaluation."""
    name: str
    value: float
    details: Dict[str, Any]


class FastMetricsEngine:
    """
    Engine for computing fast quality metrics.
    """

    def evaluate_all(
        self,
        prompt: str,
        response: str,
        expected_format: Optional[str] = None
    ) -> Dict[str, FastMetricResult]:
        """
        Compute all fast metrics for a response.
        """
        results = {}

        # Length metrics
        results["response_length"] = self._eval_length(response)
        results["response_word_count"] = self._eval_word_count(response)

        # Structure metrics
        results["repetition"] = self._eval_repetition(response)
        results["sentence_count"] = self._eval_sentence_count(response)

        # Format compliance
        if expected_format:
            results["format_compliance"] = self._eval_format(response, expected_format)

        # Content metrics
        results["contains_refusal"] = self._eval_refusal(response)
        results["contains_uncertainty"] = self._eval_uncertainty(response)
        results["contains_code"] = self._eval_code_presence(response)

        # Safety quick checks
        results["url_presence"] = self._eval_url_presence(response)
        results["email_presence"] = self._eval_email_presence(response)

        return results

    def _eval_length(self, response: str) -> FastMetricResult:
        """Evaluate response length."""
        length = len(response)
        # Score based on reasonable length (100-2000 chars ideal)
        if 100 <= length <= 2000:
            score = 1.0
        elif length < 100:
            score = length / 100
        else:
            score = max(0, 1 - (length - 2000) / 5000)

        return FastMetricResult(
            name="response_length",
            value=score,
            details={"char_count": length}
        )

    def _eval_word_count(self, response: str) -> FastMetricResult:
        """Evaluate word count."""
        words = response.split()
        word_count = len(words)

        # Score based on reasonable word count (20-500 words ideal)
        if 20 <= word_count <= 500:
            score = 1.0
        elif word_count < 20:
            score = word_count / 20
        else:
            score = max(0, 1 - (word_count - 500) / 1000)

        return FastMetricResult(
            name="word_count",
            value=score,
            details={"word_count": word_count}
        )

    def _eval_repetition(self, response: str) -> FastMetricResult:
        """Evaluate repetition (lower repetition = higher score)."""
        words = response.lower().split()
        if len(words) < 5:
            return FastMetricResult(
                name="repetition",
                value=1.0,
                details={"unique_ratio": 1.0}
            )

        # Check trigram repetition
        trigrams = [tuple(words[i:i+3]) for i in range(len(words)-2)]
        unique_ratio = len(set(trigrams)) / len(trigrams) if trigrams else 1.0

        # Check for repeated phrases
        repeated_phrases = self._find_repeated_phrases(response)

        return FastMetricResult(
            name="repetition",
            value=unique_ratio,
            details={
                "unique_trigram_ratio": unique_ratio,
                "repeated_phrases": repeated_phrases[:5]
            }
        )

    def _find_repeated_phrases(self, text: str, min_length: int = 5) -> List[str]:
        """Find repeated phrases in text."""
        words = text.split()
        phrases = []

        for length in range(min_length, min(20, len(words) // 2)):
            for i in range(len(words) - length):
                phrase = " ".join(words[i:i+length])
                remaining = " ".join(words[i+length:])
                if phrase in remaining:
                    phrases.append(phrase)

        return list(set(phrases))

    def _eval_sentence_count(self, response: str) -> FastMetricResult:
        """Evaluate sentence structure."""
        sentences = re.split(r'[.!?]+', response)
        sentences = [s.strip() for s in sentences if s.strip()]
        count = len(sentences)

        # Calculate average sentence length
        avg_length = np.mean([len(s.split()) for s in sentences]) if sentences else 0

        return FastMetricResult(
            name="sentence_count",
            value=1.0 if 2 <= count <= 20 else 0.5,
            details={
                "count": count,
                "avg_words_per_sentence": avg_length
            }
        )

    def _eval_format(self, response: str, expected_format: str) -> FastMetricResult:
        """Evaluate format compliance."""
        if expected_format == "json":
            try:
                json.loads(response)
                return FastMetricResult("format_compliance", 1.0, {"valid_json": True})
            except json.JSONDecodeError as e:
                return FastMetricResult("format_compliance", 0.0, {"error": str(e)})

        elif expected_format == "markdown":
            md_elements = {
                "headers": len(re.findall(r'^#{1,6}\s', response, re.MULTILINE)),
                "bold": len(re.findall(r'\*\*[^*]+\*\*', response)),
                "lists": len(re.findall(r'^\s*[-*]\s', response, re.MULTILINE)),
                "code_blocks": len(re.findall(r'```', response)) // 2
            }
            total_elements = sum(md_elements.values())
            score = min(1.0, total_elements / 3)
            return FastMetricResult("format_compliance", score, md_elements)

        return FastMetricResult("format_compliance", 1.0, {"format": "plain"})

    def _eval_refusal(self, response: str) -> FastMetricResult:
        """Check if response contains refusal patterns."""
        refusal_patterns = [
            r"I cannot",
            r"I'm unable to",
            r"I am not able to",
            r"I don't have the ability",
            r"I can't help with",
            r"I'm not allowed to",
            r"against my guidelines",
            r"I must decline"
        ]

        for pattern in refusal_patterns:
            if re.search(pattern, response, re.IGNORECASE):
                return FastMetricResult(
                    "contains_refusal",
                    1.0,
                    {"pattern_matched": pattern}
                )

        return FastMetricResult("contains_refusal", 0.0, {})

    def _eval_uncertainty(self, response: str) -> FastMetricResult:
        """Check if response contains uncertainty indicators."""
        uncertainty_patterns = [
            r"I'm not sure",
            r"I don't know",
            r"I'm uncertain",
            r"It's possible that",
            r"might be",
            r"could be",
            r"I believe",
            r"I think",
            r"approximately",
            r"roughly"
        ]

        matches = []
        for pattern in uncertainty_patterns:
            if re.search(pattern, response, re.IGNORECASE):
                matches.append(pattern)

        score = len(matches) / len(uncertainty_patterns)
        return FastMetricResult(
            "contains_uncertainty",
            score,
            {"patterns_matched": matches}
        )

    def _eval_code_presence(self, response: str) -> FastMetricResult:
        """Detect code blocks in response."""
        code_block_pattern = r'```[\s\S]*?```'
        inline_code_pattern = r'`[^`]+`'

        code_blocks = re.findall(code_block_pattern, response)
        inline_code = re.findall(inline_code_pattern, response)

        return FastMetricResult(
            "contains_code",
            1.0 if code_blocks or inline_code else 0.0,
            {
                "code_blocks": len(code_blocks),
                "inline_code": len(inline_code)
            }
        )

    def _eval_url_presence(self, response: str) -> FastMetricResult:
        """Check for URLs in response."""
        url_pattern = r'https?://[^\s<>"\']+|www\.[^\s<>"\']+\.[^\s<>"\']+'
        urls = re.findall(url_pattern, response)

        return FastMetricResult(
            "url_presence",
            1.0 if urls else 0.0,
            {"urls": urls[:5]}
        )

    def _eval_email_presence(self, response: str) -> FastMetricResult:
        """Check for email addresses in response."""
        email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        emails = re.findall(email_pattern, response)

        return FastMetricResult(
            "email_presence",
            1.0 if emails else 0.0,
            {"email_count": len(emails)}
        )
```

## 10.3.3 Drift Detection

### Input Distribution Drift

```python
"""
Drift detection for LLM input and output distributions.
"""

from dataclasses import dataclass
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime, timedelta
from enum import Enum
import numpy as np
from scipy import stats
from collections import defaultdict


class DriftType(Enum):
    """Types of drift to detect."""
    INPUT_DISTRIBUTION = "input_distribution"
    OUTPUT_DISTRIBUTION = "output_distribution"
    PERFORMANCE = "performance"
    EMBEDDING = "embedding"


@dataclass
class DriftResult:
    """Result of drift detection."""
    drift_type: DriftType
    detected: bool
    severity: str  # "none", "low", "medium", "high"
    metric_name: str
    baseline_value: float
    current_value: float
    p_value: Optional[float] = None
    details: Dict[str, Any] = None


class InputDriftDetector:
    """
    Detect drift in input distributions.
    """

    def __init__(
        self,
        window_size: int = 1000,
        significance_level: float = 0.05
    ):
        self.window_size = window_size
        self.significance_level = significance_level
        self.baseline_stats: Dict[str, Any] = {}

    def set_baseline(self, features: Dict[str, List[float]]):
        """
        Set baseline statistics from reference data.
        """
        for feature_name, values in features.items():
            self.baseline_stats[feature_name] = {
                "mean": np.mean(values),
                "std": np.std(values),
                "min": np.min(values),
                "max": np.max(values),
                "percentiles": np.percentile(values, [25, 50, 75]),
                "histogram": np.histogram(values, bins=50),
                "values": values[-self.window_size:]  # Keep recent for KS test
            }

    def detect_drift(
        self,
        current_features: Dict[str, List[float]]
    ) -> List[DriftResult]:
        """
        Detect drift in current features compared to baseline.
        """
        results = []

        for feature_name, current_values in current_features.items():
            if feature_name not in self.baseline_stats:
                continue

            baseline = self.baseline_stats[feature_name]

            # Kolmogorov-Smirnov test
            ks_result = self._ks_test(baseline["values"], current_values)

            # Population Stability Index (PSI)
            psi = self._calculate_psi(baseline["histogram"], current_values)

            # Mean shift detection
            mean_shift = abs(np.mean(current_values) - baseline["mean"]) / baseline["std"]

            # Determine severity
            severity = self._determine_severity(ks_result, psi, mean_shift)

            results.append(DriftResult(
                drift_type=DriftType.INPUT_DISTRIBUTION,
                detected=severity != "none",
                severity=severity,
                metric_name=feature_name,
                baseline_value=baseline["mean"],
                current_value=np.mean(current_values),
                p_value=ks_result.pvalue,
                details={
                    "ks_statistic": ks_result.statistic,
                    "psi": psi,
                    "mean_shift_zscore": mean_shift,
                    "baseline_std": baseline["std"]
                }
            ))

        return results

    def _ks_test(
        self,
        baseline: List[float],
        current: List[float]
    ) -> stats.KstestResult:
        """Perform Kolmogorov-Smirnov test."""
        return stats.ks_2samp(baseline, current)

    def _calculate_psi(
        self,
        baseline_hist: Tuple[np.ndarray, np.ndarray],
        current_values: List[float]
    ) -> float:
        """Calculate Population Stability Index."""
        baseline_counts, bin_edges = baseline_hist

        # Create histogram for current values using same bins
        current_counts, _ = np.histogram(current_values, bins=bin_edges)

        # Convert to proportions
        baseline_prop = baseline_counts / (baseline_counts.sum() + 1e-10)
        current_prop = current_counts / (current_counts.sum() + 1e-10)

        # Add small constant to avoid log(0)
        baseline_prop = np.clip(baseline_prop, 1e-10, 1)
        current_prop = np.clip(current_prop, 1e-10, 1)

        # Calculate PSI
        psi = np.sum((current_prop - baseline_prop) * np.log(current_prop / baseline_prop))

        return psi

    def _determine_severity(
        self,
        ks_result: stats.KstestResult,
        psi: float,
        mean_shift: float
    ) -> str:
        """Determine drift severity based on multiple indicators."""
        score = 0

        # KS test
        if ks_result.pvalue < 0.001:
            score += 3
        elif ks_result.pvalue < 0.01:
            score += 2
        elif ks_result.pvalue < self.significance_level:
            score += 1

        # PSI thresholds (industry standard)
        if psi > 0.25:
            score += 3
        elif psi > 0.1:
            score += 2
        elif psi > 0.05:
            score += 1

        # Mean shift
        if mean_shift > 3:
            score += 3
        elif mean_shift > 2:
            score += 2
        elif mean_shift > 1:
            score += 1

        # Map score to severity
        if score >= 6:
            return "high"
        elif score >= 3:
            return "medium"
        elif score >= 1:
            return "low"
        return "none"


class OutputDriftDetector:
    """
    Detect drift in output distributions.
    """

    def __init__(self, window_size: int = 1000):
        self.window_size = window_size
        self.baseline_stats: Dict[str, Any] = {}

    def set_baseline(self, outputs: Dict[str, List[Any]]):
        """Set baseline output statistics."""
        for metric_name, values in outputs.items():
            if isinstance(values[0], (int, float)):
                self.baseline_stats[metric_name] = {
                    "type": "numeric",
                    "mean": np.mean(values),
                    "std": np.std(values),
                    "values": values[-self.window_size:]
                }
            elif isinstance(values[0], str):
                # For categorical outputs (like finish_reason)
                value_counts = defaultdict(int)
                for v in values:
                    value_counts[v] += 1
                total = len(values)

                self.baseline_stats[metric_name] = {
                    "type": "categorical",
                    "distribution": {k: v/total for k, v in value_counts.items()},
                    "total": total
                }

    def detect_drift(
        self,
        current_outputs: Dict[str, List[Any]]
    ) -> List[DriftResult]:
        """Detect drift in current outputs."""
        results = []

        for metric_name, current_values in current_outputs.items():
            if metric_name not in self.baseline_stats:
                continue

            baseline = self.baseline_stats[metric_name]

            if baseline["type"] == "numeric":
                result = self._detect_numeric_drift(metric_name, baseline, current_values)
            else:
                result = self._detect_categorical_drift(metric_name, baseline, current_values)

            results.append(result)

        return results

    def _detect_numeric_drift(
        self,
        metric_name: str,
        baseline: Dict[str, Any],
        current_values: List[float]
    ) -> DriftResult:
        """Detect drift in numeric output metric."""
        current_mean = np.mean(current_values)
        current_std = np.std(current_values)

        # Welch's t-test for mean comparison
        t_stat, p_value = stats.ttest_ind(
            baseline["values"],
            current_values,
            equal_var=False
        )

        # Effect size (Cohen's d)
        pooled_std = np.sqrt((baseline["std"]**2 + current_std**2) / 2)
        cohens_d = abs(current_mean - baseline["mean"]) / pooled_std if pooled_std > 0 else 0

        # Determine severity
        if p_value < 0.001 and cohens_d > 0.8:
            severity = "high"
        elif p_value < 0.01 and cohens_d > 0.5:
            severity = "medium"
        elif p_value < 0.05 and cohens_d > 0.2:
            severity = "low"
        else:
            severity = "none"

        return DriftResult(
            drift_type=DriftType.OUTPUT_DISTRIBUTION,
            detected=severity != "none",
            severity=severity,
            metric_name=metric_name,
            baseline_value=baseline["mean"],
            current_value=current_mean,
            p_value=p_value,
            details={
                "t_statistic": t_stat,
                "cohens_d": cohens_d,
                "baseline_std": baseline["std"],
                "current_std": current_std
            }
        )

    def _detect_categorical_drift(
        self,
        metric_name: str,
        baseline: Dict[str, Any],
        current_values: List[str]
    ) -> DriftResult:
        """Detect drift in categorical output metric."""
        # Calculate current distribution
        current_counts = defaultdict(int)
        for v in current_values:
            current_counts[v] += 1
        total = len(current_values)
        current_dist = {k: v/total for k, v in current_counts.items()}

        # Chi-square test
        all_categories = set(baseline["distribution"].keys()) | set(current_dist.keys())

        baseline_expected = [baseline["distribution"].get(c, 0) * total for c in all_categories]
        current_observed = [current_counts.get(c, 0) for c in all_categories]

        # Handle zero expected values
        baseline_expected = [max(e, 0.001) for e in baseline_expected]

        chi2, p_value = stats.chisquare(current_observed, baseline_expected)

        # Calculate distribution difference
        dist_diff = sum(
            abs(baseline["distribution"].get(c, 0) - current_dist.get(c, 0))
            for c in all_categories
        ) / 2

        # Determine severity
        if p_value < 0.001 and dist_diff > 0.2:
            severity = "high"
        elif p_value < 0.01 and dist_diff > 0.1:
            severity = "medium"
        elif p_value < 0.05 and dist_diff > 0.05:
            severity = "low"
        else:
            severity = "none"

        return DriftResult(
            drift_type=DriftType.OUTPUT_DISTRIBUTION,
            detected=severity != "none",
            severity=severity,
            metric_name=metric_name,
            baseline_value=0,  # Not applicable for categorical
            current_value=0,
            p_value=p_value,
            details={
                "chi2_statistic": chi2,
                "distribution_difference": dist_diff,
                "baseline_distribution": baseline["distribution"],
                "current_distribution": current_dist
            }
        )


class EmbeddingDriftDetector:
    """
    Detect drift in embedding space.
    """

    def __init__(
        self,
        embedding_dim: int,
        reference_embeddings: Optional[np.ndarray] = None
    ):
        self.embedding_dim = embedding_dim
        self.reference_embeddings = reference_embeddings
        self.reference_centroid = None
        self.reference_distances = None

        if reference_embeddings is not None:
            self._compute_reference_stats()

    def _compute_reference_stats(self):
        """Compute reference statistics."""
        self.reference_centroid = np.mean(self.reference_embeddings, axis=0)
        self.reference_distances = np.linalg.norm(
            self.reference_embeddings - self.reference_centroid,
            axis=1
        )

    def set_reference(self, embeddings: np.ndarray):
        """Set reference embeddings."""
        self.reference_embeddings = embeddings
        self._compute_reference_stats()

    def detect_drift(self, current_embeddings: np.ndarray) -> DriftResult:
        """Detect drift in embedding space."""
        if self.reference_centroid is None:
            raise ValueError("Reference embeddings not set")

        # Compute current statistics
        current_centroid = np.mean(current_embeddings, axis=0)
        current_distances = np.linalg.norm(
            current_embeddings - self.reference_centroid,
            axis=1
        )

        # Centroid shift
        centroid_shift = np.linalg.norm(current_centroid - self.reference_centroid)

        # Maximum Mean Discrepancy (simplified)
        mmd = self._compute_mmd(self.reference_embeddings, current_embeddings)

        # Distance distribution comparison
        ks_stat, ks_p = stats.ks_2samp(self.reference_distances, current_distances)

        # Determine severity
        severity = self._determine_severity(centroid_shift, mmd, ks_p)

        return DriftResult(
            drift_type=DriftType.EMBEDDING,
            detected=severity != "none",
            severity=severity,
            metric_name="embedding_drift",
            baseline_value=0,
            current_value=centroid_shift,
            p_value=ks_p,
            details={
                "centroid_shift": centroid_shift,
                "mmd": mmd,
                "ks_statistic": ks_stat,
                "mean_distance_baseline": np.mean(self.reference_distances),
                "mean_distance_current": np.mean(current_distances)
            }
        )

    def _compute_mmd(
        self,
        X: np.ndarray,
        Y: np.ndarray,
        kernel: str = "rbf",
        gamma: float = 1.0
    ) -> float:
        """Compute Maximum Mean Discrepancy."""
        n_x, n_y = len(X), len(Y)

        if kernel == "rbf":
            # RBF kernel
            def k(a, b):
                diff = a[:, np.newaxis, :] - b[np.newaxis, :, :]
                return np.exp(-gamma * np.sum(diff**2, axis=2))
        else:
            # Linear kernel
            def k(a, b):
                return a @ b.T

        # Sample subsets for efficiency
        max_samples = 500
        if n_x > max_samples:
            X = X[np.random.choice(n_x, max_samples, replace=False)]
        if n_y > max_samples:
            Y = Y[np.random.choice(n_y, max_samples, replace=False)]

        K_xx = k(X, X)
        K_yy = k(Y, Y)
        K_xy = k(X, Y)

        mmd = (
            np.mean(K_xx) +
            np.mean(K_yy) -
            2 * np.mean(K_xy)
        )

        return max(0, mmd)

    def _determine_severity(
        self,
        centroid_shift: float,
        mmd: float,
        ks_p: float
    ) -> str:
        """Determine drift severity."""
        # Normalize centroid shift by embedding dimension
        normalized_shift = centroid_shift / np.sqrt(self.embedding_dim)

        score = 0

        if normalized_shift > 0.5:
            score += 3
        elif normalized_shift > 0.2:
            score += 2
        elif normalized_shift > 0.1:
            score += 1

        if mmd > 0.1:
            score += 3
        elif mmd > 0.05:
            score += 2
        elif mmd > 0.01:
            score += 1

        if ks_p < 0.001:
            score += 3
        elif ks_p < 0.01:
            score += 2
        elif ks_p < 0.05:
            score += 1

        if score >= 6:
            return "high"
        elif score >= 3:
            return "medium"
        elif score >= 1:
            return "low"
        return "none"
```

### Performance Drift

```python
"""
Performance drift detection for LLM systems.
"""

from dataclasses import dataclass
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
import numpy as np
from scipy import stats


class PerformanceDriftDetector:
    """
    Detect performance degradation over time.
    """

    def __init__(
        self,
        metrics_to_track: List[str] = None,
        window_size: int = 1000,
        min_samples: int = 100
    ):
        self.metrics_to_track = metrics_to_track or [
            "latency_ms",
            "ttft_ms",
            "tokens_per_second",
            "error_rate"
        ]
        self.window_size = window_size
        self.min_samples = min_samples
        self.baseline_stats: Dict[str, Dict[str, float]] = {}

    def set_baseline(self, metrics_data: Dict[str, List[float]]):
        """Set baseline performance metrics."""
        for metric_name, values in metrics_data.items():
            if metric_name in self.metrics_to_track:
                self.baseline_stats[metric_name] = {
                    "mean": np.mean(values),
                    "std": np.std(values),
                    "median": np.median(values),
                    "p95": np.percentile(values, 95),
                    "p99": np.percentile(values, 99),
                    "values": values[-self.window_size:]
                }

    def detect_drift(
        self,
        current_metrics: Dict[str, List[float]]
    ) -> List[DriftResult]:
        """Detect performance drift."""
        results = []

        for metric_name, current_values in current_metrics.items():
            if metric_name not in self.baseline_stats:
                continue

            if len(current_values) < self.min_samples:
                continue

            baseline = self.baseline_stats[metric_name]
            result = self._analyze_metric_drift(metric_name, baseline, current_values)
            results.append(result)

        return results

    def _analyze_metric_drift(
        self,
        metric_name: str,
        baseline: Dict[str, Any],
        current_values: List[float]
    ) -> DriftResult:
        """Analyze drift for a single metric."""
        current_mean = np.mean(current_values)
        current_std = np.std(current_values)
        current_p95 = np.percentile(current_values, 95)
        current_p99 = np.percentile(current_values, 99)

        # Statistical tests
        t_stat, t_p = stats.ttest_ind(
            baseline["values"],
            current_values,
            equal_var=False
        )

        # Mann-Whitney U test (non-parametric)
        u_stat, u_p = stats.mannwhitneyu(
            baseline["values"],
            current_values,
            alternative='two-sided'
        )

        # Calculate percentage changes
        mean_change_pct = (current_mean - baseline["mean"]) / baseline["mean"] * 100
        p95_change_pct = (current_p95 - baseline["p95"]) / baseline["p95"] * 100
        p99_change_pct = (current_p99 - baseline["p99"]) / baseline["p99"] * 100

        # Determine if this is degradation
        is_higher_worse = metric_name in ["latency_ms", "ttft_ms", "error_rate"]

        if is_higher_worse:
            degradation = mean_change_pct > 0
            severity = self._severity_for_increase(
                mean_change_pct, p95_change_pct, min(t_p, u_p)
            )
        else:
            degradation = mean_change_pct < 0
            severity = self._severity_for_decrease(
                mean_change_pct, p95_change_pct, min(t_p, u_p)
            )

        return DriftResult(
            drift_type=DriftType.PERFORMANCE,
            detected=severity != "none" and degradation,
            severity=severity if degradation else "none",
            metric_name=metric_name,
            baseline_value=baseline["mean"],
            current_value=current_mean,
            p_value=min(t_p, u_p),
            details={
                "mean_change_pct": mean_change_pct,
                "p95_change_pct": p95_change_pct,
                "p99_change_pct": p99_change_pct,
                "t_statistic": t_stat,
                "u_statistic": u_stat,
                "baseline_p95": baseline["p95"],
                "current_p95": current_p95,
                "is_degradation": degradation
            }
        )

    def _severity_for_increase(
        self,
        mean_change_pct: float,
        p95_change_pct: float,
        p_value: float
    ) -> str:
        """Determine severity for metrics where higher is worse."""
        if p_value >= 0.05:
            return "none"

        if mean_change_pct > 50 or p95_change_pct > 100:
            return "high"
        elif mean_change_pct > 20 or p95_change_pct > 50:
            return "medium"
        elif mean_change_pct > 10 or p95_change_pct > 20:
            return "low"
        return "none"

    def _severity_for_decrease(
        self,
        mean_change_pct: float,
        p95_change_pct: float,
        p_value: float
    ) -> str:
        """Determine severity for metrics where lower is worse."""
        if p_value >= 0.05:
            return "none"

        # Note: mean_change_pct is negative for decrease
        if mean_change_pct < -30:
            return "high"
        elif mean_change_pct < -15:
            return "medium"
        elif mean_change_pct < -5:
            return "low"
        return "none"
```

## 10.3.4 Feedback Integration

### Explicit Feedback Collection

```python
"""
Feedback collection and integration for LLM quality monitoring.
"""

from dataclasses import dataclass, field
from typing import Dict, Any, List, Optional, Callable
from datetime import datetime
from enum import Enum
import asyncio


class FeedbackType(Enum):
    """Types of user feedback."""
    THUMBS_UP = "thumbs_up"
    THUMBS_DOWN = "thumbs_down"
    RATING = "rating"
    REGENERATE = "regenerate"
    EDIT = "edit"
    COPY = "copy"
    SHARE = "share"
    REPORT = "report"
    SURVEY = "survey"


@dataclass
class UserFeedback:
    """User feedback record."""
    feedback_id: str
    request_id: str
    user_id: str
    feedback_type: FeedbackType
    value: Any  # Depends on feedback type
    timestamp: datetime
    response_text: Optional[str] = None
    edited_text: Optional[str] = None
    report_reason: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)


class FeedbackCollector:
    """
    Collect and process user feedback for quality monitoring.
    """

    def __init__(
        self,
        storage: 'FeedbackStorage',
        quality_monitor: 'QualityMonitor' = None
    ):
        self.storage = storage
        self.quality_monitor = quality_monitor
        self._listeners: List[Callable] = []

    def add_listener(self, callback: Callable[[UserFeedback], None]):
        """Add feedback listener."""
        self._listeners.append(callback)

    async def record_thumbs_feedback(
        self,
        request_id: str,
        user_id: str,
        is_positive: bool,
        response_text: Optional[str] = None
    ) -> UserFeedback:
        """Record thumbs up/down feedback."""
        feedback = UserFeedback(
            feedback_id=f"fb_{request_id}_{datetime.utcnow().timestamp()}",
            request_id=request_id,
            user_id=user_id,
            feedback_type=FeedbackType.THUMBS_UP if is_positive else FeedbackType.THUMBS_DOWN,
            value=1 if is_positive else 0,
            timestamp=datetime.utcnow(),
            response_text=response_text
        )

        await self._process_feedback(feedback)
        return feedback

    async def record_rating(
        self,
        request_id: str,
        user_id: str,
        rating: int,  # 1-5
        dimensions: Optional[Dict[str, int]] = None
    ) -> UserFeedback:
        """Record numeric rating feedback."""
        feedback = UserFeedback(
            feedback_id=f"fb_{request_id}_{datetime.utcnow().timestamp()}",
            request_id=request_id,
            user_id=user_id,
            feedback_type=FeedbackType.RATING,
            value=rating,
            timestamp=datetime.utcnow(),
            metadata={"dimensions": dimensions} if dimensions else {}
        )

        await self._process_feedback(feedback)
        return feedback

    async def record_regeneration(
        self,
        request_id: str,
        user_id: str,
        original_response: str,
        regeneration_count: int = 1
    ) -> UserFeedback:
        """Record response regeneration (implicit negative feedback)."""
        feedback = UserFeedback(
            feedback_id=f"fb_{request_id}_{datetime.utcnow().timestamp()}",
            request_id=request_id,
            user_id=user_id,
            feedback_type=FeedbackType.REGENERATE,
            value=regeneration_count,
            timestamp=datetime.utcnow(),
            response_text=original_response
        )

        await self._process_feedback(feedback)
        return feedback

    async def record_edit(
        self,
        request_id: str,
        user_id: str,
        original_response: str,
        edited_response: str
    ) -> UserFeedback:
        """Record user edit of response (implicit feedback)."""
        # Calculate edit distance
        edit_distance = self._calculate_edit_distance(original_response, edited_response)
        edit_ratio = edit_distance / max(len(original_response), 1)

        feedback = UserFeedback(
            feedback_id=f"fb_{request_id}_{datetime.utcnow().timestamp()}",
            request_id=request_id,
            user_id=user_id,
            feedback_type=FeedbackType.EDIT,
            value=edit_ratio,
            timestamp=datetime.utcnow(),
            response_text=original_response,
            edited_text=edited_response,
            metadata={"edit_distance": edit_distance}
        )

        await self._process_feedback(feedback)
        return feedback

    async def record_copy(
        self,
        request_id: str,
        user_id: str,
        copied_text: str
    ) -> UserFeedback:
        """Record copy action (implicit positive feedback)."""
        feedback = UserFeedback(
            feedback_id=f"fb_{request_id}_{datetime.utcnow().timestamp()}",
            request_id=request_id,
            user_id=user_id,
            feedback_type=FeedbackType.COPY,
            value=1,
            timestamp=datetime.utcnow(),
            response_text=copied_text,
            metadata={"copied_length": len(copied_text)}
        )

        await self._process_feedback(feedback)
        return feedback

    async def record_report(
        self,
        request_id: str,
        user_id: str,
        reason: str,
        response_text: str,
        additional_info: Optional[str] = None
    ) -> UserFeedback:
        """Record content report."""
        feedback = UserFeedback(
            feedback_id=f"fb_{request_id}_{datetime.utcnow().timestamp()}",
            request_id=request_id,
            user_id=user_id,
            feedback_type=FeedbackType.REPORT,
            value=reason,
            timestamp=datetime.utcnow(),
            response_text=response_text,
            report_reason=reason,
            metadata={"additional_info": additional_info}
        )

        await self._process_feedback(feedback)
        return feedback

    async def _process_feedback(self, feedback: UserFeedback):
        """Process and store feedback."""
        # Store feedback
        await self.storage.store(feedback)

        # Update quality metrics if monitor is configured
        if self.quality_monitor:
            await self._update_quality_metrics(feedback)

        # Notify listeners
        for listener in self._listeners:
            try:
                if asyncio.iscoroutinefunction(listener):
                    await listener(feedback)
                else:
                    listener(feedback)
            except Exception as e:
                print(f"Feedback listener error: {e}")

    async def _update_quality_metrics(self, feedback: UserFeedback):
        """Update quality metrics based on feedback."""
        # Map feedback to quality scores
        score_mapping = {
            FeedbackType.THUMBS_UP: ("user_satisfaction", 1.0),
            FeedbackType.THUMBS_DOWN: ("user_satisfaction", 0.0),
            FeedbackType.RATING: ("user_rating", feedback.value / 5.0),
            FeedbackType.REGENERATE: ("regeneration_penalty", 0.0),
            FeedbackType.COPY: ("engagement_score", 1.0),
            FeedbackType.REPORT: ("safety_issue", 0.0),
        }

        if feedback.feedback_type in score_mapping:
            metric_name, score = score_mapping[feedback.feedback_type]
            # Store as quality score
            # Implementation depends on quality monitor

    def _calculate_edit_distance(self, s1: str, s2: str) -> int:
        """Calculate Levenshtein edit distance."""
        if len(s1) < len(s2):
            return self._calculate_edit_distance(s2, s1)

        if len(s2) == 0:
            return len(s1)

        previous_row = range(len(s2) + 1)
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row

        return previous_row[-1]


class FeedbackAggregator:
    """
    Aggregate feedback data for analysis.
    """

    def __init__(self, storage: 'FeedbackStorage'):
        self.storage = storage

    async def get_satisfaction_rate(
        self,
        model_id: str,
        time_range: tuple[datetime, datetime]
    ) -> Dict[str, float]:
        """Calculate user satisfaction metrics."""
        feedbacks = await self.storage.get_feedbacks(
            model_id=model_id,
            time_range=time_range,
            feedback_types=[FeedbackType.THUMBS_UP, FeedbackType.THUMBS_DOWN]
        )

        if not feedbacks:
            return {"satisfaction_rate": None, "sample_size": 0}

        positive = sum(1 for f in feedbacks if f.feedback_type == FeedbackType.THUMBS_UP)
        total = len(feedbacks)

        return {
            "satisfaction_rate": positive / total,
            "positive_count": positive,
            "negative_count": total - positive,
            "sample_size": total
        }

    async def get_average_rating(
        self,
        model_id: str,
        time_range: tuple[datetime, datetime]
    ) -> Dict[str, float]:
        """Calculate average rating metrics."""
        feedbacks = await self.storage.get_feedbacks(
            model_id=model_id,
            time_range=time_range,
            feedback_types=[FeedbackType.RATING]
        )

        if not feedbacks:
            return {"average_rating": None, "sample_size": 0}

        ratings = [f.value for f in feedbacks]

        return {
            "average_rating": np.mean(ratings),
            "rating_std": np.std(ratings),
            "rating_distribution": {
                str(i): sum(1 for r in ratings if r == i)
                for i in range(1, 6)
            },
            "sample_size": len(ratings)
        }

    async def get_implicit_feedback_metrics(
        self,
        model_id: str,
        time_range: tuple[datetime, datetime]
    ) -> Dict[str, float]:
        """Calculate implicit feedback metrics."""
        all_feedbacks = await self.storage.get_feedbacks(
            model_id=model_id,
            time_range=time_range
        )

        # Get total request count for the period
        total_requests = await self.storage.get_request_count(model_id, time_range)

        regenerations = [f for f in all_feedbacks if f.feedback_type == FeedbackType.REGENERATE]
        edits = [f for f in all_feedbacks if f.feedback_type == FeedbackType.EDIT]
        copies = [f for f in all_feedbacks if f.feedback_type == FeedbackType.COPY]

        return {
            "regeneration_rate": len(regenerations) / total_requests if total_requests > 0 else 0,
            "edit_rate": len(edits) / total_requests if total_requests > 0 else 0,
            "copy_rate": len(copies) / total_requests if total_requests > 0 else 0,
            "average_edit_ratio": np.mean([e.value for e in edits]) if edits else 0,
            "total_requests": total_requests
        }

    async def identify_problem_patterns(
        self,
        model_id: str,
        time_range: tuple[datetime, datetime],
        min_occurrences: int = 5
    ) -> List[Dict[str, Any]]:
        """Identify patterns in negative feedback."""
        negative_feedbacks = await self.storage.get_feedbacks(
            model_id=model_id,
            time_range=time_range,
            feedback_types=[FeedbackType.THUMBS_DOWN, FeedbackType.REPORT, FeedbackType.REGENERATE]
        )

        # Group by common characteristics
        patterns = []

        # Group by report reason
        report_reasons = {}
        for f in negative_feedbacks:
            if f.feedback_type == FeedbackType.REPORT and f.report_reason:
                reason = f.report_reason
                if reason not in report_reasons:
                    report_reasons[reason] = []
                report_reasons[reason].append(f)

        for reason, feedbacks in report_reasons.items():
            if len(feedbacks) >= min_occurrences:
                patterns.append({
                    "pattern_type": "report_reason",
                    "pattern_value": reason,
                    "occurrence_count": len(feedbacks),
                    "sample_request_ids": [f.request_id for f in feedbacks[:5]]
                })

        return patterns
```

## 10.3.5 Quality Alerting

### Alert Rule Engine

```python
"""
Quality alerting system for LLM monitoring.
"""

from dataclasses import dataclass, field
from typing import Dict, Any, List, Optional, Callable
from datetime import datetime, timedelta
from enum import Enum
import asyncio


class AlertSeverity(Enum):
    """Alert severity levels."""
    INFO = "info"
    WARNING = "warning"
    CRITICAL = "critical"
    EMERGENCY = "emergency"


class AlertCondition(Enum):
    """Types of alert conditions."""
    THRESHOLD = "threshold"
    TREND = "trend"
    ANOMALY = "anomaly"
    RATE_OF_CHANGE = "rate_of_change"


@dataclass
class QualityAlertRule:
    """Definition of a quality alert rule."""
    rule_id: str
    name: str
    description: str
    metric_name: str
    condition: AlertCondition
    threshold: Optional[float] = None
    comparison: str = "lt"  # lt, gt, eq, lte, gte
    window_minutes: int = 15
    evaluation_interval_seconds: int = 60
    severity: AlertSeverity = AlertSeverity.WARNING
    enabled: bool = True
    labels: Dict[str, str] = field(default_factory=dict)

    # For trend-based alerts
    trend_direction: Optional[str] = None  # "increasing", "decreasing"
    trend_threshold_pct: Optional[float] = None

    # For anomaly detection
    std_deviations: Optional[float] = None


@dataclass
class QualityAlert:
    """Generated quality alert."""
    alert_id: str
    rule_id: str
    rule_name: str
    severity: AlertSeverity
    metric_name: str
    metric_value: float
    threshold: Optional[float]
    message: str
    fired_at: datetime
    resolved_at: Optional[datetime] = None
    labels: Dict[str, str] = field(default_factory=dict)
    annotations: Dict[str, Any] = field(default_factory=dict)


class QualityAlertEngine:
    """
    Alert engine for LLM quality monitoring.
    """

    # Standard quality alert rules
    STANDARD_RULES = [
        QualityAlertRule(
            rule_id="quality_satisfaction_low",
            name="Low User Satisfaction",
            description="User satisfaction rate has dropped below threshold",
            metric_name="thumbs_up_rate",
            condition=AlertCondition.THRESHOLD,
            threshold=0.70,
            comparison="lt",
            window_minutes=60,
            severity=AlertSeverity.WARNING
        ),
        QualityAlertRule(
            rule_id="quality_satisfaction_critical",
            name="Critical User Satisfaction Drop",
            description="User satisfaction rate is critically low",
            metric_name="thumbs_up_rate",
            condition=AlertCondition.THRESHOLD,
            threshold=0.50,
            comparison="lt",
            window_minutes=30,
            severity=AlertSeverity.CRITICAL
        ),
        QualityAlertRule(
            rule_id="quality_regeneration_high",
            name="High Regeneration Rate",
            description="Users are frequently regenerating responses",
            metric_name="regeneration_rate",
            condition=AlertCondition.THRESHOLD,
            threshold=0.15,
            comparison="gt",
            window_minutes=30,
            severity=AlertSeverity.WARNING
        ),
        QualityAlertRule(
            rule_id="quality_hallucination_spike",
            name="Hallucination Rate Increase",
            description="Detected increase in hallucination rate",
            metric_name="hallucination_rate",
            condition=AlertCondition.THRESHOLD,
            threshold=0.05,
            comparison="gt",
            window_minutes=60,
            severity=AlertSeverity.CRITICAL
        ),
        QualityAlertRule(
            rule_id="quality_safety_degradation",
            name="Safety Score Degradation",
            description="Model safety scores have degraded",
            metric_name="toxicity_score",
            condition=AlertCondition.THRESHOLD,
            threshold=0.95,
            comparison="lt",
            window_minutes=15,
            severity=AlertSeverity.EMERGENCY
        ),
        QualityAlertRule(
            rule_id="quality_relevance_trend",
            name="Declining Relevance Trend",
            description="Response relevance is trending downward",
            metric_name="relevance_score",
            condition=AlertCondition.TREND,
            trend_direction="decreasing",
            trend_threshold_pct=10,
            window_minutes=120,
            severity=AlertSeverity.WARNING
        ),
        QualityAlertRule(
            rule_id="quality_coherence_anomaly",
            name="Coherence Score Anomaly",
            description="Unusual coherence score pattern detected",
            metric_name="coherence_score",
            condition=AlertCondition.ANOMALY,
            std_deviations=3,
            window_minutes=60,
            severity=AlertSeverity.WARNING
        ),
    ]

    def __init__(
        self,
        metrics_store: 'MetricsStore',
        alert_manager: 'AlertManager'
    ):
        self.metrics_store = metrics_store
        self.alert_manager = alert_manager
        self.rules: Dict[str, QualityAlertRule] = {}
        self.active_alerts: Dict[str, QualityAlert] = {}
        self._running = False

        # Register standard rules
        for rule in self.STANDARD_RULES:
            self.register_rule(rule)

    def register_rule(self, rule: QualityAlertRule):
        """Register an alert rule."""
        self.rules[rule.rule_id] = rule

    def unregister_rule(self, rule_id: str):
        """Unregister an alert rule."""
        self.rules.pop(rule_id, None)

    async def start(self):
        """Start the alert evaluation loop."""
        self._running = True
        asyncio.create_task(self._evaluation_loop())

    async def stop(self):
        """Stop the alert evaluation loop."""
        self._running = False

    async def _evaluation_loop(self):
        """Main evaluation loop."""
        while self._running:
            for rule_id, rule in self.rules.items():
                if rule.enabled:
                    try:
                        await self._evaluate_rule(rule)
                    except Exception as e:
                        print(f"Error evaluating rule {rule_id}: {e}")

            await asyncio.sleep(60)  # Evaluate every minute

    async def _evaluate_rule(self, rule: QualityAlertRule):
        """Evaluate a single alert rule."""
        # Get metric values for window
        end_time = datetime.utcnow()
        start_time = end_time - timedelta(minutes=rule.window_minutes)

        values = await self.metrics_store.get_metric_values(
            metric_name=rule.metric_name,
            time_range=(start_time, end_time)
        )

        if not values:
            return

        # Evaluate based on condition type
        should_alert = False
        current_value = np.mean(values)
        message = ""

        if rule.condition == AlertCondition.THRESHOLD:
            should_alert, message = self._evaluate_threshold(rule, current_value)

        elif rule.condition == AlertCondition.TREND:
            should_alert, message = self._evaluate_trend(rule, values)

        elif rule.condition == AlertCondition.ANOMALY:
            should_alert, message = self._evaluate_anomaly(rule, values, current_value)

        elif rule.condition == AlertCondition.RATE_OF_CHANGE:
            should_alert, message = self._evaluate_rate_of_change(rule, values)

        # Handle alert state
        if should_alert:
            if rule.rule_id not in self.active_alerts:
                await self._fire_alert(rule, current_value, message)
        else:
            if rule.rule_id in self.active_alerts:
                await self._resolve_alert(rule.rule_id)

    def _evaluate_threshold(
        self,
        rule: QualityAlertRule,
        value: float
    ) -> tuple[bool, str]:
        """Evaluate threshold condition."""
        comparisons = {
            "lt": lambda v, t: v < t,
            "gt": lambda v, t: v > t,
            "eq": lambda v, t: abs(v - t) < 0.001,
            "lte": lambda v, t: v <= t,
            "gte": lambda v, t: v >= t,
        }

        should_alert = comparisons[rule.comparison](value, rule.threshold)
        comparison_text = {
            "lt": "below",
            "gt": "above",
            "lte": "at or below",
            "gte": "at or above"
        }.get(rule.comparison, rule.comparison)

        message = (
            f"{rule.metric_name} is {value:.3f}, "
            f"which is {comparison_text} threshold of {rule.threshold}"
        )

        return should_alert, message

    def _evaluate_trend(
        self,
        rule: QualityAlertRule,
        values: List[float]
    ) -> tuple[bool, str]:
        """Evaluate trend condition."""
        if len(values) < 10:
            return False, ""

        # Split into first and second half
        mid = len(values) // 2
        first_half_mean = np.mean(values[:mid])
        second_half_mean = np.mean(values[mid:])

        if first_half_mean == 0:
            return False, ""

        change_pct = (second_half_mean - first_half_mean) / first_half_mean * 100

        if rule.trend_direction == "decreasing":
            should_alert = change_pct < -rule.trend_threshold_pct
            message = f"{rule.metric_name} decreased by {abs(change_pct):.1f}%"
        else:
            should_alert = change_pct > rule.trend_threshold_pct
            message = f"{rule.metric_name} increased by {change_pct:.1f}%"

        return should_alert, message

    def _evaluate_anomaly(
        self,
        rule: QualityAlertRule,
        values: List[float],
        current_value: float
    ) -> tuple[bool, str]:
        """Evaluate anomaly condition using z-score."""
        if len(values) < 30:
            return False, ""

        mean = np.mean(values)
        std = np.std(values)

        if std == 0:
            return False, ""

        z_score = abs(current_value - mean) / std
        should_alert = z_score > rule.std_deviations

        message = (
            f"{rule.metric_name} value {current_value:.3f} is "
            f"{z_score:.1f} standard deviations from mean {mean:.3f}"
        )

        return should_alert, message

    def _evaluate_rate_of_change(
        self,
        rule: QualityAlertRule,
        values: List[float]
    ) -> tuple[bool, str]:
        """Evaluate rate of change condition."""
        if len(values) < 2:
            return False, ""

        # Calculate rate of change
        rates = np.diff(values)
        avg_rate = np.mean(rates)

        # This would need threshold configuration
        # Placeholder logic
        return False, ""

    async def _fire_alert(
        self,
        rule: QualityAlertRule,
        value: float,
        message: str
    ):
        """Fire a new alert."""
        alert = QualityAlert(
            alert_id=f"alert_{rule.rule_id}_{datetime.utcnow().timestamp()}",
            rule_id=rule.rule_id,
            rule_name=rule.name,
            severity=rule.severity,
            metric_name=rule.metric_name,
            metric_value=value,
            threshold=rule.threshold,
            message=message,
            fired_at=datetime.utcnow(),
            labels=rule.labels,
            annotations={"description": rule.description}
        )

        self.active_alerts[rule.rule_id] = alert
        await self.alert_manager.send_alert(alert)

    async def _resolve_alert(self, rule_id: str):
        """Resolve an active alert."""
        if rule_id in self.active_alerts:
            alert = self.active_alerts[rule_id]
            alert.resolved_at = datetime.utcnow()
            await self.alert_manager.resolve_alert(alert)
            del self.active_alerts[rule_id]


class HumanReviewTrigger:
    """
    Trigger human review based on quality signals.
    """

    def __init__(
        self,
        review_queue: 'ReviewQueue',
        threshold_config: Dict[str, float] = None
    ):
        self.review_queue = review_queue
        self.thresholds = threshold_config or {
            "safety_score": 0.9,
            "hallucination_probability": 0.3,
            "user_report": True,
            "low_confidence": 0.5
        }

    async def check_and_trigger(
        self,
        request_id: str,
        scores: Dict[str, float],
        feedback: Optional[UserFeedback] = None
    ) -> bool:
        """
        Check if human review should be triggered.
        """
        trigger_reasons = []

        # Safety threshold
        if scores.get("safety_score", 1.0) < self.thresholds["safety_score"]:
            trigger_reasons.append("low_safety_score")

        # Hallucination probability
        if scores.get("hallucination_probability", 0) > self.thresholds["hallucination_probability"]:
            trigger_reasons.append("high_hallucination_risk")

        # User report
        if feedback and feedback.feedback_type == FeedbackType.REPORT:
            trigger_reasons.append("user_report")

        # Low model confidence
        if scores.get("model_confidence", 1.0) < self.thresholds["low_confidence"]:
            trigger_reasons.append("low_model_confidence")

        # Multiple negative feedback
        if scores.get("negative_feedback_count", 0) >= 3:
            trigger_reasons.append("multiple_negative_feedback")

        if trigger_reasons:
            await self.review_queue.add_for_review(
                request_id=request_id,
                reasons=trigger_reasons,
                scores=scores,
                priority=self._calculate_priority(trigger_reasons, scores)
            )
            return True

        return False

    def _calculate_priority(
        self,
        reasons: List[str],
        scores: Dict[str, float]
    ) -> int:
        """Calculate review priority (1 = highest)."""
        priority_score = 5  # Default medium priority

        if "user_report" in reasons:
            priority_score -= 2

        if "low_safety_score" in reasons:
            safety = scores.get("safety_score", 1.0)
            if safety < 0.7:
                priority_score -= 3
            else:
                priority_score -= 1

        if "high_hallucination_risk" in reasons:
            priority_score -= 1

        return max(1, min(5, priority_score))
```

## 10.3.6 Quality Dashboards

### Dashboard Configuration

```python
"""
Quality monitoring dashboard configuration.
"""

from dataclasses import dataclass
from typing import Dict, Any, List
import json


@dataclass
class QualityDashboardConfig:
    """Configuration for quality monitoring dashboard."""

    title: str = "LLM Quality Monitoring"
    refresh_interval: str = "1m"

    def generate_grafana_dashboard(self) -> Dict[str, Any]:
        """Generate Grafana dashboard JSON."""
        return {
            "title": self.title,
            "refresh": self.refresh_interval,
            "panels": self._generate_panels(),
            "templating": self._generate_variables(),
            "time": {"from": "now-24h", "to": "now"}
        }

    def _generate_panels(self) -> List[Dict[str, Any]]:
        """Generate dashboard panels."""
        panels = []
        panel_id = 1

        # User Satisfaction Panel
        panels.append({
            "id": panel_id,
            "title": "User Satisfaction Rate",
            "type": "gauge",
            "gridPos": {"x": 0, "y": 0, "w": 6, "h": 4},
            "targets": [{
                "expr": '''
                    sum(rate(llm_feedback_total{feedback_type="thumbs_up"}[1h]))
                    / sum(rate(llm_feedback_total[1h]))
                ''',
                "legendFormat": "Satisfaction Rate"
            }],
            "fieldConfig": {
                "defaults": {
                    "thresholds": {
                        "steps": [
                            {"value": 0, "color": "red"},
                            {"value": 0.7, "color": "yellow"},
                            {"value": 0.85, "color": "green"}
                        ]
                    },
                    "min": 0,
                    "max": 1,
                    "unit": "percentunit"
                }
            }
        })
        panel_id += 1

        # Quality Scores Over Time
        panels.append({
            "id": panel_id,
            "title": "Quality Scores Over Time",
            "type": "timeseries",
            "gridPos": {"x": 6, "y": 0, "w": 12, "h": 8},
            "targets": [
                {
                    "expr": 'avg(llm_quality_score{metric="relevance"}) by (model)',
                    "legendFormat": "{{model}} - Relevance"
                },
                {
                    "expr": 'avg(llm_quality_score{metric="coherence"}) by (model)',
                    "legendFormat": "{{model}} - Coherence"
                },
                {
                    "expr": 'avg(llm_quality_score{metric="factual_accuracy"}) by (model)',
                    "legendFormat": "{{model}} - Factual Accuracy"
                }
            ],
            "fieldConfig": {
                "defaults": {
                    "min": 0,
                    "max": 1
                }
            }
        })
        panel_id += 1

        # Safety Metrics
        panels.append({
            "id": panel_id,
            "title": "Safety Metrics",
            "type": "stat",
            "gridPos": {"x": 18, "y": 0, "w": 6, "h": 4},
            "targets": [
                {
                    "expr": 'avg(llm_quality_score{metric="toxicity_score"})',
                    "legendFormat": "Safety Score"
                }
            ],
            "fieldConfig": {
                "defaults": {
                    "thresholds": {
                        "steps": [
                            {"value": 0, "color": "red"},
                            {"value": 0.95, "color": "yellow"},
                            {"value": 0.99, "color": "green"}
                        ]
                    }
                }
            }
        })
        panel_id += 1

        # Drift Detection Status
        panels.append({
            "id": panel_id,
            "title": "Drift Detection",
            "type": "table",
            "gridPos": {"x": 0, "y": 8, "w": 12, "h": 6},
            "targets": [{
                "expr": 'llm_drift_score',
                "format": "table",
                "instant": True
            }],
            "transformations": [
                {"id": "organize", "options": {
                    "excludeByName": {"Time": True},
                    "renameByName": {
                        "drift_type": "Type",
                        "severity": "Severity",
                        "Value": "Score"
                    }
                }}
            ]
        })
        panel_id += 1

        # Feedback Distribution
        panels.append({
            "id": panel_id,
            "title": "Feedback Distribution",
            "type": "piechart",
            "gridPos": {"x": 12, "y": 8, "w": 6, "h": 6},
            "targets": [{
                "expr": 'sum(llm_feedback_total) by (feedback_type)',
                "legendFormat": "{{feedback_type}}"
            }]
        })
        panel_id += 1

        # Regeneration Rate
        panels.append({
            "id": panel_id,
            "title": "Regeneration Rate",
            "type": "timeseries",
            "gridPos": {"x": 18, "y": 8, "w": 6, "h": 6},
            "targets": [{
                "expr": '''
                    sum(rate(llm_feedback_total{feedback_type="regenerate"}[5m]))
                    / sum(rate(llm_requests_total[5m]))
                ''',
                "legendFormat": "Regeneration Rate"
            }],
            "fieldConfig": {
                "defaults": {
                    "unit": "percentunit",
                    "thresholds": {
                        "steps": [
                            {"value": 0, "color": "green"},
                            {"value": 0.1, "color": "yellow"},
                            {"value": 0.2, "color": "red"}
                        ]
                    }
                }
            }
        })
        panel_id += 1

        # Active Alerts
        panels.append({
            "id": panel_id,
            "title": "Active Quality Alerts",
            "type": "table",
            "gridPos": {"x": 0, "y": 14, "w": 24, "h": 6},
            "targets": [{
                "expr": 'ALERTS{alertstate="firing", job="llm-quality"}',
                "format": "table"
            }],
            "transformations": [
                {"id": "organize", "options": {
                    "excludeByName": {"Time": True, "__name__": True},
                    "renameByName": {
                        "alertname": "Alert",
                        "severity": "Severity",
                        "alertstate": "State"
                    }
                }}
            ]
        })

        return panels

    def _generate_variables(self) -> Dict[str, Any]:
        """Generate dashboard template variables."""
        return {
            "list": [
                {
                    "name": "model",
                    "type": "query",
                    "query": 'label_values(llm_quality_score, model)',
                    "multi": True,
                    "includeAll": True
                },
                {
                    "name": "environment",
                    "type": "custom",
                    "options": [
                        {"text": "Production", "value": "production"},
                        {"text": "Staging", "value": "staging"}
                    ],
                    "current": {"text": "Production", "value": "production"}
                },
                {
                    "name": "time_window",
                    "type": "interval",
                    "options": [
                        {"text": "1h", "value": "1h"},
                        {"text": "6h", "value": "6h"},
                        {"text": "24h", "value": "24h"},
                        {"text": "7d", "value": "7d"}
                    ],
                    "current": {"text": "24h", "value": "24h"}
                }
            ]
        }


def generate_prometheus_rules() -> str:
    """Generate Prometheus recording and alerting rules."""
    return """
groups:
  - name: llm_quality_recording_rules
    interval: 1m
    rules:
      # User satisfaction rate
      - record: llm:satisfaction_rate:1h
        expr: |
          sum(rate(llm_feedback_total{feedback_type="thumbs_up"}[1h]))
          / sum(rate(llm_feedback_total{feedback_type=~"thumbs_up|thumbs_down"}[1h]))

      # Quality score averages by model
      - record: llm:quality_score:avg_1h
        expr: |
          avg by (model, metric) (
            avg_over_time(llm_quality_score[1h])
          )

      # Regeneration rate
      - record: llm:regeneration_rate:5m
        expr: |
          sum(rate(llm_feedback_total{feedback_type="regenerate"}[5m]))
          / sum(rate(llm_requests_total[5m]))

  - name: llm_quality_alerts
    rules:
      - alert: LLMSatisfactionLow
        expr: llm:satisfaction_rate:1h < 0.70
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "Low user satisfaction rate"
          description: "User satisfaction is {{ $value | humanizePercentage }}"

      - alert: LLMSatisfactionCritical
        expr: llm:satisfaction_rate:1h < 0.50
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Critical user satisfaction drop"
          description: "User satisfaction is {{ $value | humanizePercentage }}"

      - alert: LLMSafetyDegraded
        expr: avg(llm_quality_score{metric="toxicity_score"}) < 0.95
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Safety score degradation"
          description: "Safety score is {{ $value }}"

      - alert: LLMHighRegenerationRate
        expr: llm:regeneration_rate:5m > 0.15
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High regeneration rate"
          description: "{{ $value | humanizePercentage }} of responses being regenerated"

      - alert: LLMQualityDriftDetected
        expr: llm_drift_score{severity="high"} > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Quality drift detected"
          description: "High severity drift in {{ $labels.drift_type }}"
"""
```

## Summary

This guide covered comprehensive model quality monitoring for LLM systems:

1. **Quality Dimensions**: Output quality, safety, consistency, user satisfaction, factual accuracy, and instruction following
2. **Online Evaluation**: Real-time quality scoring with sampling strategies and LLM-as-judge
3. **Drift Detection**: Input distribution, output distribution, performance, and embedding drift
4. **Feedback Integration**: Explicit (thumbs, ratings) and implicit (regeneration, edits, copy) feedback
5. **Quality Alerting**: Threshold, trend, and anomaly-based alerts with human review triggers
6. **Quality Dashboards**: Grafana configurations and Prometheus rules

Key takeaways:
- Monitor multiple quality dimensions, not just one metric
- Combine automated evaluation with user feedback
- Implement drift detection for early warning
- Use sampling strategies for cost-effective online evaluation
- Build alerting with appropriate severity levels
- Create dashboards that surface actionable insights
