# Document 10.2: LLM Logging & Tracing Guide

## Purpose

This guide provides comprehensive strategies for implementing logging and distributed tracing in LLM systems. Proper observability is critical for debugging, performance optimization, compliance, and understanding system behavior at scale.

## Prerequisites

- Understanding of LLM inference pipelines
- Familiarity with logging frameworks (Python logging, structlog)
- Basic knowledge of distributed systems concepts
- Experience with observability tools (Prometheus, Grafana, Jaeger)

## 10.2.1 Logging Strategy

### Log Level Framework

Effective logging requires consistent use of log levels across the entire LLM system:

```python
"""
LLM logging framework with appropriate log levels and context management.
"""

import logging
import structlog
from enum import Enum
from typing import Any, Dict, Optional
from dataclasses import dataclass, field
from datetime import datetime
import hashlib
import json
from contextlib import contextmanager
from functools import wraps
import threading


class LogLevel(Enum):
    """Log levels with LLM-specific guidance."""
    DEBUG = "debug"      # Token-level operations, cache hits/misses, batch details
    INFO = "info"        # Request start/end, model loaded, generation complete
    WARNING = "warning"  # High latency, cache eviction, rate limit approaching
    ERROR = "error"      # Generation failed, model error, timeout
    CRITICAL = "critical"  # Model unavailable, OOM, system failure


@dataclass
class LLMLogContext:
    """Context for LLM request logging."""
    request_id: str
    trace_id: str
    span_id: str
    user_id: Optional[str] = None
    session_id: Optional[str] = None
    model_id: Optional[str] = None
    environment: str = "production"
    service_name: str = "llm-service"
    extra: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        """Convert context to dictionary for logging."""
        return {
            "request_id": self.request_id,
            "trace_id": self.trace_id,
            "span_id": self.span_id,
            "user_id": self.user_id,
            "session_id": self.session_id,
            "model_id": self.model_id,
            "environment": self.environment,
            "service_name": self.service_name,
            **self.extra
        }


class LLMLogger:
    """
    Structured logger for LLM systems with privacy-aware logging.
    """

    # Thread-local storage for context
    _context = threading.local()

    def __init__(
        self,
        service_name: str = "llm-service",
        log_level: LogLevel = LogLevel.INFO,
        enable_pii_filtering: bool = True,
        log_prompts: bool = False,
        log_responses: bool = False
    ):
        self.service_name = service_name
        self.log_level = log_level
        self.enable_pii_filtering = enable_pii_filtering
        self.log_prompts = log_prompts
        self.log_responses = log_responses

        # Configure structlog
        self._configure_structlog()
        self.logger = structlog.get_logger(service_name)

        # PII patterns for filtering
        self.pii_patterns = self._compile_pii_patterns()

    def _configure_structlog(self):
        """Configure structlog with JSON output."""
        structlog.configure(
            processors=[
                structlog.contextvars.merge_contextvars,
                structlog.processors.add_log_level,
                structlog.processors.TimeStamper(fmt="iso"),
                structlog.processors.StackInfoRenderer(),
                structlog.processors.UnicodeDecoder(),
                self._pii_filter_processor,
                structlog.processors.JSONRenderer()
            ],
            wrapper_class=structlog.make_filtering_bound_logger(
                logging.getLevelName(self.log_level.value.upper())
            ),
            context_class=dict,
            logger_factory=structlog.PrintLoggerFactory(),
            cache_logger_on_first_use=True,
        )

    def _compile_pii_patterns(self):
        """Compile regex patterns for PII detection."""
        import re
        return {
            "email": re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'),
            "phone": re.compile(r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b'),
            "ssn": re.compile(r'\b\d{3}-\d{2}-\d{4}\b'),
            "credit_card": re.compile(r'\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b'),
            "api_key": re.compile(r'\b(sk-|pk-|api[_-]?key[_-]?)[a-zA-Z0-9]{20,}\b', re.I),
        }

    def _pii_filter_processor(self, logger, method_name, event_dict):
        """Filter PII from log events."""
        if not self.enable_pii_filtering:
            return event_dict

        def redact_value(value: Any) -> Any:
            if not isinstance(value, str):
                return value

            result = value
            for pattern_name, pattern in self.pii_patterns.items():
                result = pattern.sub(f"[REDACTED_{pattern_name.upper()}]", result)
            return result

        def redact_dict(d: Dict) -> Dict:
            return {
                k: redact_dict(v) if isinstance(v, dict)
                   else redact_value(v)
                for k, v in d.items()
            }

        return redact_dict(event_dict)

    @contextmanager
    def request_context(self, context: LLMLogContext):
        """Context manager for request-scoped logging."""
        token = structlog.contextvars.bind_contextvars(**context.to_dict())
        try:
            yield self
        finally:
            structlog.contextvars.unbind_contextvars(*context.to_dict().keys())

    def log_request_start(
        self,
        model: str,
        prompt_tokens: int,
        max_tokens: int,
        temperature: float,
        prompt_hash: Optional[str] = None
    ):
        """Log the start of an LLM request."""
        log_data = {
            "event": "llm_request_start",
            "model": model,
            "prompt_tokens": prompt_tokens,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "timestamp": datetime.utcnow().isoformat()
        }

        if prompt_hash:
            log_data["prompt_hash"] = prompt_hash

        self.logger.info(**log_data)

    def log_request_end(
        self,
        model: str,
        prompt_tokens: int,
        completion_tokens: int,
        total_tokens: int,
        latency_ms: float,
        ttft_ms: Optional[float] = None,
        status: str = "success",
        error: Optional[str] = None
    ):
        """Log the completion of an LLM request."""
        log_data = {
            "event": "llm_request_end",
            "model": model,
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "total_tokens": total_tokens,
            "latency_ms": latency_ms,
            "tokens_per_second": (completion_tokens / latency_ms * 1000) if latency_ms > 0 else 0,
            "status": status,
            "timestamp": datetime.utcnow().isoformat()
        }

        if ttft_ms is not None:
            log_data["ttft_ms"] = ttft_ms

        if error:
            log_data["error"] = error
            self.logger.error(**log_data)
        else:
            self.logger.info(**log_data)

    def log_prompt(self, prompt: str, prompt_type: str = "user"):
        """Log prompt content (only if enabled)."""
        if not self.log_prompts:
            # Log hash only for debugging without exposing content
            prompt_hash = hashlib.sha256(prompt.encode()).hexdigest()[:16]
            self.logger.debug(
                event="prompt_received",
                prompt_hash=prompt_hash,
                prompt_length=len(prompt),
                prompt_type=prompt_type
            )
            return

        self.logger.info(
            event="prompt_content",
            prompt=prompt,
            prompt_length=len(prompt),
            prompt_type=prompt_type
        )

    def log_response(self, response: str, finish_reason: str = "stop"):
        """Log response content (only if enabled)."""
        if not self.log_responses:
            response_hash = hashlib.sha256(response.encode()).hexdigest()[:16]
            self.logger.debug(
                event="response_generated",
                response_hash=response_hash,
                response_length=len(response),
                finish_reason=finish_reason
            )
            return

        self.logger.info(
            event="response_content",
            response=response,
            response_length=len(response),
            finish_reason=finish_reason
        )

    def log_cache_event(
        self,
        cache_type: str,
        hit: bool,
        key_hash: str,
        latency_ms: float
    ):
        """Log cache hit/miss events."""
        self.logger.debug(
            event="cache_access",
            cache_type=cache_type,
            cache_hit=hit,
            key_hash=key_hash,
            latency_ms=latency_ms
        )

    def log_model_event(
        self,
        event_type: str,
        model_id: str,
        details: Optional[Dict[str, Any]] = None
    ):
        """Log model lifecycle events."""
        self.logger.info(
            event=f"model_{event_type}",
            model_id=model_id,
            details=details or {}
        )

    def log_error(
        self,
        error_type: str,
        error_message: str,
        stack_trace: Optional[str] = None,
        recoverable: bool = True
    ):
        """Log error events."""
        log_method = self.logger.error if recoverable else self.logger.critical
        log_method(
            event="error",
            error_type=error_type,
            error_message=error_message,
            stack_trace=stack_trace,
            recoverable=recoverable
        )


def log_llm_call(logger: LLMLogger):
    """Decorator for logging LLM function calls."""
    def decorator(func):
        @wraps(func)
        async def async_wrapper(*args, **kwargs):
            import time
            start_time = time.perf_counter()

            try:
                result = await func(*args, **kwargs)
                latency_ms = (time.perf_counter() - start_time) * 1000

                logger.logger.info(
                    event="function_call",
                    function=func.__name__,
                    latency_ms=latency_ms,
                    status="success"
                )
                return result
            except Exception as e:
                latency_ms = (time.perf_counter() - start_time) * 1000
                logger.log_error(
                    error_type=type(e).__name__,
                    error_message=str(e),
                    recoverable=True
                )
                raise

        @wraps(func)
        def sync_wrapper(*args, **kwargs):
            import time
            start_time = time.perf_counter()

            try:
                result = func(*args, **kwargs)
                latency_ms = (time.perf_counter() - start_time) * 1000

                logger.logger.info(
                    event="function_call",
                    function=func.__name__,
                    latency_ms=latency_ms,
                    status="success"
                )
                return result
            except Exception as e:
                latency_ms = (time.perf_counter() - start_time) * 1000
                logger.log_error(
                    error_type=type(e).__name__,
                    error_message=str(e),
                    recoverable=True
                )
                raise

        import asyncio
        if asyncio.iscoroutinefunction(func):
            return async_wrapper
        return sync_wrapper
    return decorator
```

### Log Content Guidelines

```python
"""
Guidelines for what to log and what to exclude in LLM systems.
"""

from dataclasses import dataclass
from typing import List, Dict, Any, Optional
from enum import Enum


class LogCategory(Enum):
    """Categories of loggable information."""
    ALWAYS_LOG = "always"           # Always safe to log
    CONFIGURABLE = "configurable"   # Log based on configuration
    NEVER_LOG = "never"             # Never log (security/privacy)


@dataclass
class LoggableField:
    """Definition of a loggable field."""
    name: str
    category: LogCategory
    description: str
    example: Any


# Comprehensive logging guidelines
LOGGING_GUIDELINES = {
    "request_metadata": [
        LoggableField("request_id", LogCategory.ALWAYS_LOG,
                     "Unique request identifier", "req_abc123"),
        LoggableField("timestamp", LogCategory.ALWAYS_LOG,
                     "ISO8601 timestamp", "2024-01-15T10:30:00Z"),
        LoggableField("model_id", LogCategory.ALWAYS_LOG,
                     "Model identifier", "gpt-4-turbo"),
        LoggableField("api_version", LogCategory.ALWAYS_LOG,
                     "API version used", "2024-01-01"),
        LoggableField("client_ip", LogCategory.CONFIGURABLE,
                     "Client IP (may be PII)", "192.168.1.1"),
        LoggableField("user_agent", LogCategory.ALWAYS_LOG,
                     "Client user agent", "python-requests/2.28"),
    ],

    "timing_metrics": [
        LoggableField("total_latency_ms", LogCategory.ALWAYS_LOG,
                     "Total request latency", 1250.5),
        LoggableField("ttft_ms", LogCategory.ALWAYS_LOG,
                     "Time to first token", 180.2),
        LoggableField("queue_time_ms", LogCategory.ALWAYS_LOG,
                     "Time spent in queue", 50.0),
        LoggableField("inference_time_ms", LogCategory.ALWAYS_LOG,
                     "Model inference time", 1000.3),
        LoggableField("preprocessing_time_ms", LogCategory.ALWAYS_LOG,
                     "Input preprocessing time", 20.0),
    ],

    "token_metrics": [
        LoggableField("prompt_tokens", LogCategory.ALWAYS_LOG,
                     "Number of input tokens", 150),
        LoggableField("completion_tokens", LogCategory.ALWAYS_LOG,
                     "Number of output tokens", 500),
        LoggableField("total_tokens", LogCategory.ALWAYS_LOG,
                     "Total tokens used", 650),
        LoggableField("tokens_per_second", LogCategory.ALWAYS_LOG,
                     "Generation throughput", 45.5),
        LoggableField("cache_tokens", LogCategory.ALWAYS_LOG,
                     "Tokens served from cache", 50),
    ],

    "model_parameters": [
        LoggableField("temperature", LogCategory.ALWAYS_LOG,
                     "Sampling temperature", 0.7),
        LoggableField("max_tokens", LogCategory.ALWAYS_LOG,
                     "Maximum output tokens", 1000),
        LoggableField("top_p", LogCategory.ALWAYS_LOG,
                     "Nucleus sampling parameter", 0.9),
        LoggableField("frequency_penalty", LogCategory.ALWAYS_LOG,
                     "Frequency penalty value", 0.0),
        LoggableField("presence_penalty", LogCategory.ALWAYS_LOG,
                     "Presence penalty value", 0.0),
        LoggableField("stop_sequences", LogCategory.CONFIGURABLE,
                     "Stop sequences (may reveal intent)", ["\\n\\n"]),
    ],

    "content_fields": [
        LoggableField("prompt", LogCategory.CONFIGURABLE,
                     "User prompt content", "What is..."),
        LoggableField("response", LogCategory.CONFIGURABLE,
                     "Model response content", "The answer..."),
        LoggableField("system_prompt", LogCategory.CONFIGURABLE,
                     "System prompt content", "You are..."),
        LoggableField("prompt_hash", LogCategory.ALWAYS_LOG,
                     "SHA256 hash of prompt", "a1b2c3..."),
        LoggableField("response_hash", LogCategory.ALWAYS_LOG,
                     "SHA256 hash of response", "d4e5f6..."),
    ],

    "never_log": [
        LoggableField("api_key", LogCategory.NEVER_LOG,
                     "API authentication key", "sk-..."),
        LoggableField("password", LogCategory.NEVER_LOG,
                     "User password", "***"),
        LoggableField("ssn", LogCategory.NEVER_LOG,
                     "Social Security Number", "***-**-****"),
        LoggableField("credit_card", LogCategory.NEVER_LOG,
                     "Credit card number", "****-****-****-****"),
        LoggableField("private_key", LogCategory.NEVER_LOG,
                     "Cryptographic private key", "-----BEGIN..."),
        LoggableField("session_token", LogCategory.NEVER_LOG,
                     "Session authentication token", "sess_..."),
    ],
}


class LogContentFilter:
    """
    Filter log content based on guidelines and configuration.
    """

    def __init__(
        self,
        allow_prompts: bool = False,
        allow_responses: bool = False,
        allow_client_ip: bool = False,
        custom_filters: Optional[List[str]] = None
    ):
        self.allow_prompts = allow_prompts
        self.allow_responses = allow_responses
        self.allow_client_ip = allow_client_ip
        self.custom_filters = custom_filters or []

        # Build filter set
        self._build_filter_set()

    def _build_filter_set(self):
        """Build the set of fields to filter."""
        self.filtered_fields = set()

        # Add never-log fields
        for field in LOGGING_GUIDELINES["never_log"]:
            self.filtered_fields.add(field.name)

        # Add configurable fields based on settings
        if not self.allow_prompts:
            self.filtered_fields.add("prompt")
            self.filtered_fields.add("system_prompt")

        if not self.allow_responses:
            self.filtered_fields.add("response")

        if not self.allow_client_ip:
            self.filtered_fields.add("client_ip")

        # Add custom filters
        self.filtered_fields.update(self.custom_filters)

    def filter_log_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Filter sensitive fields from log data."""
        filtered = {}

        for key, value in data.items():
            if key in self.filtered_fields:
                filtered[key] = "[FILTERED]"
            elif isinstance(value, dict):
                filtered[key] = self.filter_log_data(value)
            elif isinstance(value, list):
                filtered[key] = [
                    self.filter_log_data(item) if isinstance(item, dict) else item
                    for item in value
                ]
            else:
                filtered[key] = value

        return filtered

    def should_log_field(self, field_name: str) -> bool:
        """Check if a field should be logged."""
        return field_name not in self.filtered_fields
```

## 10.2.2 Structured Logging

### JSON Log Format

```python
"""
Structured JSON logging for LLM systems.
"""

import json
import logging
from datetime import datetime, timezone
from typing import Any, Dict, Optional
from dataclasses import dataclass, asdict
import uuid
from contextvars import ContextVar


# Context variables for request tracking
current_request_id: ContextVar[str] = ContextVar('request_id', default='')
current_trace_id: ContextVar[str] = ContextVar('trace_id', default='')
current_span_id: ContextVar[str] = ContextVar('span_id', default='')
current_user_id: ContextVar[str] = ContextVar('user_id', default='')


@dataclass
class StructuredLogEntry:
    """Structured log entry with all required fields."""
    timestamp: str
    level: str
    message: str
    service: str
    version: str
    environment: str

    # Correlation fields
    request_id: str
    trace_id: str
    span_id: str
    parent_span_id: Optional[str] = None

    # Optional context
    user_id: Optional[str] = None
    session_id: Optional[str] = None

    # Event-specific data
    event_type: Optional[str] = None
    event_data: Optional[Dict[str, Any]] = None

    # Error information
    error_type: Optional[str] = None
    error_message: Optional[str] = None
    stack_trace: Optional[str] = None

    # Performance metrics
    duration_ms: Optional[float] = None

    def to_json(self) -> str:
        """Convert to JSON string."""
        data = {k: v for k, v in asdict(self).items() if v is not None}
        return json.dumps(data, default=str)


class JSONFormatter(logging.Formatter):
    """
    JSON log formatter for structured logging.
    """

    def __init__(
        self,
        service_name: str = "llm-service",
        version: str = "1.0.0",
        environment: str = "production"
    ):
        super().__init__()
        self.service_name = service_name
        self.version = version
        self.environment = environment

    def format(self, record: logging.LogRecord) -> str:
        """Format log record as JSON."""
        # Get context from context variables
        request_id = current_request_id.get() or str(uuid.uuid4())
        trace_id = current_trace_id.get() or str(uuid.uuid4())
        span_id = current_span_id.get() or str(uuid.uuid4())
        user_id = current_user_id.get() or None

        # Build base log entry
        log_entry = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(),
            "service": self.service_name,
            "version": self.version,
            "environment": self.environment,
            "request_id": request_id,
            "trace_id": trace_id,
            "span_id": span_id,
        }

        # Add user context
        if user_id:
            log_entry["user_id"] = user_id

        # Add source information
        log_entry["source"] = {
            "file": record.pathname,
            "line": record.lineno,
            "function": record.funcName
        }

        # Add extra fields
        if hasattr(record, 'extra_data'):
            log_entry["data"] = record.extra_data

        # Add exception information
        if record.exc_info:
            import traceback
            log_entry["error"] = {
                "type": record.exc_info[0].__name__ if record.exc_info[0] else None,
                "message": str(record.exc_info[1]) if record.exc_info[1] else None,
                "stack_trace": ''.join(traceback.format_exception(*record.exc_info))
            }

        return json.dumps(log_entry, default=str)


class CorrelationIDMiddleware:
    """
    ASGI middleware for correlation ID management.
    """

    def __init__(self, app, header_name: str = "X-Request-ID"):
        self.app = app
        self.header_name = header_name

    async def __call__(self, scope, receive, send):
        if scope["type"] != "http":
            await self.app(scope, receive, send)
            return

        # Extract or generate correlation IDs
        headers = dict(scope.get("headers", []))

        request_id = headers.get(
            self.header_name.lower().encode(),
            str(uuid.uuid4()).encode()
        ).decode()

        trace_id = headers.get(
            b"x-trace-id",
            str(uuid.uuid4()).encode()
        ).decode()

        span_id = str(uuid.uuid4())

        # Set context variables
        request_id_token = current_request_id.set(request_id)
        trace_id_token = current_trace_id.set(trace_id)
        span_id_token = current_span_id.set(span_id)

        # Add correlation headers to response
        async def send_wrapper(message):
            if message["type"] == "http.response.start":
                headers = list(message.get("headers", []))
                headers.append((b"x-request-id", request_id.encode()))
                headers.append((b"x-trace-id", trace_id.encode()))
                message["headers"] = headers
            await send(message)

        try:
            await self.app(scope, receive, send_wrapper)
        finally:
            # Reset context variables
            current_request_id.reset(request_id_token)
            current_trace_id.reset(trace_id_token)
            current_span_id.reset(span_id_token)


class StructuredLogger:
    """
    High-level structured logger for LLM applications.
    """

    def __init__(
        self,
        name: str,
        service_name: str = "llm-service",
        version: str = "1.0.0",
        environment: str = "production",
        log_level: int = logging.INFO
    ):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(log_level)

        # Remove existing handlers
        self.logger.handlers.clear()

        # Add JSON formatter handler
        handler = logging.StreamHandler()
        handler.setFormatter(JSONFormatter(service_name, version, environment))
        self.logger.addHandler(handler)

    def _log(
        self,
        level: int,
        message: str,
        event_type: Optional[str] = None,
        **kwargs
    ):
        """Internal logging method with extra data."""
        extra_data = {"event_type": event_type, **kwargs} if event_type or kwargs else None

        record = self.logger.makeRecord(
            self.logger.name,
            level,
            "(unknown)",
            0,
            message,
            (),
            None
        )

        if extra_data:
            record.extra_data = extra_data

        self.logger.handle(record)

    def info(self, message: str, **kwargs):
        self._log(logging.INFO, message, **kwargs)

    def debug(self, message: str, **kwargs):
        self._log(logging.DEBUG, message, **kwargs)

    def warning(self, message: str, **kwargs):
        self._log(logging.WARNING, message, **kwargs)

    def error(self, message: str, **kwargs):
        self._log(logging.ERROR, message, **kwargs)

    def critical(self, message: str, **kwargs):
        self._log(logging.CRITICAL, message, **kwargs)

    # LLM-specific logging methods
    def log_inference_start(
        self,
        model: str,
        prompt_tokens: int,
        parameters: Dict[str, Any]
    ):
        """Log inference start event."""
        self.info(
            f"Starting inference with {model}",
            event_type="inference_start",
            model=model,
            prompt_tokens=prompt_tokens,
            parameters=parameters
        )

    def log_inference_complete(
        self,
        model: str,
        prompt_tokens: int,
        completion_tokens: int,
        duration_ms: float,
        finish_reason: str
    ):
        """Log inference completion event."""
        self.info(
            f"Inference complete: {completion_tokens} tokens in {duration_ms:.1f}ms",
            event_type="inference_complete",
            model=model,
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens,
            duration_ms=duration_ms,
            finish_reason=finish_reason,
            tokens_per_second=completion_tokens / (duration_ms / 1000) if duration_ms > 0 else 0
        )

    def log_cache_event(self, cache_type: str, hit: bool, key: str):
        """Log cache access event."""
        self.debug(
            f"Cache {'hit' if hit else 'miss'}: {cache_type}",
            event_type="cache_access",
            cache_type=cache_type,
            cache_hit=hit,
            cache_key_hash=key[:16] if len(key) > 16 else key
        )
```

### Timestamp Standardization

```python
"""
Timestamp handling and standardization for distributed LLM systems.
"""

from datetime import datetime, timezone
from typing import Optional, Union
import time


class TimestampHandler:
    """
    Standardized timestamp handling for logs across distributed systems.
    """

    # ISO 8601 format with microseconds and timezone
    ISO_FORMAT = "%Y-%m-%dT%H:%M:%S.%f%z"
    ISO_FORMAT_SHORT = "%Y-%m-%dT%H:%M:%S%z"

    @staticmethod
    def now() -> datetime:
        """Get current UTC timestamp."""
        return datetime.now(timezone.utc)

    @staticmethod
    def now_iso() -> str:
        """Get current timestamp as ISO 8601 string."""
        return datetime.now(timezone.utc).isoformat()

    @staticmethod
    def now_epoch_ms() -> int:
        """Get current timestamp as epoch milliseconds."""
        return int(time.time() * 1000)

    @staticmethod
    def now_epoch_ns() -> int:
        """Get current timestamp as epoch nanoseconds (for high precision)."""
        return time.time_ns()

    @staticmethod
    def from_epoch_ms(epoch_ms: int) -> datetime:
        """Convert epoch milliseconds to datetime."""
        return datetime.fromtimestamp(epoch_ms / 1000, tz=timezone.utc)

    @staticmethod
    def from_epoch_ns(epoch_ns: int) -> datetime:
        """Convert epoch nanoseconds to datetime."""
        return datetime.fromtimestamp(epoch_ns / 1e9, tz=timezone.utc)

    @staticmethod
    def parse_iso(timestamp_str: str) -> datetime:
        """Parse ISO 8601 timestamp string."""
        # Handle 'Z' suffix
        if timestamp_str.endswith('Z'):
            timestamp_str = timestamp_str[:-1] + '+00:00'
        return datetime.fromisoformat(timestamp_str)

    @staticmethod
    def to_iso(dt: datetime) -> str:
        """Convert datetime to ISO 8601 string."""
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.isoformat()

    @staticmethod
    def duration_ms(start: datetime, end: Optional[datetime] = None) -> float:
        """Calculate duration in milliseconds."""
        if end is None:
            end = datetime.now(timezone.utc)
        return (end - start).total_seconds() * 1000


class HighPrecisionTimer:
    """
    High-precision timer for measuring LLM operation latencies.
    """

    def __init__(self):
        self._start_ns: Optional[int] = None
        self._end_ns: Optional[int] = None
        self._checkpoints: dict[str, int] = {}

    def start(self) -> 'HighPrecisionTimer':
        """Start the timer."""
        self._start_ns = time.perf_counter_ns()
        self._checkpoints = {}
        return self

    def checkpoint(self, name: str) -> float:
        """Record a checkpoint and return elapsed milliseconds."""
        if self._start_ns is None:
            raise RuntimeError("Timer not started")

        checkpoint_ns = time.perf_counter_ns()
        self._checkpoints[name] = checkpoint_ns
        return (checkpoint_ns - self._start_ns) / 1e6

    def stop(self) -> float:
        """Stop the timer and return total elapsed milliseconds."""
        if self._start_ns is None:
            raise RuntimeError("Timer not started")

        self._end_ns = time.perf_counter_ns()
        return self.elapsed_ms

    @property
    def elapsed_ms(self) -> float:
        """Get elapsed milliseconds."""
        if self._start_ns is None:
            return 0.0

        end = self._end_ns or time.perf_counter_ns()
        return (end - self._start_ns) / 1e6

    @property
    def elapsed_us(self) -> float:
        """Get elapsed microseconds."""
        return self.elapsed_ms * 1000

    def get_checkpoint_ms(self, name: str) -> Optional[float]:
        """Get milliseconds at a specific checkpoint from start."""
        if name not in self._checkpoints or self._start_ns is None:
            return None
        return (self._checkpoints[name] - self._start_ns) / 1e6

    def get_segment_ms(self, start_checkpoint: str, end_checkpoint: str) -> Optional[float]:
        """Get milliseconds between two checkpoints."""
        if start_checkpoint not in self._checkpoints or end_checkpoint not in self._checkpoints:
            return None
        return (self._checkpoints[end_checkpoint] - self._checkpoints[start_checkpoint]) / 1e6

    def to_dict(self) -> dict:
        """Export timer data as dictionary."""
        result = {
            "total_ms": self.elapsed_ms,
            "checkpoints": {}
        }

        if self._start_ns:
            for name, ns in self._checkpoints.items():
                result["checkpoints"][name] = (ns - self._start_ns) / 1e6

        return result
```

## 10.2.3 Distributed Tracing

### OpenTelemetry Integration

```python
"""
OpenTelemetry integration for distributed tracing in LLM systems.
"""

from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.sdk.resources import Resource, SERVICE_NAME, SERVICE_VERSION
from opentelemetry.trace import Status, StatusCode, SpanKind
from opentelemetry.trace.propagation.tracecontext import TraceContextTextMapPropagator
from opentelemetry.propagators import set_global_textmap
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
from opentelemetry.instrumentation.httpx import HTTPXClientInstrumentor
from opentelemetry.instrumentation.redis import RedisInstrumentor
from opentelemetry.semconv.trace import SpanAttributes

from typing import Any, Dict, Optional, Callable
from functools import wraps
from contextlib import contextmanager
import asyncio


class LLMTracingConfig:
    """Configuration for LLM tracing."""

    def __init__(
        self,
        service_name: str = "llm-service",
        service_version: str = "1.0.0",
        environment: str = "production",
        otlp_endpoint: Optional[str] = None,
        jaeger_host: Optional[str] = None,
        jaeger_port: int = 6831,
        sampling_rate: float = 1.0,
        enable_auto_instrumentation: bool = True
    ):
        self.service_name = service_name
        self.service_version = service_version
        self.environment = environment
        self.otlp_endpoint = otlp_endpoint
        self.jaeger_host = jaeger_host
        self.jaeger_port = jaeger_port
        self.sampling_rate = sampling_rate
        self.enable_auto_instrumentation = enable_auto_instrumentation


class LLMTracer:
    """
    Distributed tracer for LLM systems with OpenTelemetry.
    """

    # Custom span attribute names for LLM operations
    LLM_MODEL_NAME = "llm.model.name"
    LLM_MODEL_PROVIDER = "llm.model.provider"
    LLM_PROMPT_TOKENS = "llm.usage.prompt_tokens"
    LLM_COMPLETION_TOKENS = "llm.usage.completion_tokens"
    LLM_TOTAL_TOKENS = "llm.usage.total_tokens"
    LLM_TEMPERATURE = "llm.request.temperature"
    LLM_MAX_TOKENS = "llm.request.max_tokens"
    LLM_FINISH_REASON = "llm.response.finish_reason"
    LLM_TTFT_MS = "llm.latency.ttft_ms"
    LLM_CACHE_HIT = "llm.cache.hit"

    def __init__(self, config: LLMTracingConfig):
        self.config = config
        self._setup_tracing()

    def _setup_tracing(self):
        """Initialize OpenTelemetry tracing."""
        # Create resource
        resource = Resource.create({
            SERVICE_NAME: self.config.service_name,
            SERVICE_VERSION: self.config.service_version,
            "deployment.environment": self.config.environment
        })

        # Create tracer provider
        provider = TracerProvider(resource=resource)

        # Add exporters
        if self.config.otlp_endpoint:
            otlp_exporter = OTLPSpanExporter(endpoint=self.config.otlp_endpoint)
            provider.add_span_processor(BatchSpanProcessor(otlp_exporter))

        if self.config.jaeger_host:
            jaeger_exporter = JaegerExporter(
                agent_host_name=self.config.jaeger_host,
                agent_port=self.config.jaeger_port
            )
            provider.add_span_processor(BatchSpanProcessor(jaeger_exporter))

        # Set global tracer provider
        trace.set_tracer_provider(provider)

        # Set up context propagation
        set_global_textmap(TraceContextTextMapPropagator())

        # Get tracer
        self.tracer = trace.get_tracer(
            self.config.service_name,
            self.config.service_version
        )

        # Auto-instrument if enabled
        if self.config.enable_auto_instrumentation:
            self._setup_auto_instrumentation()

    def _setup_auto_instrumentation(self):
        """Set up automatic instrumentation for common libraries."""
        try:
            HTTPXClientInstrumentor().instrument()
        except Exception:
            pass

        try:
            RedisInstrumentor().instrument()
        except Exception:
            pass

    @contextmanager
    def trace_llm_request(
        self,
        operation_name: str,
        model: str,
        provider: str = "unknown",
        attributes: Optional[Dict[str, Any]] = None
    ):
        """
        Context manager for tracing LLM requests.
        """
        with self.tracer.start_as_current_span(
            operation_name,
            kind=SpanKind.CLIENT,
            attributes={
                self.LLM_MODEL_NAME: model,
                self.LLM_MODEL_PROVIDER: provider,
                **(attributes or {})
            }
        ) as span:
            try:
                yield LLMSpanContext(span, self)
            except Exception as e:
                span.set_status(Status(StatusCode.ERROR, str(e)))
                span.record_exception(e)
                raise

    @contextmanager
    def trace_operation(
        self,
        operation_name: str,
        kind: SpanKind = SpanKind.INTERNAL,
        attributes: Optional[Dict[str, Any]] = None
    ):
        """
        Context manager for tracing general operations.
        """
        with self.tracer.start_as_current_span(
            operation_name,
            kind=kind,
            attributes=attributes or {}
        ) as span:
            try:
                yield span
            except Exception as e:
                span.set_status(Status(StatusCode.ERROR, str(e)))
                span.record_exception(e)
                raise

    def trace_function(
        self,
        operation_name: Optional[str] = None,
        attributes: Optional[Dict[str, Any]] = None
    ):
        """
        Decorator for tracing function calls.
        """
        def decorator(func: Callable):
            name = operation_name or func.__name__

            @wraps(func)
            async def async_wrapper(*args, **kwargs):
                with self.trace_operation(name, attributes=attributes):
                    return await func(*args, **kwargs)

            @wraps(func)
            def sync_wrapper(*args, **kwargs):
                with self.trace_operation(name, attributes=attributes):
                    return func(*args, **kwargs)

            if asyncio.iscoroutinefunction(func):
                return async_wrapper
            return sync_wrapper

        return decorator


class LLMSpanContext:
    """
    Context wrapper for LLM spans with convenience methods.
    """

    def __init__(self, span, tracer: LLMTracer):
        self.span = span
        self.tracer = tracer

    def set_request_params(
        self,
        temperature: float,
        max_tokens: int,
        prompt_tokens: Optional[int] = None,
        **kwargs
    ):
        """Set request parameters on the span."""
        self.span.set_attribute(self.tracer.LLM_TEMPERATURE, temperature)
        self.span.set_attribute(self.tracer.LLM_MAX_TOKENS, max_tokens)

        if prompt_tokens is not None:
            self.span.set_attribute(self.tracer.LLM_PROMPT_TOKENS, prompt_tokens)

        for key, value in kwargs.items():
            self.span.set_attribute(f"llm.request.{key}", value)

    def set_response_metrics(
        self,
        completion_tokens: int,
        total_tokens: int,
        finish_reason: str,
        ttft_ms: Optional[float] = None
    ):
        """Set response metrics on the span."""
        self.span.set_attribute(self.tracer.LLM_COMPLETION_TOKENS, completion_tokens)
        self.span.set_attribute(self.tracer.LLM_TOTAL_TOKENS, total_tokens)
        self.span.set_attribute(self.tracer.LLM_FINISH_REASON, finish_reason)

        if ttft_ms is not None:
            self.span.set_attribute(self.tracer.LLM_TTFT_MS, ttft_ms)

    def set_cache_hit(self, hit: bool):
        """Record cache hit/miss."""
        self.span.set_attribute(self.tracer.LLM_CACHE_HIT, hit)

    def add_event(self, name: str, attributes: Optional[Dict[str, Any]] = None):
        """Add an event to the span."""
        self.span.add_event(name, attributes=attributes or {})

    def set_error(self, error: Exception):
        """Mark span as error."""
        self.span.set_status(Status(StatusCode.ERROR, str(error)))
        self.span.record_exception(error)


class TracePropagator:
    """
    Utility for propagating trace context across service boundaries.
    """

    def __init__(self):
        self.propagator = TraceContextTextMapPropagator()

    def inject_headers(self, headers: Dict[str, str]) -> Dict[str, str]:
        """Inject trace context into headers."""
        self.propagator.inject(headers)
        return headers

    def extract_context(self, headers: Dict[str, str]):
        """Extract trace context from headers."""
        return self.propagator.extract(headers)

    @staticmethod
    def get_current_trace_id() -> Optional[str]:
        """Get current trace ID."""
        span = trace.get_current_span()
        if span and span.get_span_context().is_valid:
            return format(span.get_span_context().trace_id, '032x')
        return None

    @staticmethod
    def get_current_span_id() -> Optional[str]:
        """Get current span ID."""
        span = trace.get_current_span()
        if span and span.get_span_context().is_valid:
            return format(span.get_span_context().span_id, '016x')
        return None
```

### Sampling Strategies

```python
"""
Sampling strategies for distributed tracing in LLM systems.
"""

from opentelemetry.sdk.trace.sampling import (
    Sampler,
    SamplingResult,
    Decision,
    ParentBased,
    TraceIdRatioBased,
    ALWAYS_ON,
    ALWAYS_OFF
)
from opentelemetry.trace import SpanKind, Link
from opentelemetry.util.types import Attributes
from typing import Optional, Sequence, Callable
import random


class LLMAdaptiveSampler(Sampler):
    """
    Adaptive sampler that adjusts sampling based on LLM request characteristics.

    - Always samples errors
    - Higher sampling for slow requests
    - Lower sampling for cached responses
    - Configurable base rate
    """

    def __init__(
        self,
        base_rate: float = 0.1,
        error_rate: float = 1.0,
        slow_threshold_ms: float = 5000,
        slow_rate: float = 0.5,
        cache_hit_rate: float = 0.01
    ):
        self.base_rate = base_rate
        self.error_rate = error_rate
        self.slow_threshold_ms = slow_threshold_ms
        self.slow_rate = slow_rate
        self.cache_hit_rate = cache_hit_rate

    def should_sample(
        self,
        parent_context: Optional["Context"],
        trace_id: int,
        name: str,
        kind: SpanKind = None,
        attributes: Attributes = None,
        links: Sequence[Link] = None,
    ) -> SamplingResult:
        """Determine if span should be sampled."""
        attributes = attributes or {}

        # Always sample errors
        if attributes.get("error", False):
            return SamplingResult(Decision.RECORD_AND_SAMPLE, attributes)

        # Determine sampling rate based on attributes
        sampling_rate = self.base_rate

        # Check for cache hit
        if attributes.get("llm.cache.hit", False):
            sampling_rate = self.cache_hit_rate

        # Check for slow requests (if duration is known)
        latency_ms = attributes.get("llm.latency.total_ms", 0)
        if latency_ms > self.slow_threshold_ms:
            sampling_rate = max(sampling_rate, self.slow_rate)

        # Use trace ID for consistent sampling decision
        threshold = trace_id & 0xFFFFFFFF  # Lower 32 bits
        if threshold < (sampling_rate * 0xFFFFFFFF):
            return SamplingResult(Decision.RECORD_AND_SAMPLE, attributes)

        return SamplingResult(Decision.DROP, attributes)

    def get_description(self) -> str:
        return f"LLMAdaptiveSampler(base={self.base_rate})"


class HeadBasedSampler(Sampler):
    """
    Head-based sampling that makes sampling decision at trace start.
    Ensures all spans in a trace have the same sampling decision.
    """

    def __init__(self, rate: float = 0.1):
        self.rate = rate

    def should_sample(
        self,
        parent_context: Optional["Context"],
        trace_id: int,
        name: str,
        kind: SpanKind = None,
        attributes: Attributes = None,
        links: Sequence[Link] = None,
    ) -> SamplingResult:
        """Sample based on trace ID for consistency."""
        # Use trace ID to make deterministic decision
        threshold = trace_id & 0xFFFFFFFF

        if threshold < (self.rate * 0xFFFFFFFF):
            return SamplingResult(Decision.RECORD_AND_SAMPLE, attributes or {})

        return SamplingResult(Decision.DROP, attributes or {})

    def get_description(self) -> str:
        return f"HeadBasedSampler({self.rate})"


class TailBasedSamplerConfig:
    """
    Configuration for tail-based sampling (typically done by collector).
    This class generates the configuration for external collectors.
    """

    def __init__(
        self,
        error_rate: float = 1.0,
        latency_threshold_ms: float = 5000,
        latency_rate: float = 0.5,
        base_rate: float = 0.1
    ):
        self.error_rate = error_rate
        self.latency_threshold_ms = latency_threshold_ms
        self.latency_rate = latency_rate
        self.base_rate = base_rate

    def to_otel_collector_config(self) -> dict:
        """Generate OpenTelemetry Collector tail sampling config."""
        return {
            "processors": {
                "tail_sampling": {
                    "decision_wait": "30s",
                    "num_traces": 100000,
                    "expected_new_traces_per_sec": 1000,
                    "policies": [
                        {
                            "name": "errors-policy",
                            "type": "status_code",
                            "status_code": {"status_codes": ["ERROR"]},
                        },
                        {
                            "name": "slow-traces-policy",
                            "type": "latency",
                            "latency": {
                                "threshold_ms": int(self.latency_threshold_ms)
                            },
                        },
                        {
                            "name": "probabilistic-policy",
                            "type": "probabilistic",
                            "probabilistic": {
                                "sampling_percentage": self.base_rate * 100
                            },
                        },
                    ],
                }
            }
        }


def create_sampler(
    strategy: str = "adaptive",
    base_rate: float = 0.1,
    **kwargs
) -> Sampler:
    """
    Factory function to create appropriate sampler.

    Strategies:
    - "always_on": Sample everything (development)
    - "always_off": Sample nothing (emergency)
    - "ratio": Fixed ratio sampling
    - "adaptive": LLM-aware adaptive sampling
    - "parent_based": Respect parent sampling decision
    """
    if strategy == "always_on":
        return ALWAYS_ON

    if strategy == "always_off":
        return ALWAYS_OFF

    if strategy == "ratio":
        return TraceIdRatioBased(base_rate)

    if strategy == "adaptive":
        return LLMAdaptiveSampler(base_rate=base_rate, **kwargs)

    if strategy == "parent_based":
        root_sampler = LLMAdaptiveSampler(base_rate=base_rate, **kwargs)
        return ParentBased(root=root_sampler)

    raise ValueError(f"Unknown sampling strategy: {strategy}")
```

## 10.2.4 Log Aggregation

### ELK Stack Configuration

```python
"""
ELK Stack (Elasticsearch, Logstash, Kibana) configuration for LLM logs.
"""

from dataclasses import dataclass
from typing import Dict, Any, List, Optional
import json


@dataclass
class ElasticsearchConfig:
    """Elasticsearch configuration for LLM logs."""
    hosts: List[str]
    index_prefix: str = "llm-logs"
    shards: int = 3
    replicas: int = 1
    refresh_interval: str = "5s"

    def get_index_template(self) -> Dict[str, Any]:
        """Generate Elasticsearch index template for LLM logs."""
        return {
            "index_patterns": [f"{self.index_prefix}-*"],
            "settings": {
                "number_of_shards": self.shards,
                "number_of_replicas": self.replicas,
                "refresh_interval": self.refresh_interval,
                "index.mapping.total_fields.limit": 2000,
            },
            "mappings": {
                "properties": {
                    # Timestamp and identifiers
                    "@timestamp": {"type": "date"},
                    "request_id": {"type": "keyword"},
                    "trace_id": {"type": "keyword"},
                    "span_id": {"type": "keyword"},
                    "user_id": {"type": "keyword"},
                    "session_id": {"type": "keyword"},

                    # Service metadata
                    "service": {"type": "keyword"},
                    "version": {"type": "keyword"},
                    "environment": {"type": "keyword"},
                    "host": {"type": "keyword"},

                    # Log metadata
                    "level": {"type": "keyword"},
                    "logger": {"type": "keyword"},
                    "message": {"type": "text"},
                    "event_type": {"type": "keyword"},

                    # LLM-specific fields
                    "model": {"type": "keyword"},
                    "model_provider": {"type": "keyword"},
                    "prompt_tokens": {"type": "integer"},
                    "completion_tokens": {"type": "integer"},
                    "total_tokens": {"type": "integer"},
                    "tokens_per_second": {"type": "float"},

                    # Timing metrics
                    "latency_ms": {"type": "float"},
                    "ttft_ms": {"type": "float"},
                    "queue_time_ms": {"type": "float"},
                    "inference_time_ms": {"type": "float"},

                    # Request parameters
                    "temperature": {"type": "float"},
                    "max_tokens": {"type": "integer"},
                    "top_p": {"type": "float"},

                    # Cache metrics
                    "cache_hit": {"type": "boolean"},
                    "cache_type": {"type": "keyword"},

                    # Error fields
                    "error_type": {"type": "keyword"},
                    "error_message": {"type": "text"},
                    "stack_trace": {"type": "text"},

                    # Content hashes (not actual content)
                    "prompt_hash": {"type": "keyword"},
                    "response_hash": {"type": "keyword"},
                }
            }
        }

    def get_ilm_policy(self, retention_days: int = 30) -> Dict[str, Any]:
        """Generate Index Lifecycle Management policy."""
        return {
            "policy": {
                "phases": {
                    "hot": {
                        "min_age": "0ms",
                        "actions": {
                            "rollover": {
                                "max_size": "50GB",
                                "max_age": "1d"
                            },
                            "set_priority": {
                                "priority": 100
                            }
                        }
                    },
                    "warm": {
                        "min_age": "7d",
                        "actions": {
                            "shrink": {
                                "number_of_shards": 1
                            },
                            "forcemerge": {
                                "max_num_segments": 1
                            },
                            "set_priority": {
                                "priority": 50
                            }
                        }
                    },
                    "cold": {
                        "min_age": "14d",
                        "actions": {
                            "set_priority": {
                                "priority": 0
                            },
                            "freeze": {}
                        }
                    },
                    "delete": {
                        "min_age": f"{retention_days}d",
                        "actions": {
                            "delete": {}
                        }
                    }
                }
            }
        }


def generate_logstash_config(
    elasticsearch_hosts: List[str],
    index_prefix: str = "llm-logs"
) -> str:
    """Generate Logstash pipeline configuration."""
    return f"""
input {{
  beats {{
    port => 5044
  }}

  tcp {{
    port => 5000
    codec => json_lines
  }}
}}

filter {{
  # Parse JSON logs
  if [message] =~ /^\\{{/ {{
    json {{
      source => "message"
    }}
  }}

  # Parse timestamp
  date {{
    match => ["timestamp", "ISO8601"]
    target => "@timestamp"
  }}

  # Add geo-ip for client IPs (if present and allowed)
  if [client_ip] {{
    geoip {{
      source => "client_ip"
      target => "geoip"
    }}
  }}

  # Calculate derived metrics
  if [completion_tokens] and [latency_ms] {{
    ruby {{
      code => "
        latency_s = event.get('latency_ms').to_f / 1000
        if latency_s > 0
          event.set('tokens_per_second', event.get('completion_tokens').to_f / latency_s)
        end
      "
    }}
  }}

  # Categorize log level
  if [level] == "ERROR" or [level] == "CRITICAL" {{
    mutate {{
      add_tag => ["error"]
    }}
  }}

  # Tag slow requests
  if [latency_ms] and [latency_ms] > 5000 {{
    mutate {{
      add_tag => ["slow_request"]
    }}
  }}

  # Remove sensitive fields
  mutate {{
    remove_field => ["api_key", "password", "secret", "token"]
  }}
}}

output {{
  elasticsearch {{
    hosts => {json.dumps(elasticsearch_hosts)}
    index => "{index_prefix}-%{{+YYYY.MM.dd}}"
    ilm_enabled => true
    ilm_rollover_alias => "{index_prefix}"
    ilm_policy => "{index_prefix}-policy"
  }}

  # Debug output (disable in production)
  # stdout {{ codec => rubydebug }}
}}
"""


def generate_kibana_dashboard() -> Dict[str, Any]:
    """Generate Kibana dashboard configuration for LLM monitoring."""
    return {
        "title": "LLM Service Dashboard",
        "panels": [
            {
                "title": "Request Rate",
                "type": "line",
                "query": {
                    "aggs": {
                        "requests": {
                            "date_histogram": {
                                "field": "@timestamp",
                                "fixed_interval": "1m"
                            }
                        }
                    }
                }
            },
            {
                "title": "Latency Percentiles",
                "type": "line",
                "query": {
                    "aggs": {
                        "latency_percentiles": {
                            "date_histogram": {
                                "field": "@timestamp",
                                "fixed_interval": "1m"
                            },
                            "aggs": {
                                "p50": {"percentiles": {"field": "latency_ms", "percents": [50]}},
                                "p95": {"percentiles": {"field": "latency_ms", "percents": [95]}},
                                "p99": {"percentiles": {"field": "latency_ms", "percents": [99]}}
                            }
                        }
                    }
                }
            },
            {
                "title": "Token Usage by Model",
                "type": "bar",
                "query": {
                    "aggs": {
                        "by_model": {
                            "terms": {"field": "model"},
                            "aggs": {
                                "total_tokens": {"sum": {"field": "total_tokens"}}
                            }
                        }
                    }
                }
            },
            {
                "title": "Error Rate",
                "type": "line",
                "query": {
                    "query": {"term": {"level": "ERROR"}},
                    "aggs": {
                        "errors": {
                            "date_histogram": {
                                "field": "@timestamp",
                                "fixed_interval": "1m"
                            }
                        }
                    }
                }
            },
            {
                "title": "Cache Hit Rate",
                "type": "gauge",
                "query": {
                    "aggs": {
                        "cache_hits": {
                            "filter": {"term": {"cache_hit": True}}
                        },
                        "total": {
                            "value_count": {"field": "request_id"}
                        }
                    }
                }
            }
        ]
    }
```

### Loki + Grafana Configuration

```python
"""
Grafana Loki configuration for LLM log aggregation.
"""

from dataclasses import dataclass
from typing import Dict, Any, List, Optional
import yaml


@dataclass
class LokiConfig:
    """Loki configuration for LLM logs."""

    storage_backend: str = "filesystem"  # filesystem, s3, gcs
    retention_days: int = 30
    chunk_target_size: int = 1572864  # 1.5MB
    max_chunk_age: str = "1h"

    def generate_config(self) -> Dict[str, Any]:
        """Generate Loki configuration."""
        return {
            "auth_enabled": False,
            "server": {
                "http_listen_port": 3100,
                "grpc_listen_port": 9096
            },
            "common": {
                "path_prefix": "/loki",
                "storage": {
                    "filesystem": {
                        "chunks_directory": "/loki/chunks",
                        "rules_directory": "/loki/rules"
                    }
                },
                "replication_factor": 1,
                "ring": {
                    "instance_addr": "127.0.0.1",
                    "kvstore": {
                        "store": "inmemory"
                    }
                }
            },
            "schema_config": {
                "configs": [
                    {
                        "from": "2024-01-01",
                        "store": "boltdb-shipper",
                        "object_store": self.storage_backend,
                        "schema": "v11",
                        "index": {
                            "prefix": "llm_index_",
                            "period": "24h"
                        }
                    }
                ]
            },
            "ruler": {
                "alertmanager_url": "http://alertmanager:9093"
            },
            "limits_config": {
                "retention_period": f"{self.retention_days * 24}h",
                "enforce_metric_name": False,
                "ingestion_rate_mb": 16,
                "ingestion_burst_size_mb": 32,
                "max_streams_per_user": 10000,
                "max_entries_limit_per_query": 50000
            },
            "chunk_store_config": {
                "max_look_back_period": "0s"
            },
            "table_manager": {
                "retention_deletes_enabled": True,
                "retention_period": f"{self.retention_days * 24}h"
            }
        }

    def to_yaml(self) -> str:
        """Export configuration as YAML."""
        return yaml.dump(self.generate_config(), default_flow_style=False)


class PromtailConfig:
    """Promtail configuration for shipping logs to Loki."""

    def __init__(
        self,
        loki_url: str = "http://loki:3100/loki/api/v1/push",
        log_paths: List[str] = None
    ):
        self.loki_url = loki_url
        self.log_paths = log_paths or ["/var/log/llm-service/*.log"]

    def generate_config(self) -> Dict[str, Any]:
        """Generate Promtail configuration."""
        return {
            "server": {
                "http_listen_port": 9080,
                "grpc_listen_port": 0
            },
            "positions": {
                "filename": "/tmp/positions.yaml"
            },
            "clients": [
                {"url": self.loki_url}
            ],
            "scrape_configs": [
                {
                    "job_name": "llm-service",
                    "static_configs": [
                        {
                            "targets": ["localhost"],
                            "labels": {
                                "job": "llm-service",
                                "__path__": path
                            }
                        }
                        for path in self.log_paths
                    ],
                    "pipeline_stages": [
                        # Parse JSON logs
                        {
                            "json": {
                                "expressions": {
                                    "level": "level",
                                    "service": "service",
                                    "model": "model",
                                    "request_id": "request_id",
                                    "trace_id": "trace_id",
                                    "latency_ms": "latency_ms",
                                    "error_type": "error_type"
                                }
                            }
                        },
                        # Add labels
                        {
                            "labels": {
                                "level": "",
                                "service": "",
                                "model": ""
                            }
                        },
                        # Parse timestamp
                        {
                            "timestamp": {
                                "source": "timestamp",
                                "format": "RFC3339"
                            }
                        },
                        # Add metrics
                        {
                            "metrics": {
                                "llm_request_latency": {
                                    "type": "Histogram",
                                    "description": "LLM request latency",
                                    "source": "latency_ms",
                                    "config": {
                                        "buckets": [100, 250, 500, 1000, 2500, 5000, 10000]
                                    }
                                }
                            }
                        }
                    ]
                }
            ]
        }

    def to_yaml(self) -> str:
        """Export configuration as YAML."""
        return yaml.dump(self.generate_config(), default_flow_style=False)


def generate_grafana_loki_datasource() -> Dict[str, Any]:
    """Generate Grafana datasource configuration for Loki."""
    return {
        "apiVersion": 1,
        "datasources": [
            {
                "name": "Loki",
                "type": "loki",
                "access": "proxy",
                "url": "http://loki:3100",
                "jsonData": {
                    "maxLines": 1000,
                    "derivedFields": [
                        {
                            "name": "TraceID",
                            "matcherRegex": '"trace_id":"([^"]+)"',
                            "url": "http://jaeger:16686/trace/${__value.raw}",
                            "datasourceUid": "jaeger"
                        }
                    ]
                }
            }
        ]
    }


def generate_loki_alert_rules() -> Dict[str, Any]:
    """Generate Loki alerting rules for LLM service."""
    return {
        "groups": [
            {
                "name": "llm-service-alerts",
                "rules": [
                    {
                        "alert": "HighErrorRate",
                        "expr": """
                            sum(rate({job="llm-service"} |= "ERROR" [5m]))
                            / sum(rate({job="llm-service"} [5m]))
                            > 0.05
                        """,
                        "for": "5m",
                        "labels": {
                            "severity": "critical"
                        },
                        "annotations": {
                            "summary": "High error rate in LLM service",
                            "description": "Error rate is above 5% for the last 5 minutes"
                        }
                    },
                    {
                        "alert": "SlowRequests",
                        "expr": """
                            quantile_over_time(0.95,
                                {job="llm-service"}
                                | json
                                | unwrap latency_ms [5m]
                            ) > 5000
                        """,
                        "for": "10m",
                        "labels": {
                            "severity": "warning"
                        },
                        "annotations": {
                            "summary": "Slow LLM requests detected",
                            "description": "95th percentile latency is above 5 seconds"
                        }
                    },
                    {
                        "alert": "NoLogsReceived",
                        "expr": """
                            absent(rate({job="llm-service"} [5m]))
                        """,
                        "for": "5m",
                        "labels": {
                            "severity": "critical"
                        },
                        "annotations": {
                            "summary": "No logs received from LLM service",
                            "description": "No logs have been received in the last 5 minutes"
                        }
                    }
                ]
            }
        ]
    }
```

### Cloud Solutions

```python
"""
Cloud logging solutions for LLM systems.
"""

from dataclasses import dataclass
from typing import Dict, Any, List, Optional
import json


@dataclass
class CloudWatchConfig:
    """AWS CloudWatch Logs configuration."""

    log_group_name: str = "/llm-service/application"
    retention_days: int = 30
    region: str = "us-east-1"

    def get_log_group_config(self) -> Dict[str, Any]:
        """CloudFormation template for log group."""
        return {
            "AWSTemplateFormatVersion": "2010-09-09",
            "Resources": {
                "LLMLogGroup": {
                    "Type": "AWS::Logs::LogGroup",
                    "Properties": {
                        "LogGroupName": self.log_group_name,
                        "RetentionInDays": self.retention_days,
                        "Tags": [
                            {"Key": "Service", "Value": "llm-service"},
                            {"Key": "Environment", "Value": "production"}
                        ]
                    }
                },
                "LLMMetricFilter": {
                    "Type": "AWS::Logs::MetricFilter",
                    "Properties": {
                        "LogGroupName": {"Ref": "LLMLogGroup"},
                        "FilterPattern": '{ $.level = "ERROR" }',
                        "MetricTransformations": [
                            {
                                "MetricName": "LLMErrorCount",
                                "MetricNamespace": "LLMService",
                                "MetricValue": "1",
                                "DefaultValue": 0
                            }
                        ]
                    }
                },
                "LatencyMetricFilter": {
                    "Type": "AWS::Logs::MetricFilter",
                    "Properties": {
                        "LogGroupName": {"Ref": "LLMLogGroup"},
                        "FilterPattern": '{ $.latency_ms = * }',
                        "MetricTransformations": [
                            {
                                "MetricName": "LLMLatency",
                                "MetricNamespace": "LLMService",
                                "MetricValue": "$.latency_ms"
                            }
                        ]
                    }
                }
            }
        }

    def get_insights_queries(self) -> Dict[str, str]:
        """CloudWatch Logs Insights queries for LLM analysis."""
        return {
            "error_summary": """
                fields @timestamp, @message, error_type, error_message
                | filter level = "ERROR"
                | sort @timestamp desc
                | limit 100
            """,
            "latency_percentiles": """
                fields @timestamp, latency_ms
                | filter latency_ms > 0
                | stats
                    avg(latency_ms) as avg_latency,
                    pct(latency_ms, 50) as p50,
                    pct(latency_ms, 95) as p95,
                    pct(latency_ms, 99) as p99
                    by bin(5m)
            """,
            "token_usage_by_model": """
                fields @timestamp, model, total_tokens
                | filter total_tokens > 0
                | stats sum(total_tokens) as total by model
                | sort total desc
            """,
            "slow_requests": """
                fields @timestamp, request_id, model, latency_ms, prompt_tokens
                | filter latency_ms > 5000
                | sort latency_ms desc
                | limit 50
            """,
            "cache_hit_rate": """
                fields @timestamp, cache_hit
                | stats
                    count(*) as total,
                    sum(cache_hit) as hits
                    by bin(1h)
                | sort @timestamp
            """
        }


@dataclass
class GCPLoggingConfig:
    """Google Cloud Logging configuration."""

    project_id: str
    log_name: str = "llm-service"
    retention_days: int = 30

    def get_sink_config(self) -> Dict[str, Any]:
        """Log sink configuration for BigQuery export."""
        return {
            "name": f"llm-logs-bq-sink",
            "destination": f"bigquery.googleapis.com/projects/{self.project_id}/datasets/llm_logs",
            "filter": f'logName="projects/{self.project_id}/logs/{self.log_name}"',
            "outputVersionFormat": "V2",
            "bigqueryOptions": {
                "usePartitionedTables": True
            }
        }

    def get_log_based_metrics(self) -> List[Dict[str, Any]]:
        """Log-based metrics for Cloud Monitoring."""
        return [
            {
                "name": f"projects/{self.project_id}/metrics/llm_error_count",
                "filter": f"""
                    logName="projects/{self.project_id}/logs/{self.log_name}"
                    jsonPayload.level="ERROR"
                """,
                "metricDescriptor": {
                    "metricKind": "DELTA",
                    "valueType": "INT64",
                    "labels": [
                        {"key": "error_type", "valueType": "STRING"},
                        {"key": "model", "valueType": "STRING"}
                    ]
                },
                "labelExtractors": {
                    "error_type": "EXTRACT(jsonPayload.error_type)",
                    "model": "EXTRACT(jsonPayload.model)"
                }
            },
            {
                "name": f"projects/{self.project_id}/metrics/llm_latency",
                "filter": f"""
                    logName="projects/{self.project_id}/logs/{self.log_name}"
                    jsonPayload.latency_ms > 0
                """,
                "metricDescriptor": {
                    "metricKind": "GAUGE",
                    "valueType": "DISTRIBUTION",
                    "unit": "ms",
                    "labels": [
                        {"key": "model", "valueType": "STRING"}
                    ]
                },
                "valueExtractor": "EXTRACT(jsonPayload.latency_ms)",
                "labelExtractors": {
                    "model": "EXTRACT(jsonPayload.model)"
                },
                "bucketOptions": {
                    "exponentialBuckets": {
                        "numFiniteBuckets": 64,
                        "growthFactor": 1.4,
                        "scale": 10
                    }
                }
            }
        ]


@dataclass
class AzureMonitorConfig:
    """Azure Monitor Logs configuration."""

    workspace_id: str
    retention_days: int = 30

    def get_table_schema(self) -> Dict[str, Any]:
        """Custom log table schema."""
        return {
            "properties": {
                "schema": {
                    "name": "LLMServiceLogs_CL",
                    "columns": [
                        {"name": "TimeGenerated", "type": "datetime"},
                        {"name": "RequestId", "type": "string"},
                        {"name": "TraceId", "type": "string"},
                        {"name": "SpanId", "type": "string"},
                        {"name": "Level", "type": "string"},
                        {"name": "Model", "type": "string"},
                        {"name": "PromptTokens", "type": "int"},
                        {"name": "CompletionTokens", "type": "int"},
                        {"name": "TotalTokens", "type": "int"},
                        {"name": "LatencyMs", "type": "real"},
                        {"name": "TtftMs", "type": "real"},
                        {"name": "CacheHit", "type": "bool"},
                        {"name": "ErrorType", "type": "string"},
                        {"name": "ErrorMessage", "type": "string"}
                    ]
                },
                "retentionInDays": self.retention_days
            }
        }

    def get_kql_queries(self) -> Dict[str, str]:
        """KQL queries for Azure Monitor."""
        return {
            "error_analysis": """
                LLMServiceLogs_CL
                | where Level == "ERROR"
                | summarize Count=count() by ErrorType, bin(TimeGenerated, 1h)
                | render timechart
            """,
            "latency_analysis": """
                LLMServiceLogs_CL
                | where LatencyMs > 0
                | summarize
                    p50=percentile(LatencyMs, 50),
                    p95=percentile(LatencyMs, 95),
                    p99=percentile(LatencyMs, 99)
                    by Model, bin(TimeGenerated, 5m)
                | render timechart
            """,
            "token_usage": """
                LLMServiceLogs_CL
                | summarize TotalTokens=sum(TotalTokens) by Model, bin(TimeGenerated, 1h)
                | render columnchart
            """
        }
```

## 10.2.5 Analysis & Debugging

### Log Querying Utilities

```python
"""
Log querying and analysis utilities for LLM debugging.
"""

from dataclasses import dataclass
from typing import Dict, Any, List, Optional, Generator
from datetime import datetime, timedelta
from enum import Enum
import json
import re


class QueryBackend(Enum):
    """Supported log query backends."""
    ELASTICSEARCH = "elasticsearch"
    LOKI = "loki"
    CLOUDWATCH = "cloudwatch"


@dataclass
class LogQuery:
    """Structured log query."""
    time_range: tuple[datetime, datetime]
    filters: Dict[str, Any]
    full_text: Optional[str] = None
    limit: int = 1000
    sort_field: str = "@timestamp"
    sort_order: str = "desc"


class LogQueryBuilder:
    """
    Build log queries for different backends.
    """

    def __init__(self, backend: QueryBackend):
        self.backend = backend
        self._filters: Dict[str, Any] = {}
        self._time_range: Optional[tuple[datetime, datetime]] = None
        self._full_text: Optional[str] = None
        self._limit = 1000

    def time_range(
        self,
        start: datetime,
        end: Optional[datetime] = None
    ) -> 'LogQueryBuilder':
        """Set time range for query."""
        self._time_range = (start, end or datetime.utcnow())
        return self

    def last(self, duration: timedelta) -> 'LogQueryBuilder':
        """Query last N duration."""
        end = datetime.utcnow()
        start = end - duration
        self._time_range = (start, end)
        return self

    def filter(self, field: str, value: Any) -> 'LogQueryBuilder':
        """Add filter condition."""
        self._filters[field] = value
        return self

    def filter_in(self, field: str, values: List[Any]) -> 'LogQueryBuilder':
        """Add IN filter condition."""
        self._filters[f"{field}__in"] = values
        return self

    def filter_range(
        self,
        field: str,
        min_val: Optional[Any] = None,
        max_val: Optional[Any] = None
    ) -> 'LogQueryBuilder':
        """Add range filter."""
        self._filters[f"{field}__range"] = {"min": min_val, "max": max_val}
        return self

    def search(self, text: str) -> 'LogQueryBuilder':
        """Add full-text search."""
        self._full_text = text
        return self

    def limit(self, n: int) -> 'LogQueryBuilder':
        """Set result limit."""
        self._limit = n
        return self

    def for_request(self, request_id: str) -> 'LogQueryBuilder':
        """Filter by request ID."""
        return self.filter("request_id", request_id)

    def for_trace(self, trace_id: str) -> 'LogQueryBuilder':
        """Filter by trace ID."""
        return self.filter("trace_id", trace_id)

    def for_user(self, user_id: str) -> 'LogQueryBuilder':
        """Filter by user ID."""
        return self.filter("user_id", user_id)

    def for_model(self, model: str) -> 'LogQueryBuilder':
        """Filter by model."""
        return self.filter("model", model)

    def errors_only(self) -> 'LogQueryBuilder':
        """Filter to errors only."""
        return self.filter_in("level", ["ERROR", "CRITICAL"])

    def slow_requests(self, threshold_ms: float = 5000) -> 'LogQueryBuilder':
        """Filter to slow requests."""
        return self.filter_range("latency_ms", min_val=threshold_ms)

    def build(self) -> str:
        """Build query string for configured backend."""
        if self.backend == QueryBackend.ELASTICSEARCH:
            return self._build_elasticsearch_query()
        elif self.backend == QueryBackend.LOKI:
            return self._build_loki_query()
        elif self.backend == QueryBackend.CLOUDWATCH:
            return self._build_cloudwatch_query()
        else:
            raise ValueError(f"Unsupported backend: {self.backend}")

    def _build_elasticsearch_query(self) -> str:
        """Build Elasticsearch query DSL."""
        must_clauses = []

        # Time range
        if self._time_range:
            must_clauses.append({
                "range": {
                    "@timestamp": {
                        "gte": self._time_range[0].isoformat(),
                        "lte": self._time_range[1].isoformat()
                    }
                }
            })

        # Filters
        for field, value in self._filters.items():
            if field.endswith("__in"):
                actual_field = field[:-4]
                must_clauses.append({"terms": {actual_field: value}})
            elif field.endswith("__range"):
                actual_field = field[:-7]
                range_clause = {}
                if value.get("min") is not None:
                    range_clause["gte"] = value["min"]
                if value.get("max") is not None:
                    range_clause["lte"] = value["max"]
                must_clauses.append({"range": {actual_field: range_clause}})
            else:
                must_clauses.append({"term": {field: value}})

        # Full text
        if self._full_text:
            must_clauses.append({
                "query_string": {"query": self._full_text}
            })

        query = {
            "query": {"bool": {"must": must_clauses}},
            "size": self._limit,
            "sort": [{"@timestamp": "desc"}]
        }

        return json.dumps(query, indent=2)

    def _build_loki_query(self) -> str:
        """Build LogQL query for Loki."""
        parts = ['{job="llm-service"}']

        # Add label filters
        label_filters = []
        for field, value in self._filters.items():
            if not field.endswith("__in") and not field.endswith("__range"):
                if field in ["level", "model", "service"]:
                    label_filters.append(f'{field}="{value}"')

        if label_filters:
            parts[0] = '{job="llm-service",' + ','.join(label_filters) + '}'

        # Add pipeline stages
        pipeline = []

        # JSON parsing
        pipeline.append("| json")

        # Line filters for full text
        if self._full_text:
            pipeline.append(f'|~ "{self._full_text}"')

        # Field filters
        for field, value in self._filters.items():
            if field.endswith("__range"):
                actual_field = field[:-7]
                if value.get("min") is not None:
                    pipeline.append(f'| {actual_field} >= {value["min"]}')
                if value.get("max") is not None:
                    pipeline.append(f'| {actual_field} <= {value["max"]}')
            elif field.endswith("__in"):
                actual_field = field[:-4]
                values_regex = "|".join(str(v) for v in value)
                pipeline.append(f'| {actual_field} =~ "({values_regex})"')

        query = parts[0] + " ".join(pipeline)

        # Add time range
        if self._time_range:
            start_ns = int(self._time_range[0].timestamp() * 1e9)
            end_ns = int(self._time_range[1].timestamp() * 1e9)
            query = f"[{start_ns}:{end_ns}] {query}"

        return query

    def _build_cloudwatch_query(self) -> str:
        """Build CloudWatch Logs Insights query."""
        parts = ["fields @timestamp, @message"]

        # Filters
        filter_conditions = []
        for field, value in self._filters.items():
            if field.endswith("__in"):
                actual_field = field[:-4]
                conditions = " or ".join(f'{actual_field} = "{v}"' for v in value)
                filter_conditions.append(f"({conditions})")
            elif field.endswith("__range"):
                actual_field = field[:-7]
                if value.get("min") is not None:
                    filter_conditions.append(f"{actual_field} >= {value['min']}")
                if value.get("max") is not None:
                    filter_conditions.append(f"{actual_field} <= {value['max']}")
            else:
                filter_conditions.append(f'{field} = "{value}"')

        if filter_conditions:
            parts.append("| filter " + " and ".join(filter_conditions))

        # Full text
        if self._full_text:
            parts.append(f'| filter @message like /{self._full_text}/')

        # Sort and limit
        parts.append("| sort @timestamp desc")
        parts.append(f"| limit {self._limit}")

        return "\n".join(parts)


class RootCauseAnalyzer:
    """
    Automated root cause analysis for LLM errors.
    """

    # Common error patterns and their likely causes
    ERROR_PATTERNS = {
        r"CUDA out of memory": {
            "cause": "GPU memory exhaustion",
            "recommendations": [
                "Reduce batch size",
                "Use model quantization",
                "Implement request queuing with memory checks",
                "Scale to larger GPU instances"
            ]
        },
        r"Connection (refused|reset|timeout)": {
            "cause": "Network connectivity issue",
            "recommendations": [
                "Check service health endpoints",
                "Verify network policies and firewalls",
                "Review load balancer configuration",
                "Check for DNS resolution issues"
            ]
        },
        r"Rate limit exceeded": {
            "cause": "API rate limiting",
            "recommendations": [
                "Implement request queuing",
                "Add exponential backoff",
                "Request rate limit increase",
                "Distribute load across API keys"
            ]
        },
        r"Context length exceeded|too many tokens": {
            "cause": "Input too long for model",
            "recommendations": [
                "Implement input truncation",
                "Use summarization for long inputs",
                "Switch to model with larger context",
                "Implement chunking strategies"
            ]
        },
        r"Model not found|model.*does not exist": {
            "cause": "Invalid model identifier",
            "recommendations": [
                "Verify model name spelling",
                "Check model availability in region",
                "Ensure model access permissions",
                "Update to correct model version"
            ]
        },
        r"Invalid API key|authentication failed": {
            "cause": "Authentication failure",
            "recommendations": [
                "Rotate API keys",
                "Verify key permissions",
                "Check key expiration",
                "Ensure correct environment variables"
            ]
        },
        r"(timeout|deadline exceeded)": {
            "cause": "Request timeout",
            "recommendations": [
                "Increase timeout settings",
                "Reduce max_tokens parameter",
                "Check model server load",
                "Implement streaming responses"
            ]
        }
    }

    def analyze_error(self, error_message: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Analyze an error message and provide root cause analysis.
        """
        analysis = {
            "error_message": error_message,
            "matched_pattern": None,
            "likely_cause": "Unknown",
            "recommendations": [],
            "context_factors": []
        }

        # Match against known patterns
        for pattern, info in self.ERROR_PATTERNS.items():
            if re.search(pattern, error_message, re.IGNORECASE):
                analysis["matched_pattern"] = pattern
                analysis["likely_cause"] = info["cause"]
                analysis["recommendations"] = info["recommendations"]
                break

        # Analyze context if provided
        if context:
            context_factors = self._analyze_context(context)
            analysis["context_factors"] = context_factors

        return analysis

    def _analyze_context(self, context: Dict[str, Any]) -> List[str]:
        """Analyze error context for contributing factors."""
        factors = []

        # Check token counts
        if context.get("prompt_tokens", 0) > 10000:
            factors.append("Very long input prompt may contribute to issues")

        # Check latency
        if context.get("latency_ms", 0) > 30000:
            factors.append("Request took exceptionally long before failing")

        # Check for concurrent requests
        if context.get("concurrent_requests", 0) > 100:
            factors.append("High concurrent request load at time of error")

        # Check GPU utilization
        if context.get("gpu_utilization", 0) > 95:
            factors.append("GPU was at near-maximum utilization")

        # Check memory
        if context.get("gpu_memory_used_pct", 0) > 90:
            factors.append("GPU memory was nearly exhausted")

        return factors

    def analyze_error_trend(
        self,
        errors: List[Dict[str, Any]],
        time_window: timedelta = timedelta(hours=1)
    ) -> Dict[str, Any]:
        """
        Analyze trends in error occurrences.
        """
        if not errors:
            return {"status": "no_errors", "trend": "stable"}

        # Group by error type
        error_types: Dict[str, List] = {}
        for error in errors:
            error_type = error.get("error_type", "unknown")
            if error_type not in error_types:
                error_types[error_type] = []
            error_types[error_type].append(error)

        # Analyze each error type
        analysis = {
            "total_errors": len(errors),
            "error_types": {},
            "dominant_error": None,
            "trend": "unknown",
            "recommendations": []
        }

        max_count = 0
        for error_type, type_errors in error_types.items():
            count = len(type_errors)
            analysis["error_types"][error_type] = {
                "count": count,
                "percentage": count / len(errors) * 100,
                "first_seen": min(e.get("timestamp", "") for e in type_errors),
                "last_seen": max(e.get("timestamp", "") for e in type_errors)
            }

            if count > max_count:
                max_count = count
                analysis["dominant_error"] = error_type

        # Determine trend
        if analysis["dominant_error"]:
            dominant_info = analysis["error_types"][analysis["dominant_error"]]
            if dominant_info["percentage"] > 80:
                analysis["trend"] = "single_dominant_error"
                analysis["recommendations"].append(
                    f"Focus on resolving {analysis['dominant_error']} errors"
                )
            else:
                analysis["trend"] = "multiple_error_types"
                analysis["recommendations"].append(
                    "Multiple error types detected - investigate common factors"
                )

        return analysis


class PerformanceProfiler:
    """
    Performance profiling from log data.
    """

    def profile_request_latency(
        self,
        logs: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Profile request latency distribution.
        """
        latencies = [
            log.get("latency_ms", 0)
            for log in logs
            if log.get("latency_ms", 0) > 0
        ]

        if not latencies:
            return {"status": "no_data"}

        latencies.sort()
        n = len(latencies)

        return {
            "count": n,
            "min": latencies[0],
            "max": latencies[-1],
            "mean": sum(latencies) / n,
            "median": latencies[n // 2],
            "p90": latencies[int(n * 0.9)],
            "p95": latencies[int(n * 0.95)],
            "p99": latencies[int(n * 0.99)],
            "std_dev": self._std_dev(latencies)
        }

    def profile_by_model(
        self,
        logs: List[Dict[str, Any]]
    ) -> Dict[str, Dict[str, Any]]:
        """
        Profile performance by model.
        """
        by_model: Dict[str, List] = {}

        for log in logs:
            model = log.get("model", "unknown")
            if model not in by_model:
                by_model[model] = []
            by_model[model].append(log)

        return {
            model: self.profile_request_latency(model_logs)
            for model, model_logs in by_model.items()
        }

    def identify_bottlenecks(
        self,
        logs: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Identify performance bottlenecks from logs.
        """
        bottlenecks = []

        # Analyze timing breakdown
        timing_fields = [
            ("queue_time_ms", "Request queuing"),
            ("preprocessing_time_ms", "Input preprocessing"),
            ("inference_time_ms", "Model inference"),
            ("postprocessing_time_ms", "Output postprocessing")
        ]

        for field, description in timing_fields:
            values = [log.get(field, 0) for log in logs if log.get(field, 0) > 0]
            if values:
                avg_time = sum(values) / len(values)
                total_time = sum(log.get("latency_ms", 0) for log in logs if log.get("latency_ms", 0) > 0)
                avg_total = total_time / len([l for l in logs if l.get("latency_ms", 0) > 0])

                if avg_total > 0:
                    pct_of_total = (avg_time / avg_total) * 100

                    if pct_of_total > 30:  # More than 30% of time
                        bottlenecks.append({
                            "component": description,
                            "avg_time_ms": avg_time,
                            "pct_of_total": pct_of_total,
                            "severity": "high" if pct_of_total > 50 else "medium"
                        })

        return sorted(bottlenecks, key=lambda x: x["pct_of_total"], reverse=True)

    @staticmethod
    def _std_dev(values: List[float]) -> float:
        """Calculate standard deviation."""
        if len(values) < 2:
            return 0.0
        mean = sum(values) / len(values)
        variance = sum((x - mean) ** 2 for x in values) / (len(values) - 1)
        return variance ** 0.5
```

## Appendix A: Logging Configuration Examples

### Python Logging Configuration

```python
"""
Production logging configuration for LLM services.
"""

import logging
import logging.config
import sys
from typing import Dict, Any


def get_production_logging_config(
    service_name: str = "llm-service",
    log_level: str = "INFO",
    log_prompts: bool = False,
    json_output: bool = True
) -> Dict[str, Any]:
    """
    Get production-ready logging configuration.
    """
    return {
        "version": 1,
        "disable_existing_loggers": False,
        "formatters": {
            "json": {
                "()": "pythonjsonlogger.jsonlogger.JsonFormatter",
                "format": "%(timestamp)s %(level)s %(name)s %(message)s",
                "timestamp": True
            },
            "standard": {
                "format": "%(asctime)s [%(levelname)s] %(name)s: %(message)s"
            }
        },
        "filters": {
            "pii_filter": {
                "()": "llm_logging.PIIFilter"
            },
            "prompt_filter": {
                "()": "llm_logging.PromptFilter",
                "allow_prompts": log_prompts
            }
        },
        "handlers": {
            "console": {
                "class": "logging.StreamHandler",
                "formatter": "json" if json_output else "standard",
                "stream": sys.stdout,
                "filters": ["pii_filter", "prompt_filter"]
            },
            "file": {
                "class": "logging.handlers.RotatingFileHandler",
                "filename": f"/var/log/{service_name}/app.log",
                "maxBytes": 104857600,  # 100MB
                "backupCount": 10,
                "formatter": "json",
                "filters": ["pii_filter", "prompt_filter"]
            },
            "error_file": {
                "class": "logging.handlers.RotatingFileHandler",
                "filename": f"/var/log/{service_name}/error.log",
                "maxBytes": 104857600,
                "backupCount": 10,
                "formatter": "json",
                "level": "ERROR"
            }
        },
        "loggers": {
            "": {
                "handlers": ["console", "file", "error_file"],
                "level": log_level,
                "propagate": True
            },
            "uvicorn": {
                "handlers": ["console"],
                "level": "INFO",
                "propagate": False
            },
            "httpx": {
                "handlers": ["console"],
                "level": "WARNING",
                "propagate": False
            }
        }
    }


def configure_logging(config: Dict[str, Any] = None):
    """Apply logging configuration."""
    if config is None:
        config = get_production_logging_config()

    logging.config.dictConfig(config)
```

### FastAPI Integration

```python
"""
FastAPI logging middleware integration.
"""

from fastapi import FastAPI, Request
from starlette.middleware.base import BaseHTTPMiddleware
import time
import uuid
import structlog


class RequestLoggingMiddleware(BaseHTTPMiddleware):
    """
    Middleware for logging HTTP requests in FastAPI LLM services.
    """

    def __init__(self, app: FastAPI, logger=None):
        super().__init__(app)
        self.logger = logger or structlog.get_logger("http")

    async def dispatch(self, request: Request, call_next):
        # Generate request ID
        request_id = request.headers.get("X-Request-ID", str(uuid.uuid4()))

        # Start timing
        start_time = time.perf_counter()

        # Add request ID to state
        request.state.request_id = request_id

        # Log request start
        self.logger.info(
            "request_started",
            request_id=request_id,
            method=request.method,
            path=request.url.path,
            client_ip=request.client.host if request.client else None
        )

        # Process request
        try:
            response = await call_next(request)

            # Calculate duration
            duration_ms = (time.perf_counter() - start_time) * 1000

            # Log request completion
            self.logger.info(
                "request_completed",
                request_id=request_id,
                method=request.method,
                path=request.url.path,
                status_code=response.status_code,
                duration_ms=duration_ms
            )

            # Add headers to response
            response.headers["X-Request-ID"] = request_id
            response.headers["X-Response-Time"] = f"{duration_ms:.2f}ms"

            return response

        except Exception as e:
            duration_ms = (time.perf_counter() - start_time) * 1000

            self.logger.error(
                "request_failed",
                request_id=request_id,
                method=request.method,
                path=request.url.path,
                error=str(e),
                duration_ms=duration_ms
            )
            raise


def setup_fastapi_logging(app: FastAPI):
    """Configure logging for FastAPI application."""
    # Add correlation ID middleware
    app.add_middleware(CorrelationIDMiddleware)

    # Add request logging middleware
    app.add_middleware(RequestLoggingMiddleware)

    # Configure structlog
    structlog.configure(
        processors=[
            structlog.contextvars.merge_contextvars,
            structlog.processors.add_log_level,
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.processors.JSONRenderer()
        ],
        wrapper_class=structlog.make_filtering_bound_logger(logging.INFO),
        context_class=dict,
        logger_factory=structlog.PrintLoggerFactory(),
        cache_logger_on_first_use=True,
    )
```

## Appendix B: OpenTelemetry Setup

### Complete Setup Script

```python
"""
Complete OpenTelemetry setup for LLM services.
"""

from opentelemetry import trace, metrics
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.sdk.metrics import MeterProvider
from opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.exporter.otlp.proto.grpc.metric_exporter import OTLPMetricExporter
from opentelemetry.sdk.resources import Resource, SERVICE_NAME, SERVICE_VERSION
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
from opentelemetry.instrumentation.httpx import HTTPXClientInstrumentor
from opentelemetry.instrumentation.redis import RedisInstrumentor
from opentelemetry.instrumentation.sqlalchemy import SQLAlchemyInstrumentor

from typing import Optional
import os


def setup_opentelemetry(
    service_name: str,
    service_version: str = "1.0.0",
    otlp_endpoint: Optional[str] = None,
    enable_metrics: bool = True,
    enable_traces: bool = True,
    sampling_rate: float = 1.0
):
    """
    Complete OpenTelemetry setup for LLM service.
    """
    # Get OTLP endpoint from environment or parameter
    endpoint = otlp_endpoint or os.getenv("OTEL_EXPORTER_OTLP_ENDPOINT", "localhost:4317")

    # Create resource
    resource = Resource.create({
        SERVICE_NAME: service_name,
        SERVICE_VERSION: service_version,
        "deployment.environment": os.getenv("ENVIRONMENT", "production"),
        "host.name": os.getenv("HOSTNAME", "unknown")
    })

    # Setup tracing
    if enable_traces:
        trace_provider = TracerProvider(
            resource=resource,
            sampler=create_sampler("adaptive", base_rate=sampling_rate)
        )

        # Add OTLP exporter
        otlp_trace_exporter = OTLPSpanExporter(endpoint=endpoint, insecure=True)
        trace_provider.add_span_processor(BatchSpanProcessor(otlp_trace_exporter))

        # Set global tracer provider
        trace.set_tracer_provider(trace_provider)

    # Setup metrics
    if enable_metrics:
        otlp_metric_exporter = OTLPMetricExporter(endpoint=endpoint, insecure=True)
        metric_reader = PeriodicExportingMetricReader(
            otlp_metric_exporter,
            export_interval_millis=60000  # Export every minute
        )

        meter_provider = MeterProvider(
            resource=resource,
            metric_readers=[metric_reader]
        )

        metrics.set_meter_provider(meter_provider)

    # Auto-instrument libraries
    _setup_auto_instrumentation()

    return {
        "tracer": trace.get_tracer(service_name),
        "meter": metrics.get_meter(service_name) if enable_metrics else None
    }


def _setup_auto_instrumentation():
    """Setup automatic instrumentation for common libraries."""
    # HTTP client
    try:
        HTTPXClientInstrumentor().instrument()
    except Exception:
        pass

    # Redis
    try:
        RedisInstrumentor().instrument()
    except Exception:
        pass

    # SQLAlchemy
    try:
        SQLAlchemyInstrumentor().instrument()
    except Exception:
        pass


def instrument_fastapi(app):
    """Instrument FastAPI application."""
    FastAPIInstrumentor.instrument_app(app)
```

## Appendix C: Log Query Examples

### Common Query Patterns

```python
"""
Example log queries for common debugging scenarios.
"""

# Elasticsearch queries
ES_QUERIES = {
    "find_slow_requests": {
        "query": {
            "bool": {
                "must": [
                    {"range": {"latency_ms": {"gte": 5000}}},
                    {"range": {"@timestamp": {"gte": "now-1h"}}}
                ]
            }
        },
        "sort": [{"latency_ms": "desc"}],
        "size": 100
    },

    "error_distribution": {
        "size": 0,
        "query": {
            "bool": {
                "must": [
                    {"term": {"level": "ERROR"}},
                    {"range": {"@timestamp": {"gte": "now-24h"}}}
                ]
            }
        },
        "aggs": {
            "by_type": {
                "terms": {"field": "error_type", "size": 20}
            }
        }
    },

    "trace_request": {
        "query": {
            "term": {"trace_id": "TRACE_ID_HERE"}
        },
        "sort": [{"@timestamp": "asc"}],
        "size": 1000
    }
}

# Loki LogQL queries
LOGQL_QUERIES = {
    "error_rate": """
        sum(rate({job="llm-service"} |= "ERROR" [5m])) by (model)
    """,

    "latency_histogram": """
        {job="llm-service"}
        | json
        | latency_ms > 0
        | unwrap latency_ms
        | histogram_over_time(latency_ms[5m])
    """,

    "top_errors": """
        {job="llm-service"}
        | json
        | level="ERROR"
        | line_format "{{.error_type}}: {{.error_message}}"
    """
}

# CloudWatch Insights queries
CLOUDWATCH_QUERIES = {
    "latency_analysis": """
        fields @timestamp, model, latency_ms
        | filter latency_ms > 0
        | stats
            count(*) as requests,
            avg(latency_ms) as avg_latency,
            pct(latency_ms, 95) as p95_latency
            by model
        | sort avg_latency desc
    """,

    "error_timeline": """
        fields @timestamp, error_type, error_message
        | filter level = "ERROR"
        | stats count(*) as error_count by bin(5m), error_type
        | sort @timestamp
    """
}
```

## Appendix D: Trace Analysis Procedures

### Debugging Workflow

```python
"""
Step-by-step trace analysis procedures.
"""

from dataclasses import dataclass
from typing import List, Dict, Any, Optional


@dataclass
class TraceAnalysisStep:
    """Single step in trace analysis procedure."""
    name: str
    description: str
    query: str
    expected_output: str
    next_steps: List[str]


TRACE_ANALYSIS_PROCEDURES = {
    "slow_request_investigation": [
        TraceAnalysisStep(
            name="Identify slow span",
            description="Find the span with the longest duration",
            query="Sort spans by duration, identify > 1s spans",
            expected_output="List of slow spans with their operations",
            next_steps=["Check if slow span is inference or I/O"]
        ),
        TraceAnalysisStep(
            name="Analyze span breakdown",
            description="Break down time spent in each operation",
            query="Group spans by operation type, sum durations",
            expected_output="Time breakdown: queue, preprocessing, inference, postprocessing",
            next_steps=["Focus optimization on highest time consumer"]
        ),
        TraceAnalysisStep(
            name="Check for queuing",
            description="Identify if request spent time waiting",
            query="Compare span start time vs request arrival time",
            expected_output="Queue wait time if significant",
            next_steps=["Scale up if queue time is high"]
        ),
        TraceAnalysisStep(
            name="Verify token counts",
            description="Check if high token count caused slowness",
            query="Extract prompt_tokens and completion_tokens from span",
            expected_output="Token counts and tokens/second rate",
            next_steps=["Consider truncation if tokens are excessive"]
        )
    ],

    "error_investigation": [
        TraceAnalysisStep(
            name="Find error span",
            description="Locate the span where error occurred",
            query="Filter spans with error status",
            expected_output="Error span with exception details",
            next_steps=["Examine error message and stack trace"]
        ),
        TraceAnalysisStep(
            name="Trace error origin",
            description="Identify root cause in span hierarchy",
            query="Walk up parent spans to find original error",
            expected_output="Root span that initiated the failure",
            next_steps=["Check span attributes for context"]
        ),
        TraceAnalysisStep(
            name="Check preceding events",
            description="Look for warnings or anomalies before error",
            query="Get all events in trace before error timestamp",
            expected_output="Timeline of events leading to error",
            next_steps=["Identify pattern or trigger"]
        ),
        TraceAnalysisStep(
            name="Compare with successful traces",
            description="Find differences from successful requests",
            query="Get similar successful trace for comparison",
            expected_output="Differences in parameters or timing",
            next_steps=["Identify unique factors in failed request"]
        )
    ]
}


def generate_analysis_report(
    trace_id: str,
    procedure: str,
    findings: List[Dict[str, Any]]
) -> str:
    """
    Generate analysis report from trace investigation.
    """
    report = f"""
# Trace Analysis Report

## Trace ID: {trace_id}
## Procedure: {procedure}
## Generated: {datetime.utcnow().isoformat()}

## Findings

"""
    for i, finding in enumerate(findings, 1):
        report += f"""
### Step {i}: {finding['step_name']}

**Query Result:**
{finding.get('query_result', 'No result')}

**Analysis:**
{finding.get('analysis', 'No analysis')}

**Recommended Action:**
{finding.get('recommendation', 'No recommendation')}

---
"""

    return report
```

## Summary

This guide covered comprehensive logging and tracing strategies for LLM systems:

1. **Logging Strategy**: Log levels, content guidelines, and PII filtering
2. **Structured Logging**: JSON format, correlation IDs, timestamp standardization
3. **Distributed Tracing**: OpenTelemetry integration, sampling strategies
4. **Log Aggregation**: ELK Stack, Loki, and cloud solutions
5. **Analysis & Debugging**: Query builders, root cause analysis, performance profiling

Key takeaways:
- Always use structured JSON logging with correlation IDs
- Implement privacy-aware logging that filters PII by default
- Use distributed tracing to understand request flow across services
- Choose sampling strategies appropriate for your traffic volume
- Build dashboards and alerts from log data
- Establish systematic debugging procedures for common issues
