> **Navigation** | [← 14.2 Cloud Cost](14.2_cloud_cost_optimization_guide.md) | [15.1 Model Migration →](../15_migration_integration/15.1_model_migration_guide.md)
>
> | | |
> |---|---|
> | **Prerequisites** | ML inference &#124; Cloud GPU instances &#124; CUDA &#124; Model deployment |
> | **Related** | [9.1 Inference Engines](../09_inference_serving/9.1_inference_engine_selection_guide.md) &#124; [6.1 Quantization](../06_model_optimization/6.1_quantization_guide.md) |
> | **Next** | [15.1 Model Migration](../15_migration_integration/15.1_model_migration_guide.md) |

# 14.3 GPU Infrastructure Optimization Guide

## Document Information
- **Version**: 1.0
- **Last Updated**: 2024-01-15
- **Owner**: ML Infrastructure Team
- **Classification**: Internal

## Purpose and Scope

This guide provides comprehensive strategies for optimizing GPU infrastructure for the Multi-Cloud RAG Platform, covering GPU selection, utilization optimization, cost management, and scaling strategies for ML inference and embedding workloads.

## Prerequisites

- Understanding of ML inference requirements
- Access to cloud GPU instances
- Familiarity with CUDA and GPU monitoring
- Knowledge of ML model deployment patterns

## 1. GPU Selection and Sizing

### 1.1 GPU Type Selection Framework

```python
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Tuple
from enum import Enum
from decimal import Decimal


class GPUProvider(Enum):
    """GPU cloud providers."""
    AWS = "aws"
    GCP = "gcp"
    AZURE = "azure"
    LAMBDA_LABS = "lambda_labs"
    COREWEAVE = "coreweave"


class WorkloadType(Enum):
    """Types of GPU workloads."""
    INFERENCE = "inference"
    EMBEDDING = "embedding"
    TRAINING = "training"
    FINE_TUNING = "fine_tuning"
    BATCH_PROCESSING = "batch_processing"


@dataclass
class GPUSpec:
    """GPU specifications."""
    name: str
    memory_gb: int
    compute_capability: float
    tensor_cores: int
    fp16_tflops: float
    fp32_tflops: float
    memory_bandwidth_gbps: int
    nvlink: bool = False
    multi_instance_gpu: bool = False


@dataclass
class GPUInstanceType:
    """Cloud GPU instance type."""
    provider: GPUProvider
    instance_type: str
    gpu_spec: GPUSpec
    gpu_count: int
    vcpus: int
    memory_gb: int
    hourly_price: Decimal
    spot_price: Optional[Decimal] = None
    availability: str = "high"  # high, medium, low


class GPUCatalog:
    """Catalog of available GPU options."""

    def __init__(self):
        self.gpu_specs = self._initialize_gpu_specs()
        self.instance_types = self._initialize_instance_types()

    def _initialize_gpu_specs(self) -> Dict[str, GPUSpec]:
        return {
            "A100_40GB": GPUSpec(
                name="NVIDIA A100 40GB",
                memory_gb=40,
                compute_capability=8.0,
                tensor_cores=432,
                fp16_tflops=312,
                fp32_tflops=156,
                memory_bandwidth_gbps=1555,
                nvlink=True,
                multi_instance_gpu=True
            ),
            "A100_80GB": GPUSpec(
                name="NVIDIA A100 80GB",
                memory_gb=80,
                compute_capability=8.0,
                tensor_cores=432,
                fp16_tflops=312,
                fp32_tflops=156,
                memory_bandwidth_gbps=2039,
                nvlink=True,
                multi_instance_gpu=True
            ),
            "H100_80GB": GPUSpec(
                name="NVIDIA H100 80GB",
                memory_gb=80,
                compute_capability=9.0,
                tensor_cores=528,
                fp16_tflops=989,
                fp32_tflops=494,
                memory_bandwidth_gbps=3350,
                nvlink=True,
                multi_instance_gpu=True
            ),
            "A10G": GPUSpec(
                name="NVIDIA A10G",
                memory_gb=24,
                compute_capability=8.6,
                tensor_cores=288,
                fp16_tflops=125,
                fp32_tflops=31.2,
                memory_bandwidth_gbps=600,
                nvlink=False,
                multi_instance_gpu=False
            ),
            "L4": GPUSpec(
                name="NVIDIA L4",
                memory_gb=24,
                compute_capability=8.9,
                tensor_cores=240,
                fp16_tflops=121,
                fp32_tflops=30.3,
                memory_bandwidth_gbps=300,
                nvlink=False,
                multi_instance_gpu=False
            ),
            "T4": GPUSpec(
                name="NVIDIA T4",
                memory_gb=16,
                compute_capability=7.5,
                tensor_cores=320,
                fp16_tflops=65,
                fp32_tflops=8.1,
                memory_bandwidth_gbps=300,
                nvlink=False,
                multi_instance_gpu=False
            )
        }

    def _initialize_instance_types(self) -> List[GPUInstanceType]:
        return [
            # AWS instances
            GPUInstanceType(
                provider=GPUProvider.AWS,
                instance_type="p4d.24xlarge",
                gpu_spec=self.gpu_specs["A100_40GB"],
                gpu_count=8,
                vcpus=96,
                memory_gb=1152,
                hourly_price=Decimal("32.77"),
                spot_price=Decimal("9.83"),
                availability="medium"
            ),
            GPUInstanceType(
                provider=GPUProvider.AWS,
                instance_type="p5.48xlarge",
                gpu_spec=self.gpu_specs["H100_80GB"],
                gpu_count=8,
                vcpus=192,
                memory_gb=2048,
                hourly_price=Decimal("98.32"),
                spot_price=Decimal("29.50"),
                availability="low"
            ),
            GPUInstanceType(
                provider=GPUProvider.AWS,
                instance_type="g5.xlarge",
                gpu_spec=self.gpu_specs["A10G"],
                gpu_count=1,
                vcpus=4,
                memory_gb=16,
                hourly_price=Decimal("1.01"),
                spot_price=Decimal("0.30"),
                availability="high"
            ),
            GPUInstanceType(
                provider=GPUProvider.AWS,
                instance_type="g5.2xlarge",
                gpu_spec=self.gpu_specs["A10G"],
                gpu_count=1,
                vcpus=8,
                memory_gb=32,
                hourly_price=Decimal("1.21"),
                spot_price=Decimal("0.36"),
                availability="high"
            ),
            GPUInstanceType(
                provider=GPUProvider.AWS,
                instance_type="g5.4xlarge",
                gpu_spec=self.gpu_specs["A10G"],
                gpu_count=1,
                vcpus=16,
                memory_gb=64,
                hourly_price=Decimal("1.62"),
                spot_price=Decimal("0.49"),
                availability="high"
            ),
            GPUInstanceType(
                provider=GPUProvider.AWS,
                instance_type="g4dn.xlarge",
                gpu_spec=self.gpu_specs["T4"],
                gpu_count=1,
                vcpus=4,
                memory_gb=16,
                hourly_price=Decimal("0.526"),
                spot_price=Decimal("0.16"),
                availability="high"
            ),
            # GCP instances
            GPUInstanceType(
                provider=GPUProvider.GCP,
                instance_type="a2-highgpu-1g",
                gpu_spec=self.gpu_specs["A100_40GB"],
                gpu_count=1,
                vcpus=12,
                memory_gb=85,
                hourly_price=Decimal("3.67"),
                spot_price=Decimal("1.10"),
                availability="medium"
            ),
            GPUInstanceType(
                provider=GPUProvider.GCP,
                instance_type="g2-standard-4",
                gpu_spec=self.gpu_specs["L4"],
                gpu_count=1,
                vcpus=4,
                memory_gb=16,
                hourly_price=Decimal("0.70"),
                spot_price=Decimal("0.21"),
                availability="high"
            ),
            # Azure instances
            GPUInstanceType(
                provider=GPUProvider.AZURE,
                instance_type="Standard_NC24ads_A100_v4",
                gpu_spec=self.gpu_specs["A100_80GB"],
                gpu_count=1,
                vcpus=24,
                memory_gb=220,
                hourly_price=Decimal("3.67"),
                spot_price=Decimal("0.73"),
                availability="medium"
            ),
            GPUInstanceType(
                provider=GPUProvider.AZURE,
                instance_type="Standard_NC4as_T4_v3",
                gpu_spec=self.gpu_specs["T4"],
                gpu_count=1,
                vcpus=4,
                memory_gb=28,
                hourly_price=Decimal("0.53"),
                spot_price=Decimal("0.11"),
                availability="high"
            )
        ]

    def find_instances(
        self,
        min_gpu_memory: int = 0,
        min_gpus: int = 1,
        provider: Optional[GPUProvider] = None,
        max_hourly_price: Optional[Decimal] = None
    ) -> List[GPUInstanceType]:
        """Find instances matching criteria."""
        results = []

        for instance in self.instance_types:
            if instance.gpu_spec.memory_gb < min_gpu_memory:
                continue
            if instance.gpu_count < min_gpus:
                continue
            if provider and instance.provider != provider:
                continue
            if max_hourly_price and instance.hourly_price > max_hourly_price:
                continue

            results.append(instance)

        return sorted(results, key=lambda x: x.hourly_price)


class GPUSizingCalculator:
    """Calculate optimal GPU sizing for workloads."""

    def __init__(self, catalog: GPUCatalog):
        self.catalog = catalog

        # Model memory requirements (approximate, in GB)
        self.model_memory_requirements = {
            "gpt-4": 80,  # Would need multiple GPUs
            "llama-70b": 140,  # Needs multi-GPU
            "llama-13b": 26,
            "llama-7b": 14,
            "mistral-7b": 14,
            "embedding-large": 2,
            "embedding-small": 0.5
        }

    def estimate_memory_requirement(
        self,
        model_name: str,
        batch_size: int = 1,
        sequence_length: int = 2048,
        precision: str = "fp16"
    ) -> float:
        """Estimate GPU memory requirement for a model."""
        base_memory = self.model_memory_requirements.get(model_name, 8)

        # Adjust for batch size and sequence length
        # KV cache grows with batch * sequence
        kv_cache_factor = 1 + (batch_size * sequence_length / 2048) * 0.1

        # Adjust for precision
        precision_factors = {
            "fp32": 2.0,
            "fp16": 1.0,
            "int8": 0.5,
            "int4": 0.25
        }
        precision_factor = precision_factors.get(precision, 1.0)

        # Add overhead for activations, optimizer states (if training)
        overhead = 1.2

        return base_memory * precision_factor * kv_cache_factor * overhead

    def recommend_instance(
        self,
        model_name: str,
        workload_type: WorkloadType,
        target_throughput: float,
        max_latency_ms: int,
        budget_hourly: Optional[Decimal] = None
    ) -> Dict:
        """Recommend GPU instance for workload."""
        memory_required = self.estimate_memory_requirement(model_name)

        # Find instances with enough memory
        candidates = self.catalog.find_instances(
            min_gpu_memory=int(memory_required * 1.1),  # 10% buffer
            max_hourly_price=budget_hourly
        )

        if not candidates:
            # Try multi-GPU configurations
            candidates = self.catalog.find_instances(min_gpus=2)
            candidates = [
                c for c in candidates
                if c.gpu_spec.memory_gb * c.gpu_count >= memory_required
            ]

        if not candidates:
            return {"error": "No suitable instances found"}

        # Score candidates based on workload type
        scored = []
        for candidate in candidates:
            score = self._score_instance(candidate, workload_type, target_throughput)
            scored.append((candidate, score))

        scored.sort(key=lambda x: x[1], reverse=True)
        best = scored[0][0]

        return {
            "recommended_instance": best.instance_type,
            "provider": best.provider.value,
            "gpu": best.gpu_spec.name,
            "gpu_count": best.gpu_count,
            "gpu_memory_gb": best.gpu_spec.memory_gb,
            "hourly_cost": float(best.hourly_price),
            "spot_price": float(best.spot_price) if best.spot_price else None,
            "memory_required_gb": memory_required,
            "memory_headroom_percent": (best.gpu_spec.memory_gb - memory_required) / best.gpu_spec.memory_gb * 100,
            "alternatives": [
                {
                    "instance": s[0].instance_type,
                    "hourly_cost": float(s[0].hourly_price)
                }
                for s in scored[1:4]
            ]
        }

    def _score_instance(
        self,
        instance: GPUInstanceType,
        workload_type: WorkloadType,
        target_throughput: float
    ) -> float:
        """Score an instance for a workload."""
        score = 0.0

        # Cost efficiency
        cost_per_tflop = float(instance.hourly_price) / instance.gpu_spec.fp16_tflops
        score -= cost_per_tflop * 10  # Penalize high cost

        # Throughput capability
        estimated_throughput = instance.gpu_spec.fp16_tflops * instance.gpu_count
        if estimated_throughput >= target_throughput:
            score += 50  # Meets requirement

        # Availability
        availability_scores = {"high": 20, "medium": 10, "low": 0}
        score += availability_scores.get(instance.availability, 0)

        # Workload-specific scoring
        if workload_type == WorkloadType.INFERENCE:
            # Prefer single large GPU for inference
            if instance.gpu_count == 1 and instance.gpu_spec.memory_gb >= 24:
                score += 30
        elif workload_type == WorkloadType.EMBEDDING:
            # Embeddings can use smaller GPUs efficiently
            if instance.gpu_spec.memory_gb <= 24:
                score += 20
        elif workload_type == WorkloadType.BATCH_PROCESSING:
            # Prefer spot instances for batch
            if instance.spot_price:
                score += 40

        return score
```

### 1.2 Model-GPU Compatibility Matrix

```python
from dataclasses import dataclass
from typing import Dict, List, Optional


@dataclass
class ModelGPUCompatibility:
    """Compatibility information for model-GPU combinations."""
    model_name: str
    gpu_name: str
    compatible: bool
    min_gpu_count: int
    recommended_precision: str
    expected_throughput: float  # tokens/second
    expected_latency_ms: float
    memory_usage_gb: float
    notes: str = ""


class CompatibilityMatrix:
    """Model-GPU compatibility matrix."""

    def __init__(self):
        self.compatibilities: List[ModelGPUCompatibility] = self._initialize_matrix()

    def _initialize_matrix(self) -> List[ModelGPUCompatibility]:
        return [
            # LLaMA 70B
            ModelGPUCompatibility(
                model_name="llama-70b",
                gpu_name="H100_80GB",
                compatible=True,
                min_gpu_count=1,
                recommended_precision="fp16",
                expected_throughput=40,
                expected_latency_ms=100,
                memory_usage_gb=140,
                notes="With tensor parallelism across 2 GPUs for full precision"
            ),
            ModelGPUCompatibility(
                model_name="llama-70b",
                gpu_name="A100_80GB",
                compatible=True,
                min_gpu_count=2,
                recommended_precision="fp16",
                expected_throughput=25,
                expected_latency_ms=160,
                memory_usage_gb=140,
                notes="Requires 2 GPUs with NVLink"
            ),
            ModelGPUCompatibility(
                model_name="llama-70b",
                gpu_name="A100_40GB",
                compatible=True,
                min_gpu_count=4,
                recommended_precision="int8",
                expected_throughput=20,
                expected_latency_ms=200,
                memory_usage_gb=70,
                notes="INT8 quantization required"
            ),
            # LLaMA 13B
            ModelGPUCompatibility(
                model_name="llama-13b",
                gpu_name="A100_40GB",
                compatible=True,
                min_gpu_count=1,
                recommended_precision="fp16",
                expected_throughput=80,
                expected_latency_ms=50,
                memory_usage_gb=26
            ),
            ModelGPUCompatibility(
                model_name="llama-13b",
                gpu_name="A10G",
                compatible=True,
                min_gpu_count=1,
                recommended_precision="int8",
                expected_throughput=40,
                expected_latency_ms=100,
                memory_usage_gb=13,
                notes="INT8 quantization recommended"
            ),
            # LLaMA 7B / Mistral 7B
            ModelGPUCompatibility(
                model_name="llama-7b",
                gpu_name="A10G",
                compatible=True,
                min_gpu_count=1,
                recommended_precision="fp16",
                expected_throughput=60,
                expected_latency_ms=65,
                memory_usage_gb=14
            ),
            ModelGPUCompatibility(
                model_name="llama-7b",
                gpu_name="L4",
                compatible=True,
                min_gpu_count=1,
                recommended_precision="fp16",
                expected_throughput=55,
                expected_latency_ms=70,
                memory_usage_gb=14
            ),
            ModelGPUCompatibility(
                model_name="llama-7b",
                gpu_name="T4",
                compatible=True,
                min_gpu_count=1,
                recommended_precision="int8",
                expected_throughput=25,
                expected_latency_ms=160,
                memory_usage_gb=7,
                notes="INT8 quantization recommended for 16GB memory"
            ),
            # Embedding models
            ModelGPUCompatibility(
                model_name="embedding-large",
                gpu_name="T4",
                compatible=True,
                min_gpu_count=1,
                recommended_precision="fp16",
                expected_throughput=1000,
                expected_latency_ms=5,
                memory_usage_gb=2
            ),
            ModelGPUCompatibility(
                model_name="embedding-large",
                gpu_name="L4",
                compatible=True,
                min_gpu_count=1,
                recommended_precision="fp16",
                expected_throughput=1500,
                expected_latency_ms=3,
                memory_usage_gb=2
            ),
            ModelGPUCompatibility(
                model_name="embedding-small",
                gpu_name="T4",
                compatible=True,
                min_gpu_count=1,
                recommended_precision="fp16",
                expected_throughput=3000,
                expected_latency_ms=2,
                memory_usage_gb=0.5
            )
        ]

    def get_compatible_gpus(self, model_name: str) -> List[ModelGPUCompatibility]:
        """Get compatible GPUs for a model."""
        return [
            c for c in self.compatibilities
            if c.model_name == model_name and c.compatible
        ]

    def get_best_gpu_for_model(
        self,
        model_name: str,
        optimize_for: str = "throughput"  # throughput, latency, cost
    ) -> Optional[ModelGPUCompatibility]:
        """Get the best GPU for a model based on optimization criteria."""
        compatible = self.get_compatible_gpus(model_name)
        if not compatible:
            return None

        if optimize_for == "throughput":
            return max(compatible, key=lambda x: x.expected_throughput)
        elif optimize_for == "latency":
            return min(compatible, key=lambda x: x.expected_latency_ms)
        else:
            return compatible[0]

    def generate_compatibility_report(self) -> str:
        """Generate a compatibility report."""
        report = "# Model-GPU Compatibility Matrix\n\n"

        models = set(c.model_name for c in self.compatibilities)

        for model in sorted(models):
            report += f"## {model}\n\n"
            report += "| GPU | Min Count | Precision | Throughput (tok/s) | Latency (ms) | Memory (GB) |\n"
            report += "|-----|-----------|-----------|-------------------|--------------|-------------|\n"

            for compat in self.get_compatible_gpus(model):
                report += f"| {compat.gpu_name} | {compat.min_gpu_count} | {compat.recommended_precision} | "
                report += f"{compat.expected_throughput:.0f} | {compat.expected_latency_ms:.0f} | {compat.memory_usage_gb:.1f} |\n"

            report += "\n"

        return report
```

## 2. GPU Utilization Optimization

### 2.1 GPU Monitoring and Metrics

```python
from dataclasses import dataclass
from typing import Dict, List, Optional
from datetime import datetime, timedelta
import subprocess
import json


@dataclass
class GPUMetrics:
    """GPU utilization metrics."""
    gpu_index: int
    gpu_name: str
    utilization_percent: float
    memory_used_mb: int
    memory_total_mb: int
    memory_utilization_percent: float
    temperature_celsius: int
    power_draw_watts: float
    power_limit_watts: float
    processes: List[Dict]


class GPUMonitor:
    """Monitor GPU utilization and performance."""

    def __init__(self):
        self.metrics_history: List[Dict] = []

    def get_current_metrics(self) -> List[GPUMetrics]:
        """Get current GPU metrics using nvidia-smi."""
        try:
            result = subprocess.run(
                [
                    "nvidia-smi",
                    "--query-gpu=index,name,utilization.gpu,memory.used,memory.total,temperature.gpu,power.draw,power.limit",
                    "--format=csv,noheader,nounits"
                ],
                capture_output=True,
                text=True
            )

            metrics = []
            for line in result.stdout.strip().split("\n"):
                if not line:
                    continue

                parts = [p.strip() for p in line.split(",")]
                gpu_idx = int(parts[0])

                # Get processes on this GPU
                processes = self._get_gpu_processes(gpu_idx)

                memory_used = int(parts[3])
                memory_total = int(parts[4])

                metrics.append(GPUMetrics(
                    gpu_index=gpu_idx,
                    gpu_name=parts[1],
                    utilization_percent=float(parts[2]),
                    memory_used_mb=memory_used,
                    memory_total_mb=memory_total,
                    memory_utilization_percent=(memory_used / memory_total * 100) if memory_total else 0,
                    temperature_celsius=int(parts[5]),
                    power_draw_watts=float(parts[6]) if parts[6] != "[N/A]" else 0,
                    power_limit_watts=float(parts[7]) if parts[7] != "[N/A]" else 0,
                    processes=processes
                ))

            return metrics

        except Exception as e:
            return []

    def _get_gpu_processes(self, gpu_index: int) -> List[Dict]:
        """Get processes running on a GPU."""
        try:
            result = subprocess.run(
                [
                    "nvidia-smi",
                    f"--id={gpu_index}",
                    "--query-compute-apps=pid,process_name,used_memory",
                    "--format=csv,noheader,nounits"
                ],
                capture_output=True,
                text=True
            )

            processes = []
            for line in result.stdout.strip().split("\n"):
                if not line:
                    continue
                parts = [p.strip() for p in line.split(",")]
                if len(parts) >= 3:
                    processes.append({
                        "pid": int(parts[0]),
                        "name": parts[1],
                        "memory_mb": int(parts[2])
                    })

            return processes

        except Exception:
            return []

    def record_metrics(self):
        """Record current metrics to history."""
        metrics = self.get_current_metrics()
        self.metrics_history.append({
            "timestamp": datetime.utcnow().isoformat(),
            "gpus": [
                {
                    "index": m.gpu_index,
                    "utilization": m.utilization_percent,
                    "memory_utilization": m.memory_utilization_percent,
                    "temperature": m.temperature_celsius,
                    "power": m.power_draw_watts
                }
                for m in metrics
            ]
        })

    def get_utilization_summary(
        self,
        hours: int = 24
    ) -> Dict:
        """Get utilization summary for a time period."""
        cutoff = datetime.utcnow() - timedelta(hours=hours)

        recent = [
            m for m in self.metrics_history
            if datetime.fromisoformat(m["timestamp"]) >= cutoff
        ]

        if not recent:
            return {}

        gpu_stats = {}
        for record in recent:
            for gpu in record["gpus"]:
                idx = gpu["index"]
                if idx not in gpu_stats:
                    gpu_stats[idx] = {
                        "utilization": [],
                        "memory": [],
                        "temperature": [],
                        "power": []
                    }

                gpu_stats[idx]["utilization"].append(gpu["utilization"])
                gpu_stats[idx]["memory"].append(gpu["memory_utilization"])
                gpu_stats[idx]["temperature"].append(gpu["temperature"])
                gpu_stats[idx]["power"].append(gpu["power"])

        summary = {}
        for idx, stats in gpu_stats.items():
            summary[f"gpu_{idx}"] = {
                "avg_utilization": sum(stats["utilization"]) / len(stats["utilization"]),
                "max_utilization": max(stats["utilization"]),
                "min_utilization": min(stats["utilization"]),
                "avg_memory": sum(stats["memory"]) / len(stats["memory"]),
                "max_memory": max(stats["memory"]),
                "avg_temperature": sum(stats["temperature"]) / len(stats["temperature"]),
                "max_temperature": max(stats["temperature"]),
                "avg_power": sum(stats["power"]) / len(stats["power"])
            }

        return summary

    def identify_optimization_opportunities(self) -> List[Dict]:
        """Identify GPU optimization opportunities."""
        opportunities = []
        summary = self.get_utilization_summary(hours=24)

        for gpu_id, stats in summary.items():
            # Low utilization
            if stats["avg_utilization"] < 30:
                opportunities.append({
                    "gpu": gpu_id,
                    "type": "underutilization",
                    "severity": "high" if stats["avg_utilization"] < 10 else "medium",
                    "current_value": f"{stats['avg_utilization']:.1f}%",
                    "recommendation": "Consider consolidating workloads or downsizing GPU"
                })

            # Low memory utilization
            if stats["avg_memory"] < 40:
                opportunities.append({
                    "gpu": gpu_id,
                    "type": "memory_underutilization",
                    "severity": "medium",
                    "current_value": f"{stats['avg_memory']:.1f}%",
                    "recommendation": "Consider using smaller GPU or increasing batch size"
                })

            # High temperature
            if stats["max_temperature"] > 80:
                opportunities.append({
                    "gpu": gpu_id,
                    "type": "thermal_throttling_risk",
                    "severity": "high",
                    "current_value": f"{stats['max_temperature']}°C",
                    "recommendation": "Check cooling, reduce clock speeds, or reduce load"
                })

            # Sporadic utilization (high variance)
            utilization_variance = stats["max_utilization"] - stats["min_utilization"]
            if utilization_variance > 70 and stats["avg_utilization"] < 50:
                opportunities.append({
                    "gpu": gpu_id,
                    "type": "bursty_workload",
                    "severity": "medium",
                    "current_value": f"Variance: {utilization_variance:.1f}%",
                    "recommendation": "Consider request batching or autoscaling"
                })

        return opportunities


class GPUProfiler:
    """Profile GPU workloads for optimization."""

    def __init__(self):
        self.profiles: Dict[str, Dict] = {}

    def profile_inference(
        self,
        model_name: str,
        batch_sizes: List[int],
        sequence_lengths: List[int]
    ) -> Dict:
        """Profile inference performance across configurations."""
        results = []

        for batch_size in batch_sizes:
            for seq_len in sequence_lengths:
                # This would actually run inference benchmarks
                # Simplified for illustration
                throughput = self._estimate_throughput(model_name, batch_size, seq_len)
                latency = self._estimate_latency(model_name, batch_size, seq_len)
                memory = self._estimate_memory(model_name, batch_size, seq_len)

                results.append({
                    "batch_size": batch_size,
                    "sequence_length": seq_len,
                    "throughput_tokens_per_sec": throughput,
                    "latency_ms": latency,
                    "memory_gb": memory,
                    "tokens_per_dollar": throughput / 1.0  # Assuming $1/hour
                })

        self.profiles[model_name] = {
            "results": results,
            "optimal_batch_size": self._find_optimal_batch_size(results),
            "profiled_at": datetime.utcnow().isoformat()
        }

        return self.profiles[model_name]

    def _estimate_throughput(
        self,
        model_name: str,
        batch_size: int,
        seq_len: int
    ) -> float:
        """Estimate throughput (simplified model)."""
        base_throughput = 100  # tokens/sec for batch=1
        batch_efficiency = min(1.0, 0.5 + 0.1 * batch_size)
        return base_throughput * batch_size * batch_efficiency

    def _estimate_latency(
        self,
        model_name: str,
        batch_size: int,
        seq_len: int
    ) -> float:
        """Estimate latency (simplified model)."""
        base_latency = 50  # ms
        return base_latency * (1 + 0.05 * batch_size) * (seq_len / 1024)

    def _estimate_memory(
        self,
        model_name: str,
        batch_size: int,
        seq_len: int
    ) -> float:
        """Estimate memory usage (simplified model)."""
        base_memory = 8  # GB
        kv_cache = batch_size * seq_len * 0.001  # GB
        return base_memory + kv_cache

    def _find_optimal_batch_size(self, results: List[Dict]) -> int:
        """Find optimal batch size balancing throughput and latency."""
        # Maximize throughput while keeping latency reasonable
        valid = [r for r in results if r["latency_ms"] < 200]
        if not valid:
            return 1

        best = max(valid, key=lambda x: x["tokens_per_dollar"])
        return best["batch_size"]
```

### 2.2 Batching and Request Optimization

```python
from dataclasses import dataclass
from typing import Dict, List, Optional, Callable
from datetime import datetime
import asyncio
import queue
import threading
from collections import deque


@dataclass
class InferenceRequest:
    """An inference request."""
    request_id: str
    prompt: str
    max_tokens: int
    created_at: datetime
    priority: int = 0


@dataclass
class BatchedInferenceRequest:
    """A batched inference request."""
    batch_id: str
    requests: List[InferenceRequest]
    created_at: datetime


class DynamicBatcher:
    """Dynamic batching for inference requests."""

    def __init__(
        self,
        max_batch_size: int = 32,
        max_wait_ms: int = 50,
        max_tokens_per_batch: int = 4096
    ):
        self.max_batch_size = max_batch_size
        self.max_wait_ms = max_wait_ms
        self.max_tokens_per_batch = max_tokens_per_batch

        self.pending_requests: deque = deque()
        self.batch_ready_event = asyncio.Event()
        self._lock = threading.Lock()

        # Metrics
        self.batches_created = 0
        self.requests_processed = 0
        self.total_wait_time_ms = 0

    async def add_request(self, request: InferenceRequest) -> str:
        """Add a request to the batcher."""
        with self._lock:
            self.pending_requests.append(request)

            # Check if we should form a batch immediately
            if self._should_batch_now():
                self.batch_ready_event.set()

        return request.request_id

    def _should_batch_now(self) -> bool:
        """Check if we should form a batch immediately."""
        if len(self.pending_requests) >= self.max_batch_size:
            return True

        # Check total tokens
        total_tokens = sum(
            len(r.prompt.split()) + r.max_tokens
            for r in self.pending_requests
        )
        if total_tokens >= self.max_tokens_per_batch:
            return True

        return False

    async def get_batch(self) -> Optional[BatchedInferenceRequest]:
        """Get the next batch of requests."""
        # Wait for either max_wait_ms or enough requests
        try:
            await asyncio.wait_for(
                self.batch_ready_event.wait(),
                timeout=self.max_wait_ms / 1000
            )
        except asyncio.TimeoutError:
            pass

        with self._lock:
            self.batch_ready_event.clear()

            if not self.pending_requests:
                return None

            # Form batch
            batch_requests = []
            total_tokens = 0

            while self.pending_requests and len(batch_requests) < self.max_batch_size:
                request = self.pending_requests[0]
                request_tokens = len(request.prompt.split()) + request.max_tokens

                if total_tokens + request_tokens > self.max_tokens_per_batch:
                    break

                batch_requests.append(self.pending_requests.popleft())
                total_tokens += request_tokens

            if not batch_requests:
                return None

            batch = BatchedInferenceRequest(
                batch_id=f"batch-{self.batches_created}",
                requests=batch_requests,
                created_at=datetime.utcnow()
            )

            self.batches_created += 1
            self.requests_processed += len(batch_requests)

            # Track wait time
            for req in batch_requests:
                wait_ms = (batch.created_at - req.created_at).total_seconds() * 1000
                self.total_wait_time_ms += wait_ms

            return batch

    def get_metrics(self) -> Dict:
        """Get batcher metrics."""
        avg_wait = (
            self.total_wait_time_ms / self.requests_processed
            if self.requests_processed else 0
        )
        avg_batch_size = (
            self.requests_processed / self.batches_created
            if self.batches_created else 0
        )

        return {
            "batches_created": self.batches_created,
            "requests_processed": self.requests_processed,
            "pending_requests": len(self.pending_requests),
            "average_wait_time_ms": avg_wait,
            "average_batch_size": avg_batch_size,
            "max_batch_size": self.max_batch_size
        }


class ContinuousBatching:
    """Continuous batching for LLM inference."""

    def __init__(
        self,
        max_batch_size: int = 64,
        max_total_tokens: int = 16384
    ):
        self.max_batch_size = max_batch_size
        self.max_total_tokens = max_total_tokens

        self.running_requests: Dict[str, Dict] = {}
        self.waiting_requests: deque = deque()

    def add_request(
        self,
        request_id: str,
        prompt_tokens: int,
        max_new_tokens: int
    ):
        """Add a new request."""
        self.waiting_requests.append({
            "request_id": request_id,
            "prompt_tokens": prompt_tokens,
            "max_new_tokens": max_new_tokens,
            "generated_tokens": 0
        })

    def can_add_to_batch(self, request: Dict) -> bool:
        """Check if a request can be added to current batch."""
        if len(self.running_requests) >= self.max_batch_size:
            return False

        current_tokens = sum(
            r["prompt_tokens"] + r["generated_tokens"]
            for r in self.running_requests.values()
        )
        new_tokens = request["prompt_tokens"]

        return current_tokens + new_tokens <= self.max_total_tokens

    def step(self) -> Dict:
        """Execute one generation step."""
        # Add waiting requests if possible
        while self.waiting_requests:
            request = self.waiting_requests[0]
            if self.can_add_to_batch(request):
                self.waiting_requests.popleft()
                self.running_requests[request["request_id"]] = request
            else:
                break

        # Simulate generation step
        completed = []
        for req_id, request in list(self.running_requests.items()):
            request["generated_tokens"] += 1

            if request["generated_tokens"] >= request["max_new_tokens"]:
                completed.append(req_id)
                del self.running_requests[req_id]

        return {
            "running_requests": len(self.running_requests),
            "waiting_requests": len(self.waiting_requests),
            "completed": completed,
            "total_tokens": sum(
                r["prompt_tokens"] + r["generated_tokens"]
                for r in self.running_requests.values()
            )
        }

    def get_status(self) -> Dict:
        """Get current batch status."""
        return {
            "running_requests": len(self.running_requests),
            "waiting_requests": len(self.waiting_requests),
            "total_tokens_in_batch": sum(
                r["prompt_tokens"] + r["generated_tokens"]
                for r in self.running_requests.values()
            ),
            "utilization": len(self.running_requests) / self.max_batch_size * 100
        }
```

## 3. GPU Cost Optimization

### 3.1 Spot Instance Management

```python
from dataclasses import dataclass
from typing import Dict, List, Optional, Callable
from datetime import datetime, timedelta
from decimal import Decimal
import boto3
import asyncio


@dataclass
class SpotInstanceConfig:
    """Configuration for spot instances."""
    instance_types: List[str]
    allocation_strategy: str  # lowest-price, capacity-optimized, diversified
    max_price: Optional[Decimal] = None
    target_capacity: int = 1
    interruption_behavior: str = "terminate"  # terminate, stop, hibernate


class GPUSpotManager:
    """Manage GPU spot instances."""

    def __init__(self, region: str = "us-east-1"):
        self.ec2 = boto3.client("ec2", region_name=region)
        self.region = region

    def get_spot_price_history(
        self,
        instance_types: List[str],
        hours: int = 24
    ) -> Dict[str, List[Dict]]:
        """Get spot price history for instance types."""
        end_time = datetime.utcnow()
        start_time = end_time - timedelta(hours=hours)

        response = self.ec2.describe_spot_price_history(
            InstanceTypes=instance_types,
            StartTime=start_time,
            EndTime=end_time,
            ProductDescriptions=["Linux/UNIX"]
        )

        prices: Dict[str, List[Dict]] = {}
        for record in response["SpotPriceHistory"]:
            instance_type = record["InstanceType"]
            if instance_type not in prices:
                prices[instance_type] = []

            prices[instance_type].append({
                "timestamp": record["Timestamp"].isoformat(),
                "price": float(record["SpotPrice"]),
                "az": record["AvailabilityZone"]
            })

        return prices

    def analyze_spot_savings(
        self,
        instance_types: List[str]
    ) -> Dict[str, Dict]:
        """Analyze potential spot savings."""
        price_history = self.get_spot_price_history(instance_types, hours=168)  # 1 week

        # Get on-demand prices
        on_demand_prices = self._get_on_demand_prices(instance_types)

        analysis = {}
        for instance_type, prices in price_history.items():
            if not prices:
                continue

            spot_prices = [p["price"] for p in prices]
            avg_spot = sum(spot_prices) / len(spot_prices)
            min_spot = min(spot_prices)
            max_spot = max(spot_prices)
            on_demand = on_demand_prices.get(instance_type, 0)

            analysis[instance_type] = {
                "on_demand_hourly": on_demand,
                "avg_spot_hourly": avg_spot,
                "min_spot_hourly": min_spot,
                "max_spot_hourly": max_spot,
                "avg_savings_percent": ((on_demand - avg_spot) / on_demand * 100) if on_demand else 0,
                "max_savings_percent": ((on_demand - min_spot) / on_demand * 100) if on_demand else 0,
                "spot_price_variance": max_spot - min_spot,
                "estimated_interruption_rate": self._estimate_interruption_rate(prices)
            }

        return analysis

    def _get_on_demand_prices(self, instance_types: List[str]) -> Dict[str, float]:
        """Get on-demand prices for instance types."""
        # Simplified - would use AWS Pricing API
        prices = {
            "g5.xlarge": 1.01,
            "g5.2xlarge": 1.21,
            "g5.4xlarge": 1.62,
            "g4dn.xlarge": 0.526,
            "g4dn.2xlarge": 0.752,
            "p4d.24xlarge": 32.77
        }
        return {t: prices.get(t, 1.0) for t in instance_types}

    def _estimate_interruption_rate(self, price_history: List[Dict]) -> float:
        """Estimate interruption rate based on price volatility."""
        if len(price_history) < 2:
            return 0.05

        prices = [p["price"] for p in price_history]
        avg = sum(prices) / len(prices)
        variance = sum((p - avg) ** 2 for p in prices) / len(prices)

        # Higher variance suggests higher interruption rate
        return min(0.3, variance / avg * 10)

    def create_spot_fleet(
        self,
        config: SpotInstanceConfig,
        launch_template_id: str
    ) -> str:
        """Create a spot fleet request."""
        launch_specs = []
        for instance_type in config.instance_types:
            spec = {
                "InstanceType": instance_type,
                "LaunchTemplateConfigs": [{
                    "LaunchTemplateSpecification": {
                        "LaunchTemplateId": launch_template_id,
                        "Version": "$Latest"
                    },
                    "Overrides": [{
                        "InstanceType": instance_type
                    }]
                }]
            }
            launch_specs.append(spec)

        request_config = {
            "IamFleetRole": "arn:aws:iam::ACCOUNT:role/aws-ec2-spot-fleet-role",
            "AllocationStrategy": config.allocation_strategy,
            "TargetCapacity": config.target_capacity,
            "TerminateInstancesWithExpiration": True,
            "Type": "maintain",
            "LaunchTemplateConfigs": launch_specs
        }

        if config.max_price:
            request_config["SpotPrice"] = str(config.max_price)

        response = self.ec2.request_spot_fleet(SpotFleetRequestConfig=request_config)

        return response["SpotFleetRequestId"]

    def setup_interruption_handler(
        self,
        handler: Callable[[str], None]
    ):
        """Setup handler for spot interruption notices."""
        # This would typically be done through EventBridge/CloudWatch Events
        # The handler would be called when interruption notice is received
        pass


class GPUCapacityReservationManager:
    """Manage GPU capacity reservations."""

    def __init__(self, region: str = "us-east-1"):
        self.ec2 = boto3.client("ec2", region_name=region)

    def create_capacity_reservation(
        self,
        instance_type: str,
        instance_count: int,
        availability_zone: str,
        end_date: Optional[datetime] = None
    ) -> str:
        """Create a capacity reservation."""
        params = {
            "InstanceType": instance_type,
            "InstancePlatform": "Linux/UNIX",
            "InstanceCount": instance_count,
            "AvailabilityZone": availability_zone,
            "InstanceMatchCriteria": "targeted"
        }

        if end_date:
            params["EndDateType"] = "limited"
            params["EndDate"] = end_date
        else:
            params["EndDateType"] = "unlimited"

        response = self.ec2.create_capacity_reservation(**params)

        return response["CapacityReservation"]["CapacityReservationId"]

    def get_capacity_utilization(
        self,
        reservation_id: str
    ) -> Dict:
        """Get utilization of a capacity reservation."""
        response = self.ec2.describe_capacity_reservations(
            CapacityReservationIds=[reservation_id]
        )

        if not response["CapacityReservations"]:
            return {}

        reservation = response["CapacityReservations"][0]

        available = reservation["AvailableInstanceCount"]
        total = reservation["TotalInstanceCount"]
        used = total - available

        return {
            "reservation_id": reservation_id,
            "instance_type": reservation["InstanceType"],
            "total_capacity": total,
            "used_capacity": used,
            "available_capacity": available,
            "utilization_percent": (used / total * 100) if total else 0
        }
```

### 3.2 Multi-Instance GPU (MIG) Optimization

```python
from dataclasses import dataclass
from typing import Dict, List, Optional
import subprocess
import json


@dataclass
class MIGInstance:
    """A MIG instance configuration."""
    profile: str
    gpu_memory_gb: int
    compute_units: int
    max_instances_per_gpu: int


@dataclass
class MIGPartition:
    """A MIG partition."""
    instance_id: str
    profile: str
    gpu_index: int
    memory_gb: int
    compute_units: int
    in_use: bool = False


class MIGManager:
    """Manage Multi-Instance GPU configurations."""

    def __init__(self):
        # MIG profiles for A100
        self.profiles = {
            "1g.5gb": MIGInstance("1g.5gb", 5, 1, 7),
            "2g.10gb": MIGInstance("2g.10gb", 10, 2, 3),
            "3g.20gb": MIGInstance("3g.20gb", 20, 3, 2),
            "4g.20gb": MIGInstance("4g.20gb", 20, 4, 1),
            "7g.40gb": MIGInstance("7g.40gb", 40, 7, 1)
        }

    def enable_mig(self, gpu_index: int) -> bool:
        """Enable MIG mode on a GPU."""
        try:
            subprocess.run(
                ["nvidia-smi", "-i", str(gpu_index), "-mig", "1"],
                check=True
            )
            return True
        except subprocess.CalledProcessError:
            return False

    def disable_mig(self, gpu_index: int) -> bool:
        """Disable MIG mode on a GPU."""
        try:
            subprocess.run(
                ["nvidia-smi", "-i", str(gpu_index), "-mig", "0"],
                check=True
            )
            return True
        except subprocess.CalledProcessError:
            return False

    def create_mig_instances(
        self,
        gpu_index: int,
        profile: str,
        count: int
    ) -> List[str]:
        """Create MIG instances on a GPU."""
        if profile not in self.profiles:
            raise ValueError(f"Unknown profile: {profile}")

        profile_info = self.profiles[profile]
        if count > profile_info.max_instances_per_gpu:
            raise ValueError(f"Cannot create {count} instances of {profile}")

        created = []
        try:
            for _ in range(count):
                result = subprocess.run(
                    [
                        "nvidia-smi", "mig", "-i", str(gpu_index),
                        "-cgi", profile, "-C"
                    ],
                    capture_output=True,
                    text=True,
                    check=True
                )
                # Parse instance ID from output
                created.append(f"mig-{gpu_index}-{len(created)}")

            return created
        except subprocess.CalledProcessError as e:
            return created

    def list_mig_instances(self, gpu_index: int) -> List[MIGPartition]:
        """List MIG instances on a GPU."""
        try:
            result = subprocess.run(
                [
                    "nvidia-smi", "mig", "-i", str(gpu_index), "-lgi"
                ],
                capture_output=True,
                text=True,
                check=True
            )

            # Parse output (simplified)
            partitions = []
            # Actual parsing would depend on nvidia-smi output format
            return partitions

        except subprocess.CalledProcessError:
            return []

    def recommend_mig_configuration(
        self,
        workloads: List[Dict]
    ) -> Dict:
        """Recommend MIG configuration for workloads."""
        # Analyze workloads
        total_memory_needed = sum(w.get("memory_gb", 5) for w in workloads)
        workload_count = len(workloads)

        # Find best profile configuration
        recommendations = []

        for profile, info in self.profiles.items():
            if info.gpu_memory_gb >= max(w.get("memory_gb", 5) for w in workloads):
                instances_needed = min(workload_count, info.max_instances_per_gpu)
                utilization = workload_count / instances_needed * 100

                recommendations.append({
                    "profile": profile,
                    "instances_per_gpu": instances_needed,
                    "gpu_memory_per_instance": info.gpu_memory_gb,
                    "workloads_per_instance": workload_count / instances_needed,
                    "estimated_utilization": utilization
                })

        # Sort by utilization
        recommendations.sort(key=lambda x: x["estimated_utilization"], reverse=True)

        return {
            "workload_count": workload_count,
            "total_memory_needed_gb": total_memory_needed,
            "recommendations": recommendations[:3]
        }

    def calculate_mig_savings(
        self,
        current_config: Dict,
        proposed_config: Dict
    ) -> Dict:
        """Calculate savings from MIG configuration."""
        current_gpu_hours = current_config.get("gpu_count", 1) * 730  # hours/month
        current_cost_per_hour = current_config.get("cost_per_hour", 3.67)
        current_monthly = current_gpu_hours * current_cost_per_hour

        # With MIG, one GPU serves multiple workloads
        proposed_gpu_count = proposed_config.get("gpu_count", 1)
        proposed_monthly = proposed_gpu_count * 730 * current_cost_per_hour

        savings = current_monthly - proposed_monthly

        return {
            "current_gpus": current_config.get("gpu_count", 1),
            "proposed_gpus": proposed_gpu_count,
            "current_monthly_cost": current_monthly,
            "proposed_monthly_cost": proposed_monthly,
            "monthly_savings": savings,
            "savings_percent": (savings / current_monthly * 100) if current_monthly else 0
        }
```

## 4. GPU Auto-Scaling

### 4.1 Kubernetes GPU Autoscaling

```yaml
# GPU HPA configuration
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: llm-inference-hpa
  namespace: ml-inference
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: llm-inference
  minReplicas: 1
  maxReplicas: 10
  metrics:
    # Scale based on GPU utilization
    - type: Pods
      pods:
        metric:
          name: nvidia_gpu_duty_cycle
        target:
          type: AverageValue
          averageValue: "70"
    # Scale based on request queue length
    - type: External
      external:
        metric:
          name: inference_queue_length
          selector:
            matchLabels:
              service: llm-inference
        target:
          type: AverageValue
          averageValue: "50"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Percent
          value: 10
          periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
        - type: Percent
          value: 100
          periodSeconds: 15
        - type: Pods
          value: 4
          periodSeconds: 15
      selectPolicy: Max
```

```python
from dataclasses import dataclass
from typing import Dict, List, Optional
from kubernetes import client, config
import time


@dataclass
class GPUScalingPolicy:
    """GPU scaling policy configuration."""
    min_replicas: int
    max_replicas: int
    target_gpu_utilization: float
    target_queue_length: int
    scale_up_cooldown_seconds: int
    scale_down_cooldown_seconds: int
    scale_up_step: int
    scale_down_step: int


class KubernetesGPUAutoscaler:
    """Custom GPU autoscaler for Kubernetes."""

    def __init__(self, namespace: str = "ml-inference"):
        config.load_incluster_config()
        self.apps_v1 = client.AppsV1Api()
        self.custom_api = client.CustomObjectsApi()
        self.namespace = namespace

        self.last_scale_up = 0
        self.last_scale_down = 0

    def get_current_metrics(self, deployment_name: str) -> Dict:
        """Get current GPU and queue metrics."""
        # Would query Prometheus or custom metrics API
        return {
            "gpu_utilization": 65.0,
            "queue_length": 25,
            "current_replicas": 3,
            "ready_replicas": 3
        }

    def evaluate_scaling(
        self,
        deployment_name: str,
        policy: GPUScalingPolicy
    ) -> Dict:
        """Evaluate if scaling is needed."""
        metrics = self.get_current_metrics(deployment_name)
        current_time = time.time()

        decision = {
            "action": "none",
            "current_replicas": metrics["current_replicas"],
            "target_replicas": metrics["current_replicas"],
            "reason": ""
        }

        # Check scale up conditions
        if (metrics["gpu_utilization"] > policy.target_gpu_utilization or
            metrics["queue_length"] > policy.target_queue_length):

            if current_time - self.last_scale_up >= policy.scale_up_cooldown_seconds:
                new_replicas = min(
                    metrics["current_replicas"] + policy.scale_up_step,
                    policy.max_replicas
                )
                if new_replicas > metrics["current_replicas"]:
                    decision["action"] = "scale_up"
                    decision["target_replicas"] = new_replicas
                    decision["reason"] = f"GPU: {metrics['gpu_utilization']:.1f}%, Queue: {metrics['queue_length']}"

        # Check scale down conditions
        elif (metrics["gpu_utilization"] < policy.target_gpu_utilization * 0.5 and
              metrics["queue_length"] < policy.target_queue_length * 0.3):

            if current_time - self.last_scale_down >= policy.scale_down_cooldown_seconds:
                new_replicas = max(
                    metrics["current_replicas"] - policy.scale_down_step,
                    policy.min_replicas
                )
                if new_replicas < metrics["current_replicas"]:
                    decision["action"] = "scale_down"
                    decision["target_replicas"] = new_replicas
                    decision["reason"] = f"Low utilization: GPU {metrics['gpu_utilization']:.1f}%"

        return decision

    def apply_scaling(
        self,
        deployment_name: str,
        target_replicas: int
    ) -> bool:
        """Apply scaling decision."""
        try:
            # Patch deployment with new replica count
            body = {
                "spec": {
                    "replicas": target_replicas
                }
            }

            self.apps_v1.patch_namespaced_deployment_scale(
                name=deployment_name,
                namespace=self.namespace,
                body=body
            )

            return True

        except Exception as e:
            print(f"Scaling failed: {e}")
            return False

    def create_gpu_hpa(
        self,
        deployment_name: str,
        policy: GPUScalingPolicy
    ) -> Dict:
        """Create HPA for GPU workload."""
        hpa = {
            "apiVersion": "autoscaling/v2",
            "kind": "HorizontalPodAutoscaler",
            "metadata": {
                "name": f"{deployment_name}-gpu-hpa",
                "namespace": self.namespace
            },
            "spec": {
                "scaleTargetRef": {
                    "apiVersion": "apps/v1",
                    "kind": "Deployment",
                    "name": deployment_name
                },
                "minReplicas": policy.min_replicas,
                "maxReplicas": policy.max_replicas,
                "metrics": [
                    {
                        "type": "Pods",
                        "pods": {
                            "metric": {"name": "nvidia_gpu_duty_cycle"},
                            "target": {
                                "type": "AverageValue",
                                "averageValue": str(int(policy.target_gpu_utilization))
                            }
                        }
                    }
                ],
                "behavior": {
                    "scaleDown": {
                        "stabilizationWindowSeconds": policy.scale_down_cooldown_seconds,
                        "policies": [{
                            "type": "Pods",
                            "value": policy.scale_down_step,
                            "periodSeconds": 60
                        }]
                    },
                    "scaleUp": {
                        "stabilizationWindowSeconds": policy.scale_up_cooldown_seconds,
                        "policies": [{
                            "type": "Pods",
                            "value": policy.scale_up_step,
                            "periodSeconds": 15
                        }]
                    }
                }
            }
        }

        # Create HPA
        autoscaling_v2 = client.AutoscalingV2Api()
        result = autoscaling_v2.create_namespaced_horizontal_pod_autoscaler(
            namespace=self.namespace,
            body=hpa
        )

        return {"name": result.metadata.name, "status": "created"}
```

## 5. GPU Infrastructure Best Practices

### 5.1 Deployment Patterns

```python
from dataclasses import dataclass
from typing import Dict, List


@dataclass
class GPUDeploymentPattern:
    """GPU deployment pattern."""
    name: str
    description: str
    use_cases: List[str]
    pros: List[str]
    cons: List[str]
    configuration: Dict


class GPUDeploymentPatterns:
    """Collection of GPU deployment patterns."""

    @staticmethod
    def get_patterns() -> List[GPUDeploymentPattern]:
        return [
            GPUDeploymentPattern(
                name="Dedicated GPU per Model",
                description="Each model gets dedicated GPU(s)",
                use_cases=[
                    "Large models (>20GB)",
                    "Low-latency requirements",
                    "Consistent throughput needed"
                ],
                pros=[
                    "Predictable performance",
                    "No resource contention",
                    "Simple deployment"
                ],
                cons=[
                    "Higher cost",
                    "Potential underutilization",
                    "Not flexible"
                ],
                configuration={
                    "deployment_type": "dedicated",
                    "resource_requests": {"nvidia.com/gpu": 1},
                    "anti_affinity": True
                }
            ),
            GPUDeploymentPattern(
                name="Shared GPU with MIG",
                description="Multiple models share GPU using MIG",
                use_cases=[
                    "Small models (<10GB)",
                    "Multiple similar workloads",
                    "Cost optimization priority"
                ],
                pros=[
                    "Better GPU utilization",
                    "Cost efficient",
                    "Workload isolation"
                ],
                cons=[
                    "Only for A100/H100",
                    "Fixed partition sizes",
                    "More complex setup"
                ],
                configuration={
                    "deployment_type": "mig",
                    "mig_profile": "3g.20gb",
                    "instances_per_gpu": 2
                }
            ),
            GPUDeploymentPattern(
                name="Time-Sharing GPU",
                description="Multiple pods share GPU via scheduling",
                use_cases=[
                    "Bursty workloads",
                    "Development environments",
                    "Low-priority batch jobs"
                ],
                pros=[
                    "Maximum utilization",
                    "Flexible allocation",
                    "Easy to implement"
                ],
                cons=[
                    "Performance variability",
                    "No isolation",
                    "Memory contention"
                ],
                configuration={
                    "deployment_type": "time_shared",
                    "gpu_sharing": True,
                    "priority_class": "batch"
                }
            ),
            GPUDeploymentPattern(
                name="Multi-GPU Distributed",
                description="Model distributed across multiple GPUs",
                use_cases=[
                    "Very large models (>40GB)",
                    "High throughput requirements",
                    "Training workloads"
                ],
                pros=[
                    "Handles any model size",
                    "High throughput",
                    "Parallelization benefits"
                ],
                cons=[
                    "Complex deployment",
                    "Inter-GPU communication overhead",
                    "Higher cost"
                ],
                configuration={
                    "deployment_type": "distributed",
                    "resource_requests": {"nvidia.com/gpu": 4},
                    "parallelism": "tensor",
                    "nvlink_required": True
                }
            ),
            GPUDeploymentPattern(
                name="Spot GPU Fleet",
                description="Mix of spot and on-demand GPUs",
                use_cases=[
                    "Batch processing",
                    "Non-critical workloads",
                    "Cost-sensitive deployments"
                ],
                pros=[
                    "60-90% cost savings",
                    "High capacity availability",
                    "Automatic fallback"
                ],
                cons=[
                    "Interruption handling needed",
                    "Variable availability",
                    "More complex orchestration"
                ],
                configuration={
                    "deployment_type": "spot_fleet",
                    "spot_percentage": 80,
                    "on_demand_base": 1,
                    "interruption_tolerance": True
                }
            )
        ]

    @staticmethod
    def recommend_pattern(
        model_size_gb: float,
        latency_requirement_ms: int,
        cost_sensitivity: str,  # low, medium, high
        workload_pattern: str  # steady, bursty, batch
    ) -> GPUDeploymentPattern:
        """Recommend deployment pattern based on requirements."""
        patterns = GPUDeploymentPatterns.get_patterns()

        if model_size_gb > 40:
            return patterns[3]  # Multi-GPU Distributed

        if cost_sensitivity == "high" and workload_pattern == "batch":
            return patterns[4]  # Spot GPU Fleet

        if model_size_gb < 10 and cost_sensitivity != "low":
            return patterns[1]  # Shared GPU with MIG

        if latency_requirement_ms < 50:
            return patterns[0]  # Dedicated GPU per Model

        if workload_pattern == "bursty":
            return patterns[2]  # Time-Sharing GPU

        return patterns[0]  # Default to Dedicated
```

## Troubleshooting

### Common GPU Issues

| Issue | Symptoms | Resolution |
|-------|----------|------------|
| OOM Error | CUDA out of memory | Reduce batch size, use gradient checkpointing |
| Low Utilization | GPU <30% busy | Increase batch size, check data loading |
| Thermal Throttling | High temp, reduced clock | Check cooling, reduce load |
| Driver Mismatch | CUDA errors | Match driver and CUDA versions |
| Memory Leak | Growing memory usage | Restart pods, check for leaks |

### GPU Diagnostic Commands

```bash
# Check GPU status
nvidia-smi

# Monitor GPU in real-time
nvidia-smi dmon -s u

# Check CUDA version
nvcc --version

# Check GPU memory details
nvidia-smi --query-gpu=memory.total,memory.used,memory.free --format=csv

# Check processes using GPU
nvidia-smi pmon -s u

# Reset GPU (if stuck)
nvidia-smi --gpu-reset
```

## Related Documentation

- [14.1 Total Cost of Ownership Guide](14.1_total_cost_ownership_guide.md)
- [14.2 Cloud Cost Optimization Guide](14.2_cloud_cost_optimization_guide.md)
- [5.1 Model Serving Architecture](../05_llm_integration/5.1_model_serving_architecture_guide.md)

## Version History

| Version | Date | Author | Changes |
|---------|------|--------|---------|
| 1.0 | 2024-01-15 | ML Infrastructure Team | Initial release |
