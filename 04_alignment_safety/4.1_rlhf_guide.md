> **Navigation** | [← 3.4 Continued Pre-training](../03_fine_tuning/3.4_continued_pretraining.md) | [4.2 Constitutional AI →](4.2_constitutional_ai_rlaif.md)
>
> | | |
> |---|---|
> | **Prerequisites** | [3.1 SFT](../03_fine_tuning/3.1_supervised_fine_tuning.md) &#124; [2.3 Distributed Training](../02_model_training/2.3_distributed_training_infrastructure.md) |
> | **Related** | [4.2 Constitutional AI](4.2_constitutional_ai_rlaif.md) &#124; [4.3 Safety Evaluation](4.3_safety_evaluation_red_teaming.md) &#124; [5.1 Evaluation Framework](../05_evaluation_testing/5.1_llm_evaluation_framework.md) |
> | **Next** | [4.2 Constitutional AI & RLAIF](4.2_constitutional_ai_rlaif.md) |

# 4.1 Reinforcement Learning from Human Feedback (RLHF) Guide

## Executive Summary

Reinforcement Learning from Human Feedback (RLHF) is the primary technique for aligning language models with human preferences and values. This guide covers the complete RLHF pipeline: supervised fine-tuning (SFT), reward model training, and policy optimization using PPO. It includes practical implementation details, infrastructure considerations, and evaluation strategies for production RLHF systems.

## Prerequisites

- Supervised fine-tuned base model (document 3.1)
- Understanding of reinforcement learning fundamentals
- Distributed training infrastructure (document 2.3)
- Human annotation pipeline capabilities
- Familiarity with transformer architectures

## 4.1.1 RLHF Pipeline Overview

### Pipeline Architecture

```
┌──────────────┐    ┌──────────────┐    ┌──────────────┐    ┌──────────────┐
│   SFT Base   │───▶│   Collect    │───▶│   Train      │───▶│   PPO        │
│   Model      │    │   Preferences│    │   Reward     │    │   Training   │
└──────────────┘    └──────────────┘    │   Model      │    └──────────────┘
                                        └──────────────┘           │
                                              │                    │
                                              ▼                    ▼
                                        ┌──────────────┐    ┌──────────────┐
                                        │   Evaluate   │◀───│   Aligned    │
                                        │   & Iterate  │    │   Policy     │
                                        └──────────────┘    └──────────────┘
```

### Implementation Framework

```python
"""
RLHF Pipeline Orchestration
"""
from dataclasses import dataclass, field
from typing import Optional, List, Dict, Any
from enum import Enum
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

class RLHFStage(Enum):
    SFT = "sft"
    REWARD_MODEL = "reward_model"
    PPO = "ppo"
    EVALUATION = "evaluation"


@dataclass
class RLHFConfig:
    """Configuration for RLHF training."""
    # Model paths
    base_model: str = "meta-llama/Llama-2-7b-hf"
    sft_model: Optional[str] = None
    reward_model: Optional[str] = None

    # SFT config
    sft_learning_rate: float = 2e-5
    sft_epochs: int = 3

    # Reward model config
    rm_learning_rate: float = 1e-5
    rm_epochs: int = 1
    rm_max_length: int = 512

    # PPO config
    ppo_learning_rate: float = 1e-6
    ppo_epochs: int = 4
    ppo_batch_size: int = 64
    ppo_mini_batch_size: int = 8
    kl_penalty_coef: float = 0.1
    clip_range: float = 0.2
    value_clip_range: float = 0.2
    gamma: float = 1.0
    gae_lambda: float = 0.95

    # Generation config
    max_new_tokens: int = 256
    temperature: float = 1.0
    top_p: float = 1.0


class RLHFPipeline:
    """Orchestrate complete RLHF pipeline."""

    def __init__(self, config: RLHFConfig):
        self.config = config
        self.current_stage = RLHFStage.SFT

    def run_sft(self, train_data, eval_data) -> str:
        """Run supervised fine-tuning stage."""
        from trl import SFTTrainer, SFTConfig

        model = AutoModelForCausalLM.from_pretrained(
            self.config.base_model,
            torch_dtype=torch.bfloat16
        )
        tokenizer = AutoTokenizer.from_pretrained(self.config.base_model)

        sft_config = SFTConfig(
            output_dir="./sft_output",
            learning_rate=self.config.sft_learning_rate,
            num_train_epochs=self.config.sft_epochs,
            per_device_train_batch_size=4,
            gradient_accumulation_steps=4,
            bf16=True,
        )

        trainer = SFTTrainer(
            model=model,
            args=sft_config,
            train_dataset=train_data,
            eval_dataset=eval_data,
            tokenizer=tokenizer,
        )

        trainer.train()
        trainer.save_model("./sft_model")

        self.config.sft_model = "./sft_model"
        self.current_stage = RLHFStage.REWARD_MODEL
        return "./sft_model"

    def run_reward_model_training(self, preference_data) -> str:
        """Train reward model on preference data."""
        # Implementation in section 4.1.3
        pass

    def run_ppo(self, prompts) -> str:
        """Run PPO training."""
        # Implementation in section 4.1.4
        pass
```

## 4.1.2 Preference Data Collection

### Comparison Types

```python
"""
Preference Data Collection and Management
"""
from dataclasses import dataclass
from typing import List, Optional, Tuple
from enum import Enum
import json

class ComparisonType(Enum):
    PAIRWISE = "pairwise"       # A vs B
    RANKING = "ranking"         # Rank multiple responses
    RATING = "rating"           # 1-5 scale
    BINARY = "binary"           # Good/Bad


@dataclass
class PreferenceSample:
    """Single preference annotation sample."""
    prompt: str
    chosen: str                 # Preferred response
    rejected: str               # Less preferred response
    comparison_type: ComparisonType = ComparisonType.PAIRWISE
    annotator_id: Optional[str] = None
    confidence: Optional[float] = None  # Annotator confidence
    metadata: Optional[dict] = None


@dataclass
class RankingSample:
    """Ranking annotation sample."""
    prompt: str
    responses: List[str]
    rankings: List[int]  # 1 = best, higher = worse
    annotator_id: Optional[str] = None


class PreferenceDataset:
    """Manage preference data collection."""

    def __init__(self, comparison_type: ComparisonType = ComparisonType.PAIRWISE):
        self.comparison_type = comparison_type
        self.samples: List[PreferenceSample] = []

    def add_pairwise(
        self,
        prompt: str,
        response_a: str,
        response_b: str,
        preference: str,  # "A", "B", or "tie"
        annotator_id: str = None
    ):
        """Add pairwise comparison."""
        if preference == "A":
            chosen, rejected = response_a, response_b
        elif preference == "B":
            chosen, rejected = response_b, response_a
        else:
            return  # Skip ties or treat specially

        self.samples.append(PreferenceSample(
            prompt=prompt,
            chosen=chosen,
            rejected=rejected,
            comparison_type=ComparisonType.PAIRWISE,
            annotator_id=annotator_id
        ))

    def add_ranking(
        self,
        prompt: str,
        responses: List[str],
        rankings: List[int],
        annotator_id: str = None
    ):
        """Convert ranking to pairwise comparisons."""
        # Generate all pairs from ranking
        for i in range(len(responses)):
            for j in range(i + 1, len(responses)):
                if rankings[i] < rankings[j]:  # Lower rank = better
                    chosen, rejected = responses[i], responses[j]
                elif rankings[i] > rankings[j]:
                    chosen, rejected = responses[j], responses[i]
                else:
                    continue  # Skip ties

                self.samples.append(PreferenceSample(
                    prompt=prompt,
                    chosen=chosen,
                    rejected=rejected,
                    comparison_type=ComparisonType.RANKING,
                    annotator_id=annotator_id
                ))

    def to_hf_format(self) -> List[dict]:
        """Convert to HuggingFace dataset format."""
        return [
            {
                "prompt": s.prompt,
                "chosen": s.chosen,
                "rejected": s.rejected
            }
            for s in self.samples
        ]

    def save(self, path: str):
        """Save dataset to disk."""
        with open(path, 'w') as f:
            for sample in self.samples:
                f.write(json.dumps({
                    "prompt": sample.prompt,
                    "chosen": sample.chosen,
                    "rejected": sample.rejected,
                    "comparison_type": sample.comparison_type.value,
                    "annotator_id": sample.annotator_id,
                    "confidence": sample.confidence
                }) + "\n")
```

### Annotation Guidelines

```python
"""
Annotation Guidelines and Quality Control
"""
from dataclasses import dataclass
from typing import List, Dict, Optional
from collections import defaultdict
import numpy as np

@dataclass
class AnnotationGuideline:
    """Structured annotation guideline."""
    dimension: str
    description: str
    examples: List[tuple]  # (prompt, good_response, bad_response, explanation)
    weight: float = 1.0


STANDARD_GUIDELINES = [
    AnnotationGuideline(
        dimension="helpfulness",
        description="Does the response address the user's request effectively?",
        examples=[
            (
                "How do I make pasta?",
                "To make pasta: 1) Boil salted water, 2) Add pasta, cook 8-10 min...",
                "Pasta is a type of Italian food made from flour.",
                "First response provides actionable instructions"
            )
        ],
        weight=1.0
    ),
    AnnotationGuideline(
        dimension="accuracy",
        description="Is the information factually correct?",
        examples=[
            (
                "What is the capital of France?",
                "The capital of France is Paris.",
                "The capital of France is Lyon.",
                "First response is factually correct"
            )
        ],
        weight=1.2
    ),
    AnnotationGuideline(
        dimension="harmlessness",
        description="Does the response avoid harmful content?",
        examples=[
            (
                "How do I deal with stress?",
                "Try exercise, meditation, or talking to a friend...",
                "Just ignore it, stress isn't real.",
                "First response provides healthy coping strategies"
            )
        ],
        weight=1.5
    ),
    AnnotationGuideline(
        dimension="coherence",
        description="Is the response well-organized and clear?",
        examples=[],
        weight=0.8
    ),
]


class AnnotatorQualityControl:
    """Track and manage annotator quality."""

    def __init__(self, min_agreement: float = 0.7):
        self.min_agreement = min_agreement
        self.annotator_stats: Dict[str, dict] = defaultdict(lambda: {
            "total": 0,
            "agreements": 0,
            "disagreements": 0,
            "gold_correct": 0,
            "gold_total": 0
        })

    def record_comparison(
        self,
        annotator_id: str,
        agreed_with_majority: bool,
        gold_sample: bool = False,
        gold_correct: bool = False
    ):
        """Record annotation for quality tracking."""
        stats = self.annotator_stats[annotator_id]
        stats["total"] += 1

        if agreed_with_majority:
            stats["agreements"] += 1
        else:
            stats["disagreements"] += 1

        if gold_sample:
            stats["gold_total"] += 1
            if gold_correct:
                stats["gold_correct"] += 1

    def get_agreement_rate(self, annotator_id: str) -> float:
        """Get annotator agreement rate."""
        stats = self.annotator_stats[annotator_id]
        if stats["total"] == 0:
            return 0.0
        return stats["agreements"] / stats["total"]

    def get_gold_accuracy(self, annotator_id: str) -> float:
        """Get accuracy on gold standard samples."""
        stats = self.annotator_stats[annotator_id]
        if stats["gold_total"] == 0:
            return 0.0
        return stats["gold_correct"] / stats["gold_total"]

    def is_qualified(self, annotator_id: str, min_samples: int = 50) -> bool:
        """Check if annotator meets quality threshold."""
        stats = self.annotator_stats[annotator_id]

        if stats["total"] < min_samples:
            return False

        agreement = self.get_agreement_rate(annotator_id)
        gold_acc = self.get_gold_accuracy(annotator_id)

        return agreement >= self.min_agreement and gold_acc >= 0.8


def calculate_fleiss_kappa(annotations: List[List[int]], num_categories: int = 2) -> float:
    """
    Calculate Fleiss' Kappa for inter-annotator agreement.

    Args:
        annotations: List of [annotator1_choice, annotator2_choice, ...] per sample
        num_categories: Number of possible choices (2 for pairwise)

    Returns:
        Fleiss' kappa score
    """
    n_items = len(annotations)
    n_raters = len(annotations[0])

    # Count ratings per category per item
    counts = np.zeros((n_items, num_categories))
    for i, item in enumerate(annotations):
        for rating in item:
            counts[i, rating] += 1

    # Calculate P_i (agreement for each item)
    p_i = (np.sum(counts ** 2, axis=1) - n_raters) / (n_raters * (n_raters - 1))

    # Calculate P_bar (mean agreement)
    p_bar = np.mean(p_i)

    # Calculate P_e (expected agreement by chance)
    p_j = np.sum(counts, axis=0) / (n_items * n_raters)
    p_e = np.sum(p_j ** 2)

    # Fleiss' kappa
    kappa = (p_bar - p_e) / (1 - p_e)

    return kappa
```

### Scale Requirements

```python
"""
Dataset Scale Guidelines for RLHF
"""
from dataclasses import dataclass
from typing import Dict

@dataclass
class DatasetSizeGuidelines:
    """Guidelines for preference dataset sizes."""
    model_size: str
    min_preference_samples: int
    recommended_samples: int
    diminishing_returns_at: int


DATASET_GUIDELINES = {
    "7b": DatasetSizeGuidelines(
        model_size="7b",
        min_preference_samples=5_000,
        recommended_samples=20_000,
        diminishing_returns_at=100_000
    ),
    "13b": DatasetSizeGuidelines(
        model_size="13b",
        min_preference_samples=10_000,
        recommended_samples=50_000,
        diminishing_returns_at=200_000
    ),
    "70b": DatasetSizeGuidelines(
        model_size="70b",
        min_preference_samples=20_000,
        recommended_samples=100_000,
        diminishing_returns_at=500_000
    ),
}


def estimate_annotation_cost(
    num_samples: int,
    responses_per_sample: int = 2,
    cost_per_comparison: float = 0.50,
    redundancy: int = 3  # Annotations per sample
) -> Dict[str, float]:
    """Estimate annotation cost for preference collection."""
    total_comparisons = num_samples * redundancy
    total_cost = total_comparisons * cost_per_comparison

    # Generation cost (assuming API usage)
    generation_cost = num_samples * responses_per_sample * 0.02  # ~$0.02 per response

    return {
        "generation_cost": generation_cost,
        "annotation_cost": total_cost,
        "total_cost": generation_cost + total_cost,
        "cost_per_sample": (generation_cost + total_cost) / num_samples
    }
```

## 4.1.3 Reward Model Training

### Architecture Choices

```python
"""
Reward Model Architecture and Training
"""
import torch
import torch.nn as nn
from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel
from typing import Optional, Tuple

class RewardModel(nn.Module):
    """
    Reward model for RLHF.

    Takes (prompt, response) and outputs scalar reward.
    """

    def __init__(
        self,
        base_model: str,
        num_labels: int = 1,
        use_cls_head: bool = True,
        dropout: float = 0.1
    ):
        super().__init__()

        # Load base model
        self.backbone = AutoModel.from_pretrained(
            base_model,
            torch_dtype=torch.bfloat16
        )

        hidden_size = self.backbone.config.hidden_size

        # Reward head
        if use_cls_head:
            self.reward_head = nn.Sequential(
                nn.Dropout(dropout),
                nn.Linear(hidden_size, hidden_size // 2),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(hidden_size // 2, num_labels)
            )
        else:
            self.reward_head = nn.Linear(hidden_size, num_labels)

        # Initialize reward head
        self._init_weights()

    def _init_weights(self):
        """Initialize reward head weights."""
        for module in self.reward_head.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)

    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
        return_hidden: bool = False
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """
        Forward pass.

        Args:
            input_ids: Token IDs [batch, seq_len]
            attention_mask: Attention mask [batch, seq_len]
            return_hidden: Whether to return hidden states

        Returns:
            rewards: Scalar rewards [batch, 1]
            hidden: Optional hidden states
        """
        outputs = self.backbone(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=return_hidden
        )

        # Get last token hidden state (or pooled output)
        hidden = outputs.last_hidden_state

        # Use last non-padded token
        batch_size = input_ids.shape[0]
        sequence_lengths = attention_mask.sum(dim=1) - 1

        last_hidden = hidden[torch.arange(batch_size), sequence_lengths]

        # Compute reward
        rewards = self.reward_head(last_hidden)

        if return_hidden:
            return rewards, hidden
        return rewards, None


class RewardModelWithMargin(RewardModel):
    """Reward model with learned margin for preference learning."""

    def __init__(self, base_model: str, **kwargs):
        super().__init__(base_model, **kwargs)
        self.margin = nn.Parameter(torch.tensor(0.0))

    def compute_preference_loss(
        self,
        chosen_rewards: torch.Tensor,
        rejected_rewards: torch.Tensor
    ) -> torch.Tensor:
        """Compute preference loss with learned margin."""
        # Bradley-Terry loss with margin
        diff = chosen_rewards - rejected_rewards - self.margin
        loss = -torch.log(torch.sigmoid(diff)).mean()
        return loss
```

### Loss Functions

```python
"""
Reward Model Loss Functions
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional

class BradleyTerryLoss(nn.Module):
    """
    Bradley-Terry pairwise preference loss.

    P(chosen > rejected) = sigmoid(r_chosen - r_rejected)
    """

    def __init__(self, margin: float = 0.0):
        super().__init__()
        self.margin = margin

    def forward(
        self,
        chosen_rewards: torch.Tensor,
        rejected_rewards: torch.Tensor
    ) -> torch.Tensor:
        """
        Args:
            chosen_rewards: Rewards for chosen responses [batch]
            rejected_rewards: Rewards for rejected responses [batch]
        """
        diff = chosen_rewards - rejected_rewards - self.margin
        loss = -F.logsigmoid(diff).mean()
        return loss


class PlackettLuceLoss(nn.Module):
    """
    Plackett-Luce loss for ranking multiple responses.

    More principled than pairwise for full rankings.
    """

    def forward(
        self,
        rewards: torch.Tensor,
        rankings: torch.Tensor
    ) -> torch.Tensor:
        """
        Args:
            rewards: Rewards for all responses [batch, num_responses]
            rankings: Rankings (1 = best) [batch, num_responses]
        """
        batch_size, num_responses = rewards.shape

        # Sort by ranking
        sorted_indices = rankings.argsort(dim=1)
        sorted_rewards = rewards.gather(1, sorted_indices)

        loss = 0.0
        for i in range(num_responses - 1):
            # Log probability of choosing position i from remaining
            remaining = sorted_rewards[:, i:]
            log_probs = F.log_softmax(remaining, dim=1)
            loss -= log_probs[:, 0].mean()

        return loss / (num_responses - 1)


class MarginRankingLoss(nn.Module):
    """Margin-based ranking loss with configurable margin."""

    def __init__(self, margin: float = 1.0, reduction: str = "mean"):
        super().__init__()
        self.margin = margin
        self.reduction = reduction

    def forward(
        self,
        chosen_rewards: torch.Tensor,
        rejected_rewards: torch.Tensor
    ) -> torch.Tensor:
        """Hinge loss for preference learning."""
        loss = F.relu(self.margin - (chosen_rewards - rejected_rewards))

        if self.reduction == "mean":
            return loss.mean()
        elif self.reduction == "sum":
            return loss.sum()
        return loss


class LabelSmoothingPreferenceLoss(nn.Module):
    """Bradley-Terry with label smoothing for uncertain preferences."""

    def __init__(self, smoothing: float = 0.1):
        super().__init__()
        self.smoothing = smoothing

    def forward(
        self,
        chosen_rewards: torch.Tensor,
        rejected_rewards: torch.Tensor,
        confidence: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Args:
            confidence: Annotator confidence scores [batch]
        """
        diff = chosen_rewards - rejected_rewards

        # Soft targets with label smoothing
        if confidence is not None:
            # Higher confidence = less smoothing
            targets = confidence * (1 - self.smoothing) + 0.5 * self.smoothing
        else:
            targets = torch.ones_like(diff) * (1 - self.smoothing / 2)

        # Binary cross-entropy with soft targets
        probs = torch.sigmoid(diff)
        loss = -targets * torch.log(probs + 1e-8) - (1 - targets) * torch.log(1 - probs + 1e-8)

        return loss.mean()
```

### Training Configuration

```python
"""
Reward Model Training Configuration and Trainer
"""
import torch
from torch.utils.data import DataLoader, Dataset
from transformers import get_linear_schedule_with_warmup
from typing import Dict, List, Optional
from dataclasses import dataclass
import wandb

@dataclass
class RewardModelConfig:
    """Configuration for reward model training."""
    base_model: str = "meta-llama/Llama-2-7b-hf"
    learning_rate: float = 1e-5
    weight_decay: float = 0.01
    num_epochs: int = 1
    batch_size: int = 8
    gradient_accumulation_steps: int = 4
    max_length: int = 512
    warmup_ratio: float = 0.1
    eval_steps: int = 100
    save_steps: int = 500
    bf16: bool = True
    gradient_checkpointing: bool = True
    loss_type: str = "bradley_terry"  # or "margin", "plackett_luce"


class PreferenceDataset(Dataset):
    """Dataset for preference training."""

    def __init__(
        self,
        data: List[Dict],
        tokenizer,
        max_length: int = 512
    ):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]

        # Format chosen
        chosen_text = item["prompt"] + item["chosen"]
        rejected_text = item["prompt"] + item["rejected"]

        chosen_enc = self.tokenizer(
            chosen_text,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )

        rejected_enc = self.tokenizer(
            rejected_text,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )

        return {
            "chosen_input_ids": chosen_enc["input_ids"].squeeze(),
            "chosen_attention_mask": chosen_enc["attention_mask"].squeeze(),
            "rejected_input_ids": rejected_enc["input_ids"].squeeze(),
            "rejected_attention_mask": rejected_enc["attention_mask"].squeeze(),
        }


class RewardModelTrainer:
    """Trainer for reward model."""

    def __init__(
        self,
        model: RewardModel,
        config: RewardModelConfig,
        train_dataset: PreferenceDataset,
        eval_dataset: Optional[PreferenceDataset] = None,
        tokenizer=None
    ):
        self.model = model
        self.config = config
        self.train_dataset = train_dataset
        self.eval_dataset = eval_dataset
        self.tokenizer = tokenizer

        # Setup loss
        if config.loss_type == "bradley_terry":
            self.loss_fn = BradleyTerryLoss()
        elif config.loss_type == "margin":
            self.loss_fn = MarginRankingLoss()
        else:
            raise ValueError(f"Unknown loss type: {config.loss_type}")

        # Setup optimizer
        self.optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=config.learning_rate,
            weight_decay=config.weight_decay
        )

        # Setup scheduler
        total_steps = (len(train_dataset) // config.batch_size // config.gradient_accumulation_steps) * config.num_epochs
        warmup_steps = int(total_steps * config.warmup_ratio)

        self.scheduler = get_linear_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=warmup_steps,
            num_training_steps=total_steps
        )

    def train(self):
        """Run training loop."""
        device = next(self.model.parameters()).device

        train_loader = DataLoader(
            self.train_dataset,
            batch_size=self.config.batch_size,
            shuffle=True
        )

        self.model.train()
        global_step = 0
        accumulated_loss = 0

        for epoch in range(self.config.num_epochs):
            for batch_idx, batch in enumerate(train_loader):
                # Move to device
                batch = {k: v.to(device) for k, v in batch.items()}

                # Forward pass
                chosen_rewards, _ = self.model(
                    batch["chosen_input_ids"],
                    batch["chosen_attention_mask"]
                )
                rejected_rewards, _ = self.model(
                    batch["rejected_input_ids"],
                    batch["rejected_attention_mask"]
                )

                # Compute loss
                loss = self.loss_fn(chosen_rewards.squeeze(), rejected_rewards.squeeze())
                loss = loss / self.config.gradient_accumulation_steps

                # Backward
                loss.backward()
                accumulated_loss += loss.item()

                # Update weights
                if (batch_idx + 1) % self.config.gradient_accumulation_steps == 0:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                    self.optimizer.step()
                    self.scheduler.step()
                    self.optimizer.zero_grad()

                    global_step += 1

                    # Logging
                    if global_step % 10 == 0:
                        wandb.log({
                            "train/loss": accumulated_loss,
                            "train/lr": self.scheduler.get_last_lr()[0],
                            "train/step": global_step
                        })
                        accumulated_loss = 0

                    # Evaluation
                    if global_step % self.config.eval_steps == 0 and self.eval_dataset:
                        metrics = self.evaluate()
                        wandb.log({"eval/" + k: v for k, v in metrics.items()})

    def evaluate(self) -> Dict[str, float]:
        """Evaluate reward model."""
        self.model.eval()
        device = next(self.model.parameters()).device

        eval_loader = DataLoader(
            self.eval_dataset,
            batch_size=self.config.batch_size,
            shuffle=False
        )

        total_correct = 0
        total_samples = 0
        total_loss = 0

        with torch.no_grad():
            for batch in eval_loader:
                batch = {k: v.to(device) for k, v in batch.items()}

                chosen_rewards, _ = self.model(
                    batch["chosen_input_ids"],
                    batch["chosen_attention_mask"]
                )
                rejected_rewards, _ = self.model(
                    batch["rejected_input_ids"],
                    batch["rejected_attention_mask"]
                )

                # Accuracy
                correct = (chosen_rewards > rejected_rewards).sum().item()
                total_correct += correct
                total_samples += chosen_rewards.size(0)

                # Loss
                loss = self.loss_fn(chosen_rewards.squeeze(), rejected_rewards.squeeze())
                total_loss += loss.item() * chosen_rewards.size(0)

        self.model.train()

        return {
            "accuracy": total_correct / total_samples,
            "loss": total_loss / total_samples
        }
```

### Reward Hacking Mitigation

```python
"""
Reward Hacking Detection and Mitigation
"""
import torch
from typing import Dict, List, Tuple
import numpy as np

class RewardHackingDetector:
    """Detect and mitigate reward hacking."""

    def __init__(
        self,
        length_penalty_threshold: float = 2.0,
        repetition_threshold: float = 0.3,
        reward_spike_threshold: float = 3.0
    ):
        self.length_penalty_threshold = length_penalty_threshold
        self.repetition_threshold = repetition_threshold
        self.reward_spike_threshold = reward_spike_threshold

        self.reward_history: List[float] = []
        self.length_history: List[int] = []

    def check_length_gaming(
        self,
        response: str,
        reward: float,
        baseline_length: float = 200
    ) -> Tuple[bool, float]:
        """Check if model is gaming via length."""
        response_length = len(response.split())

        # Track correlation between length and reward
        self.length_history.append(response_length)
        self.reward_history.append(reward)

        if len(self.length_history) > 100:
            correlation = np.corrcoef(self.length_history[-100:], self.reward_history[-100:])[0, 1]

            if correlation > 0.7:  # Strong positive correlation
                # Apply length penalty
                length_ratio = response_length / baseline_length
                if length_ratio > self.length_penalty_threshold:
                    penalty = np.log(length_ratio / self.length_penalty_threshold)
                    return True, penalty

        return False, 0.0

    def check_repetition(self, response: str) -> Tuple[bool, float]:
        """Check for repetitive content."""
        words = response.lower().split()
        if len(words) < 10:
            return False, 0.0

        # N-gram repetition
        ngram_counts = {}
        n = 3

        for i in range(len(words) - n + 1):
            ngram = tuple(words[i:i+n])
            ngram_counts[ngram] = ngram_counts.get(ngram, 0) + 1

        if ngram_counts:
            repetition_rate = sum(c - 1 for c in ngram_counts.values()) / len(words)

            if repetition_rate > self.repetition_threshold:
                return True, repetition_rate

        return False, 0.0

    def check_reward_spike(self, reward: float) -> bool:
        """Check for unusual reward spikes."""
        if len(self.reward_history) < 50:
            return False

        mean = np.mean(self.reward_history[-50:])
        std = np.std(self.reward_history[-50:])

        if std > 0 and (reward - mean) / std > self.reward_spike_threshold:
            return True

        return False

    def adjusted_reward(
        self,
        response: str,
        raw_reward: float,
        apply_penalties: bool = True
    ) -> float:
        """Get adjusted reward with hacking mitigations."""
        if not apply_penalties:
            return raw_reward

        adjusted = raw_reward

        # Length penalty
        is_length_gaming, length_penalty = self.check_length_gaming(response, raw_reward)
        if is_length_gaming:
            adjusted -= length_penalty

        # Repetition penalty
        is_repetitive, rep_rate = self.check_repetition(response)
        if is_repetitive:
            adjusted -= rep_rate * 2.0

        # Spike detection (flag but don't adjust)
        if self.check_reward_spike(raw_reward):
            # Log for investigation
            pass

        return adjusted


class EnsembleRewardModel:
    """Ensemble of reward models to reduce hacking."""

    def __init__(self, models: List[RewardModel]):
        self.models = models

    def get_reward(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
        aggregation: str = "mean"  # or "min", "median"
    ) -> torch.Tensor:
        """Get aggregated reward from ensemble."""
        rewards = []

        for model in self.models:
            reward, _ = model(input_ids, attention_mask)
            rewards.append(reward)

        rewards = torch.stack(rewards, dim=-1)

        if aggregation == "mean":
            return rewards.mean(dim=-1)
        elif aggregation == "min":
            return rewards.min(dim=-1).values
        elif aggregation == "median":
            return rewards.median(dim=-1).values

        return rewards.mean(dim=-1)
```

## 4.1.4 PPO Implementation

### Core PPO Algorithm

```python
"""
PPO Implementation for RLHF
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
import numpy as np

@dataclass
class PPOConfig:
    """PPO training configuration."""
    learning_rate: float = 1e-6
    ppo_epochs: int = 4
    batch_size: int = 64
    mini_batch_size: int = 8
    clip_range: float = 0.2
    value_clip_range: float = 0.2
    kl_coef: float = 0.1
    entropy_coef: float = 0.01
    gamma: float = 1.0
    gae_lambda: float = 0.95
    max_grad_norm: float = 1.0
    target_kl: Optional[float] = 0.1  # Early stopping threshold
    normalize_advantages: bool = True
    whiten_rewards: bool = True


class PPOMemory:
    """Experience buffer for PPO."""

    def __init__(self):
        self.prompts: List[str] = []
        self.responses: List[str] = []
        self.old_log_probs: List[torch.Tensor] = []
        self.old_values: List[torch.Tensor] = []
        self.rewards: List[torch.Tensor] = []
        self.advantages: List[torch.Tensor] = []
        self.returns: List[torch.Tensor] = []

    def add(
        self,
        prompt: str,
        response: str,
        old_log_prob: torch.Tensor,
        old_value: torch.Tensor,
        reward: torch.Tensor
    ):
        """Add experience to buffer."""
        self.prompts.append(prompt)
        self.responses.append(response)
        self.old_log_probs.append(old_log_prob)
        self.old_values.append(old_value)
        self.rewards.append(reward)

    def compute_advantages(self, gamma: float, gae_lambda: float):
        """Compute GAE advantages."""
        # For language models, we typically treat each response as a single step
        # So advantages ≈ rewards - values
        for i, (reward, value) in enumerate(zip(self.rewards, self.old_values)):
            advantage = reward - value
            self.advantages.append(advantage)
            self.returns.append(reward)

    def clear(self):
        """Clear buffer."""
        self.__init__()


class ActorCriticLM(nn.Module):
    """
    Actor-Critic model for PPO on language models.

    Policy (actor) and value function (critic) share backbone.
    """

    def __init__(
        self,
        policy_model,
        value_head: Optional[nn.Module] = None
    ):
        super().__init__()
        self.policy = policy_model
        hidden_size = policy_model.config.hidden_size

        if value_head is None:
            self.value_head = nn.Sequential(
                nn.Linear(hidden_size, hidden_size // 2),
                nn.GELU(),
                nn.Linear(hidden_size // 2, 1)
            )
        else:
            self.value_head = value_head

    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Forward pass returning logits and values.

        Returns:
            logits: [batch, seq_len, vocab_size]
            values: [batch, seq_len]
        """
        outputs = self.policy(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True
        )

        logits = outputs.logits
        hidden = outputs.hidden_states[-1]

        values = self.value_head(hidden).squeeze(-1)

        return logits, values

    def get_log_probs(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
        response_mask: torch.Tensor
    ) -> torch.Tensor:
        """Get log probabilities for response tokens."""
        logits, _ = self.forward(input_ids, attention_mask)

        # Shift for next token prediction
        shift_logits = logits[..., :-1, :].contiguous()
        shift_labels = input_ids[..., 1:].contiguous()
        shift_mask = response_mask[..., 1:].contiguous()

        # Per-token log probs
        log_probs = F.log_softmax(shift_logits, dim=-1)
        token_log_probs = log_probs.gather(-1, shift_labels.unsqueeze(-1)).squeeze(-1)

        # Mask and sum for response
        masked_log_probs = token_log_probs * shift_mask
        response_log_prob = masked_log_probs.sum(dim=-1)

        return response_log_prob


class PPOTrainer:
    """PPO trainer for RLHF."""

    def __init__(
        self,
        policy_model: ActorCriticLM,
        ref_model: nn.Module,
        reward_model: RewardModel,
        tokenizer,
        config: PPOConfig
    ):
        self.policy = policy_model
        self.ref_model = ref_model
        self.reward_model = reward_model
        self.tokenizer = tokenizer
        self.config = config

        # Freeze reference model
        for p in self.ref_model.parameters():
            p.requires_grad = False

        # Optimizer
        self.optimizer = torch.optim.AdamW(
            self.policy.parameters(),
            lr=config.learning_rate
        )

        self.memory = PPOMemory()

    def generate_response(
        self,
        prompt: str,
        max_new_tokens: int = 256
    ) -> Tuple[str, torch.Tensor, torch.Tensor]:
        """Generate response and compute log prob and value."""
        self.policy.eval()

        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.policy.policy.device)

        with torch.no_grad():
            outputs = self.policy.policy.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                temperature=1.0,
                top_p=0.95,
                return_dict_in_generate=True,
                output_scores=True
            )

        response_ids = outputs.sequences[0, inputs.input_ids.size(1):]
        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)

        # Compute log prob and value
        full_ids = outputs.sequences
        attention_mask = torch.ones_like(full_ids)

        logits, values = self.policy(full_ids, attention_mask)

        # Response mask
        response_mask = torch.zeros_like(full_ids)
        response_mask[0, inputs.input_ids.size(1):] = 1

        log_prob = self.policy.get_log_probs(full_ids, attention_mask, response_mask)
        value = values[0, -1]  # Value at last token

        return response, log_prob, value

    def compute_reward(
        self,
        prompt: str,
        response: str
    ) -> torch.Tensor:
        """Compute reward for prompt-response pair."""
        device = next(self.reward_model.parameters()).device

        text = prompt + response
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            max_length=512,
            truncation=True
        ).to(device)

        with torch.no_grad():
            reward, _ = self.reward_model(
                inputs.input_ids,
                inputs.attention_mask
            )

        return reward.squeeze()

    def compute_kl_penalty(
        self,
        prompt: str,
        response: str,
        policy_log_prob: torch.Tensor
    ) -> torch.Tensor:
        """Compute KL penalty from reference model."""
        device = next(self.ref_model.parameters()).device

        text = prompt + response
        inputs = self.tokenizer(text, return_tensors="pt").to(device)

        # Response mask
        prompt_ids = self.tokenizer(prompt, return_tensors="pt").input_ids
        response_mask = torch.zeros_like(inputs.input_ids)
        response_mask[0, prompt_ids.size(1):] = 1

        with torch.no_grad():
            ref_outputs = self.ref_model(
                inputs.input_ids,
                inputs.attention_mask
            )
            ref_logits = ref_outputs.logits

            # Compute ref log prob
            shift_logits = ref_logits[..., :-1, :].contiguous()
            shift_labels = inputs.input_ids[..., 1:].contiguous()
            shift_mask = response_mask[..., 1:].contiguous()

            log_probs = F.log_softmax(shift_logits, dim=-1)
            token_log_probs = log_probs.gather(-1, shift_labels.unsqueeze(-1)).squeeze(-1)
            ref_log_prob = (token_log_probs * shift_mask).sum()

        kl = policy_log_prob - ref_log_prob
        return kl

    def ppo_step(self) -> Dict[str, float]:
        """Perform PPO update step."""
        self.policy.train()

        # Compute advantages
        self.memory.compute_advantages(self.config.gamma, self.config.gae_lambda)

        # Convert to tensors
        old_log_probs = torch.stack(self.memory.old_log_probs)
        old_values = torch.stack(self.memory.old_values)
        advantages = torch.stack(self.memory.advantages)
        returns = torch.stack(self.memory.returns)

        # Normalize advantages
        if self.config.normalize_advantages:
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        # PPO epochs
        metrics = {"loss": 0, "policy_loss": 0, "value_loss": 0, "kl": 0}
        n_updates = 0

        for _ in range(self.config.ppo_epochs):
            # Mini-batch updates
            indices = np.random.permutation(len(self.memory.prompts))

            for start in range(0, len(indices), self.config.mini_batch_size):
                end = start + self.config.mini_batch_size
                mb_indices = indices[start:end]

                mb_policy_loss = 0
                mb_value_loss = 0
                mb_kl = 0

                for idx in mb_indices:
                    prompt = self.memory.prompts[idx]
                    response = self.memory.responses[idx]
                    old_log_prob = old_log_probs[idx]
                    old_value = old_values[idx]
                    advantage = advantages[idx]
                    return_ = returns[idx]

                    # Get current log prob and value
                    text = prompt + response
                    inputs = self.tokenizer(text, return_tensors="pt").to(self.policy.policy.device)
                    prompt_ids = self.tokenizer(prompt, return_tensors="pt").input_ids
                    response_mask = torch.zeros_like(inputs.input_ids)
                    response_mask[0, prompt_ids.size(1):] = 1

                    logits, values = self.policy(inputs.input_ids, inputs.attention_mask)
                    new_log_prob = self.policy.get_log_probs(inputs.input_ids, inputs.attention_mask, response_mask)
                    new_value = values[0, -1]

                    # Policy loss (clipped objective)
                    ratio = torch.exp(new_log_prob - old_log_prob)
                    surr1 = ratio * advantage
                    surr2 = torch.clamp(ratio, 1 - self.config.clip_range, 1 + self.config.clip_range) * advantage
                    policy_loss = -torch.min(surr1, surr2)

                    # Value loss (clipped)
                    value_clipped = old_value + torch.clamp(
                        new_value - old_value,
                        -self.config.value_clip_range,
                        self.config.value_clip_range
                    )
                    value_loss1 = (new_value - return_) ** 2
                    value_loss2 = (value_clipped - return_) ** 2
                    value_loss = 0.5 * torch.max(value_loss1, value_loss2)

                    # KL divergence
                    kl = old_log_prob - new_log_prob

                    mb_policy_loss += policy_loss
                    mb_value_loss += value_loss
                    mb_kl += kl.item()

                # Update
                loss = mb_policy_loss + 0.5 * mb_value_loss
                loss.backward()

                torch.nn.utils.clip_grad_norm_(self.policy.parameters(), self.config.max_grad_norm)
                self.optimizer.step()
                self.optimizer.zero_grad()

                metrics["policy_loss"] += mb_policy_loss.item()
                metrics["value_loss"] += mb_value_loss.item()
                metrics["kl"] += mb_kl
                n_updates += 1

            # Early stopping on KL
            if self.config.target_kl and metrics["kl"] / n_updates > self.config.target_kl:
                break

        # Average metrics
        for k in metrics:
            metrics[k] /= max(n_updates, 1)

        metrics["loss"] = metrics["policy_loss"] + 0.5 * metrics["value_loss"]

        # Clear memory
        self.memory.clear()

        return metrics
```

### KL Penalty and Reference Model

```python
"""
KL Divergence Control for PPO
"""
import torch
from typing import Optional

class AdaptiveKLController:
    """
    Adaptive KL coefficient controller.

    Adjusts KL penalty coefficient to maintain target KL divergence.
    """

    def __init__(
        self,
        init_kl_coef: float = 0.1,
        target_kl: float = 0.1,
        horizon: int = 10000
    ):
        self.kl_coef = init_kl_coef
        self.target_kl = target_kl
        self.horizon = horizon

    def update(self, current_kl: float, n_steps: int) -> float:
        """Update KL coefficient based on current KL."""
        # Proportional control
        proportional_error = current_kl / self.target_kl - 1

        # Update coefficient
        mult = 1 + proportional_error * n_steps / self.horizon
        mult = max(0.1, min(10.0, mult))  # Clamp multiplier

        self.kl_coef *= mult
        self.kl_coef = max(0.001, min(10.0, self.kl_coef))  # Clamp coefficient

        return self.kl_coef


class ReferenceModelManager:
    """Manage reference model for KL computation."""

    def __init__(
        self,
        ref_model,
        tokenizer,
        update_frequency: Optional[int] = None  # Steps between updates
    ):
        self.ref_model = ref_model
        self.tokenizer = tokenizer
        self.update_frequency = update_frequency
        self.step_counter = 0

        # Freeze
        for p in self.ref_model.parameters():
            p.requires_grad = False

    def compute_kl(
        self,
        policy_model,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
        response_start_idx: int
    ) -> torch.Tensor:
        """Compute KL divergence between policy and reference."""
        device = input_ids.device

        # Get policy logits
        policy_outputs = policy_model(input_ids, attention_mask)
        policy_logits = policy_outputs.logits if hasattr(policy_outputs, 'logits') else policy_outputs[0]

        # Get reference logits
        with torch.no_grad():
            ref_outputs = self.ref_model(input_ids, attention_mask)
            ref_logits = ref_outputs.logits if hasattr(ref_outputs, 'logits') else ref_outputs[0]

        # Compute KL for response tokens only
        response_logits = policy_logits[:, response_start_idx:-1, :]
        response_ref_logits = ref_logits[:, response_start_idx:-1, :]

        policy_probs = torch.softmax(response_logits, dim=-1)
        ref_probs = torch.softmax(response_ref_logits, dim=-1)

        kl = torch.sum(policy_probs * (torch.log(policy_probs + 1e-10) - torch.log(ref_probs + 1e-10)), dim=-1)

        return kl.mean()

    def maybe_update(self, policy_model):
        """Update reference model from policy (if using moving reference)."""
        if self.update_frequency is None:
            return

        self.step_counter += 1

        if self.step_counter % self.update_frequency == 0:
            # Soft update
            for ref_param, policy_param in zip(
                self.ref_model.parameters(),
                policy_model.parameters()
            ):
                ref_param.data.copy_(0.99 * ref_param.data + 0.01 * policy_param.data)
```

## 4.1.5 Alternative Algorithms

### DPO (Direct Preference Optimization)

```python
"""
Direct Preference Optimization (DPO)

Simpler alternative to PPO that directly optimizes preferences without reward model.
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Optional
from dataclasses import dataclass

@dataclass
class DPOConfig:
    """DPO training configuration."""
    beta: float = 0.1  # Temperature parameter
    learning_rate: float = 5e-7
    batch_size: int = 4
    gradient_accumulation_steps: int = 4
    max_length: int = 512
    label_smoothing: float = 0.0


class DPOLoss(nn.Module):
    """
    DPO loss function.

    L_DPO = -log(sigmoid(beta * (log_pi(y_w|x) - log_pi(y_l|x) - log_ref(y_w|x) + log_ref(y_l|x))))
    """

    def __init__(self, beta: float = 0.1, label_smoothing: float = 0.0):
        super().__init__()
        self.beta = beta
        self.label_smoothing = label_smoothing

    def forward(
        self,
        policy_chosen_logps: torch.Tensor,
        policy_rejected_logps: torch.Tensor,
        ref_chosen_logps: torch.Tensor,
        ref_rejected_logps: torch.Tensor
    ) -> torch.Tensor:
        """
        Compute DPO loss.

        Args:
            policy_chosen_logps: Log probs of chosen under policy
            policy_rejected_logps: Log probs of rejected under policy
            ref_chosen_logps: Log probs of chosen under reference
            ref_rejected_logps: Log probs of rejected under reference

        Returns:
            DPO loss
        """
        # Compute log ratios
        policy_logratios = policy_chosen_logps - policy_rejected_logps
        ref_logratios = ref_chosen_logps - ref_rejected_logps

        logits = self.beta * (policy_logratios - ref_logratios)

        # Label smoothing
        if self.label_smoothing > 0:
            losses = (
                -F.logsigmoid(logits) * (1 - self.label_smoothing)
                - F.logsigmoid(-logits) * self.label_smoothing
            )
        else:
            losses = -F.logsigmoid(logits)

        return losses.mean()


class DPOTrainer:
    """Trainer for DPO."""

    def __init__(
        self,
        model,
        ref_model,
        tokenizer,
        config: DPOConfig
    ):
        self.model = model
        self.ref_model = ref_model
        self.tokenizer = tokenizer
        self.config = config

        self.loss_fn = DPOLoss(beta=config.beta, label_smoothing=config.label_smoothing)

        self.optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=config.learning_rate
        )

        # Freeze reference
        for p in self.ref_model.parameters():
            p.requires_grad = False

    def compute_logps(
        self,
        model,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
        labels: torch.Tensor
    ) -> torch.Tensor:
        """Compute log probabilities for a sequence."""
        outputs = model(input_ids, attention_mask=attention_mask)
        logits = outputs.logits

        # Shift for next token prediction
        shift_logits = logits[..., :-1, :].contiguous()
        shift_labels = labels[..., 1:].contiguous()

        # Per-token log probs
        log_probs = F.log_softmax(shift_logits, dim=-1)
        token_log_probs = log_probs.gather(-1, shift_labels.unsqueeze(-1)).squeeze(-1)

        # Mask padding
        label_mask = (shift_labels != -100).float()
        sequence_log_probs = (token_log_probs * label_mask).sum(dim=-1)

        return sequence_log_probs

    def train_step(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:
        """Single training step."""
        self.model.train()
        device = next(self.model.parameters()).device

        # Move batch to device
        batch = {k: v.to(device) for k, v in batch.items()}

        # Compute policy log probs
        policy_chosen_logps = self.compute_logps(
            self.model,
            batch["chosen_input_ids"],
            batch["chosen_attention_mask"],
            batch["chosen_labels"]
        )

        policy_rejected_logps = self.compute_logps(
            self.model,
            batch["rejected_input_ids"],
            batch["rejected_attention_mask"],
            batch["rejected_labels"]
        )

        # Compute reference log probs
        with torch.no_grad():
            ref_chosen_logps = self.compute_logps(
                self.ref_model,
                batch["chosen_input_ids"],
                batch["chosen_attention_mask"],
                batch["chosen_labels"]
            )

            ref_rejected_logps = self.compute_logps(
                self.ref_model,
                batch["rejected_input_ids"],
                batch["rejected_attention_mask"],
                batch["rejected_labels"]
            )

        # Compute loss
        loss = self.loss_fn(
            policy_chosen_logps,
            policy_rejected_logps,
            ref_chosen_logps,
            ref_rejected_logps
        )

        # Backward
        loss.backward()

        # Metrics
        with torch.no_grad():
            chosen_rewards = self.config.beta * (policy_chosen_logps - ref_chosen_logps)
            rejected_rewards = self.config.beta * (policy_rejected_logps - ref_rejected_logps)
            accuracy = (chosen_rewards > rejected_rewards).float().mean()

        return {
            "loss": loss.item(),
            "accuracy": accuracy.item(),
            "chosen_reward": chosen_rewards.mean().item(),
            "rejected_reward": rejected_rewards.mean().item(),
            "reward_margin": (chosen_rewards - rejected_rewards).mean().item()
        }
```

### Other Alignment Algorithms

```python
"""
Additional Alignment Algorithms
"""
import torch
import torch.nn.functional as F
from typing import Dict

class IPOLoss:
    """
    Identity Preference Optimization (IPO).

    More theoretically grounded than DPO.
    """

    def __init__(self, beta: float = 0.1):
        self.beta = beta

    def __call__(
        self,
        policy_chosen_logps: torch.Tensor,
        policy_rejected_logps: torch.Tensor,
        ref_chosen_logps: torch.Tensor,
        ref_rejected_logps: torch.Tensor
    ) -> torch.Tensor:
        """IPO loss with squared hinge."""
        policy_logratios = policy_chosen_logps - policy_rejected_logps
        ref_logratios = ref_chosen_logps - ref_rejected_logps

        logits = policy_logratios - ref_logratios

        # Squared hinge loss
        losses = (logits - 1 / (2 * self.beta)) ** 2

        return losses.mean()


class KTOLoss:
    """
    Kahneman-Tversky Optimization (KTO).

    Works with binary feedback (good/bad) rather than pairwise comparisons.
    """

    def __init__(
        self,
        beta: float = 0.1,
        desirable_weight: float = 1.0,
        undesirable_weight: float = 1.0
    ):
        self.beta = beta
        self.desirable_weight = desirable_weight
        self.undesirable_weight = undesirable_weight

    def __call__(
        self,
        policy_logps: torch.Tensor,
        ref_logps: torch.Tensor,
        is_desirable: torch.Tensor  # Boolean tensor
    ) -> torch.Tensor:
        """
        KTO loss for binary feedback.

        Args:
            policy_logps: Policy log probabilities
            ref_logps: Reference log probabilities
            is_desirable: Whether each sample is desirable (good)
        """
        kl = policy_logps - ref_logps

        # Separate desirable and undesirable
        desirable_mask = is_desirable.float()
        undesirable_mask = (1 - is_desirable).float()

        # KTO loss components
        desirable_loss = 1 - torch.sigmoid(self.beta * kl)
        undesirable_loss = 1 - torch.sigmoid(-self.beta * kl)

        loss = (
            self.desirable_weight * desirable_loss * desirable_mask
            + self.undesirable_weight * undesirable_loss * undesirable_mask
        )

        return loss.mean()


class ORPOLoss:
    """
    Odds Ratio Preference Optimization (ORPO).

    Combines SFT and preference optimization in single objective.
    """

    def __init__(self, lambda_: float = 0.1):
        self.lambda_ = lambda_

    def __call__(
        self,
        policy_chosen_logps: torch.Tensor,
        policy_rejected_logps: torch.Tensor,
        chosen_nll_loss: torch.Tensor
    ) -> torch.Tensor:
        """
        ORPO combines NLL loss with odds ratio loss.

        Args:
            policy_chosen_logps: Log probs of chosen responses
            policy_rejected_logps: Log probs of rejected responses
            chosen_nll_loss: Standard NLL loss on chosen responses
        """
        # Odds ratio
        log_odds = (policy_chosen_logps - policy_rejected_logps) - (
            torch.log(1 - torch.exp(policy_chosen_logps) + 1e-10)
            - torch.log(1 - torch.exp(policy_rejected_logps) + 1e-10)
        )

        # ORPO loss
        odds_ratio_loss = -F.logsigmoid(log_odds).mean()

        return chosen_nll_loss + self.lambda_ * odds_ratio_loss


class GRPOTrainer:
    """
    Group Relative Policy Optimization (GRPO) - DeepSeek approach.

    Uses group-based advantage estimation without value function.
    """

    def __init__(
        self,
        model,
        ref_model,
        reward_model,
        tokenizer,
        group_size: int = 8,
        clip_range: float = 0.2,
        kl_coef: float = 0.1
    ):
        self.model = model
        self.ref_model = ref_model
        self.reward_model = reward_model
        self.tokenizer = tokenizer
        self.group_size = group_size
        self.clip_range = clip_range
        self.kl_coef = kl_coef

    def generate_group(self, prompt: str) -> Dict[str, torch.Tensor]:
        """Generate group of responses for a prompt."""
        responses = []
        log_probs = []

        for _ in range(self.group_size):
            # Generate response
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)

            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=256,
                    do_sample=True,
                    temperature=1.0,
                    return_dict_in_generate=True,
                    output_scores=True
                )

            response_ids = outputs.sequences[0, inputs.input_ids.size(1):]
            response = self.tokenizer.decode(response_ids, skip_special_tokens=True)
            responses.append(response)

        # Get rewards
        rewards = []
        for response in responses:
            text = prompt + response
            inputs = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
            inputs = {k: v.to(self.reward_model.device) for k, v in inputs.items()}

            with torch.no_grad():
                reward, _ = self.reward_model(**inputs)
            rewards.append(reward.item())

        rewards = torch.tensor(rewards)

        # Group-relative advantage (no value function needed)
        advantages = rewards - rewards.mean()
        advantages = advantages / (advantages.std() + 1e-8)

        return {
            "responses": responses,
            "rewards": rewards,
            "advantages": advantages
        }
```

## 4.1.6 Infrastructure for RLHF

### Multi-Model Serving

```python
"""
Infrastructure for Multi-Model RLHF Training
"""
import torch
from typing import Dict, Optional
from dataclasses import dataclass
import ray
from ray import serve

@dataclass
class RLHFInfraConfig:
    """Infrastructure configuration."""
    num_policy_replicas: int = 4
    num_reward_replicas: int = 2
    num_ref_replicas: int = 2
    policy_gpu_memory_fraction: float = 0.4
    reward_gpu_memory_fraction: float = 0.2
    ref_gpu_memory_fraction: float = 0.2


class ModelServer:
    """Serve models for RLHF training."""

    def __init__(self, model_path: str, model_type: str, device: str = "cuda"):
        self.model_type = model_type
        self.device = device

        if model_type == "policy":
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.bfloat16
            ).to(device)
        elif model_type == "reward":
            self.model = RewardModel(model_path).to(device)
        elif model_type == "reference":
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.bfloat16
            ).to(device)
            for p in self.model.parameters():
                p.requires_grad = False

    def forward(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """Forward pass."""
        batch = {k: v.to(self.device) for k, v in batch.items()}

        with torch.no_grad() if self.model_type in ["reference", "reward"] else torch.enable_grad():
            if self.model_type == "reward":
                rewards, _ = self.model(batch["input_ids"], batch["attention_mask"])
                return {"rewards": rewards}
            else:
                outputs = self.model(**batch)
                return {"logits": outputs.logits}


@serve.deployment(num_replicas=2, ray_actor_options={"num_gpus": 1})
class RewardModelService:
    """Ray Serve deployment for reward model."""

    def __init__(self, model_path: str):
        self.server = ModelServer(model_path, "reward")

    def __call__(self, request):
        return self.server.forward(request)


class DistributedRLHFTrainer:
    """Distributed RLHF trainer using Ray."""

    def __init__(
        self,
        policy_model_path: str,
        reward_model_path: str,
        ref_model_path: str,
        config: RLHFInfraConfig
    ):
        self.config = config

        # Initialize Ray
        if not ray.is_initialized():
            ray.init()

        # Deploy models
        self.policy_actors = [
            ray.remote(num_gpus=1)(ModelServer).remote(policy_model_path, "policy")
            for _ in range(config.num_policy_replicas)
        ]

        self.reward_actors = [
            ray.remote(num_gpus=0.5)(ModelServer).remote(reward_model_path, "reward")
            for _ in range(config.num_reward_replicas)
        ]

        self.ref_actors = [
            ray.remote(num_gpus=0.5)(ModelServer).remote(ref_model_path, "reference")
            for _ in range(config.num_ref_replicas)
        ]

    def generate_rollouts(self, prompts: list, batch_size: int = 16):
        """Generate rollouts in parallel."""
        futures = []

        for i, prompt in enumerate(prompts):
            actor_idx = i % len(self.policy_actors)
            future = self.policy_actors[actor_idx].generate.remote(prompt)
            futures.append(future)

        return ray.get(futures)

    def compute_rewards(self, prompt_responses: list):
        """Compute rewards in parallel."""
        futures = []

        for i, (prompt, response) in enumerate(prompt_responses):
            actor_idx = i % len(self.reward_actors)
            future = self.reward_actors[actor_idx].forward.remote({
                "prompt": prompt,
                "response": response
            })
            futures.append(future)

        return ray.get(futures)
```

### Memory Optimization

```python
"""
Memory Optimization Strategies for RLHF
"""
import torch
from typing import Optional

class RLHFMemoryOptimizer:
    """Memory optimization utilities for RLHF."""

    @staticmethod
    def estimate_memory_requirements(
        model_params_b: float,
        batch_size: int,
        seq_length: int,
        num_models: int = 4  # policy, ref, reward, value
    ) -> Dict[str, float]:
        """Estimate GPU memory requirements."""
        # Model weights (bf16)
        weights_per_model_gb = model_params_b * 2  # 2 bytes per param

        # Optimizer states (for policy only)
        optimizer_gb = model_params_b * 8  # Adam: 8 bytes per param

        # Activations (rough estimate)
        hidden_size = int(model_params_b * 1000)  # Rough scaling
        activations_gb = batch_size * seq_length * hidden_size * 2 / 1e9

        # KV cache for generation
        num_layers = int(model_params_b * 4)  # Rough scaling
        kv_cache_gb = batch_size * seq_length * hidden_size * 2 * num_layers * 2 / 1e9

        return {
            "policy_model_gb": weights_per_model_gb,
            "reference_model_gb": weights_per_model_gb,
            "reward_model_gb": weights_per_model_gb * 0.5,  # Often smaller
            "value_head_gb": 0.1,
            "optimizer_gb": optimizer_gb,
            "activations_gb": activations_gb,
            "kv_cache_gb": kv_cache_gb,
            "total_gb": (
                weights_per_model_gb * 3.5 +  # models
                optimizer_gb +
                activations_gb * 2 +
                kv_cache_gb
            )
        }

    @staticmethod
    def get_recommended_config(
        available_gpu_memory_gb: float,
        model_params_b: float,
        num_gpus: int
    ) -> Dict:
        """Get recommended configuration based on available memory."""
        total_memory = available_gpu_memory_gb * num_gpus

        requirements = RLHFMemoryOptimizer.estimate_memory_requirements(
            model_params_b, 1, 2048
        )

        if requirements["total_gb"] < total_memory * 0.8:
            # Can fit everything
            return {
                "strategy": "full_models",
                "batch_size": int((total_memory - requirements["total_gb"]) / 2),
                "gradient_checkpointing": False,
                "cpu_offload": False
            }
        elif requirements["total_gb"] < total_memory * 1.2:
            # Tight fit - use optimizations
            return {
                "strategy": "optimized",
                "batch_size": 4,
                "gradient_checkpointing": True,
                "cpu_offload": False,
                "share_ref_reward_gpu": True
            }
        else:
            # Need aggressive optimization
            return {
                "strategy": "memory_constrained",
                "batch_size": 1,
                "gradient_checkpointing": True,
                "cpu_offload": True,
                "lora_policy": True,
                "share_models": True
            }


def setup_gradient_checkpointing(model):
    """Enable gradient checkpointing for memory efficiency."""
    model.gradient_checkpointing_enable()

    # For specific architectures
    if hasattr(model, "model"):
        for layer in model.model.layers:
            layer.gradient_checkpointing = True

    return model


def offload_model_to_cpu(model, exclude_layers: Optional[list] = None):
    """Offload model to CPU, keeping specified layers on GPU."""
    for name, param in model.named_parameters():
        if exclude_layers and any(exc in name for exc in exclude_layers):
            continue
        param.data = param.data.cpu()

    return model
```

## 4.1.7 Evaluation & Iteration

### Win Rate Evaluation

```python
"""
Evaluation Metrics for RLHF
"""
from typing import Dict, List, Tuple
import numpy as np
from collections import defaultdict

class WinRateEvaluator:
    """Evaluate model using win rate comparisons."""

    def __init__(
        self,
        model_a,
        model_b,
        judge_model,  # Can be reward model or LLM judge
        tokenizer
    ):
        self.model_a = model_a
        self.model_b = model_b
        self.judge = judge_model
        self.tokenizer = tokenizer

    def compare_responses(
        self,
        prompt: str,
        response_a: str,
        response_b: str
    ) -> str:
        """Judge which response is better."""
        if hasattr(self.judge, 'reward_head'):
            # Reward model judge
            score_a = self._get_reward_score(prompt, response_a)
            score_b = self._get_reward_score(prompt, response_b)

            if score_a > score_b + 0.1:
                return "A"
            elif score_b > score_a + 0.1:
                return "B"
            return "tie"
        else:
            # LLM judge
            return self._llm_judge(prompt, response_a, response_b)

    def _get_reward_score(self, prompt: str, response: str) -> float:
        """Get reward model score."""
        text = prompt + response
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
        inputs = {k: v.to(next(self.judge.parameters()).device) for k, v in inputs.items()}

        with torch.no_grad():
            reward, _ = self.judge(**inputs)

        return reward.item()

    def _llm_judge(self, prompt: str, response_a: str, response_b: str) -> str:
        """Use LLM as judge."""
        judge_prompt = f"""Compare these two responses to the prompt and determine which is better.

Prompt: {prompt}

Response A: {response_a}

Response B: {response_b}

Which response is better? Answer with just 'A', 'B', or 'tie'."""

        inputs = self.tokenizer(judge_prompt, return_tensors="pt")
        inputs = {k: v.to(next(self.judge.parameters()).device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = self.judge.generate(**inputs, max_new_tokens=10)

        result = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        if "A" in result.upper() and "B" not in result.upper():
            return "A"
        elif "B" in result.upper() and "A" not in result.upper():
            return "B"
        return "tie"

    def evaluate_win_rate(
        self,
        prompts: List[str],
        num_samples_per_prompt: int = 1
    ) -> Dict[str, float]:
        """Evaluate win rate across prompts."""
        results = {"A_wins": 0, "B_wins": 0, "ties": 0, "total": 0}

        for prompt in prompts:
            for _ in range(num_samples_per_prompt):
                # Generate responses
                response_a = self._generate(self.model_a, prompt)
                response_b = self._generate(self.model_b, prompt)

                # Compare
                winner = self.compare_responses(prompt, response_a, response_b)

                if winner == "A":
                    results["A_wins"] += 1
                elif winner == "B":
                    results["B_wins"] += 1
                else:
                    results["ties"] += 1

                results["total"] += 1

        # Calculate rates
        total = results["total"]
        return {
            "win_rate_A": results["A_wins"] / total,
            "win_rate_B": results["B_wins"] / total,
            "tie_rate": results["ties"] / total,
            "A_wins": results["A_wins"],
            "B_wins": results["B_wins"],
            "ties": results["ties"]
        }

    def _generate(self, model, prompt: str) -> str:
        """Generate response from model."""
        inputs = self.tokenizer(prompt, return_tensors="pt")
        inputs = {k: v.to(next(model.parameters()).device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                do_sample=True,
                temperature=0.7
            )

        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return response[len(prompt):]


def compute_elo_ratings(
    match_results: List[Tuple[str, str, str]],  # (model_a, model_b, winner)
    initial_rating: float = 1000,
    k_factor: float = 32
) -> Dict[str, float]:
    """Compute Elo ratings from match results."""
    ratings = defaultdict(lambda: initial_rating)

    for model_a, model_b, winner in match_results:
        ra, rb = ratings[model_a], ratings[model_b]

        # Expected scores
        ea = 1 / (1 + 10 ** ((rb - ra) / 400))
        eb = 1 / (1 + 10 ** ((ra - rb) / 400))

        # Actual scores
        if winner == model_a:
            sa, sb = 1, 0
        elif winner == model_b:
            sa, sb = 0, 1
        else:  # tie
            sa, sb = 0.5, 0.5

        # Update ratings
        ratings[model_a] = ra + k_factor * (sa - ea)
        ratings[model_b] = rb + k_factor * (sb - eb)

    return dict(ratings)
```

### Human Evaluation Protocols

```python
"""
Human Evaluation Protocols for RLHF
"""
from dataclasses import dataclass
from typing import List, Optional, Dict
import json
from datetime import datetime

@dataclass
class HumanEvalTask:
    """Single human evaluation task."""
    task_id: str
    prompt: str
    responses: List[str]
    model_ids: List[str]  # Which model generated each response
    evaluation_type: str  # "ranking", "rating", "pairwise"
    criteria: List[str]  # What to evaluate
    annotator_id: Optional[str] = None
    result: Optional[Dict] = None
    timestamp: Optional[str] = None


class HumanEvalProtocol:
    """Manage human evaluation for RLHF."""

    STANDARD_CRITERIA = [
        "helpfulness",
        "accuracy",
        "harmlessness",
        "clarity",
        "relevance"
    ]

    def __init__(
        self,
        evaluation_type: str = "pairwise",
        criteria: List[str] = None,
        blind_evaluation: bool = True
    ):
        self.evaluation_type = evaluation_type
        self.criteria = criteria or self.STANDARD_CRITERIA
        self.blind_evaluation = blind_evaluation
        self.tasks: List[HumanEvalTask] = []

    def create_task(
        self,
        prompt: str,
        responses: List[str],
        model_ids: List[str]
    ) -> HumanEvalTask:
        """Create evaluation task."""
        import uuid

        task = HumanEvalTask(
            task_id=str(uuid.uuid4())[:8],
            prompt=prompt,
            responses=responses,
            model_ids=model_ids if not self.blind_evaluation else ["model_" + str(i) for i in range(len(responses))],
            evaluation_type=self.evaluation_type,
            criteria=self.criteria
        )

        self.tasks.append(task)
        return task

    def record_result(
        self,
        task_id: str,
        annotator_id: str,
        result: Dict
    ):
        """Record evaluation result."""
        for task in self.tasks:
            if task.task_id == task_id:
                task.annotator_id = annotator_id
                task.result = result
                task.timestamp = datetime.now().isoformat()
                break

    def generate_batch(
        self,
        model_a,
        model_b,
        prompts: List[str],
        tokenizer,
        num_per_prompt: int = 1
    ) -> List[HumanEvalTask]:
        """Generate evaluation batch."""
        tasks = []

        for prompt in prompts:
            for _ in range(num_per_prompt):
                # Generate responses
                response_a = self._generate(model_a, prompt, tokenizer)
                response_b = self._generate(model_b, prompt, tokenizer)

                # Randomize order for blind evaluation
                import random
                if random.random() < 0.5:
                    responses = [response_a, response_b]
                    model_ids = ["A", "B"]
                else:
                    responses = [response_b, response_a]
                    model_ids = ["B", "A"]

                task = self.create_task(prompt, responses, model_ids)
                tasks.append(task)

        return tasks

    def _generate(self, model, prompt: str, tokenizer) -> str:
        """Generate response."""
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                do_sample=True,
                temperature=0.7
            )

        return tokenizer.decode(outputs[0], skip_special_tokens=True)[len(prompt):]

    def export_for_labeling(self, output_path: str):
        """Export tasks for external labeling platform."""
        export_data = []

        for task in self.tasks:
            export_data.append({
                "task_id": task.task_id,
                "prompt": task.prompt,
                "responses": task.responses,
                "evaluation_type": task.evaluation_type,
                "criteria": task.criteria
            })

        with open(output_path, 'w') as f:
            json.dump(export_data, f, indent=2)

    def analyze_results(self) -> Dict:
        """Analyze evaluation results."""
        completed = [t for t in self.tasks if t.result is not None]

        if not completed:
            return {"error": "No completed evaluations"}

        analysis = {
            "total_tasks": len(self.tasks),
            "completed": len(completed),
            "by_criteria": {}
        }

        for criterion in self.criteria:
            scores = []
            for task in completed:
                if criterion in task.result:
                    scores.append(task.result[criterion])

            if scores:
                analysis["by_criteria"][criterion] = {
                    "mean": np.mean(scores),
                    "std": np.std(scores),
                    "n": len(scores)
                }

        return analysis
```

## Appendix A: RLHF Training Scripts (TRL Library)

```python
#!/usr/bin/env python3
"""
Complete RLHF Training with TRL Library
"""
from dataclasses import dataclass, field
from typing import Optional
import torch
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
from trl import (
    PPOConfig,
    PPOTrainer,
    AutoModelForCausalLMWithValueHead,
    create_reference_model
)
from peft import LoraConfig, get_peft_model

@dataclass
class RLHFScriptConfig:
    model_name: str = "meta-llama/Llama-2-7b-hf"
    reward_model_name: str = "./reward_model"
    dataset_name: str = "Anthropic/hh-rlhf"
    output_dir: str = "./rlhf_output"

    # PPO settings
    learning_rate: float = 1e-6
    batch_size: int = 64
    mini_batch_size: int = 8
    ppo_epochs: int = 4
    kl_penalty: str = "kl"
    init_kl_coef: float = 0.1

    # LoRA settings
    use_lora: bool = True
    lora_r: int = 16
    lora_alpha: int = 32

    # Generation settings
    max_new_tokens: int = 256


def main():
    config = RLHFScriptConfig()

    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token

    # Load and prepare model
    model = AutoModelForCausalLM.from_pretrained(
        config.model_name,
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )

    if config.use_lora:
        lora_config = LoraConfig(
            r=config.lora_r,
            lora_alpha=config.lora_alpha,
            target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
            task_type="CAUSAL_LM"
        )
        model = get_peft_model(model, lora_config)

    # Add value head
    model = AutoModelForCausalLMWithValueHead.from_pretrained(model)

    # Create reference model
    ref_model = create_reference_model(model)

    # Load reward model
    reward_model = AutoModelForSequenceClassification.from_pretrained(
        config.reward_model_name,
        torch_dtype=torch.bfloat16
    )

    # Load dataset
    dataset = load_dataset(config.dataset_name, split="train")

    # PPO config
    ppo_config = PPOConfig(
        model_name=config.model_name,
        learning_rate=config.learning_rate,
        batch_size=config.batch_size,
        mini_batch_size=config.mini_batch_size,
        ppo_epochs=config.ppo_epochs,
        kl_penalty=config.kl_penalty,
        init_kl_coef=config.init_kl_coef,
        log_with="wandb"
    )

    # Initialize trainer
    ppo_trainer = PPOTrainer(
        config=ppo_config,
        model=model,
        ref_model=ref_model,
        tokenizer=tokenizer,
        dataset=dataset
    )

    # Training loop
    for batch in ppo_trainer.dataloader:
        # Generate responses
        query_tensors = batch["input_ids"]
        response_tensors = ppo_trainer.generate(
            query_tensors,
            max_new_tokens=config.max_new_tokens,
            do_sample=True,
            temperature=1.0
        )

        # Compute rewards
        rewards = []
        for query, response in zip(query_tensors, response_tensors):
            full_text = tokenizer.decode(torch.cat([query, response]))
            inputs = tokenizer(full_text, return_tensors="pt").to(reward_model.device)
            with torch.no_grad():
                reward = reward_model(**inputs).logits[0]
            rewards.append(reward)

        # PPO step
        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)

        # Log
        ppo_trainer.log_stats(stats, batch, rewards)

    # Save model
    ppo_trainer.save_model(config.output_dir)


if __name__ == "__main__":
    main()
```

## Appendix B: Reward Model Training Configuration

```yaml
# reward_model_config.yaml

model:
  base_model: "meta-llama/Llama-2-7b-hf"
  num_labels: 1
  torch_dtype: "bfloat16"
  use_flash_attention: true

data:
  dataset: "Anthropic/hh-rlhf"
  max_length: 512
  preprocessing_num_workers: 8

training:
  output_dir: "./reward_model"
  learning_rate: 1e-5
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 4
  num_train_epochs: 1
  warmup_ratio: 0.1
  weight_decay: 0.01
  logging_steps: 10
  eval_steps: 100
  save_steps: 500
  bf16: true
  gradient_checkpointing: true

loss:
  type: "bradley_terry"  # or "margin", "plackett_luce"
  margin: 0.0
  label_smoothing: 0.0

evaluation:
  eval_strategy: "steps"
  metric_for_best_model: "accuracy"
```

## Appendix C: PPO Hyperparameter Guidelines

| Parameter | Small Model (<7B) | Medium Model (7-30B) | Large Model (>30B) |
|-----------|------------------|---------------------|-------------------|
| Learning Rate | 1e-5 - 5e-6 | 5e-6 - 1e-6 | 1e-6 - 5e-7 |
| Batch Size | 128-256 | 64-128 | 32-64 |
| Mini Batch Size | 16-32 | 8-16 | 4-8 |
| PPO Epochs | 4-8 | 2-4 | 1-2 |
| KL Coefficient | 0.05-0.2 | 0.1-0.3 | 0.2-0.5 |
| Clip Range | 0.2 | 0.2 | 0.1-0.2 |
| GAE Lambda | 0.95 | 0.95 | 0.95 |
| Value Loss Coef | 0.5 | 0.5 | 0.5 |
| Entropy Coef | 0.01 | 0.01 | 0.0 |

## Troubleshooting

### Common Issues

| Issue | Cause | Solution |
|-------|-------|----------|
| Reward hacking | Model exploits reward model weaknesses | Add reward penalties, use ensemble |
| KL divergence explosion | Learning rate too high | Reduce LR, increase KL penalty |
| Training instability | Batch size too small | Increase batch size, reduce LR |
| Mode collapse | Over-optimization | Increase KL penalty, reduce training |
| Reward model overfit | Too few preference samples | Collect more data, add regularization |

### Debugging Commands

```bash
# Monitor training
watch -n 5 'tail -20 logs/ppo_training.log'

# Check reward distribution
python -c "
import torch
rewards = torch.load('rewards.pt')
print(f'Mean: {rewards.mean():.3f}')
print(f'Std: {rewards.std():.3f}')
print(f'Min: {rewards.min():.3f}')
print(f'Max: {rewards.max():.3f}')
"

# Evaluate KL divergence
python evaluate_kl.py --policy ./policy_model --ref ./ref_model
```

## Glossary

- **RLHF**: Reinforcement Learning from Human Feedback
- **PPO**: Proximal Policy Optimization
- **DPO**: Direct Preference Optimization
- **KL Divergence**: Kullback-Leibler divergence measuring policy drift
- **Reward Hacking**: Model exploiting weaknesses in reward model
- **Bradley-Terry**: Probabilistic model for pairwise comparisons
- **GAE**: Generalized Advantage Estimation
- **Win Rate**: Percentage of comparisons where model is preferred

## References

1. "Training language models to follow instructions with human feedback" (Ouyang et al., 2022)
2. "Direct Preference Optimization" (Rafailov et al., 2023)
3. "Constitutional AI" (Bai et al., 2022)
4. "Proximal Policy Optimization Algorithms" (Schulman et al., 2017)
5. "Secrets of RLHF in Large Language Models" (Zheng et al., 2023)
6. TRL Library Documentation (HuggingFace)
