> **Navigation** | [← 4.1 RLHF](4.1_rlhf_guide.md) | [4.3 Safety Evaluation →](4.3_safety_evaluation_red_teaming.md)
>
> | | |
> |---|---|
> | **Prerequisites** | [4.1 RLHF](4.1_rlhf_guide.md) &#124; [3.1 SFT](../03_fine_tuning/3.1_supervised_fine_tuning.md) |
> | **Related** | [4.3 Safety Evaluation](4.3_safety_evaluation_red_teaming.md) &#124; [4.4 Bias & Fairness](4.4_bias_fairness_evaluation.md) |
> | **Next** | [4.3 Safety Evaluation & Red Teaming](4.3_safety_evaluation_red_teaming.md) |

# 4.2 Constitutional AI & RLAIF Guide

## Executive Summary

Constitutional AI (CAI) and Reinforcement Learning from AI Feedback (RLAIF) are techniques for aligning language models using AI-generated feedback rather than human annotations. CAI uses a set of constitutional principles to guide model behavior through self-critique and revision, while RLAIF uses AI models to generate preference labels. These approaches enable scalable alignment while reducing reliance on expensive human annotation.

## Prerequisites

- Understanding of RLHF fundamentals (document 4.1)
- Supervised fine-tuned base model (document 3.1)
- Access to capable AI models for feedback generation
- Familiarity with prompt engineering

## 4.2.1 Constitutional AI Overview

### Principles-Based Alignment

Constitutional AI operates on the principle that AI systems can be guided by explicit rules and values. Rather than learning preferences implicitly from human feedback, CAI makes the desired behavior explicit through a "constitution" - a set of principles that define how the AI should behave.

### CAI Pipeline

```
┌──────────────────┐
│  User Prompt     │
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│ Initial Response │ ◄── May contain problematic content
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│ Self-Critique    │ ◄── "Identify issues based on principles"
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│ Revision         │ ◄── "Rewrite following the critique"
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│ Iterate if needed│
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│ Final Response   │
└──────────────────┘
```

### Implementation Framework

```python
from dataclasses import dataclass, field
from typing import List, Dict, Optional
from enum import Enum

class PrincipleCategory(Enum):
    HELPFULNESS = "helpfulness"
    HARMLESSNESS = "harmlessness"
    HONESTY = "honesty"
    CUSTOM = "custom"


@dataclass
class ConstitutionalPrinciple:
    """A single principle in the constitution."""
    name: str
    description: str
    category: PrincipleCategory
    critique_prompt: str
    revision_prompt: str
    weight: float = 1.0
    examples: List[Dict] = field(default_factory=list)


@dataclass
class Constitution:
    """Complete constitution for AI alignment."""
    name: str
    principles: List[ConstitutionalPrinciple]
    version: str = "1.0"

    def get_principles_by_category(self, category: PrincipleCategory) -> List[ConstitutionalPrinciple]:
        return [p for p in self.principles if p.category == category]

    def sample_principle(self, weights: Optional[List[float]] = None) -> ConstitutionalPrinciple:
        import random
        if weights is None:
            weights = [p.weight for p in self.principles]
        return random.choices(self.principles, weights=weights, k=1)[0]
```

### Self-Critique and Revision

```python
class ConstitutionalAI:
    """Constitutional AI with self-critique and revision."""

    def __init__(
        self,
        model,
        tokenizer,
        constitution: Constitution,
        max_iterations: int = 3
    ):
        self.model = model
        self.tokenizer = tokenizer
        self.constitution = constitution
        self.max_iterations = max_iterations

    def critique_response(
        self,
        prompt: str,
        response: str,
        principle: ConstitutionalPrinciple
    ) -> str:
        """Generate critique based on constitutional principle."""
        critique_template = (
            "Human: {prompt}\n\n"
            "Assistant: {response}\n\n"
            "Critique Request: {critique_prompt}\n\n"
            "Critique:"
        )

        critique_input = critique_template.format(
            prompt=prompt,
            response=response,
            critique_prompt=principle.critique_prompt
        )

        inputs = self.tokenizer(critique_input, return_tensors="pt").to(self.model.device)

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True
            )

        return self.tokenizer.decode(outputs[0], skip_special_tokens=True).split("Critique:")[-1].strip()

    def revise_response(
        self,
        prompt: str,
        response: str,
        critique: str,
        principle: ConstitutionalPrinciple
    ) -> str:
        """Generate revised response based on critique."""
        revision_template = (
            "Human: {prompt}\n\n"
            "Assistant: {response}\n\n"
            "Critique: {critique}\n\n"
            "Revision Request: {revision_prompt}\n\n"
            "Revised Response:"
        )

        revision_input = revision_template.format(
            prompt=prompt,
            response=response,
            critique=critique,
            revision_prompt=principle.revision_prompt
        )

        inputs = self.tokenizer(revision_input, return_tensors="pt").to(self.model.device)

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=0.7,
                do_sample=True
            )

        return self.tokenizer.decode(outputs[0], skip_special_tokens=True).split("Revised Response:")[-1].strip()

    def constitutional_revision(
        self,
        prompt: str,
        initial_response: str
    ) -> Dict:
        """Full constitutional revision pipeline."""
        current_response = initial_response
        history = [{
            "iteration": 0,
            "response": initial_response,
            "critique": None,
            "principle": None
        }]

        for i in range(self.max_iterations):
            # Sample a principle
            principle = self.constitution.sample_principle()

            # Generate critique
            critique = self.critique_response(prompt, current_response, principle)

            # Check if critique indicates issues
            if self._critique_indicates_issues(critique):
                # Generate revision
                revised = self.revise_response(prompt, current_response, critique, principle)
                current_response = revised

                history.append({
                    "iteration": i + 1,
                    "response": revised,
                    "critique": critique,
                    "principle": principle.name
                })
            else:
                # No issues found, stop iterating
                break

        return {
            "final_response": current_response,
            "history": history,
            "num_iterations": len(history) - 1
        }

    def _critique_indicates_issues(self, critique: str) -> bool:
        """Check if critique indicates problems that need revision."""
        negative_indicators = [
            "problematic", "harmful", "incorrect", "misleading",
            "inappropriate", "unsafe", "biased", "unfair"
        ]
        positive_indicators = [
            "no issues", "appropriate", "safe", "accurate",
            "helpful", "well-written"
        ]

        critique_lower = critique.lower()

        has_negative = any(ind in critique_lower for ind in negative_indicators)
        has_positive = any(ind in critique_lower for ind in positive_indicators)

        return has_negative and not has_positive
```

## 4.2.2 Constitution Design

### Helpfulness Principles

```python
HELPFULNESS_PRINCIPLES = [
    ConstitutionalPrinciple(
        name="task_completion",
        description="The response should directly address and complete the user's request",
        category=PrincipleCategory.HELPFULNESS,
        critique_prompt=(
            "Identify specific ways the response fails to address the user's request. "
            "Does it answer the question directly? Does it provide actionable information?"
        ),
        revision_prompt=(
            "Rewrite the response to directly and completely address the user's request. "
            "Ensure all parts of the question are answered with actionable information."
        ),
        weight=1.0
    ),
    ConstitutionalPrinciple(
        name="clarity",
        description="The response should be clear, well-organized, and easy to understand",
        category=PrincipleCategory.HELPFULNESS,
        critique_prompt=(
            "Identify parts of the response that are unclear, poorly organized, "
            "or difficult to understand. Note any jargon that should be explained."
        ),
        revision_prompt=(
            "Rewrite the response to be clearer and better organized. "
            "Use simple language and explain any technical terms."
        ),
        weight=0.8
    ),
    ConstitutionalPrinciple(
        name="relevance",
        description="The response should stay focused on the topic without unnecessary tangents",
        category=PrincipleCategory.HELPFULNESS,
        critique_prompt=(
            "Identify any parts of the response that are off-topic or unnecessary. "
            "Note if the response includes irrelevant information."
        ),
        revision_prompt=(
            "Rewrite the response to focus only on relevant information. "
            "Remove tangents and keep the response concise and on-topic."
        ),
        weight=0.7
    ),
]
```

### Harmlessness Principles

```python
HARMLESSNESS_PRINCIPLES = [
    ConstitutionalPrinciple(
        name="no_harmful_content",
        description="The response should not contain content that could cause harm",
        category=PrincipleCategory.HARMLESSNESS,
        critique_prompt=(
            "Identify any content in the response that could be harmful, dangerous, "
            "or could be used to cause harm to individuals or groups. "
            "Consider physical harm, psychological harm, and societal harm."
        ),
        revision_prompt=(
            "Rewrite the response to remove all harmful content. "
            "If the request itself is harmful, politely decline and explain why."
        ),
        weight=2.0  # Higher weight for safety
    ),
    ConstitutionalPrinciple(
        name="no_discrimination",
        description="The response should not discriminate against protected groups",
        category=PrincipleCategory.HARMLESSNESS,
        critique_prompt=(
            "Identify any content that discriminates against, stereotypes, or "
            "demeans individuals based on race, gender, religion, nationality, "
            "disability, age, or other protected characteristics."
        ),
        revision_prompt=(
            "Rewrite the response to be fair and inclusive. "
            "Remove stereotypes and ensure equal treatment of all groups."
        ),
        weight=1.5
    ),
    ConstitutionalPrinciple(
        name="legal_compliance",
        description="The response should not assist with illegal activities",
        category=PrincipleCategory.HARMLESSNESS,
        critique_prompt=(
            "Identify any content that provides instructions or assistance for "
            "illegal activities, including fraud, violence, or other crimes."
        ),
        revision_prompt=(
            "Rewrite the response to refuse assistance with illegal activities. "
            "Provide legal alternatives if appropriate."
        ),
        weight=2.0
    ),
    ConstitutionalPrinciple(
        name="privacy_protection",
        description="The response should protect privacy and not expose personal information",
        category=PrincipleCategory.HARMLESSNESS,
        critique_prompt=(
            "Identify any content that could compromise privacy, including requests "
            "for personal information or instructions for obtaining it improperly."
        ),
        revision_prompt=(
            "Rewrite the response to protect privacy. Do not include or request "
            "personal identifying information. Suggest privacy-respecting alternatives."
        ),
        weight=1.5
    ),
]
```

### Honesty Principles

```python
HONESTY_PRINCIPLES = [
    ConstitutionalPrinciple(
        name="factual_accuracy",
        description="The response should contain accurate, verifiable information",
        category=PrincipleCategory.HONESTY,
        critique_prompt=(
            "Identify any factual claims in the response that are incorrect, "
            "misleading, or cannot be verified. Note unsupported assertions."
        ),
        revision_prompt=(
            "Rewrite the response to correct factual errors. "
            "Remove claims that cannot be verified or add appropriate caveats."
        ),
        weight=1.2
    ),
    ConstitutionalPrinciple(
        name="uncertainty_acknowledgment",
        description="The response should acknowledge uncertainty when appropriate",
        category=PrincipleCategory.HONESTY,
        critique_prompt=(
            "Identify claims made with false confidence. Does the response "
            "present uncertain information as definite? Does it acknowledge "
            "the limits of its knowledge?"
        ),
        revision_prompt=(
            "Rewrite the response to appropriately convey uncertainty. "
            "Use hedging language for uncertain claims and acknowledge knowledge limits."
        ),
        weight=0.9
    ),
    ConstitutionalPrinciple(
        name="no_deception",
        description="The response should not attempt to deceive or manipulate",
        category=PrincipleCategory.HONESTY,
        critique_prompt=(
            "Identify any attempts to deceive, manipulate, or mislead the user. "
            "Note any half-truths or misleading framing."
        ),
        revision_prompt=(
            "Rewrite the response to be straightforward and honest. "
            "Remove any manipulative framing or misleading information."
        ),
        weight=1.5
    ),
    ConstitutionalPrinciple(
        name="source_attribution",
        description="The response should attribute claims to sources when possible",
        category=PrincipleCategory.HONESTY,
        critique_prompt=(
            "Identify claims that would benefit from source attribution. "
            "Note any claims presented as fact without supporting evidence."
        ),
        revision_prompt=(
            "Rewrite the response to include source attribution where appropriate. "
            "Distinguish between established facts and opinions."
        ),
        weight=0.7
    ),
]
```

### Custom Domain-Specific Principles

```python
def create_domain_constitution(domain: str) -> List[ConstitutionalPrinciple]:
    """Create domain-specific constitutional principles."""

    domain_principles = {
        "medical": [
            ConstitutionalPrinciple(
                name="medical_disclaimer",
                description="Medical responses must include appropriate disclaimers",
                category=PrincipleCategory.CUSTOM,
                critique_prompt=(
                    "Check if the medical response includes appropriate disclaimers. "
                    "Does it recommend consulting a healthcare professional? "
                    "Does it clarify it's not a substitute for medical advice?"
                ),
                revision_prompt=(
                    "Rewrite to include medical disclaimers. Add recommendations to "
                    "consult healthcare professionals and clarify limitations."
                ),
                weight=2.0
            ),
            ConstitutionalPrinciple(
                name="evidence_based",
                description="Medical information should be evidence-based",
                category=PrincipleCategory.CUSTOM,
                critique_prompt=(
                    "Identify any medical claims not supported by evidence. "
                    "Note any alternative medicine claims presented as proven."
                ),
                revision_prompt=(
                    "Rewrite to focus on evidence-based medicine. "
                    "Remove or caveat unproven claims."
                ),
                weight=1.5
            ),
        ],
        "legal": [
            ConstitutionalPrinciple(
                name="legal_disclaimer",
                description="Legal responses must include disclaimers and recommend counsel",
                category=PrincipleCategory.CUSTOM,
                critique_prompt=(
                    "Check if legal response disclaims providing legal advice. "
                    "Does it recommend consulting an attorney? "
                    "Does it note jurisdictional variations?"
                ),
                revision_prompt=(
                    "Rewrite to include legal disclaimers. Recommend consulting "
                    "an attorney and note that laws vary by jurisdiction."
                ),
                weight=2.0
            ),
        ],
        "financial": [
            ConstitutionalPrinciple(
                name="investment_disclaimer",
                description="Financial responses must disclaim investment advice",
                category=PrincipleCategory.CUSTOM,
                critique_prompt=(
                    "Check if financial response includes investment disclaimers. "
                    "Does it clarify this is not personalized financial advice? "
                    "Does it recommend consulting a financial advisor?"
                ),
                revision_prompt=(
                    "Rewrite to include investment disclaimers. Clarify this is "
                    "not personalized advice and recommend consulting professionals."
                ),
                weight=2.0
            ),
        ],
    }

    return domain_principles.get(domain, [])
```

## 4.2.3 RLAIF Implementation

### AI Preference Labeling

```python
"""
RLAIF: Reinforcement Learning from AI Feedback
"""
import torch
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass

@dataclass
class AIPreference:
    """AI-generated preference annotation."""
    prompt: str
    response_a: str
    response_b: str
    preference: str  # "A", "B", or "tie"
    reasoning: str
    confidence: float
    principle_used: Optional[str] = None


class AILabeler:
    """AI model for generating preference labels."""

    def __init__(
        self,
        model,
        tokenizer,
        constitution: Optional[Constitution] = None,
        temperature: float = 0.3  # Lower for more consistent labeling
    ):
        self.model = model
        self.tokenizer = tokenizer
        self.constitution = constitution
        self.temperature = temperature

    def generate_preference(
        self,
        prompt: str,
        response_a: str,
        response_b: str,
        principle: Optional[ConstitutionalPrinciple] = None
    ) -> AIPreference:
        """Generate preference label for response pair."""

        if principle:
            criteria = principle.description
        else:
            criteria = (
                "helpfulness, harmlessness, and honesty. Consider which response "
                "better addresses the user's needs while being safe and truthful."
            )

        labeling_prompt = self._create_labeling_prompt(
            prompt, response_a, response_b, criteria
        )

        inputs = self.tokenizer(labeling_prompt, return_tensors="pt").to(self.model.device)

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=self.temperature,
                do_sample=True
            )

        output_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Parse the response
        preference, reasoning, confidence = self._parse_preference_output(output_text)

        return AIPreference(
            prompt=prompt,
            response_a=response_a,
            response_b=response_b,
            preference=preference,
            reasoning=reasoning,
            confidence=confidence,
            principle_used=principle.name if principle else None
        )

    def _create_labeling_prompt(
        self,
        prompt: str,
        response_a: str,
        response_b: str,
        criteria: str
    ) -> str:
        """Create prompt for preference labeling."""
        return f"""Compare the following two responses to a user prompt and determine which is better.

User Prompt: {prompt}

Response A: {response_a}

Response B: {response_b}

Evaluation Criteria: {criteria}

Instructions:
1. Analyze both responses based on the criteria
2. Provide your reasoning
3. State your preference (A, B, or tie)
4. Rate your confidence (high, medium, low)

Your evaluation:
"""

    def _parse_preference_output(self, output: str) -> Tuple[str, str, float]:
        """Parse the preference output from the model."""
        output_lower = output.lower()

        # Determine preference
        if "response a is better" in output_lower or "prefer response a" in output_lower or "choose a" in output_lower:
            preference = "A"
        elif "response b is better" in output_lower or "prefer response b" in output_lower or "choose b" in output_lower:
            preference = "B"
        elif "tie" in output_lower or "equally" in output_lower:
            preference = "tie"
        else:
            # Default to tie if unclear
            preference = "tie"

        # Determine confidence
        if "high confidence" in output_lower or "clearly" in output_lower or "significantly" in output_lower:
            confidence = 0.9
        elif "low confidence" in output_lower or "slightly" in output_lower or "marginally" in output_lower:
            confidence = 0.6
        else:
            confidence = 0.75

        # Extract reasoning (everything before the final verdict)
        reasoning = output.strip()

        return preference, reasoning, confidence
```

### Critique Generation

```python
class CritiqueGenerator:
    """Generate detailed critiques for responses."""

    def __init__(self, model, tokenizer, constitution: Constitution):
        self.model = model
        self.tokenizer = tokenizer
        self.constitution = constitution

    def generate_critique(
        self,
        prompt: str,
        response: str,
        principles: Optional[List[ConstitutionalPrinciple]] = None
    ) -> Dict:
        """Generate comprehensive critique of a response."""

        if principles is None:
            principles = self.constitution.principles

        critiques = {}

        for principle in principles:
            critique = self._critique_for_principle(prompt, response, principle)
            critiques[principle.name] = {
                "category": principle.category.value,
                "critique": critique["critique"],
                "severity": critique["severity"],
                "suggestions": critique["suggestions"]
            }

        # Aggregate overall assessment
        overall_severity = max(c["severity"] for c in critiques.values())
        needs_revision = overall_severity > 0.3

        return {
            "critiques": critiques,
            "overall_severity": overall_severity,
            "needs_revision": needs_revision,
            "primary_issues": [
                name for name, c in critiques.items()
                if c["severity"] > 0.5
            ]
        }

    def _critique_for_principle(
        self,
        prompt: str,
        response: str,
        principle: ConstitutionalPrinciple
    ) -> Dict:
        """Generate critique for a single principle."""

        critique_prompt = f"""Analyze this response for compliance with the following principle:

Principle: {principle.name}
Description: {principle.description}

User Prompt: {prompt}

Response: {response}

Provide:
1. A critique identifying any violations of this principle
2. Severity rating (none, minor, moderate, severe)
3. Specific suggestions for improvement

Analysis:
"""

        inputs = self.tokenizer(critique_prompt, return_tensors="pt").to(self.model.device)

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.3,
                do_sample=True
            )

        output_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Parse severity
        severity_map = {"none": 0.0, "minor": 0.3, "moderate": 0.6, "severe": 0.9}
        severity = 0.3  # Default

        for sev_name, sev_value in severity_map.items():
            if sev_name in output_text.lower():
                severity = sev_value
                break

        return {
            "critique": output_text,
            "severity": severity,
            "suggestions": self._extract_suggestions(output_text)
        }

    def _extract_suggestions(self, text: str) -> List[str]:
        """Extract suggestions from critique text."""
        suggestions = []

        # Look for suggestion patterns
        lines = text.split("\n")
        for line in lines:
            line = line.strip()
            if any(indicator in line.lower() for indicator in ["suggest", "should", "could", "recommend", "consider"]):
                suggestions.append(line)

        return suggestions[:5]  # Limit to 5 suggestions
```

### Revision Generation

```python
class RevisionGenerator:
    """Generate revisions based on critiques."""

    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer

    def generate_revision(
        self,
        prompt: str,
        original_response: str,
        critiques: Dict
    ) -> str:
        """Generate revised response addressing critiques."""

        # Format critiques for the revision prompt
        critique_summary = self._format_critiques(critiques)

        revision_prompt = f"""Revise the following response to address the identified issues.

User Prompt: {prompt}

Original Response: {original_response}

Issues Identified:
{critique_summary}

Generate an improved response that addresses all identified issues while maintaining helpfulness:

Revised Response:
"""

        inputs = self.tokenizer(revision_prompt, return_tensors="pt").to(self.model.device)

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=0.7,
                do_sample=True
            )

        output_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Extract the revised response
        if "Revised Response:" in output_text:
            revised = output_text.split("Revised Response:")[-1].strip()
        else:
            revised = output_text.strip()

        return revised

    def _format_critiques(self, critiques: Dict) -> str:
        """Format critiques for revision prompt."""
        formatted = []

        for principle_name, critique_data in critiques.get("critiques", {}).items():
            if critique_data["severity"] > 0.2:
                formatted.append(f"- {principle_name}: {critique_data['critique'][:200]}...")
                for suggestion in critique_data.get("suggestions", [])[:2]:
                    formatted.append(f"  Suggestion: {suggestion}")

        return "\n".join(formatted)
```

## 4.2.4 Self-Improvement Loops

### Iterative Refinement Pipeline

```python
class SelfImprovementPipeline:
    """Pipeline for iterative self-improvement using CAI."""

    def __init__(
        self,
        model,
        tokenizer,
        constitution: Constitution,
        max_iterations: int = 5,
        improvement_threshold: float = 0.1
    ):
        self.model = model
        self.tokenizer = tokenizer
        self.constitution = constitution
        self.max_iterations = max_iterations
        self.improvement_threshold = improvement_threshold

        self.critique_generator = CritiqueGenerator(model, tokenizer, constitution)
        self.revision_generator = RevisionGenerator(model, tokenizer)
        self.ai_labeler = AILabeler(model, tokenizer, constitution)

    def run_improvement_loop(
        self,
        prompt: str,
        initial_response: str
    ) -> Dict:
        """Run iterative improvement until convergence or max iterations."""

        current_response = initial_response
        history = []

        for iteration in range(self.max_iterations):
            # Generate critique
            critique_result = self.critique_generator.generate_critique(
                prompt, current_response
            )

            history.append({
                "iteration": iteration,
                "response": current_response,
                "critique": critique_result,
                "severity": critique_result["overall_severity"]
            })

            # Check if revision needed
            if not critique_result["needs_revision"]:
                break

            # Generate revision
            revised_response = self.revision_generator.generate_revision(
                prompt, current_response, critique_result
            )

            # Check if revision is actually better
            preference = self.ai_labeler.generate_preference(
                prompt, revised_response, current_response
            )

            if preference.preference == "A":  # Revision is better
                current_response = revised_response
            elif preference.preference == "tie":
                # Small improvement or no improvement, might stop
                if critique_result["overall_severity"] < 0.3:
                    break
                current_response = revised_response
            else:
                # Original was better, stop revising
                break

        return {
            "final_response": current_response,
            "history": history,
            "num_iterations": len(history),
            "converged": len(history) < self.max_iterations
        }

    def batch_improve(
        self,
        prompts: List[str],
        responses: List[str]
    ) -> List[Dict]:
        """Run improvement on batch of prompt-response pairs."""
        results = []

        for prompt, response in zip(prompts, responses):
            result = self.run_improvement_loop(prompt, response)
            results.append(result)

        return results
```

### Convergence Criteria

```python
class ConvergenceCriteria:
    """Define and check convergence criteria for self-improvement."""

    def __init__(
        self,
        max_iterations: int = 5,
        min_severity_threshold: float = 0.2,
        improvement_rate_threshold: float = 0.05,
        quality_score_threshold: float = 0.8
    ):
        self.max_iterations = max_iterations
        self.min_severity_threshold = min_severity_threshold
        self.improvement_rate_threshold = improvement_rate_threshold
        self.quality_score_threshold = quality_score_threshold

    def check_convergence(self, history: List[Dict]) -> Tuple[bool, str]:
        """Check if improvement loop has converged."""

        if len(history) >= self.max_iterations:
            return True, "max_iterations_reached"

        if not history:
            return False, "no_history"

        latest = history[-1]

        # Check severity threshold
        if latest["severity"] < self.min_severity_threshold:
            return True, "severity_below_threshold"

        # Check improvement rate (if we have history)
        if len(history) >= 2:
            prev_severity = history[-2]["severity"]
            improvement = prev_severity - latest["severity"]

            if improvement < self.improvement_rate_threshold:
                return True, "improvement_rate_too_low"

        return False, "continue"

    def compute_quality_score(self, critique_result: Dict) -> float:
        """Compute overall quality score from critique."""
        critiques = critique_result.get("critiques", {})

        if not critiques:
            return 1.0

        # Weight by category importance
        category_weights = {
            "harmlessness": 2.0,
            "honesty": 1.5,
            "helpfulness": 1.0,
            "custom": 1.0
        }

        total_weight = 0
        weighted_score = 0

        for principle_name, critique_data in critiques.items():
            category = critique_data.get("category", "custom")
            weight = category_weights.get(category, 1.0)

            # Convert severity to quality (1 - severity)
            quality = 1 - critique_data["severity"]

            weighted_score += quality * weight
            total_weight += weight

        return weighted_score / total_weight if total_weight > 0 else 1.0
```

## 4.2.5 Scaling Considerations

### Cost Comparison

```python
from dataclasses import dataclass
from typing import Dict

@dataclass
class AnnotationCostEstimate:
    """Cost estimate for annotation approaches."""
    approach: str
    cost_per_sample: float
    samples_per_hour: float
    quality_score: float
    scalability: str


def compare_annotation_costs(
    num_samples: int,
    human_rate_per_hour: float = 25.0,
    api_cost_per_1k_tokens: float = 0.01
) -> Dict[str, AnnotationCostEstimate]:
    """Compare costs of different annotation approaches."""

    # Human annotation
    human_samples_per_hour = 20  # Expert rate for quality comparisons
    human_cost_per_sample = human_rate_per_hour / human_samples_per_hour

    # AI annotation (API)
    avg_tokens_per_sample = 2000  # prompt + responses + reasoning
    ai_cost_per_sample = (avg_tokens_per_sample / 1000) * api_cost_per_1k_tokens

    # Hybrid (AI generation, human verification)
    verification_rate = 60  # samples per hour
    hybrid_cost_per_sample = (
        ai_cost_per_sample +
        human_rate_per_hour / verification_rate * 0.2  # 20% verification rate
    )

    return {
        "human": AnnotationCostEstimate(
            approach="human",
            cost_per_sample=human_cost_per_sample,
            samples_per_hour=human_samples_per_hour,
            quality_score=0.95,
            scalability="low"
        ),
        "ai_only": AnnotationCostEstimate(
            approach="ai_only",
            cost_per_sample=ai_cost_per_sample,
            samples_per_hour=1000,  # API limited
            quality_score=0.80,
            scalability="high"
        ),
        "hybrid": AnnotationCostEstimate(
            approach="hybrid",
            cost_per_sample=hybrid_cost_per_sample,
            samples_per_hour=200,
            quality_score=0.90,
            scalability="medium"
        )
    }


def estimate_total_cost(
    num_samples: int,
    approach: str = "hybrid"
) -> Dict:
    """Estimate total cost for annotation project."""
    costs = compare_annotation_costs(num_samples)
    estimate = costs[approach]

    total_cost = num_samples * estimate.cost_per_sample
    total_hours = num_samples / estimate.samples_per_hour

    return {
        "approach": approach,
        "num_samples": num_samples,
        "cost_per_sample": estimate.cost_per_sample,
        "total_cost": total_cost,
        "total_hours": total_hours,
        "quality_score": estimate.quality_score
    }
```

### Quality Tradeoffs

```python
class QualityTradeoffAnalyzer:
    """Analyze quality tradeoffs between human and AI annotation."""

    def __init__(
        self,
        human_labels: List[Dict],
        ai_labels: List[Dict]
    ):
        self.human_labels = human_labels
        self.ai_labels = ai_labels

    def compute_agreement(self) -> Dict:
        """Compute agreement between human and AI labels."""
        agreements = 0
        total = 0

        for human, ai in zip(self.human_labels, self.ai_labels):
            if human["preference"] == ai["preference"]:
                agreements += 1
            total += 1

        agreement_rate = agreements / total if total > 0 else 0

        # Cohen's kappa
        # (simplified - assumes balanced classes)
        expected_agreement = 0.33  # Random for 3 classes (A, B, tie)
        kappa = (agreement_rate - expected_agreement) / (1 - expected_agreement)

        return {
            "agreement_rate": agreement_rate,
            "cohens_kappa": kappa,
            "total_comparisons": total
        }

    def analyze_disagreements(self) -> Dict:
        """Analyze where human and AI labels disagree."""
        disagreements = []

        for human, ai in zip(self.human_labels, self.ai_labels):
            if human["preference"] != ai["preference"]:
                disagreements.append({
                    "prompt": human["prompt"],
                    "human_preference": human["preference"],
                    "ai_preference": ai["preference"],
                    "human_reasoning": human.get("reasoning", ""),
                    "ai_reasoning": ai.get("reasoning", "")
                })

        # Categorize disagreements
        categories = {
            "human_stricter": 0,  # Human found issues AI missed
            "ai_stricter": 0,    # AI found issues human missed
            "different_criteria": 0,  # Different aspects prioritized
            "subjective": 0      # Genuine subjectivity
        }

        for d in disagreements:
            category = self._categorize_disagreement(d)
            categories[category] += 1

        return {
            "total_disagreements": len(disagreements),
            "disagreement_rate": len(disagreements) / len(self.human_labels),
            "categories": categories,
            "examples": disagreements[:10]  # Sample for review
        }

    def _categorize_disagreement(self, disagreement: Dict) -> str:
        """Categorize a disagreement."""
        # Simplified heuristic
        ai_reasoning = disagreement["ai_reasoning"].lower()
        human_reasoning = disagreement["human_reasoning"].lower()

        safety_keywords = ["harm", "dangerous", "unsafe", "inappropriate"]

        ai_safety_concern = any(kw in ai_reasoning for kw in safety_keywords)
        human_safety_concern = any(kw in human_reasoning for kw in safety_keywords)

        if ai_safety_concern and not human_safety_concern:
            return "ai_stricter"
        elif human_safety_concern and not ai_safety_concern:
            return "human_stricter"
        else:
            return "subjective"
```

### Verification Sampling

```python
class VerificationSampler:
    """Sample AI labels for human verification."""

    def __init__(
        self,
        verification_rate: float = 0.1,
        prioritize_uncertain: bool = True,
        prioritize_harmful: bool = True
    ):
        self.verification_rate = verification_rate
        self.prioritize_uncertain = prioritize_uncertain
        self.prioritize_harmful = prioritize_harmful

    def select_for_verification(
        self,
        ai_labels: List[AIPreference]
    ) -> List[AIPreference]:
        """Select samples for human verification."""
        n_verify = int(len(ai_labels) * self.verification_rate)

        # Score each sample for verification priority
        scored = []
        for label in ai_labels:
            score = self._compute_priority_score(label)
            scored.append((score, label))

        # Sort by priority (higher = more important to verify)
        scored.sort(key=lambda x: x[0], reverse=True)

        # Select top samples
        selected = [label for _, label in scored[:n_verify]]

        # Add random samples for unbiased estimation
        remaining = [label for _, label in scored[n_verify:]]
        import random
        random_samples = random.sample(remaining, min(len(remaining), n_verify // 5))
        selected.extend(random_samples)

        return selected

    def _compute_priority_score(self, label: AIPreference) -> float:
        """Compute verification priority score."""
        score = 0.0

        # Prioritize low confidence
        if self.prioritize_uncertain:
            score += (1 - label.confidence) * 2

        # Prioritize potential harmful content
        if self.prioritize_harmful:
            harmful_keywords = ["harm", "danger", "unsafe", "illegal"]
            reasoning_lower = label.reasoning.lower()
            for kw in harmful_keywords:
                if kw in reasoning_lower:
                    score += 1

        # Prioritize ties (harder to judge)
        if label.preference == "tie":
            score += 0.5

        return score
```

### Bias Propagation Risks

```python
class BiasPropagationAnalyzer:
    """Analyze and mitigate bias propagation in RLAIF."""

    def __init__(self, ai_model_info: Dict):
        self.model_info = ai_model_info
        self.known_biases = []

    def detect_systematic_biases(
        self,
        ai_labels: List[AIPreference],
        analysis_dimensions: List[str] = None
    ) -> Dict:
        """Detect systematic biases in AI labeling."""

        if analysis_dimensions is None:
            analysis_dimensions = [
                "response_length",
                "formality",
                "assertiveness",
                "complexity"
            ]

        biases_detected = {}

        for dimension in analysis_dimensions:
            bias = self._analyze_dimension(ai_labels, dimension)
            if bias["is_biased"]:
                biases_detected[dimension] = bias

        return {
            "dimensions_analyzed": analysis_dimensions,
            "biases_detected": biases_detected,
            "overall_bias_risk": len(biases_detected) / len(analysis_dimensions)
        }

    def _analyze_dimension(
        self,
        labels: List[AIPreference],
        dimension: str
    ) -> Dict:
        """Analyze bias along a single dimension."""

        measurements = []

        for label in labels:
            value_a = self._measure_dimension(label.response_a, dimension)
            value_b = self._measure_dimension(label.response_b, dimension)

            preferred_value = value_a if label.preference == "A" else value_b if label.preference == "B" else (value_a + value_b) / 2

            measurements.append({
                "value_a": value_a,
                "value_b": value_b,
                "preferred_value": preferred_value,
                "preference": label.preference
            })

        # Statistical analysis
        import numpy as np
        preferred_values = [m["preferred_value"] for m in measurements]
        all_values = [m["value_a"] for m in measurements] + [m["value_b"] for m in measurements]

        mean_preferred = np.mean(preferred_values)
        mean_all = np.mean(all_values)

        # Simple bias test: significant difference in means
        bias_magnitude = (mean_preferred - mean_all) / (np.std(all_values) + 1e-8)
        is_biased = abs(bias_magnitude) > 0.5  # More than 0.5 std deviation

        return {
            "dimension": dimension,
            "mean_preferred": mean_preferred,
            "mean_overall": mean_all,
            "bias_magnitude": bias_magnitude,
            "is_biased": is_biased
        }

    def _measure_dimension(self, text: str, dimension: str) -> float:
        """Measure a dimension of the text."""
        if dimension == "response_length":
            return len(text.split())
        elif dimension == "formality":
            # Simple heuristic
            formal_words = ["therefore", "furthermore", "consequently", "additionally"]
            return sum(1 for word in text.lower().split() if word in formal_words) / max(len(text.split()), 1)
        elif dimension == "assertiveness":
            # Simple heuristic
            hedging_words = ["might", "perhaps", "possibly", "maybe", "could"]
            hedge_count = sum(1 for word in text.lower().split() if word in hedging_words)
            return 1 - hedge_count / max(len(text.split()), 1)
        elif dimension == "complexity":
            # Average word length as proxy
            words = text.split()
            return sum(len(w) for w in words) / max(len(words), 1)
        else:
            return 0.0

    def mitigate_biases(
        self,
        biases: Dict,
        labels: List[AIPreference]
    ) -> List[AIPreference]:
        """Apply bias mitigation strategies."""

        mitigated_labels = []

        for label in labels:
            # Adjust confidence based on detected biases
            confidence_adjustment = 1.0

            for dimension, bias_info in biases.get("biases_detected", {}).items():
                if bias_info["is_biased"]:
                    # Reduce confidence for labels that follow the bias pattern
                    value_chosen = self._measure_dimension(
                        label.response_a if label.preference == "A" else label.response_b,
                        dimension
                    )

                    if abs(value_chosen - bias_info["mean_preferred"]) < abs(value_chosen - bias_info["mean_overall"]):
                        confidence_adjustment *= 0.9

            # Create adjusted label
            adjusted = AIPreference(
                prompt=label.prompt,
                response_a=label.response_a,
                response_b=label.response_b,
                preference=label.preference,
                reasoning=label.reasoning,
                confidence=label.confidence * confidence_adjustment,
                principle_used=label.principle_used
            )
            mitigated_labels.append(adjusted)

        return mitigated_labels
```

## Appendix A: Constitution Template Examples

### General-Purpose Constitution

```yaml
# general_constitution.yaml
name: "General Purpose AI Constitution"
version: "1.0"

principles:
  # Helpfulness
  - name: "direct_response"
    category: "helpfulness"
    description: "Responses should directly address the user's question or request"
    critique_prompt: "Does this response directly address what the user asked for?"
    revision_prompt: "Rewrite to directly answer the user's question"
    weight: 1.0

  - name: "complete_information"
    category: "helpfulness"
    description: "Responses should provide complete, actionable information"
    critique_prompt: "Is the response complete? Does it provide all necessary information?"
    revision_prompt: "Add any missing information needed for a complete answer"
    weight: 0.9

  # Harmlessness
  - name: "no_harmful_instructions"
    category: "harmlessness"
    description: "Never provide instructions for harmful activities"
    critique_prompt: "Does this response contain instructions that could cause harm?"
    revision_prompt: "Remove harmful instructions and provide safe alternatives"
    weight: 2.0

  - name: "respect_privacy"
    category: "harmlessness"
    description: "Protect user privacy and personal information"
    critique_prompt: "Does this response inappropriately request or expose personal information?"
    revision_prompt: "Remove privacy-violating content"
    weight: 1.5

  # Honesty
  - name: "factual_accuracy"
    category: "honesty"
    description: "Information provided should be factually accurate"
    critique_prompt: "Are there any factual errors or unsupported claims?"
    revision_prompt: "Correct factual errors and add appropriate caveats"
    weight: 1.2

  - name: "acknowledge_limitations"
    category: "honesty"
    description: "Acknowledge uncertainty and limitations"
    critique_prompt: "Does the response overstate certainty or hide limitations?"
    revision_prompt: "Add appropriate hedging for uncertain claims"
    weight: 0.8
```

### Safety-Focused Constitution

```yaml
# safety_constitution.yaml
name: "Safety-Focused Constitution"
version: "1.0"

principles:
  - name: "prevent_physical_harm"
    category: "harmlessness"
    description: "Never provide information that could lead to physical harm"
    critique_prompt: |
      Check if this response could lead to physical harm:
      - Instructions for weapons or violence
      - Dangerous activities without proper warnings
      - Medical advice that could harm
    revision_prompt: |
      Remove any content that could cause physical harm.
      Add safety warnings where appropriate.
      Redirect to professional resources.
    weight: 3.0

  - name: "prevent_psychological_harm"
    category: "harmlessness"
    description: "Avoid content that could cause psychological harm"
    critique_prompt: |
      Check for psychologically harmful content:
      - Manipulation tactics
      - Content that could worsen mental health
      - Bullying or harassment
    revision_prompt: |
      Remove psychologically harmful content.
      Provide supportive alternatives.
      Add resources for mental health support if relevant.
    weight: 2.0

  - name: "protect_vulnerable_groups"
    category: "harmlessness"
    description: "Extra care for content involving vulnerable populations"
    critique_prompt: |
      Check if response appropriately protects vulnerable groups:
      - Children and minors
      - People in crisis
      - Victims of abuse
    revision_prompt: |
      Ensure extra protections for vulnerable populations.
      Add appropriate resources and referrals.
      Remove any potentially exploitative content.
    weight: 2.5
```

## Appendix B: Critique Prompt Templates

```python
CRITIQUE_TEMPLATES = {
    "general": """
Analyze the following response for potential issues:

User Request: {prompt}
Response: {response}

Consider:
1. Does it fully address the user's needs?
2. Is the information accurate and complete?
3. Could it cause any harm?
4. Is it appropriately honest about limitations?

Provide a detailed critique:
""",

    "safety_focused": """
SAFETY REVIEW

User Request: {prompt}
Response: {response}

Evaluate for safety issues:
□ Physical harm risk
□ Psychological harm risk
□ Privacy violations
□ Illegal activity assistance
□ Discrimination or bias
□ Misinformation

For each issue found, explain:
- What the issue is
- How severe it is (low/medium/high)
- How it should be fixed

Safety Assessment:
""",

    "helpfulness_focused": """
HELPFULNESS REVIEW

User Request: {prompt}
Response: {response}

Evaluate helpfulness:
□ Does it answer the question directly?
□ Is the information complete?
□ Is it actionable?
□ Is it well-organized?
□ Is the language clear?

Rate helpfulness: [1-5]
Explain gaps and improvements needed:
""",

    "honesty_focused": """
HONESTY REVIEW

User Request: {prompt}
Response: {response}

Evaluate honesty:
□ Are all facts accurate?
□ Are claims properly hedged?
□ Are limitations acknowledged?
□ Is uncertainty communicated?
□ Are sources cited where needed?

Identify any honesty issues:
"""
}
```

## Appendix C: RLAIF Training Configuration

```yaml
# rlaif_config.yaml

# AI Labeler Configuration
labeler:
  model: "gpt-4"  # or local model path
  temperature: 0.3
  max_tokens: 256
  constitutional_principles: true

# Data Generation
data:
  num_comparisons: 50000
  prompts_source: "anthropic/hh-rlhf"
  responses_per_prompt: 2
  filter_ties: true
  min_confidence: 0.6

# Quality Control
quality:
  verification_rate: 0.05  # 5% human verification
  agreement_threshold: 0.8
  bias_detection: true
  bias_dimensions:
    - "response_length"
    - "formality"
    - "complexity"

# Training
training:
  base_model: "meta-llama/Llama-2-7b-hf"
  method: "dpo"  # or "ppo"
  learning_rate: 5e-7
  batch_size: 8
  epochs: 1
  beta: 0.1  # DPO temperature

# Self-Improvement
self_improvement:
  enabled: true
  max_iterations: 3
  convergence_threshold: 0.2
  improvement_rate_threshold: 0.05
```

## Appendix D: Quality Verification Procedures

```python
class QualityVerificationProcedure:
    """Standard procedures for verifying RLAIF quality."""

    def __init__(self, config: Dict):
        self.config = config

    def run_verification(
        self,
        ai_labels: List[AIPreference],
        human_annotators: List[str]
    ) -> Dict:
        """Run complete verification procedure."""

        results = {
            "sampling": {},
            "human_labels": [],
            "agreement_metrics": {},
            "bias_analysis": {},
            "recommendations": []
        }

        # 1. Sample for verification
        sampler = VerificationSampler(
            verification_rate=self.config.get("verification_rate", 0.1)
        )
        samples = sampler.select_for_verification(ai_labels)
        results["sampling"]["num_selected"] = len(samples)

        # 2. Collect human labels (placeholder - integrate with annotation platform)
        # human_labels = collect_human_labels(samples, human_annotators)
        # results["human_labels"] = human_labels

        # 3. Compute agreement metrics
        # analyzer = QualityTradeoffAnalyzer(human_labels, samples)
        # results["agreement_metrics"] = analyzer.compute_agreement()

        # 4. Analyze biases
        bias_analyzer = BiasPropagationAnalyzer({})
        results["bias_analysis"] = bias_analyzer.detect_systematic_biases(ai_labels)

        # 5. Generate recommendations
        results["recommendations"] = self._generate_recommendations(results)

        return results

    def _generate_recommendations(self, results: Dict) -> List[str]:
        """Generate recommendations based on verification results."""
        recommendations = []

        # Agreement-based recommendations
        agreement = results.get("agreement_metrics", {}).get("agreement_rate", 1.0)
        if agreement < 0.7:
            recommendations.append(
                "Low human-AI agreement detected. Consider: "
                "1) Revising constitutional principles, "
                "2) Improving labeler prompts, "
                "3) Increasing human verification rate"
            )

        # Bias-based recommendations
        bias_risk = results.get("bias_analysis", {}).get("overall_bias_risk", 0)
        if bias_risk > 0.3:
            recommendations.append(
                "Significant biases detected. Consider: "
                "1) Adjusting sampling to counter biases, "
                "2) Adding bias-aware principles, "
                "3) Increasing diversity in training data"
            )

        return recommendations
```

## Troubleshooting

### Common Issues

| Issue | Cause | Solution |
|-------|-------|----------|
| AI labels disagree with humans | Prompt design or model limitations | Refine labeling prompts, use stronger model |
| Self-improvement doesn't converge | Criteria too strict or conflicting | Relax thresholds, review principle compatibility |
| Bias in AI preferences | Model training biases | Analyze and mitigate with debiasing techniques |
| Low revision quality | Insufficient context in prompts | Provide more detailed revision instructions |
| Over-cautious responses | Harmlessness principles too aggressive | Balance safety with helpfulness weights |

### Debugging Tips

```bash
# Analyze preference distribution
python analyze_preferences.py --labels ai_labels.jsonl

# Check principle coverage
python check_coverage.py --constitution constitution.yaml --labels ai_labels.jsonl

# Measure self-improvement convergence
python convergence_analysis.py --history improvement_history.jsonl
```

## Glossary

- **Constitutional AI (CAI)**: Alignment approach using explicit principles for self-critique and revision
- **RLAIF**: Reinforcement Learning from AI Feedback - using AI-generated preference labels
- **Constitution**: Set of principles defining desired AI behavior
- **Self-Critique**: AI analyzing its own responses for issues
- **Revision**: Generating improved response based on critique
- **Scalable Oversight**: Using AI to help supervise AI at scale

## References

1. "Constitutional AI: Harmlessness from AI Feedback" (Bai et al., 2022)
2. "RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback" (Lee et al., 2023)
3. "Training a Helpful and Harmless Assistant with RLHF" (Anthropic, 2022)
4. "Self-Instruct: Aligning Language Models with Self-Generated Instructions" (Wang et al., 2023)
5. "Principle-Driven Self-Alignment of Language Models from Scratch" (Sun et al., 2023)

---

> **Navigation**
> [← 4.1 RLHF](4.1_rlhf_guide.md) | **[Index](../README.md#15-repository-structure)** | [4.3 Safety Evaluation →](4.3_safety_evaluation_red_teaming.md)
