# Document 2.2: Model Architecture Selection Guide

## Document Information
- **Version:** 1.0
- **Last Updated:** December 2025
- **Owner:** ML Research / Architecture Team
- **Category:** Model Training (Pre-training)

> **Navigation** | [← 2.1 Tokenizer Training](2.1_tokenizer_training_selection.md) | [2.3 Distributed Training →](2.3_distributed_training_infrastructure.md)
>
> | | |
> |---|---|
> | **Prerequisites** | [2.1 Tokenizer Training](2.1_tokenizer_training_selection.md) |
> | **Related** | [6.1 Quantization](../06_model_optimization/6.1_quantization_guide.md) &#124; [9.1 Inference Engine](../09_inference_serving/9.1_inference_engine_selection_guide.md) |
> | **Next** | [2.3 Distributed Training Infrastructure](2.3_distributed_training_infrastructure.md) |

---

## Executive Summary

Selecting the right model architecture is one of the most consequential decisions in LLM development. This guide covers modern transformer architectures, attention mechanisms (MHA, MQA, GQA), positional encodings (RoPE, ALiBi), normalization strategies, and scaling considerations. Architecture choices directly impact training efficiency, inference speed, and model capabilities. The guide provides decision frameworks for selecting architectures based on your constraints and objectives.

**Key Outcomes:**
- Understand modern LLM architecture components
- Select appropriate attention mechanisms for your use case
- Configure architectures for different model scales
- Make informed trade-offs between quality and efficiency

---

## Prerequisites

### Required Knowledge
- Transformer fundamentals (attention, feed-forward layers)
- Linear algebra basics (matrix operations)
- Deep learning concepts (gradients, optimization)
- GPU memory and compute concepts

### Infrastructure Requirements
- GPU for architecture experiments (A100/H100 recommended)
- Profiling tools (PyTorch Profiler, Nsight)

### Tool Installation
```bash
# PyTorch with CUDA
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# HuggingFace Transformers
pip install transformers accelerate

# FlashAttention
pip install flash-attn --no-build-isolation

# Model analysis tools
pip install fvcore  # FLOPs counting
pip install torchinfo  # Model summary
```

---

## 1. Architecture Fundamentals

### 1.1 Transformer Architecture Types

```
┌─────────────────────────────────────────────────────────────┐
│              Transformer Architecture Types                  │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  ENCODER-ONLY (Bidirectional)                               │
│  ┌───────────────────────────────────────────────────────┐  │
│  │ Examples: BERT, RoBERTa, ELECTRA                      │  │
│  │ Use cases: Classification, NER, embeddings            │  │
│  │ Attention: Full bidirectional                         │  │
│  │ Input/Output: Sequence → Sequence of embeddings       │  │
│  └───────────────────────────────────────────────────────┘  │
│                                                              │
│  DECODER-ONLY (Causal/Autoregressive)                       │
│  ┌───────────────────────────────────────────────────────┐  │
│  │ Examples: GPT, Llama, Mistral, Claude                 │  │
│  │ Use cases: Text generation, chat, reasoning           │  │
│  │ Attention: Causal (masked) - can't see future         │  │
│  │ Input/Output: Prefix → Generated continuation         │  │
│  │ ★ DOMINANT ARCHITECTURE FOR MODERN LLMs ★            │  │
│  └───────────────────────────────────────────────────────┘  │
│                                                              │
│  ENCODER-DECODER (Sequence-to-Sequence)                     │
│  ┌───────────────────────────────────────────────────────┐  │
│  │ Examples: T5, BART, Flan-T5                           │  │
│  │ Use cases: Translation, summarization                 │  │
│  │ Attention: Encoder bidirectional, decoder causal      │  │
│  │ Input/Output: Source sequence → Target sequence       │  │
│  └───────────────────────────────────────────────────────┘  │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### 1.2 Decoder-Only Architecture (Modern LLM Standard)

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from dataclasses import dataclass
from typing import Optional, Tuple

@dataclass
class LlamaConfig:
    """Configuration for Llama-style architecture"""

    # Model dimensions
    vocab_size: int = 32000
    hidden_size: int = 4096
    intermediate_size: int = 11008  # FFN hidden size
    num_hidden_layers: int = 32
    num_attention_heads: int = 32
    num_key_value_heads: int = 8  # For GQA (32/8 = 4 queries per KV)

    # Sequence length
    max_position_embeddings: int = 4096

    # Normalization
    rms_norm_eps: float = 1e-5

    # Activation
    hidden_act: str = "silu"  # SwiGLU uses SiLU

    # Positional encoding
    rope_theta: float = 10000.0
    rope_scaling: Optional[dict] = None

    # Architecture choices
    tie_word_embeddings: bool = False
    use_cache: bool = True

class LlamaRMSNorm(nn.Module):
    """RMSNorm (Root Mean Square Layer Normalization)"""

    def __init__(self, hidden_size: int, eps: float = 1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.eps = eps

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)

        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.eps)

        return self.weight * hidden_states.to(input_dtype)

class LlamaRotaryEmbedding(nn.Module):
    """Rotary Position Embedding (RoPE)"""

    def __init__(
        self,
        dim: int,
        max_position_embeddings: int = 4096,
        base: float = 10000.0,
        device: Optional[torch.device] = None
    ):
        super().__init__()
        self.dim = dim
        self.max_position_embeddings = max_position_embeddings
        self.base = base

        # Compute inverse frequencies
        inv_freq = 1.0 / (
            self.base ** (torch.arange(0, self.dim, 2, device=device).float() / self.dim)
        )
        self.register_buffer("inv_freq", inv_freq)

    def forward(
        self,
        x: torch.Tensor,
        position_ids: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        # Compute sin/cos for positions
        freqs = torch.outer(position_ids.float(), self.inv_freq)
        emb = torch.cat((freqs, freqs), dim=-1)
        return emb.cos(), emb.sin()

def rotate_half(x: torch.Tensor) -> torch.Tensor:
    """Rotate half the hidden dims of the input"""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)

def apply_rotary_pos_emb(
    q: torch.Tensor,
    k: torch.Tensor,
    cos: torch.Tensor,
    sin: torch.Tensor
) -> Tuple[torch.Tensor, torch.Tensor]:
    """Apply rotary position embeddings to Q and K"""
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed

class LlamaAttention(nn.Module):
    """Multi-head attention with Grouped Query Attention (GQA)"""

    def __init__(self, config: LlamaConfig, layer_idx: int):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx

        self.hidden_size = config.hidden_size
        self.num_heads = config.num_attention_heads
        self.head_dim = self.hidden_size // self.num_heads
        self.num_key_value_heads = config.num_key_value_heads
        self.num_key_value_groups = self.num_heads // self.num_key_value_heads

        # Projections
        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)
        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)

        # Rotary embeddings
        self.rotary_emb = LlamaRotaryEmbedding(
            self.head_dim,
            max_position_embeddings=config.max_position_embeddings,
            base=config.rope_theta
        )

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.Tensor] = None,
        past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
    ) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]:

        bsz, q_len, _ = hidden_states.size()

        # Project Q, K, V
        query_states = self.q_proj(hidden_states)
        key_states = self.k_proj(hidden_states)
        value_states = self.v_proj(hidden_states)

        # Reshape for attention
        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)

        # Apply rotary embeddings
        cos, sin = self.rotary_emb(value_states, position_ids)
        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)

        # KV cache handling
        if past_key_value is not None:
            key_states = torch.cat([past_key_value[0], key_states], dim=2)
            value_states = torch.cat([past_key_value[1], value_states], dim=2)

        past_key_value = (key_states, value_states)

        # Repeat KV for grouped query attention
        key_states = self._repeat_kv(key_states, self.num_key_value_groups)
        value_states = self._repeat_kv(value_states, self.num_key_value_groups)

        # Compute attention
        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / (self.head_dim ** 0.5)

        if attention_mask is not None:
            attn_weights = attn_weights + attention_mask

        attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
        attn_output = torch.matmul(attn_weights, value_states)

        # Reshape output
        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)

        attn_output = self.o_proj(attn_output)

        return attn_output, past_key_value

    def _repeat_kv(self, hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
        """Repeat KV heads for GQA"""
        if n_rep == 1:
            return hidden_states
        batch, num_kv_heads, slen, head_dim = hidden_states.shape
        hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_kv_heads, n_rep, slen, head_dim)
        return hidden_states.reshape(batch, num_kv_heads * n_rep, slen, head_dim)

class LlamaMLP(nn.Module):
    """SwiGLU Feed-Forward Network"""

    def __init__(self, config: LlamaConfig):
        super().__init__()
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size

        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
        self.act_fn = nn.SiLU()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # SwiGLU: SiLU(gate) * up
        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))

class LlamaDecoderLayer(nn.Module):
    """Single transformer decoder layer"""

    def __init__(self, config: LlamaConfig, layer_idx: int):
        super().__init__()
        self.hidden_size = config.hidden_size

        self.self_attn = LlamaAttention(config, layer_idx)
        self.mlp = LlamaMLP(config)

        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.Tensor] = None,
        past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
    ) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]:

        # Pre-norm architecture
        residual = hidden_states
        hidden_states = self.input_layernorm(hidden_states)

        # Self-attention
        hidden_states, past_key_value = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_value=past_key_value,
        )
        hidden_states = residual + hidden_states

        # FFN
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states

        return hidden_states, past_key_value
```

---

## 2. Attention Mechanisms

### 2.1 Multi-Head Attention (MHA) vs MQA vs GQA

```
┌─────────────────────────────────────────────────────────────┐
│              Attention Mechanism Comparison                  │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  Multi-Head Attention (MHA) - Standard                      │
│  ┌─────────────────────────────────────────────────────┐    │
│  │  Q: [h heads × d_k]   K: [h heads × d_k]            │    │
│  │  V: [h heads × d_v]                                  │    │
│  │                                                      │    │
│  │  Each head has its own K, V projections              │    │
│  │  KV Cache: O(batch × seq × h × d_k)                 │    │
│  │  Best quality, highest memory                        │    │
│  └─────────────────────────────────────────────────────┘    │
│                                                              │
│  Multi-Query Attention (MQA) - Single KV Head               │
│  ┌─────────────────────────────────────────────────────┐    │
│  │  Q: [h heads × d_k]   K: [1 head × d_k] (shared)    │    │
│  │  V: [1 head × d_v] (shared)                         │    │
│  │                                                      │    │
│  │  All query heads share single K, V                   │    │
│  │  KV Cache: O(batch × seq × 1 × d_k) = 1/h of MHA    │    │
│  │  Fastest inference, quality degradation              │    │
│  └─────────────────────────────────────────────────────┘    │
│                                                              │
│  Grouped-Query Attention (GQA) - Balanced                   │
│  ┌─────────────────────────────────────────────────────┐    │
│  │  Q: [h heads × d_k]   K: [g groups × d_k]           │    │
│  │  V: [g groups × d_v]                                │    │
│  │                                                      │    │
│  │  Groups of query heads share K, V                    │    │
│  │  KV Cache: O(batch × seq × g × d_k) = g/h of MHA    │    │
│  │  Best trade-off: near-MHA quality, MQA-like speed   │    │
│  └─────────────────────────────────────────────────────┘    │
│                                                              │
│  Memory Comparison (32 heads, 8 KV groups):                 │
│  MHA: 32 heads → 100% KV cache                              │
│  GQA:  8 groups → 25% KV cache                              │
│  MQA:  1 head  → 3.1% KV cache                              │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### 2.2 GQA Implementation

```python
class GroupedQueryAttention(nn.Module):
    """Grouped Query Attention with FlashAttention support"""

    def __init__(
        self,
        hidden_size: int,
        num_attention_heads: int,
        num_key_value_heads: int,
        head_dim: Optional[int] = None,
        dropout: float = 0.0,
        use_flash_attention: bool = True
    ):
        super().__init__()

        self.hidden_size = hidden_size
        self.num_heads = num_attention_heads
        self.num_kv_heads = num_key_value_heads
        self.head_dim = head_dim or (hidden_size // num_attention_heads)
        self.num_kv_groups = self.num_heads // self.num_kv_heads
        self.dropout = dropout
        self.use_flash_attention = use_flash_attention

        # Projections
        self.q_proj = nn.Linear(hidden_size, self.num_heads * self.head_dim, bias=False)
        self.k_proj = nn.Linear(hidden_size, self.num_kv_heads * self.head_dim, bias=False)
        self.v_proj = nn.Linear(hidden_size, self.num_kv_heads * self.head_dim, bias=False)
        self.o_proj = nn.Linear(self.num_heads * self.head_dim, hidden_size, bias=False)

        # FlashAttention
        if self.use_flash_attention:
            try:
                from flash_attn import flash_attn_func
                self.flash_attn_func = flash_attn_func
            except ImportError:
                print("FlashAttention not available, using standard attention")
                self.use_flash_attention = False

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.Tensor] = None,
        cos: Optional[torch.Tensor] = None,
        sin: Optional[torch.Tensor] = None,
        past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        output_attentions: bool = False
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple]]:

        batch_size, seq_len, _ = hidden_states.shape

        # Compute Q, K, V
        query = self.q_proj(hidden_states)
        key = self.k_proj(hidden_states)
        value = self.v_proj(hidden_states)

        # Reshape
        query = query.view(batch_size, seq_len, self.num_heads, self.head_dim)
        key = key.view(batch_size, seq_len, self.num_kv_heads, self.head_dim)
        value = value.view(batch_size, seq_len, self.num_kv_heads, self.head_dim)

        # Apply RoPE if provided
        if cos is not None and sin is not None:
            query, key = apply_rotary_pos_emb(query, key, cos, sin)

        # Handle KV cache
        if past_key_value is not None:
            key = torch.cat([past_key_value[0], key], dim=1)
            value = torch.cat([past_key_value[1], value], dim=1)

        past_key_value = (key, value) if self.use_cache else None

        # Expand KV for grouped query attention
        if self.num_kv_groups > 1:
            key = key.repeat_interleave(self.num_kv_groups, dim=2)
            value = value.repeat_interleave(self.num_kv_groups, dim=2)

        if self.use_flash_attention and not output_attentions:
            # FlashAttention path
            attn_output = self.flash_attn_func(
                query,
                key,
                value,
                dropout_p=self.dropout if self.training else 0.0,
                causal=True
            )
            attn_weights = None
        else:
            # Standard attention
            query = query.transpose(1, 2)  # [B, H, S, D]
            key = key.transpose(1, 2)
            value = value.transpose(1, 2)

            attn_weights = torch.matmul(query, key.transpose(-2, -1)) / (self.head_dim ** 0.5)

            if attention_mask is not None:
                attn_weights = attn_weights + attention_mask

            attn_weights = F.softmax(attn_weights, dim=-1)
            attn_weights = F.dropout(attn_weights, p=self.dropout, training=self.training)

            attn_output = torch.matmul(attn_weights, value)
            attn_output = attn_output.transpose(1, 2)

        # Reshape and project output
        attn_output = attn_output.reshape(batch_size, seq_len, self.hidden_size)
        attn_output = self.o_proj(attn_output)

        return attn_output, attn_weights, past_key_value

# Configuration comparison
ATTENTION_CONFIGS = {
    "llama-7b": {"num_heads": 32, "num_kv_heads": 32},  # MHA (Llama 1)
    "llama2-7b": {"num_heads": 32, "num_kv_heads": 32},  # MHA
    "llama2-70b": {"num_heads": 64, "num_kv_heads": 8},  # GQA (8 KV groups)
    "llama3-8b": {"num_heads": 32, "num_kv_heads": 8},  # GQA
    "mistral-7b": {"num_heads": 32, "num_kv_heads": 8},  # GQA
    "falcon-7b": {"num_heads": 71, "num_kv_heads": 1},  # MQA
    "falcon-180b": {"num_heads": 232, "num_kv_heads": 8},  # GQA
}
```

### 2.3 FlashAttention Integration

```python
def attention_forward_flash(
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor] = None,
    dropout_p: float = 0.0,
    is_causal: bool = True,
    scale: Optional[float] = None
) -> torch.Tensor:
    """
    FlashAttention-2 wrapper with fallbacks.

    Args:
        query: [B, S, H, D]
        key: [B, S, H_kv, D]
        value: [B, S, H_kv, D]
        attention_mask: Optional mask
        dropout_p: Dropout probability
        is_causal: Use causal masking
        scale: Attention scale (default: 1/sqrt(d))

    Returns:
        output: [B, S, H, D]
    """

    try:
        from flash_attn import flash_attn_func

        # FlashAttention expects [B, S, H, D] format
        output = flash_attn_func(
            query,
            key,
            value,
            dropout_p=dropout_p,
            causal=is_causal,
            softmax_scale=scale
        )
        return output

    except ImportError:
        # Fallback to PyTorch SDPA
        import torch.nn.functional as F

        # Transpose to [B, H, S, D]
        query = query.transpose(1, 2)
        key = key.transpose(1, 2)
        value = value.transpose(1, 2)

        output = F.scaled_dot_product_attention(
            query, key, value,
            attn_mask=attention_mask,
            dropout_p=dropout_p,
            is_causal=is_causal,
            scale=scale
        )

        return output.transpose(1, 2)

# Memory savings with FlashAttention
"""
Standard Attention Memory: O(N^2) for attention matrix
FlashAttention Memory: O(N) - doesn't materialize full attention

For sequence length 4096:
- Standard: ~67 MB per attention layer (FP16)
- Flash: ~4 MB per attention layer (FP16)

16x memory reduction!
"""
```

---

## 3. Positional Encodings

### 3.1 Rotary Position Embedding (RoPE)

```python
class RotaryEmbedding(nn.Module):
    """
    Rotary Position Embedding (RoPE).

    Key advantages:
    - Relative position encoding
    - Extrapolates beyond training length
    - No additional parameters
    """

    def __init__(
        self,
        dim: int,
        max_position_embeddings: int = 2048,
        base: float = 10000.0,
        scaling_factor: float = 1.0,
        rope_type: str = "default"  # default, linear, dynamic, yarn
    ):
        super().__init__()
        self.dim = dim
        self.max_position_embeddings = max_position_embeddings
        self.base = base
        self.scaling_factor = scaling_factor
        self.rope_type = rope_type

        self._compute_inverse_freq()

    def _compute_inverse_freq(self):
        """Compute inverse frequencies for RoPE"""

        if self.rope_type == "default":
            inv_freq = 1.0 / (
                self.base ** (torch.arange(0, self.dim, 2).float() / self.dim)
            )

        elif self.rope_type == "linear":
            # Linear interpolation for length extension
            inv_freq = 1.0 / (
                self.base ** (torch.arange(0, self.dim, 2).float() / self.dim)
            )
            inv_freq = inv_freq / self.scaling_factor

        elif self.rope_type == "dynamic":
            # Dynamic NTK scaling
            base = self.base * (
                (self.scaling_factor * self.max_position_embeddings / self.max_position_embeddings)
                - (self.scaling_factor - 1)
            ) ** (self.dim / (self.dim - 2))
            inv_freq = 1.0 / (
                base ** (torch.arange(0, self.dim, 2).float() / self.dim)
            )

        elif self.rope_type == "yarn":
            # YaRN (Yet another RoPE extensioN)
            # More sophisticated scaling for very long contexts
            inv_freq = self._compute_yarn_inv_freq()

        else:
            raise ValueError(f"Unknown rope_type: {self.rope_type}")

        self.register_buffer("inv_freq", inv_freq)

    def _compute_yarn_inv_freq(self):
        """Compute YaRN inverse frequencies"""
        # YaRN uses attention scaling and NTK-aware interpolation
        # Reference: https://arxiv.org/abs/2309.00071

        pos_freqs = self.base ** (torch.arange(0, self.dim, 2).float() / self.dim)
        inv_freq_extrapolation = 1.0 / pos_freqs
        inv_freq_interpolation = 1.0 / (self.scaling_factor * pos_freqs)

        # Blend based on wavelength
        low_freq_factor = 1.0
        high_freq_factor = 4.0

        low_freq_wavelen = self.max_position_embeddings / low_freq_factor
        high_freq_wavelen = self.max_position_embeddings / high_freq_factor

        wavelen = 2 * torch.pi / inv_freq_extrapolation

        # Compute blend ratio
        smooth = (wavelen - high_freq_wavelen) / (low_freq_wavelen - high_freq_wavelen)
        smooth = torch.clamp(smooth, 0.0, 1.0)

        inv_freq = (1 - smooth) * inv_freq_extrapolation + smooth * inv_freq_interpolation

        return inv_freq

    def forward(
        self,
        x: torch.Tensor,
        position_ids: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            x: Input tensor (used for dtype/device)
            position_ids: Position indices [B, S]

        Returns:
            cos, sin: Position embeddings
        """
        # Compute position embeddings
        inv_freq_expanded = self.inv_freq[None, :, None]  # [1, D/2, 1]
        position_ids_expanded = position_ids[:, None, :].float()  # [B, 1, S]

        freqs = inv_freq_expanded @ position_ids_expanded  # [B, D/2, S]
        freqs = freqs.transpose(1, 2)  # [B, S, D/2]

        emb = torch.cat([freqs, freqs], dim=-1)  # [B, S, D]

        cos = emb.cos()
        sin = emb.sin()

        return cos.to(x.dtype), sin.to(x.dtype)

# Length extension comparison
"""
RoPE Scaling Methods for Long Context:

1. Linear Interpolation (Position Interpolation):
   - Simple: scale position indices
   - Works up to ~2x training length
   - Quality degrades for longer

2. Dynamic NTK Scaling:
   - Adjusts base frequency
   - Better for 2-4x extension
   - Used in Code Llama

3. YaRN (Yet another RoPE extensioN):
   - Best quality for long contexts
   - Works for 8-32x extension
   - Used in Llama 3.1 (128K context)

4. Continuous Extension (ABF):
   - Attention scaling + frequency adjustment
   - Very long context (100K+)
"""
```

### 3.2 ALiBi (Attention with Linear Biases)

```python
class ALiBiPositionalBias(nn.Module):
    """
    Attention with Linear Biases (ALiBi).

    Adds a linear bias based on distance to attention scores.
    No learned parameters, excellent length generalization.
    """

    def __init__(self, num_heads: int, max_seq_len: int = 8192):
        super().__init__()
        self.num_heads = num_heads

        # Compute slopes (geometric sequence)
        slopes = self._get_alibi_slopes(num_heads)
        self.register_buffer("slopes", slopes)

        # Precompute bias matrix
        bias = self._build_alibi_bias(max_seq_len)
        self.register_buffer("bias", bias)

    def _get_alibi_slopes(self, num_heads: int) -> torch.Tensor:
        """Compute ALiBi slopes for each head"""

        def get_slopes_power_of_2(n):
            start = 2 ** (-(2 ** -(math.log2(n) - 3)))
            ratio = start
            return [start * (ratio ** i) for i in range(n)]

        if math.log2(num_heads).is_integer():
            slopes = get_slopes_power_of_2(num_heads)
        else:
            # Handle non-power-of-2 heads
            closest_power_of_2 = 2 ** math.floor(math.log2(num_heads))
            slopes = get_slopes_power_of_2(closest_power_of_2)
            extra = self._get_alibi_slopes(2 * closest_power_of_2)[0::2][:num_heads - closest_power_of_2]
            slopes = slopes + extra

        return torch.tensor(slopes)

    def _build_alibi_bias(self, max_seq_len: int) -> torch.Tensor:
        """Build ALiBi bias matrix"""

        # Create relative position matrix
        positions = torch.arange(max_seq_len)
        relative_positions = positions.unsqueeze(0) - positions.unsqueeze(1)

        # Apply slopes (causal: upper triangle is -inf)
        relative_positions = relative_positions.float()
        alibi = self.slopes.unsqueeze(-1).unsqueeze(-1) * relative_positions.unsqueeze(0)

        # Causal mask
        causal_mask = torch.triu(torch.ones(max_seq_len, max_seq_len) * float('-inf'), diagonal=1)
        alibi = alibi + causal_mask

        return alibi

    def forward(self, seq_len: int) -> torch.Tensor:
        """Get ALiBi bias for sequence length"""
        return self.bias[:, :seq_len, :seq_len]

# Usage in attention
class ALiBiAttention(nn.Module):
    def __init__(self, hidden_size, num_heads):
        super().__init__()
        self.alibi = ALiBiPositionalBias(num_heads)
        # ... other initialization

    def forward(self, hidden_states, attention_mask=None):
        # ... compute Q, K, V

        # Standard attention scores
        attn_weights = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)

        # Add ALiBi bias
        seq_len = hidden_states.shape[1]
        alibi_bias = self.alibi(seq_len)
        attn_weights = attn_weights + alibi_bias

        # ... rest of attention
```

### 3.3 Position Encoding Comparison

| Method | Learned | Relative | Extrapolation | Used By |
|--------|---------|----------|---------------|---------|
| Sinusoidal | No | No | Limited | Original Transformer |
| Learned Absolute | Yes | No | No | GPT-2, BERT |
| RoPE | No | Yes | Good | Llama, Mistral, Qwen |
| ALiBi | No | Yes | Excellent | Falcon, MPT, BLOOM |
| xPos | Partial | Yes | Good | Research |

---

## 4. Normalization Strategies

### 4.1 Pre-Norm vs Post-Norm

```python
class PreNormTransformerBlock(nn.Module):
    """Pre-norm: Norm before attention/FFN (modern default)"""

    def __init__(self, hidden_size, num_heads, ffn_size):
        super().__init__()
        self.norm1 = nn.LayerNorm(hidden_size)
        self.attn = MultiHeadAttention(hidden_size, num_heads)
        self.norm2 = nn.LayerNorm(hidden_size)
        self.ffn = FeedForward(hidden_size, ffn_size)

    def forward(self, x):
        # Pre-norm: Apply norm BEFORE sublayer
        x = x + self.attn(self.norm1(x))
        x = x + self.ffn(self.norm2(x))
        return x

class PostNormTransformerBlock(nn.Module):
    """Post-norm: Norm after attention/FFN (original Transformer)"""

    def __init__(self, hidden_size, num_heads, ffn_size):
        super().__init__()
        self.attn = MultiHeadAttention(hidden_size, num_heads)
        self.norm1 = nn.LayerNorm(hidden_size)
        self.ffn = FeedForward(hidden_size, ffn_size)
        self.norm2 = nn.LayerNorm(hidden_size)

    def forward(self, x):
        # Post-norm: Apply norm AFTER sublayer + residual
        x = self.norm1(x + self.attn(x))
        x = self.norm2(x + self.ffn(x))
        return x

"""
Pre-norm advantages:
- More stable training (gradient flow)
- Easier to train deep models
- Used by GPT-2, GPT-3, Llama, Mistral

Post-norm advantages:
- Slightly better final performance
- Original Transformer design
- Requires careful LR warmup

Modern consensus: Pre-norm with RMSNorm
"""
```

### 4.2 RMSNorm vs LayerNorm

```python
class RMSNorm(nn.Module):
    """
    Root Mean Square Layer Normalization.

    Advantages over LayerNorm:
    - ~10-15% faster (no mean computation)
    - Simpler (no centering)
    - Similar or better quality
    """

    def __init__(self, hidden_size: int, eps: float = 1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.eps = eps

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Compute RMS
        variance = x.pow(2).mean(-1, keepdim=True)
        x = x * torch.rsqrt(variance + self.eps)

        # Scale
        return self.weight * x

class LayerNorm(nn.Module):
    """Standard Layer Normalization"""

    def __init__(self, hidden_size: int, eps: float = 1e-5):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.bias = nn.Parameter(torch.zeros(hidden_size))
        self.eps = eps

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Compute mean and variance
        mean = x.mean(-1, keepdim=True)
        variance = x.var(-1, keepdim=True, unbiased=False)

        # Normalize
        x = (x - mean) / torch.sqrt(variance + self.eps)

        # Scale and shift
        return self.weight * x + self.bias

# Benchmark comparison
"""
On A100 GPU, hidden_size=4096, batch=32, seq=2048:
- LayerNorm: 0.82 ms
- RMSNorm: 0.71 ms
- Speedup: ~15%

Models using RMSNorm: Llama, Mistral, Qwen, Gemma
Models using LayerNorm: GPT-2, BERT, T5
"""
```

---

## 5. Feed-Forward Networks

### 5.1 FFN Variants

```python
class StandardFFN(nn.Module):
    """Standard Transformer FFN: Linear → ReLU → Linear"""

    def __init__(self, hidden_size: int, ffn_size: int, dropout: float = 0.0):
        super().__init__()
        self.fc1 = nn.Linear(hidden_size, ffn_size)
        self.fc2 = nn.Linear(ffn_size, hidden_size)
        self.activation = nn.ReLU()
        self.dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.fc1(x)
        x = self.activation(x)
        x = self.dropout(x)
        x = self.fc2(x)
        return x

class GatedFFN(nn.Module):
    """Gated FFN with GELU: (Linear₁ × GELU(Linear₂)) → Linear₃"""

    def __init__(self, hidden_size: int, ffn_size: int):
        super().__init__()
        self.gate_proj = nn.Linear(hidden_size, ffn_size, bias=False)
        self.up_proj = nn.Linear(hidden_size, ffn_size, bias=False)
        self.down_proj = nn.Linear(ffn_size, hidden_size, bias=False)
        self.activation = nn.GELU()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # GeGLU variant
        gate = self.activation(self.gate_proj(x))
        up = self.up_proj(x)
        return self.down_proj(gate * up)

class SwiGLUFFN(nn.Module):
    """SwiGLU FFN (Llama, Mistral standard)"""

    def __init__(self, hidden_size: int, ffn_size: int):
        super().__init__()
        self.gate_proj = nn.Linear(hidden_size, ffn_size, bias=False)
        self.up_proj = nn.Linear(hidden_size, ffn_size, bias=False)
        self.down_proj = nn.Linear(ffn_size, hidden_size, bias=False)
        self.activation = nn.SiLU()  # Swish

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # SwiGLU: SiLU(gate) × up
        return self.down_proj(self.activation(self.gate_proj(x)) * self.up_proj(x))

# FFN size calculation for SwiGLU
def compute_ffn_size(hidden_size: int, multiplier: float = 8/3) -> int:
    """
    Compute FFN intermediate size for SwiGLU.

    SwiGLU uses 3 matrices (gate, up, down) instead of 2,
    so we reduce the intermediate size to keep params constant.

    Standard FFN: 2 × hidden × 4 × hidden = 8 × hidden²
    SwiGLU: 3 × hidden × (8/3) × hidden = 8 × hidden²
    """
    ffn_size = int(hidden_size * multiplier)
    # Round to multiple of 256 for efficiency
    return ((ffn_size + 255) // 256) * 256

# Example sizes
for hidden in [2048, 4096, 8192]:
    ffn = compute_ffn_size(hidden)
    print(f"Hidden: {hidden}, FFN: {ffn}, Ratio: {ffn/hidden:.2f}")
```

---

## 6. Mixture of Experts (MoE)

### 6.1 MoE Architecture

```
┌─────────────────────────────────────────────────────────────┐
│              Mixture of Experts (MoE)                        │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  Standard FFN:                                              │
│  Input ───► [FFN] ───► Output                               │
│  Params: 2 × hidden × ffn                                    │
│                                                              │
│  MoE FFN:                                                   │
│                    ┌────► [Expert 1]                        │
│  Input ───► Router ├────► [Expert 2] ───► Weighted Sum      │
│                    ├────► [Expert 3]                        │
│                    └────► [Expert N]                        │
│                                                              │
│  Only top-k experts (usually k=2) activated per token       │
│  Total params: N × 2 × hidden × ffn                         │
│  Active params: k × 2 × hidden × ffn                        │
│                                                              │
│  Benefits:                                                  │
│  - Scale params without proportional compute                │
│  - Sparse activation → efficiency                           │
│  - Expert specialization                                    │
│                                                              │
│  Challenges:                                                │
│  - Load balancing across experts                            │
│  - Training instability                                     │
│  - Expert parallelism needed for large N                    │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### 6.2 MoE Implementation

```python
class MoERouter(nn.Module):
    """Router for Mixture of Experts"""

    def __init__(
        self,
        hidden_size: int,
        num_experts: int,
        top_k: int = 2,
        capacity_factor: float = 1.25,
        noise_std: float = 0.1
    ):
        super().__init__()
        self.num_experts = num_experts
        self.top_k = top_k
        self.capacity_factor = capacity_factor
        self.noise_std = noise_std

        self.gate = nn.Linear(hidden_size, num_experts, bias=False)

    def forward(
        self,
        hidden_states: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Args:
            hidden_states: [B, S, D]

        Returns:
            router_weights: [B, S, top_k] - weights for top-k experts
            router_indices: [B, S, top_k] - indices of top-k experts
            aux_loss: Load balancing loss
        """
        batch_size, seq_len, _ = hidden_states.shape

        # Compute router logits
        router_logits = self.gate(hidden_states)  # [B, S, num_experts]

        # Add noise during training for exploration
        if self.training and self.noise_std > 0:
            noise = torch.randn_like(router_logits) * self.noise_std
            router_logits = router_logits + noise

        # Get top-k experts
        router_probs = F.softmax(router_logits, dim=-1)
        router_weights, router_indices = torch.topk(router_probs, self.top_k, dim=-1)

        # Normalize weights
        router_weights = router_weights / router_weights.sum(dim=-1, keepdim=True)

        # Compute auxiliary loss for load balancing
        aux_loss = self._compute_load_balance_loss(router_probs)

        return router_weights, router_indices, aux_loss

    def _compute_load_balance_loss(self, router_probs: torch.Tensor) -> torch.Tensor:
        """Compute load balancing auxiliary loss"""
        # Mean routing probability per expert
        expert_usage = router_probs.mean(dim=[0, 1])  # [num_experts]

        # Ideal: uniform distribution
        ideal = 1.0 / self.num_experts

        # Loss: variance from uniform
        loss = self.num_experts * (expert_usage * (expert_usage - ideal)).sum()

        return loss

class MoELayer(nn.Module):
    """Mixture of Experts FFN layer"""

    def __init__(
        self,
        hidden_size: int,
        ffn_size: int,
        num_experts: int,
        top_k: int = 2
    ):
        super().__init__()

        self.num_experts = num_experts
        self.top_k = top_k

        # Router
        self.router = MoERouter(hidden_size, num_experts, top_k)

        # Experts (each is an FFN)
        self.experts = nn.ModuleList([
            SwiGLUFFN(hidden_size, ffn_size)
            for _ in range(num_experts)
        ])

    def forward(self, hidden_states: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            hidden_states: [B, S, D]

        Returns:
            output: [B, S, D]
            aux_loss: Load balancing loss
        """
        batch_size, seq_len, hidden_size = hidden_states.shape

        # Route
        router_weights, router_indices, aux_loss = self.router(hidden_states)

        # Initialize output
        output = torch.zeros_like(hidden_states)

        # Process each expert
        for expert_idx in range(self.num_experts):
            # Find tokens routed to this expert
            expert_mask = (router_indices == expert_idx).any(dim=-1)

            if not expert_mask.any():
                continue

            # Get tokens for this expert
            expert_input = hidden_states[expert_mask]

            # Compute expert output
            expert_output = self.experts[expert_idx](expert_input)

            # Get weights for this expert
            weight_mask = (router_indices == expert_idx)
            expert_weights = (router_weights * weight_mask.float()).sum(dim=-1)

            # Add weighted output
            output[expert_mask] += expert_weights[expert_mask].unsqueeze(-1) * expert_output

        return output, aux_loss

# Example: Mixtral-style configuration
MIXTRAL_CONFIG = {
    "hidden_size": 4096,
    "ffn_size": 14336,
    "num_experts": 8,
    "top_k": 2,
    "num_layers": 32
}
# Total params: ~47B (but only ~13B active per forward pass)
```

---

## 7. Scaling Laws & Model Sizing

### 7.1 Chinchilla Optimal Scaling

```python
import math
from typing import Tuple

def chinchilla_optimal_tokens(params_billions: float) -> float:
    """
    Compute optimal training tokens for Chinchilla scaling.

    According to Hoffmann et al. (2022):
    - Optimal ratio is ~20 tokens per parameter
    - N ∝ C^0.5, D ∝ C^0.5 (compute = N × D)
    """
    return params_billions * 1e9 * 20

def estimate_compute_flops(
    params: int,
    tokens: int,
    include_backward: bool = True
) -> float:
    """
    Estimate training FLOPs.

    Forward pass: ~2 × params × tokens (matrix multiplications)
    Backward pass: ~4 × params × tokens (gradients + updates)
    Total: ~6 × params × tokens
    """
    forward_flops = 2 * params * tokens

    if include_backward:
        return 3 * forward_flops  # 6N total
    return forward_flops

def optimal_model_for_budget(
    compute_budget_flops: float
) -> Tuple[float, float]:
    """
    Given compute budget, find optimal model size and tokens.

    From Chinchilla scaling laws:
    C = 6 × N × D
    Optimal: N ∝ C^0.5, D ∝ C^0.5

    Returns:
        (optimal_params, optimal_tokens)
    """
    # Chinchilla coefficients (approximate)
    alpha = 0.5  # N scales as C^alpha
    beta = 0.5   # D scales as C^beta

    # C = 6ND, N = k*C^0.5, D = (C/6N) = C^0.5 / (6k)
    # Empirically: optimal ratio D/N ≈ 20

    optimal_params = (compute_budget_flops / 120) ** 0.5
    optimal_tokens = 20 * optimal_params

    return optimal_params, optimal_tokens

# Scaling law examples
SCALING_EXAMPLES = {
    "1B params": {
        "chinchilla_tokens": "20B",
        "compute_flops": "1.2e20"
    },
    "7B params": {
        "chinchilla_tokens": "140B",
        "compute_flops": "5.9e21"
    },
    "70B params": {
        "chinchilla_tokens": "1.4T",
        "compute_flops": "5.9e23"
    },
    "405B params (Llama 3.1)": {
        "actual_tokens": "15T",  # Over-trained
        "compute_flops": "~4e25"
    }
}

# Note: Many modern models are "over-trained" beyond Chinchilla optimal
# because inference cost scales with model size, not training tokens.
# Smaller models trained longer can match larger models at lower inference cost.
```

### 7.2 Architecture Configurations by Scale

```python
MODEL_CONFIGS = {
    # Small models (1-3B) - Single GPU
    "1b": {
        "hidden_size": 2048,
        "num_layers": 24,
        "num_heads": 16,
        "num_kv_heads": 16,  # MHA for small models
        "ffn_size": 5504,
        "vocab_size": 32000,
        "max_seq_len": 4096,
        "params_approx": "1.3B"
    },
    "3b": {
        "hidden_size": 3072,
        "num_layers": 28,
        "num_heads": 24,
        "num_kv_heads": 8,  # GQA
        "ffn_size": 8192,
        "vocab_size": 32000,
        "max_seq_len": 4096,
        "params_approx": "3.2B"
    },

    # Medium models (7-14B) - Multi-GPU optional
    "7b": {
        "hidden_size": 4096,
        "num_layers": 32,
        "num_heads": 32,
        "num_kv_heads": 8,  # GQA: 4 queries per KV
        "ffn_size": 11008,
        "vocab_size": 32000,
        "max_seq_len": 8192,
        "params_approx": "7B"
    },
    "13b": {
        "hidden_size": 5120,
        "num_layers": 40,
        "num_heads": 40,
        "num_kv_heads": 8,
        "ffn_size": 13824,
        "vocab_size": 32000,
        "max_seq_len": 8192,
        "params_approx": "13B"
    },

    # Large models (30-70B) - Multi-GPU required
    "34b": {
        "hidden_size": 8192,
        "num_layers": 48,
        "num_heads": 64,
        "num_kv_heads": 8,
        "ffn_size": 22016,
        "vocab_size": 32000,
        "max_seq_len": 16384,
        "params_approx": "34B"
    },
    "70b": {
        "hidden_size": 8192,
        "num_layers": 80,
        "num_heads": 64,
        "num_kv_heads": 8,  # GQA with 8 KV heads
        "ffn_size": 28672,
        "vocab_size": 32000,
        "max_seq_len": 32768,
        "params_approx": "70B"
    },

    # Very large models (100B+) - Multi-node, consider MoE
    "405b": {
        "hidden_size": 16384,
        "num_layers": 126,
        "num_heads": 128,
        "num_kv_heads": 8,
        "ffn_size": 53248,
        "vocab_size": 128256,
        "max_seq_len": 131072,
        "params_approx": "405B"
    },

    # MoE variant (Mixtral-style)
    "8x7b_moe": {
        "hidden_size": 4096,
        "num_layers": 32,
        "num_heads": 32,
        "num_kv_heads": 8,
        "ffn_size": 14336,
        "num_experts": 8,
        "top_k_experts": 2,
        "vocab_size": 32000,
        "params_total": "47B",
        "params_active": "13B"
    }
}

def estimate_memory_requirements(config: dict) -> dict:
    """Estimate GPU memory requirements"""

    # Parameter memory
    params = (
        # Embeddings
        config["vocab_size"] * config["hidden_size"] +
        # Attention (Q, K, V, O projections)
        config["num_layers"] * 4 * config["hidden_size"] ** 2 +
        # FFN (gate, up, down)
        config["num_layers"] * 3 * config["hidden_size"] * config["ffn_size"] +
        # Layer norms
        config["num_layers"] * 2 * config["hidden_size"]
    )

    # Bytes per parameter
    fp32 = params * 4
    fp16 = params * 2

    # Optimizer states (Adam: 2x for momentum + variance)
    optimizer_fp32 = params * 8

    # Activations (rough estimate for batch_size=1, seq_len=2048)
    activations_per_layer = config["hidden_size"] * 2048 * 2  # FP16
    total_activations = activations_per_layer * config["num_layers"]

    return {
        "params_billions": params / 1e9,
        "model_fp16_gb": fp16 / 1e9,
        "model_fp32_gb": fp32 / 1e9,
        "optimizer_gb": optimizer_fp32 / 1e9,
        "activations_gb_per_seq": total_activations / 1e9,
        "total_training_gb": (fp16 + optimizer_fp32 + total_activations) / 1e9
    }
```

---

## 8. Architecture Decision Framework

### 8.1 Decision Tree

```
┌─────────────────────────────────────────────────────────────┐
│            Architecture Decision Framework                   │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  What is your primary use case?                             │
│  ├── Text Generation/Chat → Decoder-only                    │
│  ├── Classification/Retrieval → Encoder-only                │
│  └── Translation/Summarization → Encoder-decoder            │
│                                                              │
│  What is your model size target?                            │
│  ├── <3B params                                              │
│  │   └── MHA OK, consider GQA for inference                 │
│  ├── 3-20B params                                           │
│  │   └── GQA recommended                                    │
│  └── >20B params                                            │
│      └── GQA required, consider MoE                         │
│                                                              │
│  What is your context length?                               │
│  ├── <4K tokens → Standard RoPE                             │
│  ├── 4K-32K tokens → RoPE with scaling                      │
│  └── >32K tokens → YaRN or ALiBi                            │
│                                                              │
│  What is your inference priority?                           │
│  ├── Throughput → More KV sharing (MQA/GQA)                 │
│  ├── Latency → Smaller model, GQA                           │
│  └── Quality → MHA, larger model                            │
│                                                              │
│  Hardware constraints?                                       │
│  ├── Single GPU → <10B params, aggressive quantization      │
│  ├── 8x GPU → <100B params, tensor parallel                 │
│  └── Multi-node → Any size, full 3D parallelism             │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### 8.2 Configuration Templates

```python
def get_recommended_config(
    target_params_billions: float,
    use_case: str = "chat",
    inference_priority: str = "balanced",
    max_context: int = 8192
) -> dict:
    """Get recommended architecture configuration"""

    # Base configuration
    config = {
        "architecture": "decoder-only",
        "normalization": "rmsnorm",
        "position_encoding": "rope",
        "ffn_type": "swiglu",
        "activation": "silu"
    }

    # Size-based attention mechanism
    if target_params_billions < 3:
        config["attention"] = "mha"  # MHA for small models
        config["num_kv_groups"] = None
    elif target_params_billions < 20:
        config["attention"] = "gqa"
        config["num_kv_groups"] = 8
    else:
        config["attention"] = "gqa"
        config["num_kv_groups"] = 8
        if target_params_billions > 50:
            # Consider MoE for very large models
            config["consider_moe"] = True

    # Inference priority adjustments
    if inference_priority == "latency":
        config["num_kv_groups"] = min(config.get("num_kv_groups", 8), 4)
    elif inference_priority == "throughput":
        config["num_kv_groups"] = 1  # MQA

    # Context length configuration
    if max_context <= 4096:
        config["rope_scaling"] = None
    elif max_context <= 32768:
        config["rope_scaling"] = "linear"
        config["rope_scale_factor"] = max_context / 4096
    else:
        config["rope_scaling"] = "yarn"
        config["rope_scale_factor"] = max_context / 4096

    # Get dimensions from scaling laws
    config.update(get_dimensions_for_size(target_params_billions))

    return config

def get_dimensions_for_size(params_billions: float) -> dict:
    """Compute architecture dimensions for target parameter count"""

    # Approximate formulae based on common configurations
    if params_billions <= 1:
        hidden = 2048
        layers = 24
        heads = 16
    elif params_billions <= 3:
        hidden = 3072
        layers = 28
        heads = 24
    elif params_billions <= 7:
        hidden = 4096
        layers = 32
        heads = 32
    elif params_billions <= 13:
        hidden = 5120
        layers = 40
        heads = 40
    elif params_billions <= 34:
        hidden = 8192
        layers = 48
        heads = 64
    elif params_billions <= 70:
        hidden = 8192
        layers = 80
        heads = 64
    else:
        hidden = 16384
        layers = int(params_billions / 3)  # Rough approximation
        heads = 128

    ffn_size = int(hidden * 8 / 3)  # SwiGLU ratio
    ffn_size = ((ffn_size + 255) // 256) * 256  # Round to 256

    return {
        "hidden_size": hidden,
        "num_layers": layers,
        "num_heads": heads,
        "ffn_size": ffn_size
    }
```

---

## 9. Troubleshooting

### Common Issues

| Issue | Symptoms | Solution |
|-------|----------|----------|
| Training instability | Loss spikes, NaN | Reduce LR, add gradient clipping, check pre-norm |
| Poor quality | Low benchmark scores | Check attention mechanism, increase model size |
| OOM during training | CUDA out of memory | Reduce batch size, use gradient checkpointing |
| Slow inference | High latency | Use GQA/MQA, reduce context length |
| Length generalization | Poor perf on long context | Use RoPE scaling (YaRN), increase training length |

### Architecture Debugging

```python
def analyze_architecture(model):
    """Analyze model architecture for issues"""

    from torchinfo import summary

    # Model summary
    summary(model, input_data=torch.randint(0, 1000, (1, 512)))

    # Check for common issues
    issues = []

    # Check attention configuration
    for name, module in model.named_modules():
        if "attention" in name.lower():
            if hasattr(module, 'num_heads') and hasattr(module, 'num_kv_heads'):
                ratio = module.num_heads / module.num_kv_heads
                if ratio > 16:
                    issues.append(f"{name}: High Q/KV ratio ({ratio}x) may hurt quality")

    # Check normalization
    has_prenorm = False
    for name, module in model.named_modules():
        if "norm" in name.lower() and "pre" in name.lower():
            has_prenorm = True

    if not has_prenorm:
        issues.append("Consider pre-normalization for training stability")

    return issues
```

---

## Glossary

| Term | Definition |
|------|------------|
| **ALiBi** | Attention with Linear Biases - adds position-based bias to attention |
| **FFN** | Feed-Forward Network - the MLP after attention |
| **GQA** | Grouped Query Attention - groups of Q heads share K/V |
| **MHA** | Multi-Head Attention - standard separate K/V per head |
| **MoE** | Mixture of Experts - sparse activation of multiple FFNs |
| **MQA** | Multi-Query Attention - all Q heads share single K/V |
| **RMSNorm** | Root Mean Square Normalization |
| **RoPE** | Rotary Position Embedding |
| **SwiGLU** | Swish-Gated Linear Unit activation |

---

## References

- [GQA Paper](https://arxiv.org/abs/2305.13245)
- [RoPE Paper](https://arxiv.org/abs/2104.09864)
- [Llama Architecture](https://arxiv.org/abs/2302.13971)
- [Chinchilla Scaling Laws](https://arxiv.org/abs/2203.15556)
- [FlashAttention Paper](https://arxiv.org/abs/2205.14135)
- [Mixtral MoE](https://arxiv.org/abs/2401.04088)
- [Transformer Design Guide](https://rohitbandaru.github.io/blog/Transformer-Design-Guide-Pt2/)

---

> **Navigation**
> [← 2.1 Tokenizer](2.1_tokenizer_training_selection.md) | **[Index](../README.md#15-repository-structure)** | [2.3 Distributed Training →](2.3_distributed_training_infrastructure.md)
