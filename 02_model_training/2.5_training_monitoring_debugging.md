# Training Monitoring & Debugging Guide

## Executive Summary

Training large language models involves monitoring complex systems over extended periods where small issues can compound into catastrophic failures. This guide covers essential metrics, monitoring infrastructure, common training issues, and systematic debugging procedures to ensure successful training runs.

## Prerequisites

- Distributed training infrastructure setup (Document 2.3)
- Understanding of LLM architecture (Document 2.2)
- Familiarity with optimization algorithms (Adam, AdamW)
- Access to experiment tracking tools (W&B, MLflow)

---

## 2.5.1 Key Training Metrics

### Loss Metrics

```python
"""
Comprehensive training metrics tracking
"""
import torch
import numpy as np
from typing import Dict, List, Optional
from dataclasses import dataclass, field
from collections import deque
import time

@dataclass
class TrainingMetrics:
    """Container for all training metrics"""
    # Loss metrics
    train_loss: float = 0.0
    val_loss: float = 0.0
    per_domain_loss: Dict[str, float] = field(default_factory=dict)

    # Gradient metrics
    grad_norm: float = 0.0
    grad_norm_by_layer: Dict[str, float] = field(default_factory=dict)
    grad_variance: float = 0.0

    # Learning rate
    learning_rate: float = 0.0

    # Throughput
    tokens_per_second: float = 0.0
    samples_per_second: float = 0.0
    mfu: float = 0.0  # Model FLOPs Utilization

    # Memory
    gpu_memory_allocated: float = 0.0
    gpu_memory_reserved: float = 0.0
    peak_memory: float = 0.0

    # Timing
    step_time: float = 0.0
    forward_time: float = 0.0
    backward_time: float = 0.0
    optimizer_time: float = 0.0

class MetricsTracker:
    """
    Tracks and aggregates training metrics over time.
    """

    def __init__(
        self,
        window_size: int = 100,
        log_frequency: int = 10
    ):
        self.window_size = window_size
        self.log_frequency = log_frequency

        # Rolling windows for smoothing
        self.loss_window = deque(maxlen=window_size)
        self.grad_norm_window = deque(maxlen=window_size)
        self.throughput_window = deque(maxlen=window_size)

        # Historical data for trend analysis
        self.loss_history: List[float] = []
        self.grad_norm_history: List[float] = []

        # Anomaly detection
        self.loss_spike_threshold = 2.0  # Std deviations
        self.grad_spike_threshold = 5.0

        self.step = 0

    def update(self, metrics: TrainingMetrics) -> Dict:
        """
        Update tracker with new metrics.

        Returns:
            Dict of computed statistics and any detected anomalies
        """
        self.step += 1

        # Update windows
        self.loss_window.append(metrics.train_loss)
        self.grad_norm_window.append(metrics.grad_norm)
        self.throughput_window.append(metrics.tokens_per_second)

        # Update history
        self.loss_history.append(metrics.train_loss)
        self.grad_norm_history.append(metrics.grad_norm)

        # Compute statistics
        stats = self._compute_statistics(metrics)

        # Check for anomalies
        anomalies = self._detect_anomalies(metrics)

        return {**stats, "anomalies": anomalies}

    def _compute_statistics(self, metrics: TrainingMetrics) -> Dict:
        """Compute rolling statistics"""
        loss_array = np.array(self.loss_window)
        grad_array = np.array(self.grad_norm_window)

        return {
            "loss_mean": float(loss_array.mean()),
            "loss_std": float(loss_array.std()),
            "loss_trend": self._compute_trend(loss_array),
            "grad_norm_mean": float(grad_array.mean()),
            "grad_norm_std": float(grad_array.std()),
            "throughput_mean": float(np.mean(self.throughput_window)),
            "steps_per_second": 1.0 / metrics.step_time if metrics.step_time > 0 else 0,
        }

    def _compute_trend(self, values: np.ndarray) -> str:
        """Compute trend direction"""
        if len(values) < 10:
            return "insufficient_data"

        # Simple linear regression
        x = np.arange(len(values))
        slope = np.polyfit(x, values, 1)[0]

        if abs(slope) < 1e-6:
            return "stable"
        elif slope < 0:
            return "decreasing"
        else:
            return "increasing"

    def _detect_anomalies(self, metrics: TrainingMetrics) -> List[Dict]:
        """Detect training anomalies"""
        anomalies = []

        # Loss spike detection
        if len(self.loss_window) >= 10:
            loss_mean = np.mean(list(self.loss_window)[:-1])
            loss_std = np.std(list(self.loss_window)[:-1])

            if loss_std > 0:
                z_score = (metrics.train_loss - loss_mean) / loss_std
                if z_score > self.loss_spike_threshold:
                    anomalies.append({
                        "type": "loss_spike",
                        "severity": "high" if z_score > 5 else "medium",
                        "value": metrics.train_loss,
                        "z_score": z_score
                    })

        # Gradient spike detection
        if len(self.grad_norm_window) >= 10:
            grad_mean = np.mean(list(self.grad_norm_window)[:-1])
            grad_std = np.std(list(self.grad_norm_window)[:-1])

            if grad_std > 0:
                z_score = (metrics.grad_norm - grad_mean) / grad_std
                if z_score > self.grad_spike_threshold:
                    anomalies.append({
                        "type": "gradient_spike",
                        "severity": "high" if z_score > 10 else "medium",
                        "value": metrics.grad_norm,
                        "z_score": z_score
                    })

        # NaN/Inf detection
        if np.isnan(metrics.train_loss) or np.isinf(metrics.train_loss):
            anomalies.append({
                "type": "nan_loss",
                "severity": "critical",
                "value": metrics.train_loss
            })

        if np.isnan(metrics.grad_norm) or np.isinf(metrics.grad_norm):
            anomalies.append({
                "type": "nan_gradient",
                "severity": "critical",
                "value": metrics.grad_norm
            })

        # Training stall detection
        if len(self.loss_history) > 1000:
            recent = self.loss_history[-100:]
            earlier = self.loss_history[-1000:-900]

            if np.mean(recent) >= np.mean(earlier) * 0.99:
                anomalies.append({
                    "type": "training_stall",
                    "severity": "medium",
                    "recent_loss": np.mean(recent),
                    "earlier_loss": np.mean(earlier)
                })

        return anomalies

# Gradient metrics computation
class GradientAnalyzer:
    """
    Detailed gradient analysis for debugging.
    """

    def __init__(self, model: torch.nn.Module):
        self.model = model
        self.layer_names = []
        self.grad_hooks = []

        self._register_hooks()

    def _register_hooks(self):
        """Register backward hooks for gradient tracking"""
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.layer_names.append(name)

    def compute_gradient_metrics(self) -> Dict:
        """
        Compute comprehensive gradient metrics.
        """
        metrics = {
            "per_layer_norm": {},
            "per_layer_mean": {},
            "per_layer_std": {},
            "per_layer_max": {},
            "total_norm": 0.0,
            "max_grad": 0.0,
            "min_grad": float('inf'),
            "zero_grad_layers": [],
            "exploding_layers": [],
            "vanishing_layers": []
        }

        total_norm_sq = 0.0

        for name, param in self.model.named_parameters():
            if param.grad is not None:
                grad = param.grad.detach()

                # Per-layer metrics
                layer_norm = grad.norm().item()
                layer_mean = grad.mean().item()
                layer_std = grad.std().item()
                layer_max = grad.abs().max().item()

                metrics["per_layer_norm"][name] = layer_norm
                metrics["per_layer_mean"][name] = layer_mean
                metrics["per_layer_std"][name] = layer_std
                metrics["per_layer_max"][name] = layer_max

                total_norm_sq += layer_norm ** 2
                metrics["max_grad"] = max(metrics["max_grad"], layer_max)
                metrics["min_grad"] = min(metrics["min_grad"], grad.abs().min().item())

                # Detect issues
                if layer_norm < 1e-10:
                    metrics["zero_grad_layers"].append(name)
                elif layer_norm > 100:
                    metrics["exploding_layers"].append(name)
                elif layer_norm < 1e-7:
                    metrics["vanishing_layers"].append(name)

        metrics["total_norm"] = np.sqrt(total_norm_sq)

        return metrics

    def get_gradient_histogram(
        self,
        layer_name: str,
        num_bins: int = 50
    ) -> Dict:
        """Get histogram data for a layer's gradients"""
        for name, param in self.model.named_parameters():
            if name == layer_name and param.grad is not None:
                grad = param.grad.detach().cpu().numpy().flatten()
                hist, bin_edges = np.histogram(grad, bins=num_bins)
                return {
                    "histogram": hist.tolist(),
                    "bin_edges": bin_edges.tolist()
                }
        return {}
```

### Throughput Metrics

```python
"""
Throughput and efficiency metrics
"""
import torch
import time
from typing import Optional
from dataclasses import dataclass

@dataclass
class ThroughputMetrics:
    """Throughput and efficiency metrics"""
    tokens_per_second: float
    samples_per_second: float
    mfu: float  # Model FLOPs Utilization
    tflops: float
    gpu_utilization: float

class ThroughputTracker:
    """
    Track and compute throughput metrics.
    """

    def __init__(
        self,
        model_params: int,
        hidden_size: int,
        num_layers: int,
        sequence_length: int,
        batch_size: int,
        num_gpus: int = 1,
        gpu_peak_tflops: float = 312.0  # A100 BF16 peak
    ):
        self.model_params = model_params
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.sequence_length = sequence_length
        self.batch_size = batch_size
        self.num_gpus = num_gpus
        self.gpu_peak_tflops = gpu_peak_tflops

        # Compute theoretical FLOPs per token
        # Approximate: 6 * params for forward + backward
        self.flops_per_token = 6 * model_params

        self.start_time: Optional[float] = None
        self.tokens_processed = 0

    def start_step(self):
        """Mark start of a training step"""
        self.start_time = time.perf_counter()

    def end_step(self, tokens_in_batch: int) -> ThroughputMetrics:
        """
        Mark end of training step and compute metrics.

        Args:
            tokens_in_batch: Number of tokens processed in this step

        Returns:
            ThroughputMetrics for this step
        """
        if self.start_time is None:
            raise ValueError("Must call start_step() first")

        step_time = time.perf_counter() - self.start_time
        self.tokens_processed += tokens_in_batch

        # Compute metrics
        tokens_per_second = tokens_in_batch / step_time
        samples_per_second = self.batch_size / step_time

        # Compute achieved TFLOPs
        total_flops = tokens_in_batch * self.flops_per_token
        achieved_tflops = (total_flops / step_time) / 1e12

        # Model FLOPs Utilization
        theoretical_peak = self.gpu_peak_tflops * self.num_gpus
        mfu = achieved_tflops / theoretical_peak

        # GPU utilization (simplified - actual requires nvidia-smi)
        try:
            gpu_util = self._get_gpu_utilization()
        except Exception:
            gpu_util = 0.0

        self.start_time = None

        return ThroughputMetrics(
            tokens_per_second=tokens_per_second,
            samples_per_second=samples_per_second,
            mfu=mfu,
            tflops=achieved_tflops,
            gpu_utilization=gpu_util
        )

    def _get_gpu_utilization(self) -> float:
        """Get GPU utilization percentage"""
        try:
            import pynvml
            pynvml.nvmlInit()
            handle = pynvml.nvmlDeviceGetHandleByIndex(0)
            util = pynvml.nvmlDeviceGetUtilizationRates(handle)
            return util.gpu / 100.0
        except Exception:
            return 0.0

    def estimate_training_time(
        self,
        total_tokens: int,
        current_throughput: float
    ) -> Dict:
        """Estimate remaining training time"""
        remaining_tokens = total_tokens - self.tokens_processed

        if current_throughput > 0:
            remaining_seconds = remaining_tokens / current_throughput
            remaining_hours = remaining_seconds / 3600
            remaining_days = remaining_hours / 24

            return {
                "remaining_tokens": remaining_tokens,
                "remaining_hours": remaining_hours,
                "remaining_days": remaining_days,
                "progress_percent": (self.tokens_processed / total_tokens) * 100,
                "eta_seconds": remaining_seconds
            }
        return {}

# Memory tracking
class MemoryTracker:
    """Track GPU memory usage"""

    @staticmethod
    def get_memory_stats() -> Dict:
        """Get current memory statistics"""
        if not torch.cuda.is_available():
            return {}

        return {
            "allocated_gb": torch.cuda.memory_allocated() / 1e9,
            "reserved_gb": torch.cuda.memory_reserved() / 1e9,
            "max_allocated_gb": torch.cuda.max_memory_allocated() / 1e9,
            "max_reserved_gb": torch.cuda.max_memory_reserved() / 1e9
        }

    @staticmethod
    def reset_peak_stats():
        """Reset peak memory statistics"""
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()

    @staticmethod
    def get_memory_summary() -> str:
        """Get formatted memory summary"""
        if not torch.cuda.is_available():
            return "CUDA not available"

        stats = MemoryTracker.get_memory_stats()
        return (
            f"GPU Memory: {stats['allocated_gb']:.2f}GB allocated, "
            f"{stats['reserved_gb']:.2f}GB reserved, "
            f"Peak: {stats['max_allocated_gb']:.2f}GB"
        )
```

---

## 2.5.2 Monitoring Infrastructure

### Weights & Biases Integration

```python
"""
W&B integration for LLM training
"""
import wandb
from typing import Dict, Optional, Any
import torch

class WandBLogger:
    """
    Comprehensive W&B logging for LLM training.
    """

    def __init__(
        self,
        project: str,
        name: str,
        config: Dict,
        tags: Optional[list] = None,
        notes: Optional[str] = None,
        group: Optional[str] = None
    ):
        self.run = wandb.init(
            project=project,
            name=name,
            config=config,
            tags=tags,
            notes=notes,
            group=group,
            resume="allow"
        )

        # Define custom metrics
        wandb.define_metric("train/loss", summary="min")
        wandb.define_metric("val/loss", summary="min")
        wandb.define_metric("train/grad_norm", summary="mean")
        wandb.define_metric("train/learning_rate", summary="last")
        wandb.define_metric("throughput/tokens_per_sec", summary="mean")
        wandb.define_metric("throughput/mfu", summary="mean")

    def log_training_step(
        self,
        step: int,
        metrics: Dict[str, float],
        commit: bool = True
    ):
        """Log metrics for a training step"""
        # Prefix metrics appropriately
        log_dict = {"step": step}

        for key, value in metrics.items():
            if key.startswith("loss"):
                log_dict[f"train/{key}"] = value
            elif key.startswith("grad"):
                log_dict[f"train/{key}"] = value
            elif key in ["learning_rate", "lr"]:
                log_dict["train/learning_rate"] = value
            elif key.startswith("throughput") or key in ["tokens_per_second", "mfu"]:
                log_dict[f"throughput/{key}"] = value
            elif key.startswith("memory"):
                log_dict[f"memory/{key}"] = value
            else:
                log_dict[key] = value

        wandb.log(log_dict, step=step, commit=commit)

    def log_validation(
        self,
        step: int,
        val_loss: float,
        per_domain_loss: Optional[Dict[str, float]] = None,
        benchmarks: Optional[Dict[str, float]] = None
    ):
        """Log validation metrics"""
        log_dict = {
            "val/loss": val_loss,
            "step": step
        }

        if per_domain_loss:
            for domain, loss in per_domain_loss.items():
                log_dict[f"val/loss_{domain}"] = loss

        if benchmarks:
            for benchmark, score in benchmarks.items():
                log_dict[f"benchmarks/{benchmark}"] = score

        wandb.log(log_dict, step=step)

    def log_gradients(
        self,
        step: int,
        model: torch.nn.Module,
        log_histograms: bool = False
    ):
        """Log gradient statistics"""
        grad_metrics = {}

        for name, param in model.named_parameters():
            if param.grad is not None:
                grad = param.grad

                # Log per-layer gradient norm
                grad_metrics[f"gradients/{name}/norm"] = grad.norm().item()

                # Optionally log histograms (expensive)
                if log_histograms and step % 100 == 0:
                    wandb.log({
                        f"gradients/{name}/histogram": wandb.Histogram(
                            grad.detach().cpu().numpy().flatten()
                        )
                    }, step=step, commit=False)

        # Total gradient norm
        total_norm = sum(v ** 2 for k, v in grad_metrics.items() if "/norm" in k) ** 0.5
        grad_metrics["gradients/total_norm"] = total_norm

        wandb.log(grad_metrics, step=step, commit=False)

    def log_model_checkpoint(
        self,
        step: int,
        checkpoint_path: str,
        metadata: Optional[Dict] = None
    ):
        """Log model checkpoint as artifact"""
        artifact = wandb.Artifact(
            name=f"model-checkpoint-{step}",
            type="model",
            metadata=metadata
        )
        artifact.add_file(checkpoint_path)
        self.run.log_artifact(artifact)

    def log_attention_patterns(
        self,
        step: int,
        attention_weights: torch.Tensor,
        layer_idx: int = 0,
        head_idx: int = 0
    ):
        """Log attention pattern visualization"""
        import matplotlib.pyplot as plt

        # Get single head attention
        attn = attention_weights[0, head_idx].detach().cpu().numpy()

        fig, ax = plt.subplots(figsize=(10, 10))
        im = ax.imshow(attn, cmap='viridis')
        ax.set_title(f"Layer {layer_idx}, Head {head_idx}")
        plt.colorbar(im)

        wandb.log({
            f"attention/layer_{layer_idx}_head_{head_idx}": wandb.Image(fig)
        }, step=step)

        plt.close(fig)

    def alert(self, title: str, text: str, level: str = "WARN"):
        """Send an alert"""
        wandb.alert(
            title=title,
            text=text,
            level=getattr(wandb.AlertLevel, level)
        )

    def finish(self):
        """Finish the run"""
        wandb.finish()

# W&B configuration template
WANDB_CONFIG_TEMPLATE = {
    # Model configuration
    "model": {
        "name": "llama-7b",
        "params": 7_000_000_000,
        "hidden_size": 4096,
        "num_layers": 32,
        "num_heads": 32,
        "vocab_size": 32000,
    },
    # Training configuration
    "training": {
        "batch_size": 2048,
        "sequence_length": 4096,
        "learning_rate": 3e-4,
        "warmup_steps": 2000,
        "total_steps": 100000,
        "weight_decay": 0.1,
        "grad_clip": 1.0,
    },
    # Data configuration
    "data": {
        "total_tokens": 2_000_000_000_000,
        "data_mix": {
            "web": 0.5,
            "code": 0.15,
            "books": 0.1,
        }
    },
    # Infrastructure
    "infrastructure": {
        "num_gpus": 64,
        "gpu_type": "A100-80GB",
        "distributed_strategy": "FSDP",
    }
}
```

### Custom Prometheus Metrics

```python
"""
Prometheus metrics for LLM training monitoring
"""
from prometheus_client import Gauge, Counter, Histogram, start_http_server
from typing import Optional

class PrometheusMetrics:
    """
    Prometheus metrics exporter for training monitoring.
    """

    def __init__(self, port: int = 8000):
        # Loss metrics
        self.train_loss = Gauge(
            'llm_training_loss',
            'Current training loss',
            ['model_name', 'run_id']
        )
        self.val_loss = Gauge(
            'llm_validation_loss',
            'Current validation loss',
            ['model_name', 'run_id']
        )

        # Gradient metrics
        self.grad_norm = Gauge(
            'llm_gradient_norm',
            'Global gradient norm',
            ['model_name', 'run_id']
        )

        # Throughput metrics
        self.tokens_per_second = Gauge(
            'llm_tokens_per_second',
            'Training throughput in tokens/sec',
            ['model_name', 'run_id']
        )
        self.mfu = Gauge(
            'llm_model_flops_utilization',
            'Model FLOPs utilization',
            ['model_name', 'run_id']
        )

        # Progress metrics
        self.training_step = Counter(
            'llm_training_steps_total',
            'Total training steps completed',
            ['model_name', 'run_id']
        )
        self.tokens_processed = Counter(
            'llm_tokens_processed_total',
            'Total tokens processed',
            ['model_name', 'run_id']
        )

        # Memory metrics
        self.gpu_memory = Gauge(
            'llm_gpu_memory_bytes',
            'GPU memory usage in bytes',
            ['model_name', 'run_id', 'gpu_id', 'memory_type']
        )

        # Latency histograms
        self.step_duration = Histogram(
            'llm_step_duration_seconds',
            'Training step duration',
            ['model_name', 'run_id'],
            buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0]
        )

        # Anomaly counters
        self.loss_spikes = Counter(
            'llm_loss_spikes_total',
            'Number of loss spikes detected',
            ['model_name', 'run_id', 'severity']
        )
        self.gradient_spikes = Counter(
            'llm_gradient_spikes_total',
            'Number of gradient spikes detected',
            ['model_name', 'run_id', 'severity']
        )

        # Start HTTP server
        start_http_server(port)
        print(f"Prometheus metrics server started on port {port}")

    def update(
        self,
        model_name: str,
        run_id: str,
        metrics: dict,
        gpu_id: int = 0
    ):
        """Update all metrics"""
        labels = {'model_name': model_name, 'run_id': run_id}

        # Update loss
        if 'train_loss' in metrics:
            self.train_loss.labels(**labels).set(metrics['train_loss'])
        if 'val_loss' in metrics:
            self.val_loss.labels(**labels).set(metrics['val_loss'])

        # Update gradients
        if 'grad_norm' in metrics:
            self.grad_norm.labels(**labels).set(metrics['grad_norm'])

        # Update throughput
        if 'tokens_per_second' in metrics:
            self.tokens_per_second.labels(**labels).set(metrics['tokens_per_second'])
        if 'mfu' in metrics:
            self.mfu.labels(**labels).set(metrics['mfu'])

        # Update progress
        if 'step' in metrics:
            self.training_step.labels(**labels).inc()
        if 'tokens_in_batch' in metrics:
            self.tokens_processed.labels(**labels).inc(metrics['tokens_in_batch'])

        # Update memory
        if 'gpu_memory_allocated' in metrics:
            self.gpu_memory.labels(
                **labels, gpu_id=str(gpu_id), memory_type='allocated'
            ).set(metrics['gpu_memory_allocated'])
        if 'gpu_memory_reserved' in metrics:
            self.gpu_memory.labels(
                **labels, gpu_id=str(gpu_id), memory_type='reserved'
            ).set(metrics['gpu_memory_reserved'])

        # Update step duration
        if 'step_time' in metrics:
            self.step_duration.labels(**labels).observe(metrics['step_time'])
```

### Alert Configuration

```python
"""
Alert configuration for training monitoring
"""
from dataclasses import dataclass
from typing import Callable, List, Optional
from enum import Enum
import smtplib
from email.mime.text import MIMEText
import requests
import json

class AlertSeverity(Enum):
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"

@dataclass
class AlertRule:
    """Definition of an alert rule"""
    name: str
    condition: Callable[[dict], bool]
    severity: AlertSeverity
    message_template: str
    cooldown_seconds: int = 300  # Don't re-alert for 5 minutes

@dataclass
class AlertChannel:
    """Alert delivery channel"""
    name: str
    send_func: Callable[[str, str, AlertSeverity], None]
    min_severity: AlertSeverity = AlertSeverity.WARNING

class AlertManager:
    """
    Manages alerts for training monitoring.
    """

    def __init__(self):
        self.rules: List[AlertRule] = []
        self.channels: List[AlertChannel] = []
        self.last_alert_times: dict = {}

    def add_rule(self, rule: AlertRule):
        """Add an alert rule"""
        self.rules.append(rule)

    def add_channel(self, channel: AlertChannel):
        """Add an alert channel"""
        self.channels.append(channel)

    def check_and_alert(self, metrics: dict, step: int):
        """Check all rules and send alerts if triggered"""
        import time
        current_time = time.time()

        for rule in self.rules:
            # Check cooldown
            last_alert = self.last_alert_times.get(rule.name, 0)
            if current_time - last_alert < rule.cooldown_seconds:
                continue

            # Check condition
            if rule.condition(metrics):
                message = rule.message_template.format(
                    step=step,
                    **metrics
                )

                # Send to appropriate channels
                for channel in self.channels:
                    if rule.severity.value >= channel.min_severity.value:
                        try:
                            channel.send_func(rule.name, message, rule.severity)
                        except Exception as e:
                            print(f"Failed to send alert to {channel.name}: {e}")

                self.last_alert_times[rule.name] = current_time

# Pre-defined alert rules
def create_default_alert_rules() -> List[AlertRule]:
    """Create default alert rules for LLM training"""
    return [
        AlertRule(
            name="loss_spike",
            condition=lambda m: m.get('loss_spike_detected', False),
            severity=AlertSeverity.WARNING,
            message_template="Loss spike detected at step {step}. Current loss: {train_loss:.4f}",
            cooldown_seconds=300
        ),
        AlertRule(
            name="nan_loss",
            condition=lambda m: m.get('nan_detected', False),
            severity=AlertSeverity.CRITICAL,
            message_template="NaN loss detected at step {step}! Training may have diverged.",
            cooldown_seconds=60
        ),
        AlertRule(
            name="gradient_explosion",
            condition=lambda m: m.get('grad_norm', 0) > 100,
            severity=AlertSeverity.ERROR,
            message_template="Gradient explosion at step {step}. Grad norm: {grad_norm:.2f}",
            cooldown_seconds=120
        ),
        AlertRule(
            name="low_throughput",
            condition=lambda m: m.get('mfu', 1.0) < 0.3,
            severity=AlertSeverity.WARNING,
            message_template="Low MFU at step {step}: {mfu:.2%}. Check for bottlenecks.",
            cooldown_seconds=600
        ),
        AlertRule(
            name="memory_pressure",
            condition=lambda m: m.get('gpu_memory_percent', 0) > 95,
            severity=AlertSeverity.WARNING,
            message_template="High GPU memory usage at step {step}: {gpu_memory_percent:.1f}%",
            cooldown_seconds=300
        ),
        AlertRule(
            name="training_stall",
            condition=lambda m: m.get('loss_trend') == 'increasing' and m.get('step', 0) > 1000,
            severity=AlertSeverity.WARNING,
            message_template="Training may be stalling at step {step}. Loss trend: increasing",
            cooldown_seconds=1800
        ),
    ]

# Alert channel implementations
def create_slack_channel(webhook_url: str) -> AlertChannel:
    """Create Slack alert channel"""
    def send_slack(title: str, message: str, severity: AlertSeverity):
        color_map = {
            AlertSeverity.INFO: "#36a64f",
            AlertSeverity.WARNING: "#ffcc00",
            AlertSeverity.ERROR: "#ff6600",
            AlertSeverity.CRITICAL: "#ff0000"
        }

        payload = {
            "attachments": [{
                "color": color_map[severity],
                "title": f"[{severity.value.upper()}] {title}",
                "text": message,
                "footer": "LLM Training Monitor"
            }]
        }

        requests.post(webhook_url, json=payload)

    return AlertChannel(name="slack", send_func=send_slack)

def create_pagerduty_channel(routing_key: str) -> AlertChannel:
    """Create PagerDuty alert channel"""
    def send_pagerduty(title: str, message: str, severity: AlertSeverity):
        # Only send critical alerts to PagerDuty
        if severity != AlertSeverity.CRITICAL:
            return

        payload = {
            "routing_key": routing_key,
            "event_action": "trigger",
            "payload": {
                "summary": f"{title}: {message}",
                "source": "llm-training-monitor",
                "severity": "critical"
            }
        }

        requests.post(
            "https://events.pagerduty.com/v2/enqueue",
            json=payload
        )

    return AlertChannel(
        name="pagerduty",
        send_func=send_pagerduty,
        min_severity=AlertSeverity.CRITICAL
    )
```

---

## 2.5.3 Common Training Issues

### Loss Spikes

```python
"""
Loss spike detection and mitigation
"""
from typing import Dict, List, Optional, Tuple
import numpy as np
import torch

class LossSpikeHandler:
    """
    Detect and handle loss spikes during training.
    """

    def __init__(
        self,
        spike_threshold: float = 2.0,  # Std deviations
        recovery_patience: int = 10,   # Steps to wait for recovery
        max_rollbacks: int = 3         # Max checkpoint rollbacks
    ):
        self.spike_threshold = spike_threshold
        self.recovery_patience = recovery_patience
        self.max_rollbacks = max_rollbacks

        self.loss_history: List[float] = []
        self.spike_count = 0
        self.rollback_count = 0
        self.in_recovery = False
        self.recovery_start_step = 0

    def check_spike(self, loss: float, step: int) -> Tuple[bool, Dict]:
        """
        Check if current loss is a spike.

        Returns:
            Tuple of (is_spike, diagnostic_info)
        """
        self.loss_history.append(loss)

        # Need enough history
        if len(self.loss_history) < 50:
            return False, {}

        # Compute statistics on recent history (excluding current)
        recent = np.array(self.loss_history[-100:-1])
        mean = recent.mean()
        std = recent.std()

        if std == 0:
            return False, {}

        z_score = (loss - mean) / std

        is_spike = z_score > self.spike_threshold

        diagnostic = {
            "z_score": z_score,
            "loss": loss,
            "mean": mean,
            "std": std,
            "threshold": self.spike_threshold
        }

        if is_spike:
            self.spike_count += 1
            self.in_recovery = True
            self.recovery_start_step = step

        return is_spike, diagnostic

    def should_rollback(self, step: int, current_loss: float) -> Tuple[bool, str]:
        """
        Determine if we should rollback to a checkpoint.

        Returns:
            Tuple of (should_rollback, reason)
        """
        if not self.in_recovery:
            return False, ""

        # Check if loss has recovered
        if len(self.loss_history) >= 50:
            pre_spike_mean = np.mean(self.loss_history[-100:-10])
            if current_loss < pre_spike_mean * 1.1:
                self.in_recovery = False
                return False, "Loss recovered naturally"

        # Check if patience exceeded
        if step - self.recovery_start_step > self.recovery_patience:
            if self.rollback_count < self.max_rollbacks:
                self.rollback_count += 1
                return True, f"Loss did not recover after {self.recovery_patience} steps"
            else:
                return False, f"Max rollbacks ({self.max_rollbacks}) exceeded"

        return False, "Waiting for recovery"

    @staticmethod
    def diagnose_spike_cause(
        metrics_before: Dict,
        metrics_during: Dict,
        batch_info: Optional[Dict] = None
    ) -> List[str]:
        """
        Diagnose probable cause of loss spike.

        Returns:
            List of probable causes
        """
        causes = []

        # Check gradient norm
        if metrics_during.get('grad_norm', 0) > 10 * metrics_before.get('grad_norm', 1):
            causes.append("Gradient explosion - consider reducing learning rate or increasing gradient clipping")

        # Check for bad batch
        if batch_info:
            if batch_info.get('contains_outliers', False):
                causes.append("Batch contains outlier samples - check data preprocessing")
            if batch_info.get('high_perplexity', False):
                causes.append("Batch has unusually high perplexity - may contain corrupted data")

        # Check learning rate
        lr_before = metrics_before.get('learning_rate', 0)
        lr_during = metrics_during.get('learning_rate', 0)
        if lr_during > lr_before * 1.5:
            causes.append("Learning rate increased significantly - check warmup schedule")

        # Check memory
        if metrics_during.get('gpu_memory_percent', 0) > 95:
            causes.append("Near OOM - gradient accumulation may have caused numerical issues")

        if not causes:
            causes.append("Unknown cause - check for data corruption or numerical instability")

        return causes

# Spike mitigation strategies
class SpikeMitigation:
    """
    Mitigation strategies for loss spikes.
    """

    @staticmethod
    def reduce_learning_rate(
        optimizer: torch.optim.Optimizer,
        factor: float = 0.5
    ):
        """Temporarily reduce learning rate"""
        for param_group in optimizer.param_groups:
            param_group['lr'] *= factor

    @staticmethod
    def increase_gradient_clipping(
        current_clip: float,
        factor: float = 0.5
    ) -> float:
        """Reduce gradient clipping threshold"""
        return current_clip * factor

    @staticmethod
    def skip_batch():
        """Signal to skip the current batch"""
        # Implementation depends on training loop
        pass

    @staticmethod
    def rollback_to_checkpoint(
        model: torch.nn.Module,
        optimizer: torch.optim.Optimizer,
        checkpoint_path: str
    ):
        """Rollback to a previous checkpoint"""
        checkpoint = torch.load(checkpoint_path)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        return checkpoint.get('step', 0)
```

### Gradient Issues

```python
"""
Gradient issue detection and debugging
"""
import torch
import torch.nn as nn
from typing import Dict, List, Tuple, Optional
import numpy as np

class GradientDebugger:
    """
    Debug gradient-related training issues.
    """

    def __init__(self, model: nn.Module):
        self.model = model
        self.grad_history: Dict[str, List[float]] = {}
        self.hooks = []

    def register_hooks(self):
        """Register hooks for gradient tracking"""
        def make_hook(name):
            def hook(grad):
                if name not in self.grad_history:
                    self.grad_history[name] = []
                self.grad_history[name].append(grad.norm().item())
                return grad
            return hook

        for name, param in self.model.named_parameters():
            if param.requires_grad:
                hook = param.register_hook(make_hook(name))
                self.hooks.append(hook)

    def remove_hooks(self):
        """Remove all gradient hooks"""
        for hook in self.hooks:
            hook.remove()
        self.hooks = []

    def diagnose_vanishing_gradients(self) -> Dict:
        """
        Diagnose vanishing gradient problem.
        """
        diagnosis = {
            "vanishing_layers": [],
            "healthy_layers": [],
            "gradient_flow_score": 0.0
        }

        for name, param in self.model.named_parameters():
            if param.grad is not None:
                grad_norm = param.grad.norm().item()

                if grad_norm < 1e-7:
                    diagnosis["vanishing_layers"].append({
                        "name": name,
                        "grad_norm": grad_norm,
                        "param_norm": param.norm().item()
                    })
                else:
                    diagnosis["healthy_layers"].append(name)

        # Compute flow score (fraction of healthy layers)
        total = len(diagnosis["vanishing_layers"]) + len(diagnosis["healthy_layers"])
        if total > 0:
            diagnosis["gradient_flow_score"] = len(diagnosis["healthy_layers"]) / total

        return diagnosis

    def diagnose_exploding_gradients(self, threshold: float = 100.0) -> Dict:
        """
        Diagnose exploding gradient problem.
        """
        diagnosis = {
            "exploding_layers": [],
            "max_grad_norm": 0.0,
            "total_grad_norm": 0.0
        }

        total_norm_sq = 0.0

        for name, param in self.model.named_parameters():
            if param.grad is not None:
                grad_norm = param.grad.norm().item()
                total_norm_sq += grad_norm ** 2

                if grad_norm > threshold:
                    diagnosis["exploding_layers"].append({
                        "name": name,
                        "grad_norm": grad_norm,
                        "max_value": param.grad.abs().max().item()
                    })

                diagnosis["max_grad_norm"] = max(diagnosis["max_grad_norm"], grad_norm)

        diagnosis["total_grad_norm"] = np.sqrt(total_norm_sq)

        return diagnosis

    def check_dead_neurons(self) -> Dict:
        """
        Check for dead neurons (always zero activation).
        """
        dead_neurons = {}

        for name, module in self.model.named_modules():
            if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d)):
                if hasattr(module, 'weight') and module.weight.grad is not None:
                    # Check for rows with zero gradient (dead output neurons)
                    grad = module.weight.grad
                    dead_rows = (grad.abs().sum(dim=-1) < 1e-10).sum().item()
                    if dead_rows > 0:
                        dead_neurons[name] = {
                            "dead_count": dead_rows,
                            "total": grad.shape[0],
                            "percent": dead_rows / grad.shape[0] * 100
                        }

        return dead_neurons

    def analyze_gradient_distribution(self) -> Dict[str, Dict]:
        """
        Analyze gradient distribution per layer.
        """
        distributions = {}

        for name, param in self.model.named_parameters():
            if param.grad is not None:
                grad = param.grad.detach().cpu().numpy().flatten()

                distributions[name] = {
                    "mean": float(np.mean(grad)),
                    "std": float(np.std(grad)),
                    "min": float(np.min(grad)),
                    "max": float(np.max(grad)),
                    "percentile_1": float(np.percentile(grad, 1)),
                    "percentile_99": float(np.percentile(grad, 99)),
                    "sparsity": float((np.abs(grad) < 1e-10).mean())
                }

        return distributions

# SPAM optimizer for spike-aware training
class SPAMAdam(torch.optim.Optimizer):
    """
    Spike-Aware Adam with Momentum Reset.
    Based on: https://arxiv.org/abs/2501.06842

    Resets momentum when detecting gradient spikes to
    prevent them from propagating through training.
    """

    def __init__(
        self,
        params,
        lr: float = 1e-4,
        betas: Tuple[float, float] = (0.9, 0.999),
        eps: float = 1e-8,
        weight_decay: float = 0.0,
        spike_threshold: float = 5.0,
        clip_threshold: float = 1.0
    ):
        defaults = dict(
            lr=lr, betas=betas, eps=eps,
            weight_decay=weight_decay,
            spike_threshold=spike_threshold,
            clip_threshold=clip_threshold
        )
        super().__init__(params, defaults)

    @torch.no_grad()
    def step(self, closure=None):
        loss = None
        if closure is not None:
            with torch.enable_grad():
                loss = closure()

        for group in self.param_groups:
            for p in group['params']:
                if p.grad is None:
                    continue

                grad = p.grad
                if grad.is_sparse:
                    raise RuntimeError("SPAM does not support sparse gradients")

                state = self.state[p]

                # Initialize state
                if len(state) == 0:
                    state['step'] = 0
                    state['exp_avg'] = torch.zeros_like(p)
                    state['exp_avg_sq'] = torch.zeros_like(p)
                    state['grad_norm_ema'] = 0.0

                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']
                beta1, beta2 = group['betas']

                state['step'] += 1

                # Compute gradient norm
                grad_norm = grad.norm().item()

                # Update gradient norm EMA
                if state['step'] == 1:
                    state['grad_norm_ema'] = grad_norm
                else:
                    state['grad_norm_ema'] = 0.99 * state['grad_norm_ema'] + 0.01 * grad_norm

                # Detect spike
                is_spike = grad_norm > group['spike_threshold'] * state['grad_norm_ema']

                if is_spike:
                    # Reset momentum
                    exp_avg.zero_()

                    # Clip gradient
                    clip_coef = group['clip_threshold'] * state['grad_norm_ema'] / (grad_norm + 1e-6)
                    grad = grad * clip_coef

                # Weight decay
                if group['weight_decay'] != 0:
                    p.add_(p, alpha=-group['lr'] * group['weight_decay'])

                # Adam update
                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)

                bias_correction1 = 1 - beta1 ** state['step']
                bias_correction2 = 1 - beta2 ** state['step']

                step_size = group['lr'] / bias_correction1
                denom = (exp_avg_sq.sqrt() / np.sqrt(bias_correction2)).add_(group['eps'])

                p.addcdiv_(exp_avg, denom, value=-step_size)

        return loss
```

### Numerical Stability

```python
"""
Numerical stability checks and fixes
"""
import torch
import torch.nn as nn
from typing import Dict, List, Optional

class NumericalStabilityChecker:
    """
    Check for numerical stability issues in training.
    """

    @staticmethod
    def check_model_parameters(model: nn.Module) -> Dict:
        """Check model parameters for numerical issues"""
        issues = {
            "nan_params": [],
            "inf_params": [],
            "very_large_params": [],
            "very_small_params": []
        }

        for name, param in model.named_parameters():
            if torch.isnan(param).any():
                issues["nan_params"].append(name)
            if torch.isinf(param).any():
                issues["inf_params"].append(name)
            if param.abs().max() > 1e6:
                issues["very_large_params"].append({
                    "name": name,
                    "max": param.abs().max().item()
                })
            if param.abs().max() < 1e-10 and param.numel() > 0:
                issues["very_small_params"].append(name)

        return issues

    @staticmethod
    def check_activations(
        model: nn.Module,
        sample_input: torch.Tensor
    ) -> Dict:
        """
        Check for numerical issues in activations.
        """
        activations = {}
        hooks = []

        def make_hook(name):
            def hook(module, input, output):
                if isinstance(output, torch.Tensor):
                    activations[name] = {
                        "has_nan": torch.isnan(output).any().item(),
                        "has_inf": torch.isinf(output).any().item(),
                        "max": output.abs().max().item(),
                        "mean": output.mean().item(),
                        "std": output.std().item()
                    }
            return hook

        # Register hooks
        for name, module in model.named_modules():
            hook = module.register_forward_hook(make_hook(name))
            hooks.append(hook)

        # Forward pass
        with torch.no_grad():
            model(sample_input)

        # Remove hooks
        for hook in hooks:
            hook.remove()

        # Find issues
        issues = {
            "nan_activations": [],
            "inf_activations": [],
            "saturated_activations": []
        }

        for name, stats in activations.items():
            if stats["has_nan"]:
                issues["nan_activations"].append(name)
            if stats["has_inf"]:
                issues["inf_activations"].append(name)
            if stats["std"] < 1e-6:
                issues["saturated_activations"].append(name)

        return {"activations": activations, "issues": issues}

    @staticmethod
    def apply_numerical_fixes(model: nn.Module):
        """
        Apply common numerical stability fixes.
        """
        for name, module in model.named_modules():
            # Fix LayerNorm eps
            if isinstance(module, nn.LayerNorm):
                if module.eps < 1e-6:
                    module.eps = 1e-6

            # Fix attention scaling
            if hasattr(module, 'scale') and module.scale is not None:
                if module.scale < 1e-4:
                    print(f"Warning: Very small attention scale in {name}")

        return model

# Mixed precision stability
class MixedPrecisionStabilizer:
    """
    Tools for stable mixed precision training.
    """

    def __init__(self, loss_scale: float = 2**16, growth_interval: int = 2000):
        self.loss_scale = loss_scale
        self.growth_interval = growth_interval
        self.growth_factor = 2.0
        self.backoff_factor = 0.5
        self.min_loss_scale = 1.0
        self.max_loss_scale = 2**24

        self.steps_since_growth = 0
        self.overflow_count = 0

    def scale_loss(self, loss: torch.Tensor) -> torch.Tensor:
        """Scale loss for mixed precision"""
        return loss * self.loss_scale

    def unscale_gradients(self, optimizer: torch.optim.Optimizer):
        """Unscale gradients after backward"""
        for group in optimizer.param_groups:
            for param in group['params']:
                if param.grad is not None:
                    param.grad.data.div_(self.loss_scale)

    def check_overflow(self, optimizer: torch.optim.Optimizer) -> bool:
        """Check for gradient overflow"""
        for group in optimizer.param_groups:
            for param in group['params']:
                if param.grad is not None:
                    if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():
                        return True
        return False

    def update_scale(self, had_overflow: bool):
        """Update loss scale based on overflow status"""
        if had_overflow:
            self.loss_scale = max(
                self.min_loss_scale,
                self.loss_scale * self.backoff_factor
            )
            self.steps_since_growth = 0
            self.overflow_count += 1
        else:
            self.steps_since_growth += 1
            if self.steps_since_growth >= self.growth_interval:
                self.loss_scale = min(
                    self.max_loss_scale,
                    self.loss_scale * self.growth_factor
                )
                self.steps_since_growth = 0

    def get_stats(self) -> Dict:
        """Get scaler statistics"""
        return {
            "loss_scale": self.loss_scale,
            "overflow_count": self.overflow_count,
            "steps_since_growth": self.steps_since_growth
        }
```

---

## 2.5.4 Debugging Procedures

### Systematic Debugging Workflow

```python
"""
Systematic debugging workflow for training issues
"""
from dataclasses import dataclass
from typing import List, Dict, Optional, Callable
from enum import Enum
import json

class IssueType(Enum):
    LOSS_SPIKE = "loss_spike"
    DIVERGENCE = "divergence"
    PLATEAU = "plateau"
    NAN_INF = "nan_inf"
    OOM = "out_of_memory"
    SLOW_TRAINING = "slow_training"
    POOR_GENERALIZATION = "poor_generalization"

@dataclass
class DebugStep:
    """A single debugging step"""
    name: str
    description: str
    action: Callable
    expected_output: str
    next_step_on_success: Optional[str]
    next_step_on_failure: Optional[str]

class DebugWorkflow:
    """
    Structured debugging workflow for training issues.
    """

    def __init__(self, issue_type: IssueType):
        self.issue_type = issue_type
        self.steps: Dict[str, DebugStep] = {}
        self.results: List[Dict] = []
        self.current_step: Optional[str] = None

        self._initialize_workflow()

    def _initialize_workflow(self):
        """Initialize workflow based on issue type"""
        if self.issue_type == IssueType.LOSS_SPIKE:
            self._init_loss_spike_workflow()
        elif self.issue_type == IssueType.DIVERGENCE:
            self._init_divergence_workflow()
        elif self.issue_type == IssueType.NAN_INF:
            self._init_nan_workflow()
        # ... other workflows

    def _init_loss_spike_workflow(self):
        """Initialize loss spike debugging workflow"""
        self.steps = {
            "check_data": DebugStep(
                name="check_data",
                description="Check the data batch that caused the spike",
                action=self._check_data_batch,
                expected_output="Data batch statistics and anomalies",
                next_step_on_success="check_gradients",
                next_step_on_failure="fix_data"
            ),
            "check_gradients": DebugStep(
                name="check_gradients",
                description="Analyze gradient norms and distributions",
                action=self._check_gradients,
                expected_output="Gradient statistics per layer",
                next_step_on_success="check_lr",
                next_step_on_failure="fix_gradients"
            ),
            "check_lr": DebugStep(
                name="check_lr",
                description="Check learning rate schedule",
                action=self._check_learning_rate,
                expected_output="Learning rate history and schedule",
                next_step_on_success="check_numerical",
                next_step_on_failure="fix_lr"
            ),
            "check_numerical": DebugStep(
                name="check_numerical",
                description="Check for numerical stability issues",
                action=self._check_numerical_stability,
                expected_output="Numerical stability report",
                next_step_on_success="done",
                next_step_on_failure="fix_numerical"
            )
        }
        self.current_step = "check_data"

    def _init_divergence_workflow(self):
        """Initialize divergence debugging workflow"""
        self.steps = {
            "check_lr_warmup": DebugStep(
                name="check_lr_warmup",
                description="Verify learning rate warmup",
                action=self._check_lr_warmup,
                expected_output="Warmup configuration and actual LR values",
                next_step_on_success="check_init",
                next_step_on_failure="fix_warmup"
            ),
            "check_init": DebugStep(
                name="check_init",
                description="Check weight initialization",
                action=self._check_initialization,
                expected_output="Weight statistics at initialization",
                next_step_on_success="check_arch",
                next_step_on_failure="fix_init"
            ),
            "check_arch": DebugStep(
                name="check_arch",
                description="Check architecture for instability patterns",
                action=self._check_architecture,
                expected_output="Architecture analysis",
                next_step_on_success="done",
                next_step_on_failure="fix_arch"
            )
        }
        self.current_step = "check_lr_warmup"

    def _init_nan_workflow(self):
        """Initialize NaN debugging workflow"""
        self.steps = {
            "locate_nan": DebugStep(
                name="locate_nan",
                description="Locate where NaN first appears",
                action=self._locate_nan,
                expected_output="Layer and operation where NaN originates",
                next_step_on_success="check_inputs",
                next_step_on_failure="manual_inspection"
            ),
            "check_inputs": DebugStep(
                name="check_inputs",
                description="Check inputs to the problematic layer",
                action=self._check_inputs,
                expected_output="Input statistics and ranges",
                next_step_on_success="check_overflow",
                next_step_on_failure="fix_inputs"
            ),
            "check_overflow": DebugStep(
                name="check_overflow",
                description="Check for overflow in operations",
                action=self._check_overflow,
                expected_output="Overflow detection results",
                next_step_on_success="done",
                next_step_on_failure="fix_precision"
            )
        }
        self.current_step = "locate_nan"

    def run_step(self, context: Dict) -> Dict:
        """Run the current debugging step"""
        if self.current_step is None:
            return {"status": "complete", "results": self.results}

        step = self.steps[self.current_step]

        print(f"\n{'='*60}")
        print(f"Step: {step.name}")
        print(f"Description: {step.description}")
        print(f"{'='*60}\n")

        try:
            result = step.action(context)
            success = result.get("success", False)

            self.results.append({
                "step": step.name,
                "result": result,
                "success": success
            })

            # Determine next step
            if success:
                self.current_step = step.next_step_on_success
            else:
                self.current_step = step.next_step_on_failure

            return result

        except Exception as e:
            error_result = {"error": str(e), "success": False}
            self.results.append({
                "step": step.name,
                "result": error_result,
                "success": False
            })
            self.current_step = step.next_step_on_failure
            return error_result

    def run_full_workflow(self, context: Dict) -> List[Dict]:
        """Run the complete debugging workflow"""
        while self.current_step is not None and self.current_step != "done":
            self.run_step(context)
        return self.results

    # Debugging action implementations
    def _check_data_batch(self, context: Dict) -> Dict:
        """Check the data batch for issues"""
        batch = context.get("batch")
        if batch is None:
            return {"success": False, "error": "No batch provided"}

        # Analyze batch
        analysis = {
            "batch_size": len(batch) if hasattr(batch, '__len__') else "unknown",
            "has_nan": False,
            "has_inf": False,
            "token_distribution": {},
            "length_stats": {}
        }

        if isinstance(batch, dict) and "input_ids" in batch:
            input_ids = batch["input_ids"]
            analysis["length_stats"] = {
                "min": input_ids.shape[-1],
                "max": input_ids.shape[-1],
                "mean": input_ids.shape[-1]
            }

        # Check for problematic values
        for key, value in batch.items() if isinstance(batch, dict) else []:
            if isinstance(value, torch.Tensor):
                if torch.isnan(value).any():
                    analysis["has_nan"] = True
                if torch.isinf(value).any():
                    analysis["has_inf"] = True

        success = not (analysis["has_nan"] or analysis["has_inf"])
        return {"success": success, "analysis": analysis}

    def _check_gradients(self, context: Dict) -> Dict:
        """Analyze gradients"""
        model = context.get("model")
        if model is None:
            return {"success": False, "error": "No model provided"}

        debugger = GradientDebugger(model)

        vanishing = debugger.diagnose_vanishing_gradients()
        exploding = debugger.diagnose_exploding_gradients()

        has_issues = (
            len(vanishing["vanishing_layers"]) > 0 or
            len(exploding["exploding_layers"]) > 0
        )

        return {
            "success": not has_issues,
            "vanishing": vanishing,
            "exploding": exploding
        }

    def _check_learning_rate(self, context: Dict) -> Dict:
        """Check learning rate"""
        optimizer = context.get("optimizer")
        scheduler = context.get("scheduler")

        if optimizer is None:
            return {"success": False, "error": "No optimizer provided"}

        current_lr = optimizer.param_groups[0]['lr']

        # Check if LR is reasonable
        if current_lr > 1e-2:
            return {
                "success": False,
                "current_lr": current_lr,
                "issue": "Learning rate too high"
            }
        if current_lr < 1e-8:
            return {
                "success": False,
                "current_lr": current_lr,
                "issue": "Learning rate too low"
            }

        return {"success": True, "current_lr": current_lr}

    def _check_numerical_stability(self, context: Dict) -> Dict:
        """Check numerical stability"""
        model = context.get("model")

        checker = NumericalStabilityChecker()
        param_issues = checker.check_model_parameters(model)

        has_issues = any(len(v) > 0 for v in param_issues.values())

        return {"success": not has_issues, "issues": param_issues}

    # Placeholder implementations for other checks
    def _check_lr_warmup(self, context: Dict) -> Dict:
        return {"success": True, "warmup_steps": context.get("warmup_steps", "unknown")}

    def _check_initialization(self, context: Dict) -> Dict:
        return {"success": True}

    def _check_architecture(self, context: Dict) -> Dict:
        return {"success": True}

    def _locate_nan(self, context: Dict) -> Dict:
        return {"success": True, "layer": "unknown"}

    def _check_inputs(self, context: Dict) -> Dict:
        return {"success": True}

    def _check_overflow(self, context: Dict) -> Dict:
        return {"success": True}

# Debugging checklist
DEBUGGING_CHECKLIST = """
## Loss Spike Investigation Checklist

### 1. Immediate Actions
- [ ] Save current checkpoint before any changes
- [ ] Record the exact step number and loss value
- [ ] Capture the batch that caused the spike (if possible)
- [ ] Check GPU memory usage at time of spike

### 2. Data Investigation
- [ ] Examine the specific batch:
  - [ ] Check for NaN/Inf values in inputs
  - [ ] Check sequence lengths
  - [ ] Check token distributions
  - [ ] Look for corrupted or malformed examples
- [ ] Check data pipeline:
  - [ ] Verify shuffling is working correctly
  - [ ] Check for data loader issues
  - [ ] Verify preprocessing is consistent

### 3. Gradient Analysis
- [ ] Check global gradient norm
- [ ] Identify layers with abnormal gradients
- [ ] Compare gradient statistics to pre-spike values
- [ ] Check for gradient accumulation issues

### 4. Learning Rate Check
- [ ] Verify current learning rate is expected
- [ ] Check warmup schedule
- [ ] Verify decay schedule
- [ ] Look for sudden LR changes

### 5. Numerical Stability
- [ ] Check for overflow in activations
- [ ] Verify attention scale factor
- [ ] Check LayerNorm eps values
- [ ] Verify loss scale (if using mixed precision)

### 6. Recovery Actions
- [ ] If spike is isolated: continue training
- [ ] If spike persists: rollback to checkpoint
- [ ] If recurring: reduce learning rate
- [ ] If severe: increase gradient clipping
"""
```

---

## 2.5.5 Checkpointing Best Practices

```python
"""
Checkpointing and model saving utilities
"""
import torch
import os
import json
import shutil
from typing import Dict, Optional, List
from pathlib import Path
from datetime import datetime
import hashlib

class CheckpointManager:
    """
    Manages training checkpoints with best practices.
    """

    def __init__(
        self,
        checkpoint_dir: str,
        max_checkpoints: int = 5,
        save_optimizer: bool = True,
        save_scheduler: bool = True
    ):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)

        self.max_checkpoints = max_checkpoints
        self.save_optimizer = save_optimizer
        self.save_scheduler = save_scheduler

        self.checkpoint_history: List[Dict] = []
        self.best_metric = float('inf')
        self.best_checkpoint: Optional[str] = None

        # Load existing history
        self._load_history()

    def _load_history(self):
        """Load checkpoint history from disk"""
        history_path = self.checkpoint_dir / "checkpoint_history.json"
        if history_path.exists():
            with open(history_path) as f:
                data = json.load(f)
                self.checkpoint_history = data.get("history", [])
                self.best_metric = data.get("best_metric", float('inf'))
                self.best_checkpoint = data.get("best_checkpoint")

    def _save_history(self):
        """Save checkpoint history to disk"""
        history_path = self.checkpoint_dir / "checkpoint_history.json"
        with open(history_path, 'w') as f:
            json.dump({
                "history": self.checkpoint_history,
                "best_metric": self.best_metric,
                "best_checkpoint": self.best_checkpoint
            }, f, indent=2)

    def save_checkpoint(
        self,
        model: torch.nn.Module,
        optimizer: torch.optim.Optimizer,
        scheduler: Optional[object],
        step: int,
        metrics: Dict,
        extra_state: Optional[Dict] = None
    ) -> str:
        """
        Save a training checkpoint.

        Returns:
            Path to saved checkpoint
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        checkpoint_name = f"checkpoint_step{step}_{timestamp}"
        checkpoint_path = self.checkpoint_dir / checkpoint_name

        checkpoint_path.mkdir(parents=True, exist_ok=True)

        # Save model state
        model_path = checkpoint_path / "model.pt"
        torch.save(model.state_dict(), model_path)

        # Save optimizer state
        if self.save_optimizer:
            optimizer_path = checkpoint_path / "optimizer.pt"
            torch.save(optimizer.state_dict(), optimizer_path)

        # Save scheduler state
        if self.save_scheduler and scheduler is not None:
            scheduler_path = checkpoint_path / "scheduler.pt"
            torch.save(scheduler.state_dict(), scheduler_path)

        # Save metadata
        metadata = {
            "step": step,
            "timestamp": timestamp,
            "metrics": metrics,
            "extra_state": extra_state or {},
            "model_hash": self._compute_hash(model_path)
        }
        metadata_path = checkpoint_path / "metadata.json"
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)

        # Update history
        self.checkpoint_history.append({
            "path": str(checkpoint_path),
            "step": step,
            "metrics": metrics,
            "timestamp": timestamp
        })

        # Check if this is the best checkpoint
        val_loss = metrics.get("val_loss", float('inf'))
        if val_loss < self.best_metric:
            self.best_metric = val_loss
            self.best_checkpoint = str(checkpoint_path)
            # Create symlink to best
            best_link = self.checkpoint_dir / "best"
            if best_link.exists():
                best_link.unlink()
            best_link.symlink_to(checkpoint_path)

        # Cleanup old checkpoints
        self._cleanup_old_checkpoints()

        # Save history
        self._save_history()

        return str(checkpoint_path)

    def load_checkpoint(
        self,
        checkpoint_path: str,
        model: torch.nn.Module,
        optimizer: Optional[torch.optim.Optimizer] = None,
        scheduler: Optional[object] = None
    ) -> Dict:
        """
        Load a checkpoint.

        Returns:
            Metadata dict with step and metrics
        """
        checkpoint_path = Path(checkpoint_path)

        # Load model
        model_path = checkpoint_path / "model.pt"
        model.load_state_dict(torch.load(model_path, map_location='cpu'))

        # Load optimizer
        if optimizer is not None:
            optimizer_path = checkpoint_path / "optimizer.pt"
            if optimizer_path.exists():
                optimizer.load_state_dict(torch.load(optimizer_path, map_location='cpu'))

        # Load scheduler
        if scheduler is not None:
            scheduler_path = checkpoint_path / "scheduler.pt"
            if scheduler_path.exists():
                scheduler.load_state_dict(torch.load(scheduler_path, map_location='cpu'))

        # Load metadata
        metadata_path = checkpoint_path / "metadata.json"
        with open(metadata_path) as f:
            metadata = json.load(f)

        return metadata

    def load_best_checkpoint(
        self,
        model: torch.nn.Module,
        optimizer: Optional[torch.optim.Optimizer] = None,
        scheduler: Optional[object] = None
    ) -> Dict:
        """Load the best checkpoint"""
        if self.best_checkpoint is None:
            raise ValueError("No best checkpoint available")
        return self.load_checkpoint(self.best_checkpoint, model, optimizer, scheduler)

    def load_latest_checkpoint(
        self,
        model: torch.nn.Module,
        optimizer: Optional[torch.optim.Optimizer] = None,
        scheduler: Optional[object] = None
    ) -> Optional[Dict]:
        """Load the most recent checkpoint"""
        if not self.checkpoint_history:
            return None

        latest = max(self.checkpoint_history, key=lambda x: x["step"])
        return self.load_checkpoint(latest["path"], model, optimizer, scheduler)

    def _cleanup_old_checkpoints(self):
        """Remove old checkpoints, keeping the best and most recent"""
        if len(self.checkpoint_history) <= self.max_checkpoints:
            return

        # Sort by step
        sorted_history = sorted(self.checkpoint_history, key=lambda x: x["step"])

        # Keep best and most recent
        to_remove = sorted_history[:-self.max_checkpoints + 1]

        for entry in to_remove:
            path = entry["path"]
            # Don't remove best checkpoint
            if path == self.best_checkpoint:
                continue

            # Remove checkpoint directory
            if os.path.exists(path):
                shutil.rmtree(path)

            # Remove from history
            self.checkpoint_history = [
                e for e in self.checkpoint_history if e["path"] != path
            ]

    def _compute_hash(self, path: Path) -> str:
        """Compute hash of a file"""
        hasher = hashlib.md5()
        with open(path, 'rb') as f:
            for chunk in iter(lambda: f.read(8192), b''):
                hasher.update(chunk)
        return hasher.hexdigest()

    def get_checkpoint_info(self) -> Dict:
        """Get information about all checkpoints"""
        return {
            "total_checkpoints": len(self.checkpoint_history),
            "best_checkpoint": self.best_checkpoint,
            "best_metric": self.best_metric,
            "latest_step": max(e["step"] for e in self.checkpoint_history) if self.checkpoint_history else 0,
            "checkpoints": self.checkpoint_history
        }
```

---

## 2.5.6 Experiment Tracking

```python
"""
Experiment tracking and reproducibility
"""
import json
import hashlib
import subprocess
from dataclasses import dataclass, asdict
from typing import Dict, List, Optional
from pathlib import Path
from datetime import datetime
import yaml

@dataclass
class ExperimentConfig:
    """Complete experiment configuration"""
    # Model
    model_name: str
    model_params: int
    hidden_size: int
    num_layers: int
    num_heads: int
    vocab_size: int
    max_seq_length: int

    # Training
    batch_size: int
    learning_rate: float
    warmup_steps: int
    total_steps: int
    weight_decay: float
    grad_clip: float

    # Data
    data_mix: Dict[str, float]
    total_tokens: int

    # Infrastructure
    num_gpus: int
    distributed_strategy: str

    # Optional
    seed: int = 42
    notes: str = ""

class ExperimentTracker:
    """
    Track experiments for reproducibility.
    """

    def __init__(self, experiments_dir: str):
        self.experiments_dir = Path(experiments_dir)
        self.experiments_dir.mkdir(parents=True, exist_ok=True)
        self.current_experiment: Optional[str] = None

    def start_experiment(
        self,
        config: ExperimentConfig,
        name: Optional[str] = None
    ) -> str:
        """
        Start a new experiment.

        Returns:
            Experiment ID
        """
        # Generate experiment ID
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        config_hash = hashlib.md5(
            json.dumps(asdict(config), sort_keys=True).encode()
        ).hexdigest()[:8]

        exp_id = f"{name or config.model_name}_{timestamp}_{config_hash}"

        # Create experiment directory
        exp_dir = self.experiments_dir / exp_id
        exp_dir.mkdir(parents=True, exist_ok=True)

        # Save configuration
        config_path = exp_dir / "config.yaml"
        with open(config_path, 'w') as f:
            yaml.dump(asdict(config), f, default_flow_style=False)

        # Save git info
        git_info = self._get_git_info()
        git_path = exp_dir / "git_info.json"
        with open(git_path, 'w') as f:
            json.dump(git_info, f, indent=2)

        # Save environment
        env_info = self._get_environment_info()
        env_path = exp_dir / "environment.json"
        with open(env_path, 'w') as f:
            json.dump(env_info, f, indent=2)

        self.current_experiment = exp_id

        return exp_id

    def log_metrics(
        self,
        step: int,
        metrics: Dict[str, float],
        phase: str = "train"
    ):
        """Log metrics for the current experiment"""
        if self.current_experiment is None:
            raise ValueError("No active experiment")

        exp_dir = self.experiments_dir / self.current_experiment
        metrics_file = exp_dir / f"metrics_{phase}.jsonl"

        entry = {
            "step": step,
            "timestamp": datetime.now().isoformat(),
            **metrics
        }

        with open(metrics_file, 'a') as f:
            f.write(json.dumps(entry) + "\n")

    def log_artifact(self, name: str, path: str):
        """Log an artifact (file) for the experiment"""
        if self.current_experiment is None:
            raise ValueError("No active experiment")

        exp_dir = self.experiments_dir / self.current_experiment
        artifacts_dir = exp_dir / "artifacts"
        artifacts_dir.mkdir(exist_ok=True)

        # Copy or link artifact
        import shutil
        dest = artifacts_dir / name
        shutil.copy2(path, dest)

    def end_experiment(self, final_metrics: Dict):
        """End the current experiment"""
        if self.current_experiment is None:
            return

        exp_dir = self.experiments_dir / self.current_experiment

        # Save final results
        results_path = exp_dir / "results.json"
        with open(results_path, 'w') as f:
            json.dump({
                "final_metrics": final_metrics,
                "completed_at": datetime.now().isoformat()
            }, f, indent=2)

        self.current_experiment = None

    def _get_git_info(self) -> Dict:
        """Get git repository information"""
        try:
            commit = subprocess.check_output(
                ["git", "rev-parse", "HEAD"],
                stderr=subprocess.DEVNULL
            ).decode().strip()

            branch = subprocess.check_output(
                ["git", "rev-parse", "--abbrev-ref", "HEAD"],
                stderr=subprocess.DEVNULL
            ).decode().strip()

            diff = subprocess.check_output(
                ["git", "diff", "--stat"],
                stderr=subprocess.DEVNULL
            ).decode().strip()

            return {
                "commit": commit,
                "branch": branch,
                "has_uncommitted_changes": len(diff) > 0,
                "diff_summary": diff[:1000] if diff else ""
            }
        except Exception:
            return {"error": "Git info unavailable"}

    def _get_environment_info(self) -> Dict:
        """Get environment information"""
        import torch
        import platform
        import sys

        info = {
            "python_version": sys.version,
            "platform": platform.platform(),
            "pytorch_version": torch.__version__,
            "cuda_version": torch.version.cuda if torch.cuda.is_available() else None,
            "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0,
        }

        if torch.cuda.is_available():
            info["gpu_names"] = [
                torch.cuda.get_device_name(i)
                for i in range(torch.cuda.device_count())
            ]

        return info

    def compare_experiments(
        self,
        exp_ids: List[str],
        metrics: List[str]
    ) -> Dict:
        """Compare multiple experiments"""
        comparison = {}

        for exp_id in exp_ids:
            exp_dir = self.experiments_dir / exp_id

            # Load config
            config_path = exp_dir / "config.yaml"
            with open(config_path) as f:
                config = yaml.safe_load(f)

            # Load results
            results_path = exp_dir / "results.json"
            if results_path.exists():
                with open(results_path) as f:
                    results = json.load(f)
            else:
                results = {}

            comparison[exp_id] = {
                "config": config,
                "metrics": {
                    m: results.get("final_metrics", {}).get(m)
                    for m in metrics
                }
            }

        return comparison
```

---

## Troubleshooting Quick Reference

| Symptom | Likely Cause | Quick Fix |
|---------|--------------|-----------|
| Loss suddenly spikes | Bad batch, LR too high, gradient explosion | Skip batch, reduce LR by 50%, increase grad clip |
| Loss goes to NaN | Numerical overflow, division by zero | Check for inf in activations, reduce LR |
| Loss plateaus | LR too low, stuck in local minimum | Increase LR, check data shuffle |
| Training slow | Memory bottleneck, poor GPU utilization | Increase batch size, optimize data loading |
| OOM error | Batch too large, activation memory | Reduce batch, enable gradient checkpointing |
| Gradients vanishing | Deep network, bad initialization | Use Pre-LN, check init scale |
| Gradients exploding | LR too high, no clipping | Enable gradient clipping, reduce LR |

---

## Glossary

| Term | Definition |
|------|------------|
| **MFU** | Model FLOPs Utilization - actual vs theoretical peak compute |
| **Loss Spike** | Sudden increase in training loss |
| **Gradient Clipping** | Limiting gradient magnitude to prevent explosion |
| **Warmup** | Gradual learning rate increase at training start |
| **Checkpoint** | Saved model and optimizer state |
| **Divergence** | Training loss increasing without bound |
| **Vanishing Gradients** | Gradients becoming too small to update parameters |
| **Mixed Precision** | Using FP16/BF16 with FP32 for numerical stability |

---

## References

1. Zhang, B., et al. (2024). "Spike No More: Stabilizing the Pre-training of Large Language Models"
2. Wortsman, M., et al. (2024). "SPAM: Spike-Aware Adam with Momentum Reset"
3. Shoeybi, M., et al. (2019). "Megatron-LM: Training Multi-Billion Parameter Language Models"
4. Rajbhandari, S., et al. (2020). "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models"
5. Biewald, L. (2020). "Experiment Tracking with Weights and Biases"
6. Wang, A., et al. (2023). "DeepSpeed Ulysses: System Optimizations for Training Extreme Long Sequence Transformer Models"
7. Rae, J., et al. (2021). "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"
8. Chowdhery, A., et al. (2022). "PaLM: Scaling Language Modeling with Pathways"
