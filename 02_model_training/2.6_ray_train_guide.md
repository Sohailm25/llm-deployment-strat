# Ray Train Guide

> **Navigation** | [← 2.5 Training Monitoring](2.5_training_monitoring_debugging.md) | [3.1 Supervised Fine-Tuning →](../03_fine_tuning/3.1_supervised_fine_tuning.md)
>
> | | |
> |---|---|
> | **Prerequisites** | [2.3 Distributed Training](2.3_distributed_training_infrastructure.md) |
> | **Related** | [2.5 Training Monitoring](2.5_training_monitoring_debugging.md) &#124; [8.5 LLM CI/CD](../08_mlops_lifecycle/8.5_llm_cicd_pipeline_guide.md) |
> | **Next** | [3.1 Supervised Fine-Tuning](../03_fine_tuning/3.1_supervised_fine_tuning.md) |

---

## Executive Summary

Ray Train is a distributed training library that simplifies scaling PyTorch, TensorFlow, and other ML frameworks across multiple GPUs and nodes. This guide covers Ray Train configuration for LLM pre-training and fine-tuning, including integration with DeepSpeed, FSDP, and fault-tolerant training on cloud infrastructure.

## Prerequisites

- Understanding of distributed training concepts (Document 2.3)
- PyTorch experience
- Basic familiarity with Ray framework
- Access to multi-GPU or multi-node infrastructure

---

## 2.6.1 Ray Train Fundamentals

### Why Ray Train for LLM Training?

| Feature | Benefit |
|---------|---------|
| **Fault Tolerance** | Automatic checkpoint recovery on node failures |
| **Elastic Training** | Scale workers up/down without restart |
| **Multi-framework** | Works with PyTorch, DeepSpeed, HuggingFace |
| **Cloud Native** | First-class Kubernetes and cloud support |
| **Resource Management** | Efficient GPU/CPU allocation |
| **Data Pipeline** | Integrated with Ray Data for streaming |

### Core Concepts

```python
"""
Ray Train core concepts and basic setup
"""
import ray
from ray import train
from ray.train import ScalingConfig, RunConfig, CheckpointConfig
from ray.train.torch import TorchTrainer
from ray.train.lightning import LightningTrainer
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from typing import Dict, Optional
import os

# Initialize Ray
def init_ray_cluster(
    address: Optional[str] = None,
    num_cpus: Optional[int] = None,
    num_gpus: Optional[int] = None
):
    """
    Initialize Ray cluster connection.

    Args:
        address: Ray cluster address (e.g., "auto", "ray://head-node:10001")
        num_cpus: Number of CPUs (for local mode)
        num_gpus: Number of GPUs (for local mode)
    """
    if address:
        ray.init(address=address)
    else:
        ray.init(
            num_cpus=num_cpus,
            num_gpus=num_gpus,
            ignore_reinit_error=True
        )

    print(f"Ray initialized: {ray.cluster_resources()}")

# Basic training function structure
def train_func(config: Dict):
    """
    Training function that runs on each worker.

    This function is executed on each distributed worker.
    Use train.report() to report metrics and save checkpoints.
    """
    # Get distributed training context
    world_size = train.get_context().get_world_size()
    rank = train.get_context().get_world_rank()
    local_rank = train.get_context().get_local_rank()

    print(f"Worker {rank}/{world_size} starting on local rank {local_rank}")

    # Setup model - Ray handles device placement
    model = config["model_fn"]()

    # Prepare model for distributed training
    model = train.torch.prepare_model(model)

    # Setup data loader
    train_loader = config["data_loader_fn"]()

    # Prepare data loader for distributed training
    train_loader = train.torch.prepare_data_loader(train_loader)

    # Setup optimizer
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config["learning_rate"],
        weight_decay=config["weight_decay"]
    )

    # Training loop
    for epoch in range(config["num_epochs"]):
        model.train()
        epoch_loss = 0.0

        for batch_idx, batch in enumerate(train_loader):
            optimizer.zero_grad()

            # Forward pass
            outputs = model(**batch)
            loss = outputs.loss

            # Backward pass
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

            # Report metrics periodically
            if batch_idx % config.get("log_interval", 100) == 0:
                train.report({
                    "loss": loss.item(),
                    "epoch": epoch,
                    "batch": batch_idx
                })

        # Report epoch metrics and save checkpoint
        avg_loss = epoch_loss / len(train_loader)

        with train.get_context().get_checkpoint() as checkpoint:
            torch.save(
                {"model_state_dict": model.state_dict()},
                os.path.join(checkpoint.path, "model.pt")
            )

        train.report(
            {"epoch_loss": avg_loss, "epoch": epoch},
            checkpoint=checkpoint
        )

# Scaling configuration
def create_scaling_config(
    num_workers: int,
    use_gpu: bool = True,
    resources_per_worker: Optional[Dict] = None,
    placement_strategy: str = "PACK"
) -> ScalingConfig:
    """
    Create scaling configuration for distributed training.

    Args:
        num_workers: Number of training workers
        use_gpu: Whether to use GPUs
        resources_per_worker: Custom resource requirements
        placement_strategy: "PACK" (co-locate) or "SPREAD" (distribute)

    Returns:
        ScalingConfig object
    """
    if resources_per_worker is None:
        resources_per_worker = {"GPU": 1} if use_gpu else {"CPU": 4}

    return ScalingConfig(
        num_workers=num_workers,
        use_gpu=use_gpu,
        resources_per_worker=resources_per_worker,
        placement_strategy=placement_strategy,
        # Trainer resources (for the driver)
        trainer_resources={"CPU": 1}
    )

# Run configuration
def create_run_config(
    name: str,
    storage_path: str,
    max_failures: int = 3,
    checkpoint_frequency: int = 1
) -> RunConfig:
    """
    Create run configuration with checkpointing and failure handling.

    Args:
        name: Experiment name
        storage_path: Path for checkpoints (local or cloud)
        max_failures: Maximum worker failures before stopping
        checkpoint_frequency: Save checkpoint every N epochs

    Returns:
        RunConfig object
    """
    return RunConfig(
        name=name,
        storage_path=storage_path,
        failure_config=train.FailureConfig(
            max_failures=max_failures
        ),
        checkpoint_config=CheckpointConfig(
            num_to_keep=3,
            checkpoint_score_attribute="epoch_loss",
            checkpoint_score_order="min"
        )
    )

# Example: Basic TorchTrainer setup
def create_basic_trainer(
    train_func,
    config: Dict,
    num_workers: int = 4,
    use_gpu: bool = True
) -> TorchTrainer:
    """
    Create a basic TorchTrainer.

    Args:
        train_func: Training function
        config: Training configuration
        num_workers: Number of workers
        use_gpu: Whether to use GPUs

    Returns:
        Configured TorchTrainer
    """
    trainer = TorchTrainer(
        train_loop_per_worker=train_func,
        train_loop_config=config,
        scaling_config=ScalingConfig(
            num_workers=num_workers,
            use_gpu=use_gpu
        ),
        run_config=RunConfig(
            name="basic-training",
            storage_path="/tmp/ray_results",
            checkpoint_config=CheckpointConfig(
                num_to_keep=2
            )
        )
    )

    return trainer

# Run training
def run_training(trainer: TorchTrainer):
    """Run training and return results"""
    result = trainer.fit()

    print(f"Training completed!")
    print(f"Best checkpoint: {result.best_checkpoints}")
    print(f"Final metrics: {result.metrics}")

    return result
```

### Converting Existing PyTorch Code

```python
"""
Converting existing PyTorch training code to Ray Train
"""
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import ray
from ray import train
from ray.train.torch import TorchTrainer
from ray.train import ScalingConfig

# BEFORE: Standard PyTorch training
def standard_pytorch_training():
    """Standard single-GPU PyTorch training"""

    # Setup
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    model = nn.Linear(10, 1).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    dataset = torch.randn(1000, 10)
    loader = DataLoader(dataset, batch_size=32)

    # Training loop
    for epoch in range(10):
        for batch in loader:
            batch = batch.to(device)
            loss = model(batch).sum()
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

# AFTER: Ray Train distributed version
def ray_train_distributed():
    """Ray Train distributed version"""

    def train_func(config):
        # Model creation (no .to(device) needed)
        model = nn.Linear(10, 1)

        # Let Ray handle distributed setup
        model = train.torch.prepare_model(model)

        optimizer = torch.optim.Adam(model.parameters(), lr=config["lr"])

        # Data loading
        dataset = torch.randn(1000, 10)
        loader = DataLoader(dataset, batch_size=config["batch_size"])

        # Let Ray handle data sharding
        loader = train.torch.prepare_data_loader(loader)

        # Training loop (mostly unchanged)
        for epoch in range(config["num_epochs"]):
            for batch in loader:
                # No .to(device) needed - Ray handles it
                loss = model(batch).sum()
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

            # Report metrics to Ray
            train.report({"loss": loss.item(), "epoch": epoch})

    # Create trainer
    trainer = TorchTrainer(
        train_loop_per_worker=train_func,
        train_loop_config={
            "lr": 0.001,
            "batch_size": 32,
            "num_epochs": 10
        },
        scaling_config=ScalingConfig(
            num_workers=4,
            use_gpu=True
        )
    )

    # Run training
    result = trainer.fit()
    return result

# Key changes summary:
# 1. Wrap training code in a function
# 2. Use train.torch.prepare_model() instead of model.to(device)
# 3. Use train.torch.prepare_data_loader() for automatic sharding
# 4. Use train.report() to report metrics
# 5. Remove manual device placement (.to(device))
# 6. Use TorchTrainer with ScalingConfig
```

---

## 2.6.2 LLM Training with Ray Train

### Pre-training Configuration

```python
"""
LLM pre-training with Ray Train
"""
import torch
import torch.nn as nn
from torch.utils.data import IterableDataset
import ray
from ray import train
from ray.train.torch import TorchTrainer
from ray.train import ScalingConfig, RunConfig, CheckpointConfig
from ray.data import Dataset as RayDataset
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig
from typing import Dict, Optional, Iterator
import os

class StreamingLLMDataset(IterableDataset):
    """
    Streaming dataset for LLM pre-training with Ray Train.
    """

    def __init__(
        self,
        data_path: str,
        tokenizer,
        seq_length: int = 2048,
        worker_rank: int = 0,
        world_size: int = 1
    ):
        self.data_path = data_path
        self.tokenizer = tokenizer
        self.seq_length = seq_length
        self.worker_rank = worker_rank
        self.world_size = world_size

    def __iter__(self) -> Iterator[Dict[str, torch.Tensor]]:
        """Yield tokenized sequences"""
        # Implementation for streaming data
        # Shard data across workers
        pass

def llm_pretrain_func(config: Dict):
    """
    LLM pre-training function for Ray Train workers.
    """
    # Get distributed context
    context = train.get_context()
    world_size = context.get_world_size()
    rank = context.get_world_rank()
    local_rank = context.get_local_rank()

    # Set device
    device = torch.device(f"cuda:{local_rank}")
    torch.cuda.set_device(device)

    # Model configuration
    model_config = AutoConfig.from_pretrained(
        config["model_name"],
        vocab_size=config["vocab_size"],
        hidden_size=config["hidden_size"],
        num_hidden_layers=config["num_layers"],
        num_attention_heads=config["num_heads"],
        intermediate_size=config["intermediate_size"],
        max_position_embeddings=config["max_seq_length"],
    )

    # Initialize model
    model = AutoModelForCausalLM.from_config(model_config)

    # Prepare model for distributed training (DDP by default)
    model = train.torch.prepare_model(
        model,
        move_to_device=True,
        parallel_strategy="ddp"  # or "fsdp"
    )

    # Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(config["tokenizer_name"])

    # Dataset
    train_dataset = StreamingLLMDataset(
        data_path=config["data_path"],
        tokenizer=tokenizer,
        seq_length=config["max_seq_length"],
        worker_rank=rank,
        world_size=world_size
    )

    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=config["batch_size_per_gpu"],
        num_workers=config.get("dataloader_workers", 4),
        pin_memory=True
    )

    # Don't use prepare_data_loader for IterableDataset
    # (sharding is handled in the dataset itself)

    # Optimizer
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config["learning_rate"],
        betas=(config.get("beta1", 0.9), config.get("beta2", 0.95)),
        weight_decay=config.get("weight_decay", 0.1)
    )

    # Learning rate scheduler
    from transformers import get_cosine_schedule_with_warmup

    total_steps = config["total_steps"]
    warmup_steps = config.get("warmup_steps", int(total_steps * 0.01))

    scheduler = get_cosine_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=total_steps
    )

    # Mixed precision
    scaler = torch.cuda.amp.GradScaler() if config.get("fp16", False) else None

    # Resume from checkpoint if available
    checkpoint = train.get_checkpoint()
    start_step = 0
    if checkpoint:
        with checkpoint.as_directory() as checkpoint_dir:
            state = torch.load(os.path.join(checkpoint_dir, "state.pt"))
            model.load_state_dict(state["model_state_dict"])
            optimizer.load_state_dict(state["optimizer_state_dict"])
            scheduler.load_state_dict(state["scheduler_state_dict"])
            start_step = state["step"]
            if scaler and "scaler_state_dict" in state:
                scaler.load_state_dict(state["scaler_state_dict"])

    # Training loop
    model.train()
    global_step = start_step
    running_loss = 0.0

    log_interval = config.get("log_interval", 100)
    checkpoint_interval = config.get("checkpoint_interval", 1000)
    grad_accum_steps = config.get("gradient_accumulation_steps", 1)

    for batch_idx, batch in enumerate(train_loader):
        if global_step >= total_steps:
            break

        # Move batch to device
        batch = {k: v.to(device) for k, v in batch.items()}

        # Forward pass with mixed precision
        with torch.cuda.amp.autocast(enabled=scaler is not None):
            outputs = model(**batch, labels=batch["input_ids"])
            loss = outputs.loss / grad_accum_steps

        # Backward pass
        if scaler:
            scaler.scale(loss).backward()
        else:
            loss.backward()

        running_loss += loss.item() * grad_accum_steps

        # Gradient accumulation
        if (batch_idx + 1) % grad_accum_steps == 0:
            # Gradient clipping
            if config.get("max_grad_norm"):
                if scaler:
                    scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(
                    model.parameters(),
                    config["max_grad_norm"]
                )

            # Optimizer step
            if scaler:
                scaler.step(optimizer)
                scaler.update()
            else:
                optimizer.step()

            scheduler.step()
            optimizer.zero_grad()

            global_step += 1

            # Logging
            if global_step % log_interval == 0:
                avg_loss = running_loss / log_interval
                lr = scheduler.get_last_lr()[0]

                train.report({
                    "loss": avg_loss,
                    "learning_rate": lr,
                    "step": global_step,
                    "tokens_seen": global_step * config["batch_size_per_gpu"] * world_size * config["max_seq_length"]
                })

                running_loss = 0.0

            # Checkpointing
            if global_step % checkpoint_interval == 0:
                # Save checkpoint
                checkpoint_dir = os.path.join(
                    config.get("checkpoint_dir", "/tmp/checkpoints"),
                    f"step_{global_step}"
                )
                os.makedirs(checkpoint_dir, exist_ok=True)

                state = {
                    "step": global_step,
                    "model_state_dict": model.state_dict(),
                    "optimizer_state_dict": optimizer.state_dict(),
                    "scheduler_state_dict": scheduler.state_dict(),
                }
                if scaler:
                    state["scaler_state_dict"] = scaler.state_dict()

                torch.save(state, os.path.join(checkpoint_dir, "state.pt"))

                train.report(
                    {"step": global_step, "checkpoint_saved": True},
                    checkpoint=train.Checkpoint.from_directory(checkpoint_dir)
                )

def create_llm_pretraining_trainer(config: Dict) -> TorchTrainer:
    """
    Create a TorchTrainer configured for LLM pre-training.
    """
    scaling_config = ScalingConfig(
        num_workers=config["num_workers"],
        use_gpu=True,
        resources_per_worker={
            "GPU": config.get("gpus_per_worker", 1),
            "CPU": config.get("cpus_per_worker", 8)
        }
    )

    run_config = RunConfig(
        name=config.get("run_name", "llm-pretraining"),
        storage_path=config["storage_path"],
        failure_config=train.FailureConfig(
            max_failures=config.get("max_failures", 10)
        ),
        checkpoint_config=CheckpointConfig(
            num_to_keep=config.get("checkpoints_to_keep", 3),
            checkpoint_score_attribute="loss",
            checkpoint_score_order="min"
        )
    )

    trainer = TorchTrainer(
        train_loop_per_worker=llm_pretrain_func,
        train_loop_config=config,
        scaling_config=scaling_config,
        run_config=run_config
    )

    return trainer

# Example configuration
LLM_PRETRAIN_CONFIG = {
    # Model
    "model_name": "gpt2",
    "tokenizer_name": "gpt2",
    "vocab_size": 50257,
    "hidden_size": 2048,
    "num_layers": 24,
    "num_heads": 16,
    "intermediate_size": 8192,
    "max_seq_length": 2048,

    # Training
    "batch_size_per_gpu": 4,
    "gradient_accumulation_steps": 8,
    "learning_rate": 3e-4,
    "weight_decay": 0.1,
    "max_grad_norm": 1.0,
    "total_steps": 100000,
    "warmup_steps": 2000,

    # Mixed precision
    "fp16": True,

    # Data
    "data_path": "/data/pretraining",

    # Infrastructure
    "num_workers": 8,
    "gpus_per_worker": 1,
    "cpus_per_worker": 8,

    # Logging and checkpointing
    "log_interval": 100,
    "checkpoint_interval": 1000,
    "checkpoints_to_keep": 3,
    "storage_path": "s3://my-bucket/ray-results",
    "run_name": "llm-1b-pretrain"
}
```

### Fine-tuning with Ray Train

```python
"""
LLM fine-tuning with Ray Train and HuggingFace
"""
from ray.train.huggingface import TransformersTrainer
from ray.train import ScalingConfig, RunConfig
import ray.data
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer
)
from datasets import load_dataset
import torch

def finetune_with_transformers_trainer(config: dict):
    """
    Fine-tune using HuggingFace Transformers Trainer with Ray.
    """
    from ray.train.huggingface.transformers import prepare_trainer

    # Load model and tokenizer
    model = AutoModelForCausalLM.from_pretrained(
        config["model_name"],
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    )
    tokenizer = AutoTokenizer.from_pretrained(config["model_name"])
    tokenizer.pad_token = tokenizer.eos_token

    # Load and preprocess dataset
    def preprocess_function(examples):
        # Tokenize
        tokenized = tokenizer(
            examples["text"],
            truncation=True,
            max_length=config["max_length"],
            padding="max_length"
        )
        tokenized["labels"] = tokenized["input_ids"].copy()
        return tokenized

    # Get Ray Data dataset and convert to HuggingFace
    train_dataset = ray.train.get_dataset_shard("train")
    train_hf = train_dataset.to_pandas()

    from datasets import Dataset
    train_ds = Dataset.from_pandas(train_hf)
    train_ds = train_ds.map(preprocess_function, batched=True)

    # Training arguments
    training_args = TrainingArguments(
        output_dir=config["output_dir"],
        per_device_train_batch_size=config["batch_size"],
        gradient_accumulation_steps=config["gradient_accumulation_steps"],
        learning_rate=config["learning_rate"],
        num_train_epochs=config["num_epochs"],
        warmup_ratio=0.1,
        logging_steps=10,
        save_strategy="steps",
        save_steps=config["save_steps"],
        bf16=True,
        gradient_checkpointing=config.get("gradient_checkpointing", True),
        deepspeed=config.get("deepspeed_config"),
        report_to=["none"],  # Ray handles logging
    )

    # Create trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_ds,
        tokenizer=tokenizer,
    )

    # Prepare for distributed training
    trainer = prepare_trainer(trainer)

    # Train
    trainer.train()

    # Save final model
    trainer.save_model(config["output_dir"])

def create_finetune_trainer(config: dict):
    """
    Create Ray TransformersTrainer for fine-tuning.
    """
    # Load dataset with Ray Data
    dataset = ray.data.read_json(config["data_path"])

    trainer = TransformersTrainer(
        trainer_init_per_worker=finetune_with_transformers_trainer,
        trainer_init_config=config,
        scaling_config=ScalingConfig(
            num_workers=config["num_workers"],
            use_gpu=True,
            resources_per_worker={"GPU": 1, "CPU": 4}
        ),
        run_config=RunConfig(
            name=config.get("run_name", "llm-finetune"),
            storage_path=config["storage_path"]
        ),
        datasets={"train": dataset}
    )

    return trainer
```

---

## 2.6.3 DeepSpeed Integration

### DeepSpeed ZeRO with Ray Train

```python
"""
Ray Train with DeepSpeed ZeRO integration
"""
import torch
import torch.nn as nn
from ray import train
from ray.train.torch import TorchTrainer
from ray.train import ScalingConfig, RunConfig
from transformers import AutoModelForCausalLM
import deepspeed
from typing import Dict
import json
import os

# DeepSpeed configuration for ZeRO-3
DEEPSPEED_ZERO3_CONFIG = {
    "bf16": {
        "enabled": True
    },
    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": True
        },
        "offload_param": {
            "device": "cpu",
            "pin_memory": True
        },
        "overlap_comm": True,
        "contiguous_gradients": True,
        "sub_group_size": 1e9,
        "reduce_bucket_size": "auto",
        "stage3_prefetch_bucket_size": "auto",
        "stage3_param_persistence_threshold": "auto",
        "stage3_max_live_parameters": 1e9,
        "stage3_max_reuse_distance": 1e9,
        "stage3_gather_16bit_weights_on_model_save": True
    },
    "gradient_accumulation_steps": "auto",
    "gradient_clipping": "auto",
    "steps_per_print": 100,
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "wall_clock_breakdown": False
}

def deepspeed_train_func(config: Dict):
    """
    Training function with DeepSpeed integration.
    """
    import deepspeed

    # Get distributed context
    context = train.get_context()
    world_size = context.get_world_size()
    rank = context.get_world_rank()
    local_rank = context.get_local_rank()

    # DeepSpeed requires this
    os.environ["LOCAL_RANK"] = str(local_rank)
    os.environ["RANK"] = str(rank)
    os.environ["WORLD_SIZE"] = str(world_size)

    # Load model
    model = AutoModelForCausalLM.from_pretrained(
        config["model_name"],
        torch_dtype=torch.bfloat16
    )

    # Prepare DeepSpeed config
    ds_config = config["deepspeed_config"].copy()
    ds_config["train_micro_batch_size_per_gpu"] = config["batch_size_per_gpu"]
    ds_config["gradient_accumulation_steps"] = config["gradient_accumulation_steps"]
    ds_config["gradient_clipping"] = config.get("max_grad_norm", 1.0)

    # Calculate total batch size
    total_batch_size = (
        config["batch_size_per_gpu"] *
        config["gradient_accumulation_steps"] *
        world_size
    )
    ds_config["train_batch_size"] = total_batch_size

    # Initialize DeepSpeed
    model_engine, optimizer, _, lr_scheduler = deepspeed.initialize(
        model=model,
        config=ds_config,
        model_parameters=model.parameters()
    )

    # Data loading
    train_loader = create_data_loader(config, rank, world_size)

    # Training loop
    global_step = 0

    for epoch in range(config["num_epochs"]):
        for batch in train_loader:
            # Move batch to device
            batch = {k: v.to(model_engine.device) for k, v in batch.items()}

            # Forward pass
            outputs = model_engine(**batch, labels=batch["input_ids"])
            loss = outputs.loss

            # Backward pass (DeepSpeed handles gradient accumulation)
            model_engine.backward(loss)
            model_engine.step()

            global_step += 1

            # Logging
            if global_step % config.get("log_interval", 100) == 0:
                train.report({
                    "loss": loss.item(),
                    "step": global_step,
                    "epoch": epoch
                })

            # Checkpointing
            if global_step % config.get("checkpoint_interval", 1000) == 0:
                # DeepSpeed checkpoint
                checkpoint_dir = f"/tmp/checkpoints/step_{global_step}"
                model_engine.save_checkpoint(checkpoint_dir)

                train.report(
                    {"step": global_step},
                    checkpoint=train.Checkpoint.from_directory(checkpoint_dir)
                )

            if global_step >= config["total_steps"]:
                break

def create_data_loader(config: Dict, rank: int, world_size: int):
    """Create data loader with proper sharding"""
    # Implementation depends on data format
    pass

def create_deepspeed_trainer(config: Dict) -> TorchTrainer:
    """
    Create TorchTrainer with DeepSpeed configuration.
    """
    # Ensure DeepSpeed config is included
    if "deepspeed_config" not in config:
        config["deepspeed_config"] = DEEPSPEED_ZERO3_CONFIG

    trainer = TorchTrainer(
        train_loop_per_worker=deepspeed_train_func,
        train_loop_config=config,
        scaling_config=ScalingConfig(
            num_workers=config["num_workers"],
            use_gpu=True,
            resources_per_worker={
                "GPU": 1,
                "CPU": config.get("cpus_per_worker", 8)
            }
        ),
        run_config=RunConfig(
            name=config.get("run_name", "deepspeed-training"),
            storage_path=config["storage_path"],
            failure_config=train.FailureConfig(max_failures=5)
        )
    )

    return trainer

# ZeRO Stage configurations
ZERO_STAGE_CONFIGS = {
    "zero1": {
        "zero_optimization": {
            "stage": 1,
            "reduce_bucket_size": 5e8
        }
    },
    "zero2": {
        "zero_optimization": {
            "stage": 2,
            "allgather_partitions": True,
            "allgather_bucket_size": 5e8,
            "reduce_scatter": True,
            "reduce_bucket_size": 5e8,
            "overlap_comm": True
        }
    },
    "zero3": DEEPSPEED_ZERO3_CONFIG["zero_optimization"],
    "zero3_offload": {
        **DEEPSPEED_ZERO3_CONFIG["zero_optimization"],
        "offload_optimizer": {"device": "cpu", "pin_memory": True},
        "offload_param": {"device": "cpu", "pin_memory": True}
    }
}

def get_deepspeed_config(
    zero_stage: str = "zero3",
    bf16: bool = True,
    gradient_checkpointing: bool = True
) -> Dict:
    """
    Get DeepSpeed configuration for specified ZeRO stage.
    """
    config = {
        "bf16": {"enabled": bf16},
        "fp16": {"enabled": not bf16},
        "zero_optimization": ZERO_STAGE_CONFIGS[zero_stage],
        "gradient_accumulation_steps": "auto",
        "gradient_clipping": "auto",
        "train_batch_size": "auto",
        "train_micro_batch_size_per_gpu": "auto"
    }

    if gradient_checkpointing:
        config["activation_checkpointing"] = {
            "partition_activations": True,
            "cpu_checkpointing": True,
            "contiguous_memory_optimization": True,
            "number_checkpoints": None
        }

    return config
```

---

## 2.6.4 FSDP Integration

### PyTorch FSDP with Ray Train

```python
"""
Ray Train with PyTorch FSDP integration
"""
import torch
import torch.nn as nn
from torch.distributed.fsdp import (
    FullyShardedDataParallel as FSDP,
    MixedPrecision,
    BackwardPrefetch,
    ShardingStrategy,
    CPUOffload
)
from torch.distributed.fsdp.wrap import (
    transformer_auto_wrap_policy,
    size_based_auto_wrap_policy
)
from ray import train
from ray.train.torch import TorchTrainer
from ray.train import ScalingConfig
from transformers import AutoModelForCausalLM
from transformers.models.llama.modeling_llama import LlamaDecoderLayer
from typing import Dict, Optional
import functools

def fsdp_train_func(config: Dict):
    """
    Training function with FSDP.
    """
    from torch.distributed.fsdp import FullyShardedDataParallel as FSDP

    # Get distributed context
    context = train.get_context()
    world_size = context.get_world_size()
    rank = context.get_world_rank()
    local_rank = context.get_local_rank()

    device = torch.device(f"cuda:{local_rank}")
    torch.cuda.set_device(device)

    # Load model
    model = AutoModelForCausalLM.from_pretrained(
        config["model_name"],
        torch_dtype=torch.bfloat16
    )

    # FSDP wrapping policy - wrap transformer layers
    auto_wrap_policy = functools.partial(
        transformer_auto_wrap_policy,
        transformer_layer_cls={
            LlamaDecoderLayer,  # Adjust based on model architecture
        }
    )

    # Mixed precision policy
    bf16_policy = MixedPrecision(
        param_dtype=torch.bfloat16,
        reduce_dtype=torch.bfloat16,
        buffer_dtype=torch.bfloat16
    )

    # FSDP configuration
    fsdp_config = {
        "sharding_strategy": ShardingStrategy.FULL_SHARD,
        "cpu_offload": CPUOffload(offload_params=config.get("cpu_offload", False)),
        "mixed_precision": bf16_policy,
        "backward_prefetch": BackwardPrefetch.BACKWARD_PRE,
        "auto_wrap_policy": auto_wrap_policy,
        "device_id": local_rank,
        "use_orig_params": True,  # Required for torch.compile
    }

    # Wrap model with FSDP
    model = FSDP(model, **fsdp_config)

    # Alternatively, use Ray's built-in FSDP support
    # model = train.torch.prepare_model(
    #     model,
    #     parallel_strategy="fsdp",
    #     parallel_strategy_kwargs=fsdp_config
    # )

    # Optimizer (must be created after FSDP wrapping)
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config["learning_rate"],
        weight_decay=config.get("weight_decay", 0.1)
    )

    # Data loading
    train_loader = create_data_loader(config, rank, world_size)

    # Training loop
    model.train()
    global_step = 0

    for epoch in range(config["num_epochs"]):
        for batch in train_loader:
            batch = {k: v.to(device) for k, v in batch.items()}

            # Forward pass
            with torch.cuda.amp.autocast(dtype=torch.bfloat16):
                outputs = model(**batch, labels=batch["input_ids"])
                loss = outputs.loss

            # Backward pass
            loss.backward()

            # Gradient clipping with FSDP
            if config.get("max_grad_norm"):
                model.clip_grad_norm_(config["max_grad_norm"])

            optimizer.step()
            optimizer.zero_grad()

            global_step += 1

            # Logging
            if global_step % config.get("log_interval", 100) == 0:
                train.report({
                    "loss": loss.item(),
                    "step": global_step
                })

            # Checkpointing with FSDP
            if global_step % config.get("checkpoint_interval", 1000) == 0:
                save_fsdp_checkpoint(model, optimizer, global_step, config)

def save_fsdp_checkpoint(
    model: FSDP,
    optimizer: torch.optim.Optimizer,
    step: int,
    config: Dict
):
    """
    Save FSDP checkpoint using full state dict.
    """
    from torch.distributed.fsdp import (
        FullStateDictConfig,
        StateDictType
    )

    # Get full state dict on rank 0
    full_state_config = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)

    with FSDP.state_dict_type(model, StateDictType.FULL_STATE_DICT, full_state_config):
        state_dict = model.state_dict()

        if train.get_context().get_world_rank() == 0:
            checkpoint_dir = f"{config['checkpoint_dir']}/step_{step}"
            os.makedirs(checkpoint_dir, exist_ok=True)

            torch.save({
                "step": step,
                "model_state_dict": state_dict,
            }, f"{checkpoint_dir}/model.pt")

            train.report(
                {"step": step},
                checkpoint=train.Checkpoint.from_directory(checkpoint_dir)
            )

def load_fsdp_checkpoint(
    model: FSDP,
    checkpoint_path: str
):
    """
    Load FSDP checkpoint.
    """
    from torch.distributed.fsdp import (
        FullStateDictConfig,
        StateDictType
    )

    # Load on CPU first
    state_dict = torch.load(
        f"{checkpoint_path}/model.pt",
        map_location="cpu"
    )["model_state_dict"]

    full_state_config = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)

    with FSDP.state_dict_type(model, StateDictType.FULL_STATE_DICT, full_state_config):
        model.load_state_dict(state_dict)

# FSDP sharding strategies comparison
FSDP_STRATEGIES = {
    "full_shard": {
        "strategy": ShardingStrategy.FULL_SHARD,
        "description": "Shard model, gradients, and optimizer states (ZeRO-3 equivalent)",
        "memory": "lowest",
        "communication": "highest"
    },
    "shard_grad_op": {
        "strategy": ShardingStrategy.SHARD_GRAD_OP,
        "description": "Shard gradients and optimizer states (ZeRO-2 equivalent)",
        "memory": "medium",
        "communication": "medium"
    },
    "no_shard": {
        "strategy": ShardingStrategy.NO_SHARD,
        "description": "DDP-like behavior, no sharding",
        "memory": "highest",
        "communication": "lowest"
    },
    "hybrid_shard": {
        "strategy": ShardingStrategy.HYBRID_SHARD,
        "description": "Full shard within node, replicate across nodes",
        "memory": "medium",
        "communication": "medium"
    }
}
```

---

## 2.6.5 Fault Tolerance and Elastic Training

### Automatic Failure Recovery

```python
"""
Fault-tolerant training with Ray Train
"""
import ray
from ray import train
from ray.train.torch import TorchTrainer
from ray.train import ScalingConfig, RunConfig, FailureConfig, CheckpointConfig
import torch
from typing import Dict, Optional
import os

def fault_tolerant_train_func(config: Dict):
    """
    Training function with robust fault tolerance.
    """
    context = train.get_context()
    rank = context.get_world_rank()

    # Initialize model and optimizer
    model = config["model_fn"]()
    model = train.torch.prepare_model(model)

    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config["learning_rate"]
    )

    # Load from checkpoint if resuming
    start_step = 0
    checkpoint = train.get_checkpoint()

    if checkpoint:
        with checkpoint.as_directory() as checkpoint_dir:
            checkpoint_path = os.path.join(checkpoint_dir, "checkpoint.pt")
            state = torch.load(checkpoint_path, map_location="cpu")

            model.load_state_dict(state["model_state_dict"])
            optimizer.load_state_dict(state["optimizer_state_dict"])
            start_step = state["step"]

            if rank == 0:
                print(f"Resumed from checkpoint at step {start_step}")

    # Data loader
    train_loader = config["data_loader_fn"]()
    train_loader = train.torch.prepare_data_loader(train_loader)

    # Training loop
    global_step = start_step

    while global_step < config["total_steps"]:
        for batch in train_loader:
            optimizer.zero_grad()

            outputs = model(**batch)
            loss = outputs.loss

            loss.backward()
            optimizer.step()

            global_step += 1

            # Report metrics
            if global_step % config.get("log_interval", 100) == 0:
                train.report({
                    "loss": loss.item(),
                    "step": global_step
                })

            # Save checkpoint frequently for fault tolerance
            if global_step % config.get("checkpoint_interval", 500) == 0:
                # Create checkpoint
                checkpoint_dir = f"/tmp/checkpoint_{global_step}"
                os.makedirs(checkpoint_dir, exist_ok=True)

                torch.save({
                    "step": global_step,
                    "model_state_dict": model.state_dict(),
                    "optimizer_state_dict": optimizer.state_dict(),
                    "config": config
                }, os.path.join(checkpoint_dir, "checkpoint.pt"))

                train.report(
                    {"step": global_step, "loss": loss.item()},
                    checkpoint=train.Checkpoint.from_directory(checkpoint_dir)
                )

            if global_step >= config["total_steps"]:
                break

def create_fault_tolerant_trainer(config: Dict) -> TorchTrainer:
    """
    Create trainer with comprehensive fault tolerance configuration.
    """
    # Failure configuration
    failure_config = FailureConfig(
        # Maximum number of failures to tolerate
        max_failures=config.get("max_failures", 10),
    )

    # Checkpoint configuration
    checkpoint_config = CheckpointConfig(
        # Keep recent checkpoints
        num_to_keep=config.get("checkpoints_to_keep", 5),
        # Keep best checkpoint based on metric
        checkpoint_score_attribute="loss",
        checkpoint_score_order="min"
    )

    # Run configuration with cloud storage for durability
    run_config = RunConfig(
        name=config.get("run_name", "fault-tolerant-training"),
        # Use cloud storage for checkpoint durability
        storage_path=config.get("storage_path", "s3://my-bucket/ray-results"),
        failure_config=failure_config,
        checkpoint_config=checkpoint_config,
        # Sync checkpoints frequently
        sync_config=train.SyncConfig(
            sync_period=300,  # Sync every 5 minutes
            sync_timeout=1800  # 30 minute timeout
        )
    )

    # Scaling configuration with placement for reliability
    scaling_config = ScalingConfig(
        num_workers=config["num_workers"],
        use_gpu=True,
        resources_per_worker={"GPU": 1, "CPU": 4},
        # Spread workers across nodes for fault isolation
        placement_strategy="SPREAD"
    )

    trainer = TorchTrainer(
        train_loop_per_worker=fault_tolerant_train_func,
        train_loop_config=config,
        scaling_config=scaling_config,
        run_config=run_config
    )

    return trainer

# Elastic training configuration
def create_elastic_trainer(config: Dict) -> TorchTrainer:
    """
    Create trainer that can scale workers up/down during training.

    Note: Requires Ray 2.7+ for full elastic training support.
    """
    # Elastic scaling config
    scaling_config = ScalingConfig(
        num_workers=config["num_workers"],
        use_gpu=True,
        resources_per_worker={"GPU": 1, "CPU": 4},
        # Allow scaling within range
        # min_workers=config.get("min_workers", 2),
        # max_workers=config.get("max_workers", 16),
    )

    trainer = TorchTrainer(
        train_loop_per_worker=fault_tolerant_train_func,
        train_loop_config=config,
        scaling_config=scaling_config,
        run_config=RunConfig(
            name="elastic-training",
            storage_path=config["storage_path"],
            failure_config=FailureConfig(max_failures=20)
        )
    )

    return trainer

# Graceful shutdown handling
def graceful_shutdown_train_func(config: Dict):
    """
    Training function with graceful shutdown handling.
    """
    import signal
    import sys

    shutdown_requested = False

    def signal_handler(signum, frame):
        nonlocal shutdown_requested
        print(f"Received signal {signum}, initiating graceful shutdown...")
        shutdown_requested = True

    # Register signal handlers
    signal.signal(signal.SIGTERM, signal_handler)
    signal.signal(signal.SIGINT, signal_handler)

    # Initialize training
    model = config["model_fn"]()
    model = train.torch.prepare_model(model)
    optimizer = torch.optim.AdamW(model.parameters(), lr=config["learning_rate"])

    train_loader = config["data_loader_fn"]()
    train_loader = train.torch.prepare_data_loader(train_loader)

    global_step = 0

    try:
        for batch in train_loader:
            if shutdown_requested:
                print("Shutdown requested, saving checkpoint...")
                break

            # Training step
            optimizer.zero_grad()
            outputs = model(**batch)
            loss = outputs.loss
            loss.backward()
            optimizer.step()

            global_step += 1

            # Report progress
            if global_step % 100 == 0:
                train.report({"loss": loss.item(), "step": global_step})

    finally:
        # Always save checkpoint on exit
        checkpoint_dir = f"/tmp/final_checkpoint"
        os.makedirs(checkpoint_dir, exist_ok=True)

        torch.save({
            "step": global_step,
            "model_state_dict": model.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
            "shutdown_reason": "graceful" if shutdown_requested else "completed"
        }, os.path.join(checkpoint_dir, "checkpoint.pt"))

        train.report(
            {"final_step": global_step},
            checkpoint=train.Checkpoint.from_directory(checkpoint_dir)
        )
```

---

## 2.6.6 Ray Data Integration

### Streaming Data Pipeline

```python
"""
Ray Data integration for efficient data loading
"""
import ray
from ray import train
from ray.data import Dataset
from ray.train.torch import TorchTrainer
import torch
from typing import Dict, Iterator, List
import pyarrow as pa

def create_ray_data_pipeline(
    data_paths: List[str],
    tokenizer,
    seq_length: int = 2048,
    num_workers: int = 8
) -> Dataset:
    """
    Create a Ray Data pipeline for LLM training.

    Args:
        data_paths: List of data file paths (jsonl, parquet, etc.)
        tokenizer: HuggingFace tokenizer
        seq_length: Maximum sequence length
        num_workers: Number of parallel workers for preprocessing

    Returns:
        Ray Dataset configured for streaming
    """
    # Read data
    if data_paths[0].endswith(".jsonl"):
        dataset = ray.data.read_json(data_paths)
    elif data_paths[0].endswith(".parquet"):
        dataset = ray.data.read_parquet(data_paths)
    else:
        raise ValueError(f"Unsupported file format: {data_paths[0]}")

    # Tokenization function
    def tokenize_batch(batch: Dict[str, List]) -> Dict[str, List]:
        texts = batch["text"]

        # Tokenize
        tokenized = tokenizer(
            texts,
            truncation=True,
            max_length=seq_length,
            padding="max_length",
            return_tensors="np"
        )

        return {
            "input_ids": tokenized["input_ids"].tolist(),
            "attention_mask": tokenized["attention_mask"].tolist(),
            "labels": tokenized["input_ids"].tolist()  # For causal LM
        }

    # Apply tokenization
    dataset = dataset.map_batches(
        tokenize_batch,
        batch_size=1000,
        num_cpus=num_workers
    )

    return dataset

def ray_data_train_func(config: Dict):
    """
    Training function using Ray Data for data loading.
    """
    # Get Ray Data shard for this worker
    train_data_shard = train.get_dataset_shard("train")

    # Model setup
    model = config["model_fn"]()
    model = train.torch.prepare_model(model)

    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config["learning_rate"]
    )

    device = train.torch.get_device()
    global_step = 0

    # Iterate over epochs
    for epoch in range(config["num_epochs"]):
        # Iterate over the data shard
        for batch in train_data_shard.iter_torch_batches(
            batch_size=config["batch_size"],
            prefetch_batches=2
        ):
            # Move to device
            batch = {k: v.to(device) for k, v in batch.items()}

            # Forward pass
            outputs = model(
                input_ids=batch["input_ids"],
                attention_mask=batch["attention_mask"],
                labels=batch["labels"]
            )
            loss = outputs.loss

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            global_step += 1

            # Report metrics
            if global_step % config.get("log_interval", 100) == 0:
                train.report({
                    "loss": loss.item(),
                    "step": global_step,
                    "epoch": epoch
                })

def create_ray_data_trainer(
    config: Dict,
    train_dataset: Dataset
) -> TorchTrainer:
    """
    Create TorchTrainer with Ray Data integration.
    """
    trainer = TorchTrainer(
        train_loop_per_worker=ray_data_train_func,
        train_loop_config=config,
        scaling_config=ScalingConfig(
            num_workers=config["num_workers"],
            use_gpu=True
        ),
        run_config=RunConfig(
            name="ray-data-training",
            storage_path=config["storage_path"]
        ),
        # Pass datasets to be automatically sharded
        datasets={"train": train_dataset}
    )

    return trainer

# Streaming dataset for very large data
class StreamingTokenDataset:
    """
    Streaming dataset that yields pre-tokenized batches.
    For use with Ray Data's streaming capabilities.
    """

    def __init__(
        self,
        data_source: str,
        tokenizer,
        seq_length: int = 2048,
        buffer_size: int = 10000
    ):
        self.data_source = data_source
        self.tokenizer = tokenizer
        self.seq_length = seq_length
        self.buffer_size = buffer_size
        self.token_buffer = []

    def stream_tokens(self) -> Iterator[Dict[str, torch.Tensor]]:
        """Stream tokenized sequences"""
        # Read from source
        dataset = ray.data.read_json(self.data_source)

        for batch in dataset.iter_batches(batch_size=100):
            for text in batch["text"]:
                # Tokenize
                tokens = self.tokenizer.encode(text)
                self.token_buffer.extend(tokens)

                # Yield full sequences
                while len(self.token_buffer) >= self.seq_length + 1:
                    sequence = self.token_buffer[:self.seq_length + 1]
                    self.token_buffer = self.token_buffer[self.seq_length:]

                    yield {
                        "input_ids": torch.tensor(sequence[:-1]),
                        "labels": torch.tensor(sequence[1:])
                    }
```

---

## 2.6.7 Cloud Deployment

### Kubernetes Deployment

```yaml
# ray-cluster.yaml - Ray cluster configuration for Kubernetes
apiVersion: ray.io/v1
kind: RayCluster
metadata:
  name: llm-training-cluster
spec:
  rayVersion: '2.9.0'
  headGroupSpec:
    rayStartParams:
      dashboard-host: '0.0.0.0'
    template:
      spec:
        containers:
        - name: ray-head
          image: rayproject/ray-ml:2.9.0-gpu
          resources:
            limits:
              cpu: "8"
              memory: "32Gi"
            requests:
              cpu: "4"
              memory: "16Gi"
          ports:
          - containerPort: 6379
            name: gcs
          - containerPort: 8265
            name: dashboard
          - containerPort: 10001
            name: client
  workerGroupSpecs:
  - replicas: 8
    minReplicas: 4
    maxReplicas: 16
    groupName: gpu-workers
    rayStartParams: {}
    template:
      spec:
        containers:
        - name: ray-worker
          image: rayproject/ray-ml:2.9.0-gpu
          resources:
            limits:
              cpu: "8"
              memory: "64Gi"
              nvidia.com/gpu: "1"
            requests:
              cpu: "4"
              memory: "32Gi"
              nvidia.com/gpu: "1"
          volumeMounts:
          - name: training-data
            mountPath: /data
        volumes:
        - name: training-data
          persistentVolumeClaim:
            claimName: training-data-pvc
        tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
        nodeSelector:
          cloud.google.com/gke-accelerator: nvidia-tesla-a100
```

### GKE Deployment Script

```python
"""
Deploy Ray Train job on Google Kubernetes Engine
"""
import subprocess
import yaml
from typing import Dict
import os

def deploy_ray_cluster_gke(
    cluster_name: str,
    num_workers: int,
    gpu_type: str = "nvidia-tesla-a100",
    zone: str = "us-central1-a"
):
    """
    Deploy Ray cluster on GKE.
    """
    # Create GKE cluster with GPU nodes
    create_cluster_cmd = f"""
    gcloud container clusters create {cluster_name} \
        --zone {zone} \
        --num-nodes 1 \
        --machine-type n1-standard-8

    gcloud container node-pools create gpu-pool \
        --cluster {cluster_name} \
        --zone {zone} \
        --num-nodes {num_workers} \
        --machine-type a2-highgpu-1g \
        --accelerator type={gpu_type},count=1
    """

    subprocess.run(create_cluster_cmd, shell=True, check=True)

    # Install Ray operator
    subprocess.run([
        "kubectl", "apply", "-f",
        "https://raw.githubusercontent.com/ray-project/kuberay/master/ray-operator/config/default/ray-operator.yaml"
    ], check=True)

    # Deploy Ray cluster
    subprocess.run([
        "kubectl", "apply", "-f", "ray-cluster.yaml"
    ], check=True)

def submit_training_job(
    cluster_address: str,
    config: Dict,
    script_path: str
):
    """
    Submit training job to Ray cluster.
    """
    import ray
    from ray.job_submission import JobSubmissionClient

    client = JobSubmissionClient(cluster_address)

    # Submit job
    job_id = client.submit_job(
        entrypoint=f"python {script_path}",
        runtime_env={
            "working_dir": ".",
            "pip": [
                "torch>=2.0",
                "transformers>=4.30",
                "deepspeed>=0.10"
            ],
            "env_vars": {
                "TRAINING_CONFIG": yaml.dump(config)
            }
        }
    )

    print(f"Submitted job: {job_id}")
    return job_id

# AWS deployment
def deploy_ray_cluster_aws(
    cluster_name: str,
    num_workers: int,
    instance_type: str = "p4d.24xlarge"
):
    """
    Deploy Ray cluster on AWS using ray up.
    """
    cluster_config = {
        "cluster_name": cluster_name,
        "provider": {
            "type": "aws",
            "region": "us-east-1"
        },
        "head_node": {
            "InstanceType": "m5.2xlarge",
            "ImageId": "ami-0c55b159cbfafe1f0"  # Ray AMI
        },
        "worker_nodes": {
            "InstanceType": instance_type,
            "min_workers": num_workers,
            "max_workers": num_workers
        },
        "setup_commands": [
            "pip install torch transformers deepspeed"
        ]
    }

    # Write config
    config_path = f"/tmp/{cluster_name}_config.yaml"
    with open(config_path, 'w') as f:
        yaml.dump(cluster_config, f)

    # Launch cluster
    subprocess.run(["ray", "up", config_path, "-y"], check=True)

    return config_path
```

---

## 2.6.8 Performance Optimization

### Profiling and Optimization

```python
"""
Performance optimization for Ray Train
"""
import ray
from ray import train
from ray.train.torch import TorchTrainer
from ray.train import ScalingConfig
import torch
from typing import Dict
import time

class PerformanceProfiler:
    """
    Profile Ray Train performance.
    """

    def __init__(self):
        self.timings = {}
        self.start_times = {}

    def start(self, name: str):
        self.start_times[name] = time.perf_counter()

    def stop(self, name: str):
        if name in self.start_times:
            elapsed = time.perf_counter() - self.start_times[name]
            if name not in self.timings:
                self.timings[name] = []
            self.timings[name].append(elapsed)

    def report(self) -> Dict:
        return {
            name: {
                "mean": sum(times) / len(times),
                "total": sum(times),
                "count": len(times)
            }
            for name, times in self.timings.items()
        }

def optimized_train_func(config: Dict):
    """
    Training function with performance optimizations.
    """
    import torch.backends.cudnn as cudnn

    # Enable cuDNN optimizations
    cudnn.benchmark = True
    cudnn.deterministic = False

    # Compile model for faster execution (PyTorch 2.0+)
    model = config["model_fn"]()

    if config.get("compile_model", True):
        model = torch.compile(model, mode="reduce-overhead")

    model = train.torch.prepare_model(model)

    # Optimizer with fused operations
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config["learning_rate"],
        fused=True  # Fused AdamW for better performance
    )

    # Data loading optimizations
    train_loader = torch.utils.data.DataLoader(
        config["dataset"],
        batch_size=config["batch_size"],
        num_workers=config.get("num_workers", 4),
        pin_memory=True,
        persistent_workers=True,
        prefetch_factor=2
    )
    train_loader = train.torch.prepare_data_loader(train_loader)

    # Training loop with optimizations
    model.train()
    profiler = PerformanceProfiler()

    for epoch in range(config["num_epochs"]):
        for batch_idx, batch in enumerate(train_loader):
            profiler.start("data_transfer")
            batch = {k: v.to(train.torch.get_device(), non_blocking=True)
                    for k, v in batch.items()}
            profiler.stop("data_transfer")

            profiler.start("forward")
            with torch.cuda.amp.autocast(dtype=torch.bfloat16):
                outputs = model(**batch)
                loss = outputs.loss
            profiler.stop("forward")

            profiler.start("backward")
            loss.backward()
            profiler.stop("backward")

            profiler.start("optimizer")
            optimizer.step()
            optimizer.zero_grad(set_to_none=True)  # More efficient than zero_grad()
            profiler.stop("optimizer")

            # Report metrics including profiling
            if batch_idx % 100 == 0:
                metrics = {
                    "loss": loss.item(),
                    "batch": batch_idx,
                    **{f"time_{k}": v["mean"]
                       for k, v in profiler.report().items()}
                }
                train.report(metrics)

# Memory optimization
def memory_optimized_train_func(config: Dict):
    """
    Training function optimized for memory efficiency.
    """
    # Enable gradient checkpointing
    model = config["model_fn"]()

    if hasattr(model, 'gradient_checkpointing_enable'):
        model.gradient_checkpointing_enable()

    model = train.torch.prepare_model(model)

    # Use memory-efficient attention if available
    if hasattr(torch.nn.functional, 'scaled_dot_product_attention'):
        # PyTorch 2.0+ memory-efficient attention is automatic
        pass

    # Gradient accumulation for effective larger batch sizes
    accumulation_steps = config.get("gradient_accumulation_steps", 4)

    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config["learning_rate"]
    )

    train_loader = config["data_loader_fn"]()
    train_loader = train.torch.prepare_data_loader(train_loader)

    model.train()
    optimizer.zero_grad()

    for batch_idx, batch in enumerate(train_loader):
        batch = {k: v.to(train.torch.get_device()) for k, v in batch.items()}

        # Forward with autocast
        with torch.cuda.amp.autocast(dtype=torch.bfloat16):
            outputs = model(**batch)
            loss = outputs.loss / accumulation_steps

        # Backward
        loss.backward()

        # Optimizer step after accumulation
        if (batch_idx + 1) % accumulation_steps == 0:
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

            optimizer.step()
            optimizer.zero_grad(set_to_none=True)

            # Clear cache periodically
            if batch_idx % 1000 == 0:
                torch.cuda.empty_cache()

        if batch_idx % 100 == 0:
            train.report({
                "loss": loss.item() * accumulation_steps,
                "batch": batch_idx,
                "memory_allocated_gb": torch.cuda.memory_allocated() / 1e9
            })

# Best practices configuration
OPTIMIZED_CONFIG = {
    # Training
    "batch_size": 8,
    "gradient_accumulation_steps": 4,
    "learning_rate": 3e-4,
    "num_epochs": 1,

    # Performance
    "compile_model": True,
    "num_workers": 4,

    # Memory
    "gradient_checkpointing": True,
    "mixed_precision": True,

    # Data
    "prefetch_factor": 2,
    "pin_memory": True,
    "persistent_workers": True
}
```

---

## Troubleshooting

### Common Issues and Solutions

| Issue | Cause | Solution |
|-------|-------|----------|
| Workers fail to start | Resource constraints | Check GPU availability, reduce num_workers |
| OOM errors | Batch size too large | Reduce batch size, enable gradient checkpointing |
| Slow data loading | CPU bottleneck | Increase num_workers, use Ray Data |
| Checkpoint failures | Storage issues | Use cloud storage (S3, GCS), check permissions |
| Training hangs | NCCL issues | Set NCCL_DEBUG=INFO, check network |
| Inconsistent results | Non-determinism | Set seeds, use deterministic mode |

### Debug Commands

```bash
# Check Ray cluster status
ray status

# View Ray dashboard
ray dashboard

# Check worker logs
ray logs worker-<id>

# Monitor GPU usage
watch -n 1 nvidia-smi

# Check Ray memory usage
ray memory

# Debug NCCL
export NCCL_DEBUG=INFO
export NCCL_DEBUG_SUBSYS=ALL
```

---

## Glossary

| Term | Definition |
|------|------------|
| **Ray Train** | Ray's distributed training library |
| **TorchTrainer** | Ray Train wrapper for PyTorch |
| **ScalingConfig** | Configuration for worker count and resources |
| **RunConfig** | Configuration for experiment tracking and checkpointing |
| **Ray Data** | Ray's distributed data processing library |
| **Placement Strategy** | How workers are distributed across nodes |
| **Fault Tolerance** | Ability to recover from worker failures |
| **Elastic Training** | Dynamic scaling of workers during training |

---

## References

1. Ray Documentation. "Ray Train: Scalable Model Training" https://docs.ray.io/en/latest/train/train.html
2. Ray Documentation. "Distributed Training with PyTorch" https://docs.ray.io/en/latest/train/getting-started-pytorch.html
3. Ray Documentation. "DeepSpeed Integration" https://docs.ray.io/en/latest/train/examples/pytorch/deepspeed_finetune
4. Anyscale Blog. "Ray for Fault-Tolerant Distributed LLM Fine-Tuning"
5. Google Cloud. "Train a model with PyTorch, Ray, and GKE"
6. Ray GitHub. "Ray Train Examples" https://github.com/ray-project/ray/tree/master/python/ray/train/examples
7. Moritz, P., et al. (2018). "Ray: A Distributed Framework for Emerging AI Applications"

---

> **Navigation**
> [← 2.5 Training Monitoring](2.5_training_monitoring_debugging.md) | **[Index](../README.md#15-repository-structure)** | [3.1 SFT →](../03_fine_tuning/3.1_supervised_fine_tuning.md)
