# Pre-training Data Mix & Curriculum Guide

## Executive Summary

The composition of pre-training data fundamentally shapes an LLM's capabilities, biases, and performance characteristics. This guide covers strategies for optimizing data mixtures, implementing curriculum learning approaches, and avoiding contamination issues that can invalidate evaluation results.

## Prerequisites

- Understanding of LLM pre-training basics
- Familiarity with data processing pipelines (Document 1.2)
- Knowledge of distributed training (Document 2.3)
- Access to diverse data sources

---

## 2.4.1 Data Mix Fundamentals

### Why Data Composition Matters

The training data mixture determines:
- **Knowledge coverage**: What the model "knows"
- **Reasoning patterns**: How the model thinks
- **Language style**: How the model communicates
- **Bias profile**: What assumptions the model makes
- **Task capabilities**: What the model can do well

### Quality vs Quantity Tradeoffs

```python
"""
Data mix quality analysis framework
"""
from dataclasses import dataclass
from typing import Dict, List, Optional
from enum import Enum
import numpy as np

class DataQuality(Enum):
    """Quality tiers for training data"""
    GOLD = "gold"        # Human-curated, verified
    SILVER = "silver"    # Filtered, moderate quality
    BRONZE = "bronze"    # Lightly filtered
    RAW = "raw"          # Unfiltered

@dataclass
class DataSource:
    """Represents a data source in the training mix"""
    name: str
    tokens: int
    quality: DataQuality
    domain: str
    diversity_score: float  # 0-1
    recency: Optional[str] = None
    language: str = "en"

    @property
    def effective_tokens(self) -> float:
        """Quality-weighted token count"""
        quality_weights = {
            DataQuality.GOLD: 1.0,
            DataQuality.SILVER: 0.7,
            DataQuality.BRONZE: 0.4,
            DataQuality.RAW: 0.2
        }
        return self.tokens * quality_weights[self.quality]

class DataMixOptimizer:
    """Optimize data mixture for pre-training"""

    def __init__(self, target_tokens: int):
        self.target_tokens = target_tokens
        self.sources: List[DataSource] = []
        self.domain_targets: Dict[str, float] = {}

    def add_source(self, source: DataSource):
        """Add a data source to the pool"""
        self.sources.append(source)

    def set_domain_targets(self, targets: Dict[str, float]):
        """
        Set target proportions for each domain.

        Args:
            targets: Dict mapping domain to proportion (0-1)
        """
        total = sum(targets.values())
        if abs(total - 1.0) > 0.01:
            raise ValueError(f"Domain targets must sum to 1.0, got {total}")
        self.domain_targets = targets

    def optimize_mix(self) -> Dict[str, float]:
        """
        Compute optimal sampling weights for each source.

        Returns:
            Dict mapping source name to sampling probability
        """
        weights = {}

        # Group sources by domain
        domain_sources: Dict[str, List[DataSource]] = {}
        for source in self.sources:
            if source.domain not in domain_sources:
                domain_sources[source.domain] = []
            domain_sources[source.domain].append(source)

        # Compute weights within each domain
        for domain, target_prop in self.domain_targets.items():
            if domain not in domain_sources:
                print(f"Warning: No sources for domain {domain}")
                continue

            sources = domain_sources[domain]
            total_effective = sum(s.effective_tokens for s in sources)

            for source in sources:
                # Weight by effective quality and diversity
                source_weight = (
                    source.effective_tokens / total_effective
                    * target_prop
                    * (0.5 + 0.5 * source.diversity_score)  # Diversity bonus
                )
                weights[source.name] = source_weight

        # Normalize
        total_weight = sum(weights.values())
        return {k: v / total_weight for k, v in weights.items()}

    def estimate_epochs(self, weights: Dict[str, float]) -> Dict[str, float]:
        """
        Estimate epochs per source given weights.

        Higher quality data may be seen multiple times,
        while lower quality data is seen less than once.
        """
        epochs = {}
        for source in self.sources:
            if source.name in weights:
                target_tokens = self.target_tokens * weights[source.name]
                epochs[source.name] = target_tokens / source.tokens
        return epochs

# Example usage
optimizer = DataMixOptimizer(target_tokens=2_000_000_000_000)  # 2T tokens

# Add data sources
optimizer.add_source(DataSource(
    name="common_crawl_filtered",
    tokens=800_000_000_000,
    quality=DataQuality.BRONZE,
    domain="web",
    diversity_score=0.9
))

optimizer.add_source(DataSource(
    name="fineweb_edu",
    tokens=200_000_000_000,
    quality=DataQuality.SILVER,
    domain="web",
    diversity_score=0.7
))

optimizer.add_source(DataSource(
    name="wikipedia",
    tokens=20_000_000_000,
    quality=DataQuality.GOLD,
    domain="reference",
    diversity_score=0.6
))

optimizer.add_source(DataSource(
    name="arxiv_papers",
    tokens=50_000_000_000,
    quality=DataQuality.GOLD,
    domain="scientific",
    diversity_score=0.5
))

optimizer.add_source(DataSource(
    name="github_code",
    tokens=100_000_000_000,
    quality=DataQuality.SILVER,
    domain="code",
    diversity_score=0.8
))

optimizer.add_source(DataSource(
    name="books3",
    tokens=30_000_000_000,
    quality=DataQuality.SILVER,
    domain="books",
    diversity_score=0.7
))

# Set domain targets
optimizer.set_domain_targets({
    "web": 0.50,
    "code": 0.15,
    "scientific": 0.10,
    "books": 0.10,
    "reference": 0.10,
    "conversational": 0.05
})

weights = optimizer.optimize_mix()
epochs = optimizer.estimate_epochs(weights)

print("Optimized Data Mix Weights:")
for source, weight in sorted(weights.items(), key=lambda x: -x[1]):
    print(f"  {source}: {weight:.2%} (epochs: {epochs.get(source, 0):.2f})")
```

### Domain Balance Considerations

```python
"""
Domain balance analysis and adjustment
"""
import pandas as pd
from collections import defaultdict
from typing import Tuple

class DomainBalancer:
    """Analyze and adjust domain balance in training data"""

    # Target proportions based on empirical research
    DEFAULT_TARGETS = {
        "web_crawl": (0.40, 0.60),      # (min, max)
        "books": (0.05, 0.15),
        "code": (0.05, 0.15),
        "scientific": (0.05, 0.10),
        "reference": (0.05, 0.10),
        "conversational": (0.03, 0.10),
        "news": (0.02, 0.08),
        "legal": (0.01, 0.05),
        "medical": (0.01, 0.05),
    }

    def __init__(self, domain_tokens: Dict[str, int]):
        self.domain_tokens = domain_tokens
        self.total_tokens = sum(domain_tokens.values())

    def get_current_proportions(self) -> Dict[str, float]:
        """Get current proportion of each domain"""
        return {
            domain: tokens / self.total_tokens
            for domain, tokens in self.domain_tokens.items()
        }

    def check_balance(self) -> Dict[str, Tuple[str, float]]:
        """
        Check if current proportions are within target ranges.

        Returns:
            Dict mapping domain to (status, deviation)
            status: "ok", "under", "over"
        """
        results = {}
        current = self.get_current_proportions()

        for domain, (min_prop, max_prop) in self.DEFAULT_TARGETS.items():
            prop = current.get(domain, 0.0)

            if prop < min_prop:
                results[domain] = ("under", min_prop - prop)
            elif prop > max_prop:
                results[domain] = ("over", prop - max_prop)
            else:
                results[domain] = ("ok", 0.0)

        return results

    def recommend_adjustments(self) -> List[Dict]:
        """Generate specific recommendations for rebalancing"""
        balance = self.check_balance()
        current = self.get_current_proportions()
        recommendations = []

        for domain, (status, deviation) in balance.items():
            if status == "under":
                target_tokens = (self.DEFAULT_TARGETS[domain][0] +
                               self.DEFAULT_TARGETS[domain][1]) / 2 * self.total_tokens
                current_tokens = self.domain_tokens.get(domain, 0)
                recommendations.append({
                    "domain": domain,
                    "action": "increase",
                    "current_prop": current.get(domain, 0),
                    "target_prop": self.DEFAULT_TARGETS[domain][0],
                    "tokens_needed": target_tokens - current_tokens
                })
            elif status == "over":
                target_tokens = self.DEFAULT_TARGETS[domain][1] * self.total_tokens
                current_tokens = self.domain_tokens.get(domain, 0)
                recommendations.append({
                    "domain": domain,
                    "action": "decrease",
                    "current_prop": current.get(domain, 0),
                    "target_prop": self.DEFAULT_TARGETS[domain][1],
                    "tokens_to_remove": current_tokens - target_tokens
                })

        return sorted(recommendations,
                     key=lambda x: x.get("tokens_needed", x.get("tokens_to_remove", 0)),
                     reverse=True)

# Example usage
balancer = DomainBalancer({
    "web_crawl": 700_000_000_000,
    "books": 50_000_000_000,
    "code": 100_000_000_000,
    "scientific": 30_000_000_000,
    "reference": 20_000_000_000,
    "conversational": 50_000_000_000,
    "news": 30_000_000_000,
    "legal": 10_000_000_000,
    "medical": 10_000_000_000,
})

print("Domain Balance Check:")
for domain, (status, dev) in balancer.check_balance().items():
    symbol = "✓" if status == "ok" else "↑" if status == "under" else "↓"
    print(f"  {symbol} {domain}: {status} (deviation: {dev:.2%})")
```

---

## 2.4.2 Data Categories & Proportions

### Reference Data Mix (Based on Published Models)

| Category | Proportion Range | Quality Requirements | Key Sources |
|----------|-----------------|---------------------|-------------|
| Web Crawl | 40-60% | Heavy filtering, deduplication | Common Crawl, FineWeb, RefinedWeb |
| Books | 5-15% | ISBN verified, OCR quality check | Books3, Project Gutenberg, OpenLibrary |
| Code | 5-15% | Syntax valid, license filtered | GitHub, Stack Overflow, GitLab |
| Scientific | 5-10% | Peer reviewed, citation filtered | arXiv, PubMed, Semantic Scholar |
| Conversational | 5-10% | Toxicity filtered, quality scored | Reddit, Discord dumps, Forums |
| Reference | 5-10% | Authority verified | Wikipedia, Encyclopedia, Wikibooks |
| Curated | 1-5% | Human verified instructions | FLAN, ShareGPT, OpenAssistant |

### Detailed Category Implementation

```python
"""
Data category definitions and sampling configurations
"""
from dataclasses import dataclass, field
from typing import List, Callable, Optional
import re

@dataclass
class CategoryConfig:
    """Configuration for a data category"""
    name: str
    target_proportion: float
    min_proportion: float
    max_proportion: float
    quality_filters: List[Callable]
    dedup_method: str = "minhash"
    dedup_threshold: float = 0.8
    sample_strategy: str = "uniform"  # uniform, quality_weighted, diversity_weighted

@dataclass
class WebCrawlConfig(CategoryConfig):
    """Configuration for web crawl data"""
    name: str = "web_crawl"
    target_proportion: float = 0.50
    min_proportion: float = 0.40
    max_proportion: float = 0.60
    quality_filters: List[Callable] = field(default_factory=list)

    # Web-specific settings
    min_content_length: int = 200
    max_content_length: int = 100000
    min_words: int = 50
    blocked_domains: List[str] = field(default_factory=lambda: [
        "pinterest.com", "instagram.com", "facebook.com",
        "tiktok.com", "youtube.com"  # Video sites without transcripts
    ])
    required_language_confidence: float = 0.8

    def __post_init__(self):
        self.quality_filters = [
            self._filter_length,
            self._filter_language,
            self._filter_domains,
            self._filter_quality_heuristics
        ]

    def _filter_length(self, doc: dict) -> bool:
        text = doc.get("text", "")
        word_count = len(text.split())
        return (self.min_content_length <= len(text) <= self.max_content_length
                and word_count >= self.min_words)

    def _filter_language(self, doc: dict) -> bool:
        lang_score = doc.get("language_score", 0)
        return lang_score >= self.required_language_confidence

    def _filter_domains(self, doc: dict) -> bool:
        url = doc.get("url", "")
        return not any(blocked in url for blocked in self.blocked_domains)

    def _filter_quality_heuristics(self, doc: dict) -> bool:
        """Apply FineWeb-style quality heuristics"""
        text = doc.get("text", "")

        # Check for excessive punctuation
        punct_ratio = len(re.findall(r'[^\w\s]', text)) / max(len(text), 1)
        if punct_ratio > 0.3:
            return False

        # Check for excessive capitalization
        if text.isupper():
            return False

        # Check for repeated content
        lines = text.split('\n')
        unique_lines = set(lines)
        if len(unique_lines) / max(len(lines), 1) < 0.5:
            return False

        return True

@dataclass
class CodeConfig(CategoryConfig):
    """Configuration for code data"""
    name: str = "code"
    target_proportion: float = 0.12
    min_proportion: float = 0.05
    max_proportion: float = 0.15
    quality_filters: List[Callable] = field(default_factory=list)

    # Code-specific settings
    supported_languages: List[str] = field(default_factory=lambda: [
        "python", "javascript", "typescript", "java", "c", "cpp", "go",
        "rust", "ruby", "php", "swift", "kotlin", "scala", "shell", "sql"
    ])
    min_stars: int = 5  # For GitHub repos
    excluded_licenses: List[str] = field(default_factory=lambda: [
        "AGPL-3.0", "GPL-3.0"  # Viral licenses
    ])
    max_file_size: int = 1_000_000  # 1MB
    min_file_size: int = 100  # Bytes

    def __post_init__(self):
        self.quality_filters = [
            self._filter_language,
            self._filter_size,
            self._filter_generated,
            self._filter_quality
        ]

    def _filter_language(self, doc: dict) -> bool:
        lang = doc.get("language", "").lower()
        return lang in self.supported_languages

    def _filter_size(self, doc: dict) -> bool:
        size = len(doc.get("content", "").encode())
        return self.min_file_size <= size <= self.max_file_size

    def _filter_generated(self, doc: dict) -> bool:
        """Filter out auto-generated code"""
        content = doc.get("content", "")
        generated_markers = [
            "auto-generated", "do not edit",
            "generated by", "automatically generated"
        ]
        return not any(marker in content.lower() for marker in generated_markers)

    def _filter_quality(self, doc: dict) -> bool:
        """Basic code quality checks"""
        content = doc.get("content", "")

        # Check for reasonable line lengths
        lines = content.split('\n')
        long_lines = sum(1 for line in lines if len(line) > 500)
        if long_lines / max(len(lines), 1) > 0.1:
            return False

        # Check for minified code
        avg_line_len = sum(len(l) for l in lines) / max(len(lines), 1)
        if avg_line_len > 200:
            return False

        return True

@dataclass
class ScientificConfig(CategoryConfig):
    """Configuration for scientific paper data"""
    name: str = "scientific"
    target_proportion: float = 0.08
    min_proportion: float = 0.05
    max_proportion: float = 0.10
    quality_filters: List[Callable] = field(default_factory=list)

    # Scientific-specific settings
    required_sections: List[str] = field(default_factory=lambda: [
        "abstract", "introduction", "method", "result", "conclusion"
    ])
    min_references: int = 5
    preferred_subjects: List[str] = field(default_factory=lambda: [
        "cs", "stat", "math", "physics", "econ", "q-bio"
    ])

# Example: Creating a complete data mix configuration
def create_default_mix() -> Dict[str, CategoryConfig]:
    """Create default data mix configuration"""
    return {
        "web_crawl": WebCrawlConfig(),
        "code": CodeConfig(),
        "scientific": ScientificConfig(),
        "books": CategoryConfig(
            name="books",
            target_proportion=0.10,
            min_proportion=0.05,
            max_proportion=0.15,
            quality_filters=[]
        ),
        "reference": CategoryConfig(
            name="reference",
            target_proportion=0.08,
            min_proportion=0.05,
            max_proportion=0.10,
            quality_filters=[]
        ),
        "conversational": CategoryConfig(
            name="conversational",
            target_proportion=0.07,
            min_proportion=0.03,
            max_proportion=0.10,
            quality_filters=[]
        ),
        "curated": CategoryConfig(
            name="curated",
            target_proportion=0.05,
            min_proportion=0.01,
            max_proportion=0.05,
            quality_filters=[]
        ),
    }
```

---

## 2.4.3 Data Mix Optimization

### Ablation Study Methodology

```python
"""
Data mix ablation study framework
"""
import itertools
from typing import Dict, List, Tuple
import json
from pathlib import Path

class AblationStudy:
    """Framework for systematic data mix ablation studies"""

    def __init__(
        self,
        base_config: Dict[str, float],
        model_size: str = "1B",
        tokens_per_run: int = 50_000_000_000,  # 50B tokens per ablation
    ):
        self.base_config = base_config
        self.model_size = model_size
        self.tokens_per_run = tokens_per_run
        self.results: List[Dict] = []

    def generate_variations(
        self,
        domain: str,
        proportions: List[float]
    ) -> List[Dict[str, float]]:
        """
        Generate mix variations for a single domain.
        Other domains are scaled proportionally.
        """
        variations = []
        base_prop = self.base_config[domain]
        other_total = 1 - base_prop

        for prop in proportions:
            new_config = {}
            scale_factor = (1 - prop) / other_total

            for d, p in self.base_config.items():
                if d == domain:
                    new_config[d] = prop
                else:
                    new_config[d] = p * scale_factor

            variations.append(new_config)

        return variations

    def generate_full_factorial(
        self,
        domain_levels: Dict[str, List[float]]
    ) -> List[Dict[str, float]]:
        """
        Generate full factorial design across multiple domains.
        Warning: Can produce many configurations!
        """
        domains = list(domain_levels.keys())
        levels = [domain_levels[d] for d in domains]

        variations = []
        for combo in itertools.product(*levels):
            config = dict(zip(domains, combo))

            # Fill in remaining domains from base config
            for d, p in self.base_config.items():
                if d not in config:
                    config[d] = p

            # Normalize to sum to 1
            total = sum(config.values())
            config = {d: p / total for d, p in config.items()}
            variations.append(config)

        return variations

    def record_result(
        self,
        config: Dict[str, float],
        metrics: Dict[str, float],
        training_metadata: Dict
    ):
        """Record results from an ablation run"""
        self.results.append({
            "config": config,
            "metrics": metrics,
            "metadata": training_metadata
        })

    def analyze_impact(self, metric: str = "avg_downstream") -> Dict[str, float]:
        """
        Analyze domain impact on a specific metric.

        Returns correlation between each domain's proportion
        and the target metric.
        """
        import numpy as np

        if len(self.results) < 3:
            raise ValueError("Need at least 3 results for analysis")

        impacts = {}
        metric_values = np.array([r["metrics"][metric] for r in self.results])

        for domain in self.base_config.keys():
            domain_props = np.array([
                r["config"].get(domain, 0) for r in self.results
            ])

            # Compute correlation
            if np.std(domain_props) > 0:
                correlation = np.corrcoef(domain_props, metric_values)[0, 1]
                impacts[domain] = correlation
            else:
                impacts[domain] = 0.0

        return dict(sorted(impacts.items(), key=lambda x: abs(x[1]), reverse=True))

    def save_results(self, path: str):
        """Save results to JSON"""
        with open(path, 'w') as f:
            json.dump({
                "base_config": self.base_config,
                "model_size": self.model_size,
                "tokens_per_run": self.tokens_per_run,
                "results": self.results
            }, f, indent=2)

# Example ablation study
study = AblationStudy(
    base_config={
        "web_crawl": 0.50,
        "code": 0.12,
        "scientific": 0.08,
        "books": 0.10,
        "reference": 0.08,
        "conversational": 0.07,
        "curated": 0.05
    },
    model_size="1B",
    tokens_per_run=50_000_000_000
)

# Generate variations for code proportion study
code_variations = study.generate_variations(
    domain="code",
    proportions=[0.05, 0.10, 0.15, 0.20, 0.25]
)

print("Code proportion ablation configurations:")
for i, config in enumerate(code_variations):
    print(f"\nConfig {i + 1}:")
    for domain, prop in sorted(config.items(), key=lambda x: -x[1]):
        print(f"  {domain}: {prop:.2%}")
```

### DoReMi: Domain Reweighting

```python
"""
DoReMi-style domain reweighting implementation
Based on: https://arxiv.org/abs/2305.10429
"""
import torch
import torch.nn.functional as F
from typing import Dict, List, Tuple

class DoReMi:
    """
    Domain Reweighting with Minimax Optimization.

    Finds domain weights that minimize worst-case excess loss
    compared to a reference model.
    """

    def __init__(
        self,
        domains: List[str],
        initial_weights: Dict[str, float],
        step_size: float = 0.01,
        smoothing: float = 0.1
    ):
        self.domains = domains
        self.step_size = step_size
        self.smoothing = smoothing

        # Initialize weights
        self.weights = torch.tensor([
            initial_weights.get(d, 1.0 / len(domains))
            for d in domains
        ], dtype=torch.float32)
        self.weights = self.weights / self.weights.sum()

    def update_weights(
        self,
        proxy_losses: Dict[str, float],
        reference_losses: Dict[str, float]
    ) -> Dict[str, float]:
        """
        Update domain weights based on excess loss.

        Args:
            proxy_losses: Per-domain losses from proxy model
            reference_losses: Per-domain losses from reference model

        Returns:
            Updated domain weights
        """
        # Compute excess losses
        excess_losses = torch.tensor([
            proxy_losses[d] - reference_losses[d]
            for d in self.domains
        ])

        # Exponentiated gradient update
        self.weights = self.weights * torch.exp(self.step_size * excess_losses)

        # Apply smoothing to prevent domain collapse
        uniform = torch.ones_like(self.weights) / len(self.domains)
        self.weights = (1 - self.smoothing) * self.weights + self.smoothing * uniform

        # Normalize
        self.weights = self.weights / self.weights.sum()

        return {d: w.item() for d, w in zip(self.domains, self.weights)}

    def run_optimization(
        self,
        proxy_model,
        reference_model,
        domain_dataloaders: Dict[str, 'DataLoader'],
        num_steps: int = 1000
    ) -> Dict[str, float]:
        """
        Run full DoReMi optimization loop.

        Args:
            proxy_model: Small proxy model for weight optimization
            reference_model: Reference model trained on uniform mixture
            domain_dataloaders: Per-domain data loaders
            num_steps: Number of optimization steps

        Returns:
            Final optimized domain weights
        """
        weight_history = []

        for step in range(num_steps):
            # Compute per-domain losses
            proxy_losses = {}
            reference_losses = {}

            for domain in self.domains:
                loader = domain_dataloaders[domain]
                batch = next(iter(loader))

                with torch.no_grad():
                    proxy_loss = proxy_model(batch).loss.item()
                    ref_loss = reference_model(batch).loss.item()

                proxy_losses[domain] = proxy_loss
                reference_losses[domain] = ref_loss

            # Update weights
            new_weights = self.update_weights(proxy_losses, reference_losses)
            weight_history.append(new_weights)

            if step % 100 == 0:
                print(f"Step {step}:")
                for d, w in sorted(new_weights.items(), key=lambda x: -x[1]):
                    excess = proxy_losses[d] - reference_losses[d]
                    print(f"  {d}: {w:.3f} (excess loss: {excess:.4f})")

        return dict(zip(self.domains, self.weights.tolist()))

# Example configuration
doremi = DoReMi(
    domains=["web", "code", "scientific", "books", "reference"],
    initial_weights={
        "web": 0.5,
        "code": 0.15,
        "scientific": 0.1,
        "books": 0.15,
        "reference": 0.1
    },
    step_size=0.01,
    smoothing=0.1
)
```

### Online Data Mixing (Multi-Armed Bandit)

```python
"""
Online Data Mixing using multi-armed bandit approach
"""
import numpy as np
from typing import Dict, List, Optional

class EXP3DataMixer:
    """
    EXP3 (Exponential-weight algorithm for Exploration and Exploitation)
    for online data mixing during training.
    """

    def __init__(
        self,
        domains: List[str],
        learning_rate: float = 0.1,
        min_weight: float = 0.01
    ):
        self.domains = domains
        self.n_domains = len(domains)
        self.learning_rate = learning_rate
        self.min_weight = min_weight

        # Initialize uniform weights
        self.weights = np.ones(self.n_domains) / self.n_domains

    def sample_domain(self) -> str:
        """Sample a domain according to current weights"""
        idx = np.random.choice(self.n_domains, p=self.weights)
        return self.domains[idx]

    def update(self, domain: str, reward: float):
        """
        Update weights based on observed reward (negative loss improvement).

        Args:
            domain: Domain that was sampled
            reward: Reward signal (higher is better)
        """
        idx = self.domains.index(domain)

        # Importance-weighted reward estimate
        estimated_reward = reward / self.weights[idx]

        # Exponential update
        self.weights[idx] *= np.exp(
            self.learning_rate * estimated_reward / self.n_domains
        )

        # Normalize and apply minimum weight
        self.weights = np.maximum(self.weights, self.min_weight)
        self.weights = self.weights / self.weights.sum()

    def get_weights(self) -> Dict[str, float]:
        """Get current domain weights"""
        return dict(zip(self.domains, self.weights))

class OnlineDataMixer:
    """
    Practical online data mixer for training loops.
    """

    def __init__(
        self,
        domain_datasets: Dict[str, 'Dataset'],
        initial_weights: Optional[Dict[str, float]] = None,
        update_frequency: int = 1000,
        use_exp3: bool = True
    ):
        self.domains = list(domain_datasets.keys())
        self.datasets = domain_datasets
        self.update_frequency = update_frequency

        if use_exp3:
            self.mixer = EXP3DataMixer(self.domains)
            if initial_weights:
                self.mixer.weights = np.array([
                    initial_weights.get(d, 1/len(self.domains))
                    for d in self.domains
                ])
                self.mixer.weights /= self.mixer.weights.sum()

        self.step = 0
        self.domain_losses: Dict[str, List[float]] = {d: [] for d in self.domains}

    def get_batch(self, batch_size: int) -> Tuple[Dict, str]:
        """
        Get a batch from the mixed dataset.

        Returns:
            Tuple of (batch_data, domain_name)
        """
        domain = self.mixer.sample_domain()
        dataset = self.datasets[domain]

        # Sample batch from domain
        indices = np.random.choice(len(dataset), size=batch_size, replace=True)
        batch = dataset[indices]

        return batch, domain

    def record_loss(self, domain: str, loss: float):
        """Record loss for a domain"""
        self.domain_losses[domain].append(loss)
        self.step += 1

        # Periodically update weights
        if self.step % self.update_frequency == 0:
            self._update_weights()

    def _update_weights(self):
        """Update mixer weights based on recent losses"""
        for domain in self.domains:
            if len(self.domain_losses[domain]) > 0:
                recent_losses = self.domain_losses[domain][-100:]

                # Reward is negative loss change (improvement)
                if len(recent_losses) > 1:
                    improvement = recent_losses[-100] - recent_losses[-1] if len(recent_losses) >= 100 else 0
                    self.mixer.update(domain, improvement)

        print(f"Step {self.step} - Updated weights: {self.mixer.get_weights()}")
```

---

## 2.4.4 Curriculum Learning

### Curriculum Learning Strategies

```python
"""
Curriculum learning implementations for LLM pre-training
"""
from abc import ABC, abstractmethod
from typing import Iterator, List, Callable
import numpy as np
from dataclasses import dataclass

@dataclass
class CurriculumSchedule:
    """Defines a curriculum schedule"""
    name: str
    stages: List[dict]  # Each stage defines criteria and duration

class CurriculumStrategy(ABC):
    """Base class for curriculum learning strategies"""

    @abstractmethod
    def get_difficulty(self, document: dict) -> float:
        """Compute difficulty score for a document (0-1)"""
        pass

    @abstractmethod
    def should_include(self, document: dict, progress: float) -> bool:
        """
        Determine if document should be included at current training progress.

        Args:
            document: Document to evaluate
            progress: Training progress (0-1)

        Returns:
            Whether to include the document
        """
        pass

class LengthCurriculum(CurriculumStrategy):
    """
    Short-to-long curriculum based on sequence length.
    Starts with shorter documents and gradually introduces longer ones.
    """

    def __init__(
        self,
        min_length: int = 128,
        max_length: int = 8192,
        warmup_fraction: float = 0.1
    ):
        self.min_length = min_length
        self.max_length = max_length
        self.warmup_fraction = warmup_fraction

    def get_difficulty(self, document: dict) -> float:
        text = document.get("text", "")
        length = len(text.split())

        # Normalize to 0-1
        return min(1.0, (length - self.min_length) / (self.max_length - self.min_length))

    def should_include(self, document: dict, progress: float) -> bool:
        difficulty = self.get_difficulty(document)

        # During warmup, only include easy documents
        if progress < self.warmup_fraction:
            threshold = progress / self.warmup_fraction * 0.3  # Max 30% difficulty during warmup
        else:
            # Gradually increase threshold
            threshold = 0.3 + (progress - self.warmup_fraction) / (1 - self.warmup_fraction) * 0.7

        return difficulty <= threshold

class ComplexityCurriculum(CurriculumStrategy):
    """
    Easy-to-hard curriculum based on document complexity.
    Uses multiple signals: vocabulary, sentence structure, etc.
    """

    def __init__(self):
        self.complexity_weights = {
            "vocabulary": 0.3,
            "sentence_length": 0.2,
            "perplexity": 0.3,
            "entity_density": 0.2
        }

    def get_difficulty(self, document: dict) -> float:
        text = document.get("text", "")

        scores = {}

        # Vocabulary complexity (unique words / total words)
        words = text.lower().split()
        if len(words) > 0:
            scores["vocabulary"] = len(set(words)) / len(words)
        else:
            scores["vocabulary"] = 0

        # Sentence length complexity
        sentences = text.split('.')
        avg_sentence_len = np.mean([len(s.split()) for s in sentences if s.strip()])
        scores["sentence_length"] = min(1.0, avg_sentence_len / 50)  # Normalize by 50 words

        # Perplexity (if pre-computed)
        scores["perplexity"] = document.get("perplexity", 0.5)

        # Entity density (approximation)
        capital_words = sum(1 for w in words if w and w[0].isupper())
        scores["entity_density"] = min(1.0, capital_words / max(len(words), 1) * 5)

        # Weighted combination
        difficulty = sum(
            self.complexity_weights[k] * scores.get(k, 0)
            for k in self.complexity_weights
        )

        return min(1.0, difficulty)

    def should_include(self, document: dict, progress: float) -> bool:
        difficulty = self.get_difficulty(document)

        # Piecewise linear curriculum
        if progress < 0.2:
            threshold = 0.3  # Easy only
        elif progress < 0.5:
            threshold = 0.3 + (progress - 0.2) / 0.3 * 0.3  # Up to medium
        else:
            threshold = 0.6 + (progress - 0.5) / 0.5 * 0.4  # Include hard

        return difficulty <= threshold

class QualityCurriculum(CurriculumStrategy):
    """
    Quality-based curriculum.
    Counterintuitively, start with LOWER quality data and progress to HIGHER quality.
    This ensures the model sees high-quality patterns closer to convergence.
    """

    def __init__(
        self,
        quality_field: str = "quality_score",
        reverse: bool = False  # If True, start with high quality
    ):
        self.quality_field = quality_field
        self.reverse = reverse

    def get_difficulty(self, document: dict) -> float:
        # In quality curriculum, "difficulty" is actually quality
        quality = document.get(self.quality_field, 0.5)
        return quality if not self.reverse else 1 - quality

    def should_include(self, document: dict, progress: float) -> bool:
        quality = document.get(self.quality_field, 0.5)

        if self.reverse:
            # Start with high quality, introduce low quality later
            return quality >= 1 - progress
        else:
            # Start with low quality, save high quality for later
            return quality <= progress + 0.3  # Some minimum quality floor

class DomainCurriculum(CurriculumStrategy):
    """
    Domain-based curriculum.
    Introduces domains in a specific order.
    """

    def __init__(
        self,
        domain_schedule: List[tuple],  # [(domain, start_progress, full_weight_progress)]
    ):
        """
        Args:
            domain_schedule: List of (domain, start, full_weight) tuples
                where start is when domain is introduced (0-1)
                and full_weight is when it reaches full sampling weight
        """
        self.domain_schedule = {
            domain: (start, full)
            for domain, start, full in domain_schedule
        }

    def get_difficulty(self, document: dict) -> float:
        domain = document.get("domain", "unknown")
        if domain in self.domain_schedule:
            start, full = self.domain_schedule[domain]
            return start  # Difficulty is based on when introduced
        return 0.5

    def should_include(self, document: dict, progress: float) -> bool:
        domain = document.get("domain", "unknown")

        if domain not in self.domain_schedule:
            return True  # Include unknown domains

        start, full = self.domain_schedule[domain]
        return progress >= start

    def get_domain_weight(self, domain: str, progress: float) -> float:
        """Get sampling weight for domain at current progress"""
        if domain not in self.domain_schedule:
            return 1.0

        start, full = self.domain_schedule[domain]

        if progress < start:
            return 0.0
        elif progress >= full:
            return 1.0
        else:
            # Linear ramp-up
            return (progress - start) / (full - start)

# Example curriculum configuration
def create_llm_curriculum() -> List[CurriculumStrategy]:
    """Create curriculum for LLM pre-training"""
    return [
        LengthCurriculum(
            min_length=128,
            max_length=8192,
            warmup_fraction=0.1
        ),
        ComplexityCurriculum(),
        DomainCurriculum([
            # Start with general web content
            ("web", 0.0, 0.1),
            ("reference", 0.0, 0.1),
            # Introduce code early
            ("code", 0.05, 0.15),
            # Add scientific content mid-training
            ("scientific", 0.2, 0.4),
            # High-quality curated data saved for later
            ("curated", 0.6, 0.8),
        ])
    ]
```

### Multi-Phase Pre-training

```python
"""
Multi-phase pre-training implementation
"""
from dataclasses import dataclass
from typing import Dict, List, Optional
import json

@dataclass
class TrainingPhase:
    """Configuration for a training phase"""
    name: str
    tokens: int  # Tokens to train in this phase
    data_mix: Dict[str, float]
    learning_rate: float
    warmup_tokens: int
    decay_tokens: Optional[int] = None
    sequence_length: int = 4096
    batch_size: int = 2048  # In sequences

class MultiPhaseTrainer:
    """
    Orchestrates multi-phase pre-training.

    Common patterns:
    - Phase 1: General pre-training on web data
    - Phase 2: Mid-training on high-quality data
    - Phase 3: Annealing on curated/instruction data
    """

    def __init__(self, phases: List[TrainingPhase]):
        self.phases = phases
        self.current_phase = 0
        self.tokens_in_phase = 0

    @classmethod
    def create_standard_phases(
        cls,
        total_tokens: int,
        model_size: int
    ) -> 'MultiPhaseTrainer':
        """
        Create standard 3-phase training schedule.

        Args:
            total_tokens: Total training tokens
            model_size: Model size in parameters
        """
        # Compute optimal learning rate based on scaling laws
        base_lr = 3e-4 * (model_size / 1e9) ** -0.5

        phases = [
            # Phase 1: Main pre-training (80%)
            TrainingPhase(
                name="pretraining",
                tokens=int(total_tokens * 0.8),
                data_mix={
                    "web_crawl": 0.55,
                    "code": 0.12,
                    "books": 0.10,
                    "scientific": 0.08,
                    "reference": 0.08,
                    "conversational": 0.07
                },
                learning_rate=base_lr,
                warmup_tokens=int(total_tokens * 0.01),
                decay_tokens=int(total_tokens * 0.8),
                sequence_length=4096
            ),
            # Phase 2: Mid-training (15%)
            TrainingPhase(
                name="midtraining",
                tokens=int(total_tokens * 0.15),
                data_mix={
                    "web_crawl": 0.30,  # Reduced
                    "code": 0.20,       # Increased
                    "books": 0.15,      # Increased
                    "scientific": 0.15, # Increased
                    "reference": 0.10,
                    "curated": 0.10     # Added
                },
                learning_rate=base_lr * 0.3,  # Reduced LR
                warmup_tokens=int(total_tokens * 0.005),
                sequence_length=8192  # Longer context
            ),
            # Phase 3: Annealing (5%)
            TrainingPhase(
                name="annealing",
                tokens=int(total_tokens * 0.05),
                data_mix={
                    "curated": 0.40,
                    "code": 0.25,
                    "scientific": 0.20,
                    "reference": 0.15
                },
                learning_rate=base_lr * 0.1,  # Very low LR
                warmup_tokens=0,
                decay_tokens=int(total_tokens * 0.05),  # Decay to near-zero
                sequence_length=8192
            )
        ]

        return cls(phases)

    def get_current_phase(self) -> TrainingPhase:
        """Get the current training phase"""
        return self.phases[self.current_phase]

    def step(self, tokens_processed: int):
        """
        Update trainer state after processing tokens.

        Args:
            tokens_processed: Number of tokens in this step
        """
        self.tokens_in_phase += tokens_processed

        # Check if we should advance to next phase
        current = self.get_current_phase()
        if self.tokens_in_phase >= current.tokens:
            if self.current_phase < len(self.phases) - 1:
                self.current_phase += 1
                self.tokens_in_phase = 0
                print(f"Advancing to phase: {self.get_current_phase().name}")

    def get_learning_rate(self, global_step: int, tokens_processed: int) -> float:
        """Compute learning rate for current step"""
        phase = self.get_current_phase()

        # Warmup
        if self.tokens_in_phase < phase.warmup_tokens:
            return phase.learning_rate * (self.tokens_in_phase / phase.warmup_tokens)

        # Decay
        if phase.decay_tokens:
            progress = (self.tokens_in_phase - phase.warmup_tokens) / phase.decay_tokens
            progress = min(1.0, progress)
            # Cosine decay
            return phase.learning_rate * 0.5 * (1 + np.cos(np.pi * progress))

        return phase.learning_rate

    def save_state(self, path: str):
        """Save trainer state"""
        state = {
            "current_phase": self.current_phase,
            "tokens_in_phase": self.tokens_in_phase
        }
        with open(path, 'w') as f:
            json.dump(state, f)

    def load_state(self, path: str):
        """Load trainer state"""
        with open(path) as f:
            state = json.load(f)
        self.current_phase = state["current_phase"]
        self.tokens_in_phase = state["tokens_in_phase"]

# Example usage
trainer = MultiPhaseTrainer.create_standard_phases(
    total_tokens=2_000_000_000_000,  # 2T tokens
    model_size=7_000_000_000         # 7B parameters
)

print("Training Phases:")
for i, phase in enumerate(trainer.phases):
    print(f"\nPhase {i + 1}: {phase.name}")
    print(f"  Tokens: {phase.tokens:,}")
    print(f"  Learning Rate: {phase.learning_rate:.2e}")
    print(f"  Sequence Length: {phase.sequence_length}")
    print(f"  Data Mix:")
    for domain, weight in sorted(phase.data_mix.items(), key=lambda x: -x[1]):
        print(f"    {domain}: {weight:.0%}")
```

---

## 2.4.5 Decontamination

### Train/Test Overlap Detection

```python
"""
Decontamination framework for LLM training data
"""
import hashlib
from typing import Set, List, Dict, Tuple
from collections import defaultdict
import numpy as np

class NGramDecontaminator:
    """
    N-gram based contamination detection.
    Identifies training examples that overlap with evaluation benchmarks.
    """

    def __init__(
        self,
        n: int = 13,
        threshold: float = 0.8
    ):
        """
        Args:
            n: N-gram size (13 is common for LLM decontamination)
            threshold: Fraction of n-grams that must match to flag as contaminated
        """
        self.n = n
        self.threshold = threshold
        self.benchmark_ngrams: Dict[str, Set[str]] = {}

    def _extract_ngrams(self, text: str) -> Set[str]:
        """Extract character n-grams from text"""
        text = text.lower().strip()
        text = ' '.join(text.split())  # Normalize whitespace

        ngrams = set()
        for i in range(len(text) - self.n + 1):
            ngrams.add(text[i:i + self.n])
        return ngrams

    def add_benchmark(self, name: str, examples: List[str]):
        """
        Add a benchmark's test set for contamination checking.

        Args:
            name: Benchmark name
            examples: List of benchmark examples (questions + answers)
        """
        all_ngrams = set()
        for example in examples:
            all_ngrams.update(self._extract_ngrams(example))
        self.benchmark_ngrams[name] = all_ngrams
        print(f"Added benchmark '{name}' with {len(all_ngrams):,} unique {self.n}-grams")

    def check_contamination(self, text: str) -> Dict[str, float]:
        """
        Check if text is contaminated with any benchmark.

        Returns:
            Dict mapping benchmark name to overlap fraction
        """
        text_ngrams = self._extract_ngrams(text)
        if len(text_ngrams) == 0:
            return {}

        results = {}
        for name, bench_ngrams in self.benchmark_ngrams.items():
            overlap = len(text_ngrams & bench_ngrams)
            fraction = overlap / len(text_ngrams)
            results[name] = fraction

        return results

    def is_contaminated(self, text: str) -> Tuple[bool, List[str]]:
        """
        Check if text should be removed due to contamination.

        Returns:
            Tuple of (is_contaminated, list of contaminated benchmarks)
        """
        scores = self.check_contamination(text)
        contaminated = [name for name, score in scores.items() if score >= self.threshold]
        return len(contaminated) > 0, contaminated

    def filter_dataset(
        self,
        documents: List[Dict],
        text_field: str = "text"
    ) -> Tuple[List[Dict], Dict]:
        """
        Filter contaminated documents from dataset.

        Returns:
            Tuple of (clean_documents, contamination_stats)
        """
        clean_docs = []
        stats = defaultdict(int)

        for doc in documents:
            text = doc.get(text_field, "")
            is_contam, benchmarks = self.is_contaminated(text)

            if is_contam:
                for bench in benchmarks:
                    stats[bench] += 1
            else:
                clean_docs.append(doc)

        stats["total_removed"] = len(documents) - len(clean_docs)
        stats["total_kept"] = len(clean_docs)
        stats["removal_rate"] = stats["total_removed"] / max(len(documents), 1)

        return clean_docs, dict(stats)

class EmbeddingDecontaminator:
    """
    Embedding-based contamination detection.
    More robust to paraphrasing but more expensive.
    """

    def __init__(
        self,
        embedding_model,
        similarity_threshold: float = 0.9
    ):
        self.model = embedding_model
        self.threshold = similarity_threshold
        self.benchmark_embeddings: Dict[str, np.ndarray] = {}

    def add_benchmark(self, name: str, examples: List[str]):
        """Add benchmark examples"""
        embeddings = self.model.encode(examples)
        self.benchmark_embeddings[name] = embeddings

    def check_contamination(self, texts: List[str]) -> Dict[str, List[float]]:
        """Check contamination for batch of texts"""
        text_embeddings = self.model.encode(texts)

        results = {name: [] for name in self.benchmark_embeddings}

        for text_emb in text_embeddings:
            for name, bench_embs in self.benchmark_embeddings.items():
                # Compute max similarity to any benchmark example
                similarities = np.dot(bench_embs, text_emb)
                max_sim = similarities.max()
                results[name].append(float(max_sim))

        return results

# Example contamination checking
decontaminator = NGramDecontaminator(n=13, threshold=0.8)

# Add common benchmarks
decontaminator.add_benchmark("mmlu", [
    "What is the capital of France? Paris",
    "Which planet is closest to the sun? Mercury",
    # ... more MMLU examples
])

decontaminator.add_benchmark("gsm8k", [
    "Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market? 18",
    # ... more GSM8K examples
])

# Check a document
sample_doc = "The capital of France is Paris, which is known for the Eiffel Tower."
scores = decontaminator.check_contamination(sample_doc)
print(f"Contamination scores: {scores}")
```

### Benchmark-Specific Contamination

```python
"""
Benchmark-specific contamination detection
"""
from typing import Dict, List, Set
import re

class BenchmarkContaminationChecker:
    """
    Specialized contamination checking for common LLM benchmarks.
    """

    def __init__(self):
        self.benchmark_patterns = {}
        self._initialize_patterns()

    def _initialize_patterns(self):
        """Initialize regex patterns for benchmark detection"""

        # MMLU - Multiple choice format
        self.benchmark_patterns["mmlu"] = [
            r"Question:.*\n\(A\).*\n\(B\).*\n\(C\).*\n\(D\)",
            r"The answer is \([A-D]\)",
        ]

        # GSM8K - Math word problems
        self.benchmark_patterns["gsm8k"] = [
            r"####\s*\d+",  # GSM8K answer format
            r"Step \d+:.*\n.*Step \d+:",
        ]

        # HumanEval - Code generation
        self.benchmark_patterns["humaneval"] = [
            r"def\s+\w+\([^)]*\)\s*->\s*\w+:\s*\"\"\"",
            r">>> \w+\(.*\)\n.*",
        ]

        # TruthfulQA
        self.benchmark_patterns["truthfulqa"] = [
            r"(true|false):\s*(yes|no)",
        ]

    def check_format_contamination(
        self,
        text: str,
        benchmark: str
    ) -> bool:
        """
        Check if text contains benchmark-specific format patterns.

        This catches cases where the benchmark format itself leaked,
        even if specific examples didn't.
        """
        if benchmark not in self.benchmark_patterns:
            return False

        for pattern in self.benchmark_patterns[benchmark]:
            if re.search(pattern, text, re.IGNORECASE | re.MULTILINE):
                return True
        return False

    def comprehensive_check(
        self,
        text: str,
        ngram_checker: NGramDecontaminator
    ) -> Dict[str, Dict]:
        """
        Run comprehensive contamination check.

        Returns:
            Dict with contamination results per benchmark
        """
        results = {}

        # N-gram check
        ngram_scores = ngram_checker.check_contamination(text)

        for benchmark in ngram_scores:
            results[benchmark] = {
                "ngram_overlap": ngram_scores[benchmark],
                "format_match": self.check_format_contamination(text, benchmark),
                "is_contaminated": (
                    ngram_scores[benchmark] >= ngram_checker.threshold or
                    self.check_format_contamination(text, benchmark)
                )
            }

        return results

# Contamination report generation
def generate_contamination_report(
    dataset_name: str,
    documents: List[Dict],
    decontaminator: NGramDecontaminator,
    sample_size: int = 10000
) -> Dict:
    """
    Generate a contamination report for a dataset.

    Args:
        dataset_name: Name of the dataset
        documents: List of documents
        decontaminator: Configured decontaminator
        sample_size: Number of documents to sample

    Returns:
        Contamination report dict
    """
    import random

    # Sample if dataset is large
    if len(documents) > sample_size:
        sampled = random.sample(documents, sample_size)
    else:
        sampled = documents

    # Run contamination check
    contamination_counts = defaultdict(int)
    contaminated_examples = defaultdict(list)

    for doc in sampled:
        text = doc.get("text", "")
        is_contam, benchmarks = decontaminator.is_contaminated(text)

        if is_contam:
            for bench in benchmarks:
                contamination_counts[bench] += 1
                if len(contaminated_examples[bench]) < 5:
                    contaminated_examples[bench].append(text[:500])

    report = {
        "dataset": dataset_name,
        "sample_size": len(sampled),
        "total_documents": len(documents),
        "contamination_by_benchmark": {
            bench: {
                "count": count,
                "rate": count / len(sampled),
                "examples": contaminated_examples.get(bench, [])
            }
            for bench, count in contamination_counts.items()
        },
        "overall_contamination_rate": sum(contamination_counts.values()) / len(sampled) / max(len(contamination_counts), 1)
    }

    return report
```

---

## 2.4.6 Multilingual Considerations

### Language Proportion Decisions

```python
"""
Multilingual data mix configuration
"""
from dataclasses import dataclass, field
from typing import Dict, List, Optional
import numpy as np

@dataclass
class LanguageConfig:
    """Configuration for a language in the training mix"""
    code: str  # ISO 639-1 code
    name: str
    target_proportion: float
    min_quality_score: float = 0.7
    script: str = "latin"
    resource_level: str = "high"  # high, medium, low

# Reference proportions based on published multilingual models
LANGUAGE_PROPORTIONS = {
    # High-resource languages
    "en": LanguageConfig("en", "English", 0.50, script="latin", resource_level="high"),
    "zh": LanguageConfig("zh", "Chinese", 0.08, script="han", resource_level="high"),
    "de": LanguageConfig("de", "German", 0.05, script="latin", resource_level="high"),
    "fr": LanguageConfig("fr", "French", 0.05, script="latin", resource_level="high"),
    "es": LanguageConfig("es", "Spanish", 0.05, script="latin", resource_level="high"),
    "ja": LanguageConfig("ja", "Japanese", 0.04, script="japanese", resource_level="high"),
    "ru": LanguageConfig("ru", "Russian", 0.03, script="cyrillic", resource_level="high"),
    "pt": LanguageConfig("pt", "Portuguese", 0.03, script="latin", resource_level="high"),
    "it": LanguageConfig("it", "Italian", 0.02, script="latin", resource_level="high"),
    "ko": LanguageConfig("ko", "Korean", 0.02, script="hangul", resource_level="high"),

    # Medium-resource languages
    "nl": LanguageConfig("nl", "Dutch", 0.015, script="latin", resource_level="medium"),
    "pl": LanguageConfig("pl", "Polish", 0.015, script="latin", resource_level="medium"),
    "ar": LanguageConfig("ar", "Arabic", 0.015, script="arabic", resource_level="medium"),
    "vi": LanguageConfig("vi", "Vietnamese", 0.01, script="latin", resource_level="medium"),
    "th": LanguageConfig("th", "Thai", 0.01, script="thai", resource_level="medium"),
    "tr": LanguageConfig("tr", "Turkish", 0.01, script="latin", resource_level="medium"),
    "id": LanguageConfig("id", "Indonesian", 0.01, script="latin", resource_level="medium"),
    "hi": LanguageConfig("hi", "Hindi", 0.01, script="devanagari", resource_level="medium"),

    # Low-resource (representative selection)
    "other": LanguageConfig("other", "Other Languages", 0.08, resource_level="low"),
}

class MultilingualMixer:
    """
    Manages multilingual data mixing for pre-training.
    """

    def __init__(
        self,
        language_configs: Dict[str, LanguageConfig],
        upsampling_factor: float = 2.0,
        temperature: float = 0.3
    ):
        """
        Args:
            language_configs: Language configurations
            upsampling_factor: Max upsampling for low-resource languages
            temperature: Temperature for exponential smoothing (lower = more uniform)
        """
        self.configs = language_configs
        self.upsampling_factor = upsampling_factor
        self.temperature = temperature

    def compute_sampling_weights(
        self,
        available_tokens: Dict[str, int]
    ) -> Dict[str, float]:
        """
        Compute sampling weights given available data.

        Uses temperature-based smoothing to prevent over-representation
        of high-resource languages while respecting data availability.
        """
        weights = {}
        total_available = sum(available_tokens.values())

        for lang, tokens in available_tokens.items():
            if lang not in self.configs:
                continue

            config = self.configs[lang]

            # Start with target proportion
            target = config.target_proportion

            # Compute natural proportion
            natural = tokens / total_available

            # Apply temperature smoothing
            # Higher temperature → closer to natural distribution
            # Lower temperature → closer to uniform
            smoothed = natural ** self.temperature

            # Limit upsampling for low-resource languages
            if natural < target:
                max_weight = natural * self.upsampling_factor
                target = min(target, max_weight)

            weights[lang] = smoothed * target

        # Normalize
        total = sum(weights.values())
        return {lang: w / total for lang, w in weights.items()}

    def compute_epochs_per_language(
        self,
        weights: Dict[str, float],
        available_tokens: Dict[str, int],
        target_tokens: int
    ) -> Dict[str, float]:
        """
        Compute expected epochs for each language.

        Returns:
            Dict mapping language to expected number of epochs
        """
        epochs = {}

        for lang, weight in weights.items():
            target_lang_tokens = target_tokens * weight
            available = available_tokens.get(lang, 1)
            epochs[lang] = target_lang_tokens / available

        return epochs

    def validate_mix(
        self,
        weights: Dict[str, float],
        epochs: Dict[str, float]
    ) -> List[str]:
        """
        Validate the multilingual mix and return warnings.
        """
        warnings = []

        for lang, epoch in epochs.items():
            if epoch > 5:
                warnings.append(
                    f"{lang}: {epoch:.1f} epochs may cause overfitting"
                )
            elif epoch < 0.1:
                warnings.append(
                    f"{lang}: Only {epoch:.2f} epochs, may underfit"
                )

        # Check for language imbalance
        max_weight = max(weights.values())
        min_weight = min(weights.values())

        if max_weight / min_weight > 100:
            warnings.append(
                f"Extreme language imbalance: {max_weight/min_weight:.0f}x difference"
            )

        return warnings

# Cross-lingual transfer considerations
class CrossLingualTransferAnalyzer:
    """
    Analyze expected cross-lingual transfer based on language similarity.
    """

    # Language family groupings
    LANGUAGE_FAMILIES = {
        "germanic": ["en", "de", "nl", "sv", "da", "no"],
        "romance": ["fr", "es", "it", "pt", "ro"],
        "slavic": ["ru", "pl", "cs", "uk", "bg"],
        "cjk": ["zh", "ja", "ko"],
        "indo_aryan": ["hi", "bn", "ur"],
        "semitic": ["ar", "he"],
    }

    # Script groupings
    SCRIPTS = {
        "latin": ["en", "de", "fr", "es", "it", "pt", "nl", "pl", "vi", "id", "tr"],
        "cyrillic": ["ru", "uk", "bg"],
        "han": ["zh", "ja"],  # Japanese uses Han + Kana
        "arabic": ["ar"],
        "devanagari": ["hi"],
    }

    def estimate_transfer(
        self,
        source_lang: str,
        target_lang: str
    ) -> float:
        """
        Estimate cross-lingual transfer potential (0-1).

        Higher values indicate better expected transfer.
        """
        # Same language
        if source_lang == target_lang:
            return 1.0

        transfer = 0.0

        # Same family bonus
        for family, langs in self.LANGUAGE_FAMILIES.items():
            if source_lang in langs and target_lang in langs:
                transfer += 0.4
                break

        # Same script bonus
        for script, langs in self.SCRIPTS.items():
            if source_lang in langs and target_lang in langs:
                transfer += 0.3
                break

        # English as source bonus (most training data)
        if source_lang == "en":
            transfer += 0.2

        return min(1.0, transfer)

    def recommend_low_resource_support(
        self,
        target_lang: str,
        available_langs: List[str]
    ) -> List[tuple]:
        """
        Recommend languages to include for supporting a low-resource target.

        Returns:
            List of (language, transfer_score) sorted by transfer potential
        """
        recommendations = []

        for lang in available_langs:
            if lang != target_lang:
                score = self.estimate_transfer(lang, target_lang)
                recommendations.append((lang, score))

        return sorted(recommendations, key=lambda x: -x[1])

# Example usage
mixer = MultilingualMixer(LANGUAGE_PROPORTIONS)

available = {
    "en": 1_000_000_000_000,
    "zh": 100_000_000_000,
    "de": 80_000_000_000,
    "fr": 70_000_000_000,
    "es": 60_000_000_000,
    "ja": 50_000_000_000,
    "ru": 40_000_000_000,
    "pt": 30_000_000_000,
    "other": 100_000_000_000,
}

weights = mixer.compute_sampling_weights(available)
epochs = mixer.compute_epochs_per_language(weights, available, 2_000_000_000_000)

print("Multilingual Mix Configuration:")
for lang, weight in sorted(weights.items(), key=lambda x: -x[1]):
    print(f"  {lang}: {weight:.2%} ({epochs[lang]:.2f} epochs)")

warnings = mixer.validate_mix(weights, epochs)
if warnings:
    print("\nWarnings:")
    for w in warnings:
        print(f"  ⚠ {w}")
```

---

## Troubleshooting

### Common Data Mix Issues

| Issue | Symptoms | Solution |
|-------|----------|----------|
| Domain collapse | Model excels at one domain, fails at others | Rebalance mix, add domain-specific evaluation |
| Overfitting to small domains | High quality on repeated data, poor generalization | Cap epochs per source, add diversity |
| Contamination | Suspiciously high benchmark scores | Run decontamination, check benchmark formats |
| Language imbalance | Poor multilingual performance | Adjust temperature, increase upsampling |
| Quality degradation | Training loss good but eval poor | Increase quality filtering threshold |

### Data Mix Debugging Checklist

```python
"""
Data mix debugging utilities
"""
from typing import Dict, List
import numpy as np

class DataMixDebugger:
    """Debugging utilities for data mix issues"""

    @staticmethod
    def check_domain_coverage(
        training_logs: List[Dict],
        expected_domains: List[str]
    ) -> Dict:
        """Verify all domains are being sampled"""
        seen_domains = set()
        domain_counts = {}

        for log in training_logs:
            domain = log.get("domain")
            seen_domains.add(domain)
            domain_counts[domain] = domain_counts.get(domain, 0) + 1

        missing = set(expected_domains) - seen_domains

        return {
            "missing_domains": list(missing),
            "domain_distribution": domain_counts,
            "total_samples": len(training_logs)
        }

    @staticmethod
    def detect_data_repetition(
        document_hashes: List[str],
        threshold: float = 0.01
    ) -> Dict:
        """Detect if data is being repeated excessively"""
        unique = len(set(document_hashes))
        total = len(document_hashes)
        repetition_rate = 1 - (unique / total)

        return {
            "unique_documents": unique,
            "total_samples": total,
            "repetition_rate": repetition_rate,
            "excessive_repetition": repetition_rate > threshold
        }

    @staticmethod
    def analyze_quality_distribution(
        quality_scores: List[float],
        expected_mean: float = 0.7
    ) -> Dict:
        """Analyze quality score distribution"""
        scores = np.array(quality_scores)

        return {
            "mean": float(scores.mean()),
            "std": float(scores.std()),
            "min": float(scores.min()),
            "max": float(scores.max()),
            "below_threshold": float((scores < 0.5).mean()),
            "deviation_from_expected": float(abs(scores.mean() - expected_mean))
        }
```

---

## Glossary

| Term | Definition |
|------|------------|
| **Data Mix** | The composition of different data sources used for training |
| **Curriculum Learning** | Training strategy that presents data in a meaningful order |
| **Decontamination** | Removing evaluation data from training sets |
| **DoReMi** | Domain Reweighting with Minimax Optimization |
| **N-gram** | Contiguous sequence of n items from text |
| **Upsampling** | Increasing representation of underrepresented data |
| **Temperature (mixing)** | Parameter controlling distribution smoothness |
| **Cross-lingual Transfer** | Knowledge transfer between languages |
| **Annealing** | Final training phase with high-quality data and low learning rate |
| **Domain Collapse** | When model becomes specialized to one domain |

---

## References

1. Xie, S. M., et al. (2023). "DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining"
2. Soldaini, L., et al. (2024). "Dolma: An Open Corpus of Three Trillion Tokens"
3. Penedo, G., et al. (2024). "The FineWeb Datasets: Decanting the Web for the Finest Text Data"
4. Albalak, A., et al. (2024). "A Survey on Data Selection for Language Models"
5. Longpre, S., et al. (2023). "A Pretrainer's Guide to Training Data"
6. Together AI (2024). "RedPajama-Data-v2: An Open Dataset with 30 Trillion Tokens"
7. Wei, J., et al. (2023). "Skywork: A More Open Bilingual Foundation Model"
8. Gao, L., et al. (2020). "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"
9. Brown, T., et al. (2024). "Language Models are Few-Shot Learners" - Data mixture appendix
10. Conneau, A., et al. (2020). "Unsupervised Cross-lingual Representation Learning at Scale"
