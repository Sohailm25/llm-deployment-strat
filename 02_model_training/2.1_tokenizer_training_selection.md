# Document 2.1: Tokenizer Training & Selection Guide

## Document Information
- **Version:** 1.0
- **Last Updated:** December 2025
- **Owner:** ML Engineering / Research Team
- **Category:** Model Training (Pre-training)

> **Navigation** | [‚Üê 1.6 Data Quality](../01_data_pipeline/1.6_data_quality_assurance.md) | [2.2 Model Architecture ‚Üí](2.2_model_architecture_selection.md)
>
> | | |
> |---|---|
> | **Prerequisites** | [1.1 Data Collection](../01_data_pipeline/1.1_data_collection_sourcing.md) |
> | **Related** | [2.2 Model Architecture](2.2_model_architecture_selection.md) &#124; [2.4 Pretraining Data Mix](2.4_pretraining_data_mix_curriculum.md) |
> | **Next** | [2.2 Model Architecture Selection](2.2_model_architecture_selection.md) |

---

## Executive Summary

The tokenizer is a critical but often underappreciated component of LLM systems. It directly impacts model efficiency, multilingual capability, and downstream performance. This guide covers the selection, training, and evaluation of tokenizers for LLM applications, including BPE, WordPiece, Unigram, and SentencePiece implementations. Proper tokenizer design can improve training efficiency by 20-40% and significantly enhance multilingual performance.

**Key Outcomes:**
- Select appropriate tokenization algorithms for your use case
- Train custom tokenizers optimized for your domain
- Evaluate tokenizer quality with proper metrics
- Integrate tokenizers with chat templates and special tokens

---

## Prerequisites

### Required Knowledge
- Basic NLP concepts (tokens, vocabulary)
- Python programming
- Understanding of LLM training workflow
- Unicode and text encoding basics

### Infrastructure Requirements
- Compute for tokenizer training (CPU-intensive)
- Storage for training corpus (10-100GB typical)
- GPU for downstream validation (optional)

### Tool Installation
```bash
# HuggingFace tokenizers (fast Rust implementation)
pip install tokenizers transformers

# SentencePiece (Google's implementation)
pip install sentencepiece

# tiktoken (OpenAI's BPE implementation)
pip install tiktoken

# Additional utilities
pip install datasets nltk
```

---

## 1. Tokenization Fundamentals

### 1.1 Why Tokenization Matters

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   Tokenization Impact                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                              ‚îÇ
‚îÇ  INPUT: "The quick brown fox jumps over the lazy dog"       ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ Character-level (9 chars = "tokenized")              ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ [T,h,e, ,q,u,i,c,k, ,b,r,o,w,n, ,f,o,x,...]         ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ 43 tokens ‚Üí Very long sequences, slow training      ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ Word-level                                          ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ [The, quick, brown, fox, jumps, over, the, lazy, dog]‚îÇ   ‚îÇ
‚îÇ  ‚îÇ 9 tokens ‚Üí Huge vocabulary, OOV problems            ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ Subword BPE (GPT-style)                              ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ [The, ƒ†quick, ƒ†brown, ƒ†fox, ƒ†jumps, ƒ†over, ...]    ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ 10 tokens ‚Üí Balanced vocabulary and efficiency      ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  Impact on Training:                                        ‚îÇ
‚îÇ  ‚Ä¢ Sequence length ‚Üí Memory & compute cost                  ‚îÇ
‚îÇ  ‚Ä¢ Vocabulary size ‚Üí Embedding table size                   ‚îÇ
‚îÇ  ‚Ä¢ Token fertility ‚Üí Model's view of text                   ‚îÇ
‚îÇ                                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 1.2 Tokenization Algorithm Comparison

| Algorithm | How It Works | Pros | Cons | Used By |
|-----------|--------------|------|------|---------|
| **BPE** | Iteratively merge frequent pairs | Efficient, handles rare words | Greedy, may split common words | GPT-2/3/4, Llama |
| **WordPiece** | Maximize likelihood score | Good for morphology | Slightly slower training | BERT, DistilBERT |
| **Unigram** | Probabilistic, prune from large vocab | Multiple valid tokenizations | Complex implementation | T5, ALBERT |
| **Byte-level BPE** | BPE on raw bytes | No UNK tokens ever | Slightly longer sequences | GPT-2, Llama 3 |
| **SentencePiece** | Language-agnostic, treats spaces as tokens | Multilingual, no pre-tokenization | Whitespace handling different | Llama, T5 |

### 1.3 Vocabulary Size Trade-offs

```python
"""
Vocabulary Size Impact Analysis

Smaller Vocab (8K-16K):
  + Smaller embedding tables
  + Less memory for vocabulary
  - Longer sequences (more tokens per text)
  - Worse compression ratio
  - Some concepts split across tokens

Larger Vocab (100K-256K):
  + Shorter sequences
  + Better compression
  + Common words as single tokens
  - Larger embedding tables
  - Risk of undertraining rare tokens
  - More memory overhead
"""

# Rule of thumb for vocabulary size selection
def estimate_vocab_size(
    training_tokens: int,
    min_frequency: int = 100
) -> int:
    """
    Estimate appropriate vocabulary size.

    Args:
        training_tokens: Total tokens in training corpus
        min_frequency: Minimum times each vocab item should appear

    Returns:
        Recommended vocabulary size
    """
    # Each vocab item should appear at least min_frequency times
    # to be well-trained in embeddings
    max_vocab = training_tokens // min_frequency

    # Typical ranges:
    # 1B tokens ‚Üí 10K vocab
    # 10B tokens ‚Üí 32K vocab
    # 100B+ tokens ‚Üí 100K+ vocab

    return min(max_vocab, 256000)  # Cap at 256K

# Common vocabulary sizes by model
VOCAB_SIZES = {
    "gpt-2": 50257,
    "gpt-4": 100277,  # cl100k_base
    "llama-2": 32000,
    "llama-3": 128256,  # Larger for multilingual
    "mistral": 32000,
    "qwen-2": 151936,
    "gemma": 256128,
}
```

---

## 2. Tokenization Algorithms Deep Dive

### 2.1 Byte-Pair Encoding (BPE)

BPE starts with a base vocabulary (characters or bytes) and iteratively merges the most frequent pairs.

```python
from collections import Counter, defaultdict
from typing import Dict, List, Tuple

class SimpleBPE:
    """Simple BPE implementation for educational purposes"""

    def __init__(self, vocab_size: int = 1000):
        self.vocab_size = vocab_size
        self.merges: List[Tuple[str, str]] = []
        self.vocab: Dict[str, int] = {}

    def _get_stats(self, words: Dict[str, int]) -> Counter:
        """Count frequency of adjacent pairs"""
        pairs = Counter()
        for word, freq in words.items():
            symbols = word.split()
            for i in range(len(symbols) - 1):
                pairs[(symbols[i], symbols[i + 1])] += freq
        return pairs

    def _merge_vocab(
        self,
        pair: Tuple[str, str],
        words: Dict[str, int]
    ) -> Dict[str, int]:
        """Merge all occurrences of the most frequent pair"""
        new_words = {}
        bigram = ' '.join(pair)
        replacement = ''.join(pair)

        for word, freq in words.items():
            new_word = word.replace(bigram, replacement)
            new_words[new_word] = freq

        return new_words

    def train(self, corpus: List[str]):
        """Train BPE on corpus"""
        # Initialize vocabulary with characters
        word_freqs = Counter()
        for text in corpus:
            for word in text.split():
                # Add end-of-word marker and space-separate characters
                word_repr = ' '.join(list(word)) + ' </w>'
                word_freqs[word_repr] += 1

        # Build initial vocabulary
        self.vocab = {char: i for i, char in enumerate(
            set(char for word in word_freqs for char in word.split())
        )}

        # Merge most frequent pairs until vocab_size reached
        while len(self.vocab) < self.vocab_size:
            pairs = self._get_stats(word_freqs)
            if not pairs:
                break

            best_pair = pairs.most_common(1)[0][0]
            self.merges.append(best_pair)

            # Create new token
            new_token = ''.join(best_pair)
            self.vocab[new_token] = len(self.vocab)

            # Update word representations
            word_freqs = self._merge_vocab(best_pair, word_freqs)

        print(f"Trained BPE with {len(self.vocab)} tokens and {len(self.merges)} merges")

    def encode(self, text: str) -> List[int]:
        """Encode text to token IDs"""
        tokens = []
        for word in text.split():
            word_repr = ' '.join(list(word)) + ' </w>'

            # Apply merges in order
            for merge in self.merges:
                bigram = ' '.join(merge)
                replacement = ''.join(merge)
                word_repr = word_repr.replace(bigram, replacement)

            # Convert to IDs
            for token in word_repr.split():
                if token in self.vocab:
                    tokens.append(self.vocab[token])

        return tokens

    def decode(self, ids: List[int]) -> str:
        """Decode token IDs to text"""
        id_to_token = {v: k for k, v in self.vocab.items()}
        tokens = [id_to_token.get(i, '') for i in ids]
        text = ''.join(tokens).replace('</w>', ' ')
        return text.strip()

# Usage
bpe = SimpleBPE(vocab_size=500)
bpe.train([
    "the quick brown fox",
    "the lazy dog",
    "quick brown quick brown",
    # ... more training text
])
```

### 2.2 WordPiece

WordPiece uses likelihood maximization instead of frequency counting.

```python
from tokenizers import Tokenizer
from tokenizers.models import WordPiece
from tokenizers.trainers import WordPieceTrainer
from tokenizers.pre_tokenizers import Whitespace

def train_wordpiece_tokenizer(
    files: List[str],
    vocab_size: int = 30522,
    min_frequency: int = 2,
    output_path: str = "wordpiece_tokenizer"
) -> Tokenizer:
    """Train a WordPiece tokenizer"""

    # Initialize tokenizer with WordPiece model
    tokenizer = Tokenizer(WordPiece(unk_token="[UNK]"))

    # Use whitespace pre-tokenization
    tokenizer.pre_tokenizer = Whitespace()

    # Configure trainer
    trainer = WordPieceTrainer(
        vocab_size=vocab_size,
        min_frequency=min_frequency,
        special_tokens=[
            "[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"
        ],
        continuing_subword_prefix="##"  # BERT-style prefix
    )

    # Train on files
    tokenizer.train(files, trainer)

    # Save tokenizer
    tokenizer.save(output_path + ".json")

    return tokenizer

# Key difference from BPE:
# WordPiece score = freq(ab) / (freq(a) * freq(b))
# This maximizes the likelihood of the training data
# rather than just merging most frequent pairs
```

### 2.3 Unigram (SentencePiece)

Unigram starts with a large vocabulary and iteratively removes tokens that least impact likelihood.

```python
import sentencepiece as spm

def train_sentencepiece_unigram(
    input_file: str,
    model_prefix: str,
    vocab_size: int = 32000,
    character_coverage: float = 0.9995,
    model_type: str = "unigram"
):
    """Train SentencePiece with Unigram model"""

    spm.SentencePieceTrainer.train(
        input=input_file,
        model_prefix=model_prefix,
        vocab_size=vocab_size,
        character_coverage=character_coverage,
        model_type=model_type,  # 'unigram', 'bpe', 'char', 'word'

        # Common options
        pad_id=0,
        unk_id=1,
        bos_id=2,
        eos_id=3,

        # Training options
        input_sentence_size=10000000,  # Max sentences to use
        shuffle_input_sentence=True,
        train_extremely_large_corpus=False,

        # Byte fallback for unknown characters
        byte_fallback=True,

        # Normalization
        normalization_rule_name="nmt_nfkc_cf",  # Normalize Unicode

        # Special pieces for LLMs
        user_defined_symbols=["<|system|>", "<|user|>", "<|assistant|>"],

        # Split options
        split_by_unicode_script=True,
        split_by_whitespace=True,
        split_digits=True,

        # Subword regularization (for training robustness)
        enable_differential_privacy=False,
    )

    print(f"Trained SentencePiece model: {model_prefix}.model")

    # Load and test
    sp = spm.SentencePieceProcessor(model_file=f"{model_prefix}.model")

    test_text = "Hello, how are you today?"
    tokens = sp.encode(test_text, out_type=str)
    print(f"Test: '{test_text}' -> {tokens}")

    return sp

# Train
sp = train_sentencepiece_unigram(
    input_file="training_data.txt",
    model_prefix="llm_tokenizer",
    vocab_size=32000
)

# Encode/decode
text = "Machine learning is transforming industries."
ids = sp.encode(text)  # [123, 456, 789, ...]
tokens = sp.encode(text, out_type=str)  # ['‚ñÅMachine', '‚ñÅlearning', ...]
decoded = sp.decode(ids)  # "Machine learning is transforming industries."
```

### 2.4 Byte-Level BPE (GPT-Style)

```python
from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import ByteLevel
from tokenizers.processors import ByteLevel as ByteLevelProcessor

def train_byte_level_bpe(
    files: List[str],
    vocab_size: int = 50257,
    min_frequency: int = 2,
    output_path: str = "byte_bpe_tokenizer"
) -> Tokenizer:
    """Train a byte-level BPE tokenizer (GPT-2 style)"""

    # Initialize with BPE model
    tokenizer = Tokenizer(BPE())

    # Byte-level pre-tokenization
    # This converts all text to bytes, ensuring no unknown tokens
    tokenizer.pre_tokenizer = ByteLevel(add_prefix_space=True)

    # Byte-level post-processing
    tokenizer.post_processor = ByteLevelProcessor(trim_offsets=True)

    # Configure trainer
    trainer = BpeTrainer(
        vocab_size=vocab_size,
        min_frequency=min_frequency,
        special_tokens=[
            "<|endoftext|>",
            "<|padding|>",
        ],
        initial_alphabet=ByteLevel.alphabet(),
        show_progress=True
    )

    # Train
    tokenizer.train(files, trainer)

    # Save
    tokenizer.save(output_path + ".json")

    return tokenizer

# Key benefit: Every possible byte (0-255) is in vocabulary
# This means ANY text can be encoded - no UNK tokens ever!
# Trade-off: Slightly longer sequences for non-ASCII text
```

---

## 3. Training Custom Tokenizers

### 3.1 Data Preparation

```python
from typing import List, Iterator
import json
from pathlib import Path

class TokenizerTrainingCorpus:
    """Prepare and manage corpus for tokenizer training"""

    def __init__(self, output_dir: str):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def prepare_from_jsonl(
        self,
        input_files: List[str],
        text_field: str = "text",
        output_file: str = "training_corpus.txt",
        max_samples: int = None,
        min_length: int = 100
    ) -> str:
        """Prepare training corpus from JSONL files"""

        output_path = self.output_dir / output_file
        sample_count = 0

        with open(output_path, 'w', encoding='utf-8') as f_out:
            for input_file in input_files:
                with open(input_file, 'r', encoding='utf-8') as f_in:
                    for line in f_in:
                        if max_samples and sample_count >= max_samples:
                            break

                        try:
                            data = json.loads(line)
                            text = data.get(text_field, "")

                            # Basic filtering
                            if len(text) >= min_length:
                                # Write one document per line
                                f_out.write(text.replace('\n', ' ') + '\n')
                                sample_count += 1
                        except json.JSONDecodeError:
                            continue

        print(f"Prepared {sample_count} documents in {output_path}")
        return str(output_path)

    def prepare_balanced_corpus(
        self,
        sources: Dict[str, Dict],
        target_samples_per_source: int = 100000
    ) -> str:
        """Prepare balanced corpus from multiple sources"""

        output_path = self.output_dir / "balanced_corpus.txt"

        with open(output_path, 'w', encoding='utf-8') as f_out:
            for source_name, config in sources.items():
                samples_written = 0
                file_path = config.get("file")
                text_field = config.get("text_field", "text")
                weight = config.get("weight", 1.0)

                target = int(target_samples_per_source * weight)

                with open(file_path, 'r', encoding='utf-8') as f_in:
                    for line in f_in:
                        if samples_written >= target:
                            break

                        try:
                            data = json.loads(line)
                            text = data.get(text_field, "")
                            if text:
                                f_out.write(text.replace('\n', ' ') + '\n')
                                samples_written += 1
                        except json.JSONDecodeError:
                            continue

                print(f"Added {samples_written} samples from {source_name}")

        return str(output_path)

# Usage
corpus = TokenizerTrainingCorpus("tokenizer_training")

# Option 1: Single source
corpus_file = corpus.prepare_from_jsonl(
    input_files=["data/training_data.jsonl"],
    text_field="text",
    max_samples=1_000_000
)

# Option 2: Balanced multi-source
corpus_file = corpus.prepare_balanced_corpus(
    sources={
        "web": {"file": "data/web.jsonl", "weight": 0.5},
        "books": {"file": "data/books.jsonl", "weight": 0.2},
        "code": {"file": "data/code.jsonl", "weight": 0.2},
        "wiki": {"file": "data/wiki.jsonl", "weight": 0.1},
    },
    target_samples_per_source=200_000
)
```

### 3.2 Complete Training Pipeline

```python
from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors
from tokenizers.normalizers import NFD, Lowercase, StripAccents, Sequence
from typing import Optional, List
import json

class LLMTokenizerTrainer:
    """Complete tokenizer training pipeline for LLMs"""

    def __init__(
        self,
        vocab_size: int = 32000,
        min_frequency: int = 2,
        algorithm: str = "bpe",  # bpe, wordpiece, unigram
        byte_level: bool = True
    ):
        self.vocab_size = vocab_size
        self.min_frequency = min_frequency
        self.algorithm = algorithm
        self.byte_level = byte_level
        self.tokenizer = None

    def _create_tokenizer(self) -> Tokenizer:
        """Create tokenizer with appropriate model"""

        if self.algorithm == "bpe":
            model = models.BPE()
        elif self.algorithm == "wordpiece":
            model = models.WordPiece(unk_token="[UNK]")
        elif self.algorithm == "unigram":
            model = models.Unigram()
        else:
            raise ValueError(f"Unknown algorithm: {self.algorithm}")

        return Tokenizer(model)

    def _configure_pre_tokenization(self, tokenizer: Tokenizer):
        """Configure pre-tokenization"""

        if self.byte_level:
            tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(
                add_prefix_space=False,
                trim_offsets=True
            )
        else:
            # Standard whitespace + punctuation splitting
            tokenizer.pre_tokenizer = pre_tokenizers.Sequence([
                pre_tokenizers.WhitespaceSplit(),
                pre_tokenizers.Punctuation()
            ])

    def _configure_decoder(self, tokenizer: Tokenizer):
        """Configure decoder"""

        if self.byte_level:
            tokenizer.decoder = decoders.ByteLevel()
        else:
            tokenizer.decoder = decoders.WordPiece(prefix="##")

    def _get_trainer(self, special_tokens: List[str]):
        """Get appropriate trainer"""

        if self.algorithm == "bpe":
            return trainers.BpeTrainer(
                vocab_size=self.vocab_size,
                min_frequency=self.min_frequency,
                special_tokens=special_tokens,
                initial_alphabet=pre_tokenizers.ByteLevel.alphabet() if self.byte_level else [],
                show_progress=True
            )
        elif self.algorithm == "wordpiece":
            return trainers.WordPieceTrainer(
                vocab_size=self.vocab_size,
                min_frequency=self.min_frequency,
                special_tokens=special_tokens,
                continuing_subword_prefix="##"
            )
        elif self.algorithm == "unigram":
            return trainers.UnigramTrainer(
                vocab_size=self.vocab_size,
                special_tokens=special_tokens,
                show_progress=True
            )

    def train(
        self,
        files: List[str],
        special_tokens: List[str] = None
    ) -> Tokenizer:
        """Train tokenizer on files"""

        if special_tokens is None:
            special_tokens = [
                "<|begin_of_text|>",
                "<|end_of_text|>",
                "<|pad|>",
                "<|unk|>",
                "<|system|>",
                "<|user|>",
                "<|assistant|>",
                "<|tool|>",
                "<|tool_result|>",
            ]

        # Create and configure tokenizer
        self.tokenizer = self._create_tokenizer()
        self._configure_pre_tokenization(self.tokenizer)
        self._configure_decoder(self.tokenizer)

        # Get trainer
        trainer = self._get_trainer(special_tokens)

        # Train
        print(f"Training {self.algorithm} tokenizer with vocab_size={self.vocab_size}")
        self.tokenizer.train(files, trainer)

        # Add post-processor for special tokens
        self.tokenizer.post_processor = processors.TemplateProcessing(
            single="<|begin_of_text|> $A <|end_of_text|>",
            pair="<|begin_of_text|> $A <|end_of_text|> <|begin_of_text|> $B <|end_of_text|>",
            special_tokens=[
                ("<|begin_of_text|>", self.tokenizer.token_to_id("<|begin_of_text|>")),
                ("<|end_of_text|>", self.tokenizer.token_to_id("<|end_of_text|>")),
            ],
        )

        print(f"Trained tokenizer with {self.tokenizer.get_vocab_size()} tokens")
        return self.tokenizer

    def save(self, path: str):
        """Save tokenizer"""
        if self.tokenizer:
            self.tokenizer.save(path)
            print(f"Saved tokenizer to {path}")

    def load(self, path: str) -> Tokenizer:
        """Load tokenizer"""
        self.tokenizer = Tokenizer.from_file(path)
        return self.tokenizer

# Usage
trainer = LLMTokenizerTrainer(
    vocab_size=32000,
    min_frequency=2,
    algorithm="bpe",
    byte_level=True
)

tokenizer = trainer.train(
    files=["training_corpus.txt"],
    special_tokens=[
        "<|begin_of_text|>",
        "<|end_of_text|>",
        "<|pad|>",
        "<|system|>",
        "<|user|>",
        "<|assistant|>",
    ]
)

trainer.save("my_tokenizer.json")
```

### 3.3 SentencePiece Training Configuration

```python
import sentencepiece as spm
from typing import List, Optional

def train_production_sentencepiece(
    input_files: List[str],
    model_prefix: str,
    vocab_size: int = 32000,
    model_type: str = "bpe",
    user_defined_symbols: Optional[List[str]] = None,
    normalization: str = "nmt_nfkc_cf",
    byte_fallback: bool = True
):
    """Train production-ready SentencePiece tokenizer"""

    # Concatenate input files
    combined_input = f"{model_prefix}_input.txt"
    with open(combined_input, 'w', encoding='utf-8') as f_out:
        for input_file in input_files:
            with open(input_file, 'r', encoding='utf-8') as f_in:
                for line in f_in:
                    f_out.write(line)

    # Default user-defined symbols for LLM chat
    if user_defined_symbols is None:
        user_defined_symbols = [
            "<|begin_of_text|>",
            "<|end_of_text|>",
            "<|start_header_id|>",
            "<|end_header_id|>",
            "<|eot_id|>",  # End of turn
            "<|python_tag|>",
            "<|reserved_special_token_0|>",
            "<|reserved_special_token_1|>",
            "<|reserved_special_token_2|>",
            "<|reserved_special_token_3|>",
        ]

    # Training command arguments
    train_args = {
        'input': combined_input,
        'model_prefix': model_prefix,
        'vocab_size': vocab_size,
        'model_type': model_type,  # 'bpe' or 'unigram'

        # Special token IDs
        'pad_id': 0,
        'unk_id': 1,
        'bos_id': 2,
        'eos_id': 3,

        # Character coverage (important for multilingual)
        'character_coverage': 0.9999,

        # Normalization
        'normalization_rule_name': normalization,

        # Byte fallback for unknown characters
        'byte_fallback': byte_fallback,

        # User-defined symbols
        'user_defined_symbols': ','.join(user_defined_symbols),

        # Splitting rules
        'split_by_unicode_script': True,
        'split_by_whitespace': True,
        'split_by_number': True,
        'split_digits': True,

        # Max sentence length
        'max_sentence_length': 4192,

        # Training size limits
        'input_sentence_size': 50000000,  # 50M sentences max
        'shuffle_input_sentence': True,

        # For very large corpus
        'train_extremely_large_corpus': True,

        # Vocabulary filtering
        'hard_vocab_limit': True,

        # Required coverage
        'required_chars': '',  # Comma-separated chars that must be in vocab
    }

    # Train
    spm.SentencePieceTrainer.train(**train_args)

    # Validate
    sp = spm.SentencePieceProcessor(model_file=f"{model_prefix}.model")

    print(f"Trained SentencePiece model: {model_prefix}.model")
    print(f"Vocabulary size: {sp.get_piece_size()}")

    # Test encoding
    test_texts = [
        "Hello, world!",
        "„Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïå",  # Japanese
        "def foo(): return 42",  # Code
        "üöÄ Rocket emoji",  # Emoji
    ]

    for text in test_texts:
        tokens = sp.encode(text, out_type=str)
        print(f"  '{text}' -> {tokens} ({len(tokens)} tokens)")

    return sp

# Usage
sp = train_production_sentencepiece(
    input_files=["corpus_part1.txt", "corpus_part2.txt"],
    model_prefix="llama_style_tokenizer",
    vocab_size=32000,
    model_type="bpe"
)
```

---

## 4. Tokenizer Evaluation

### 4.1 Evaluation Metrics

```python
from typing import List, Dict, Tuple
import numpy as np
from collections import Counter

class TokenizerEvaluator:
    """Evaluate tokenizer quality"""

    def __init__(self, tokenizer):
        self.tokenizer = tokenizer

    def fertility(self, texts: List[str]) -> Dict[str, float]:
        """
        Calculate fertility (tokens per word/character).
        Lower is generally better (more compressed).
        """
        total_tokens = 0
        total_words = 0
        total_chars = 0

        for text in texts:
            if hasattr(self.tokenizer, 'encode'):
                tokens = self.tokenizer.encode(text)
                if hasattr(tokens, 'ids'):
                    num_tokens = len(tokens.ids)
                else:
                    num_tokens = len(tokens)
            else:
                num_tokens = len(self.tokenizer.tokenize(text))

            total_tokens += num_tokens
            total_words += len(text.split())
            total_chars += len(text)

        return {
            "tokens_per_word": total_tokens / total_words if total_words > 0 else 0,
            "tokens_per_char": total_tokens / total_chars if total_chars > 0 else 0,
            "compression_ratio": total_chars / total_tokens if total_tokens > 0 else 0
        }

    def unknown_token_rate(self, texts: List[str]) -> float:
        """
        Calculate rate of unknown tokens.
        Should be 0 for byte-level tokenizers.
        """
        total_tokens = 0
        unk_tokens = 0

        # Get UNK token ID
        if hasattr(self.tokenizer, 'token_to_id'):
            unk_id = self.tokenizer.token_to_id("<unk>") or self.tokenizer.token_to_id("[UNK]")
        else:
            unk_id = None

        for text in texts:
            if hasattr(self.tokenizer, 'encode'):
                encoding = self.tokenizer.encode(text)
                tokens = encoding.ids if hasattr(encoding, 'ids') else encoding
            else:
                tokens = self.tokenizer.encode(text)

            total_tokens += len(tokens)
            if unk_id is not None:
                unk_tokens += sum(1 for t in tokens if t == unk_id)

        return unk_tokens / total_tokens if total_tokens > 0 else 0

    def vocabulary_coverage(
        self,
        texts: List[str],
        top_k: int = 10000
    ) -> Dict[str, float]:
        """Analyze vocabulary coverage patterns"""

        token_counts = Counter()

        for text in texts:
            if hasattr(self.tokenizer, 'encode'):
                encoding = self.tokenizer.encode(text)
                tokens = encoding.ids if hasattr(encoding, 'ids') else encoding
            else:
                tokens = self.tokenizer.encode(text)

            token_counts.update(tokens)

        total_occurrences = sum(token_counts.values())
        unique_tokens = len(token_counts)

        # Calculate coverage by top tokens
        sorted_tokens = token_counts.most_common()
        top_k_coverage = sum(c for _, c in sorted_tokens[:top_k]) / total_occurrences

        return {
            "unique_tokens_used": unique_tokens,
            "total_occurrences": total_occurrences,
            f"top_{top_k}_coverage": top_k_coverage,
            "vocabulary_utilization": unique_tokens / self.tokenizer.get_vocab_size()
        }

    def encoding_speed(
        self,
        texts: List[str],
        warmup_rounds: int = 3
    ) -> Dict[str, float]:
        """Benchmark encoding speed"""
        import time

        # Warmup
        for _ in range(warmup_rounds):
            for text in texts[:100]:
                self.tokenizer.encode(text)

        # Benchmark
        start_time = time.perf_counter()
        total_chars = 0

        for text in texts:
            self.tokenizer.encode(text)
            total_chars += len(text)

        elapsed = time.perf_counter() - start_time

        return {
            "total_time_seconds": elapsed,
            "chars_per_second": total_chars / elapsed,
            "texts_per_second": len(texts) / elapsed
        }

    def roundtrip_accuracy(self, texts: List[str]) -> float:
        """Check if encode->decode produces original text"""

        correct = 0
        for text in texts:
            if hasattr(self.tokenizer, 'encode'):
                encoded = self.tokenizer.encode(text)
                ids = encoded.ids if hasattr(encoded, 'ids') else encoded
                decoded = self.tokenizer.decode(ids)
            else:
                ids = self.tokenizer.encode(text)
                decoded = self.tokenizer.decode(ids)

            # Normalize for comparison
            if decoded.strip() == text.strip():
                correct += 1

        return correct / len(texts)

    def multilingual_analysis(
        self,
        texts_by_language: Dict[str, List[str]]
    ) -> Dict[str, Dict]:
        """Analyze tokenizer performance across languages"""

        results = {}

        for language, texts in texts_by_language.items():
            fertility = self.fertility(texts)
            unk_rate = self.unknown_token_rate(texts)

            results[language] = {
                "tokens_per_word": fertility["tokens_per_word"],
                "tokens_per_char": fertility["tokens_per_char"],
                "unknown_rate": unk_rate
            }

        return results

    def generate_report(self, eval_texts: List[str]) -> Dict:
        """Generate comprehensive evaluation report"""

        report = {
            "vocab_size": self.tokenizer.get_vocab_size(),
            "fertility": self.fertility(eval_texts),
            "unknown_rate": self.unknown_token_rate(eval_texts),
            "coverage": self.vocabulary_coverage(eval_texts),
            "speed": self.encoding_speed(eval_texts),
            "roundtrip_accuracy": self.roundtrip_accuracy(eval_texts)
        }

        return report

# Usage
from tokenizers import Tokenizer

tokenizer = Tokenizer.from_file("my_tokenizer.json")
evaluator = TokenizerEvaluator(tokenizer)

eval_texts = [
    "This is a test sentence.",
    "Machine learning is transforming industries.",
    # ... more test texts
]

report = evaluator.generate_report(eval_texts)
print(f"Vocabulary Size: {report['vocab_size']}")
print(f"Tokens per Word: {report['fertility']['tokens_per_word']:.2f}")
print(f"Unknown Rate: {report['unknown_rate']:.4f}")
print(f"Roundtrip Accuracy: {report['roundtrip_accuracy']:.2%}")
```

### 4.2 Downstream Task Validation

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from typing import List, Dict

class DownstreamValidator:
    """Validate tokenizer with downstream tasks"""

    def __init__(self, tokenizer_path: str, model_path: str = None):
        # Load custom tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)

        # Optionally load model for validation
        if model_path:
            self.model = AutoModelForCausalLM.from_pretrained(model_path)
        else:
            self.model = None

    def validate_special_tokens(self) -> Dict:
        """Validate special token handling"""

        required_tokens = {
            "bos_token": self.tokenizer.bos_token,
            "eos_token": self.tokenizer.eos_token,
            "pad_token": self.tokenizer.pad_token,
            "unk_token": self.tokenizer.unk_token,
        }

        results = {}
        for name, token in required_tokens.items():
            token_id = self.tokenizer.convert_tokens_to_ids(token) if token else None
            results[name] = {
                "token": token,
                "id": token_id,
                "present": token is not None and token_id is not None
            }

        return results

    def validate_chat_template(
        self,
        messages: List[Dict[str, str]]
    ) -> str:
        """Validate chat template formatting"""

        try:
            formatted = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            return formatted
        except Exception as e:
            return f"Error: {e}"

    def validate_truncation(
        self,
        long_text: str,
        max_length: int = 2048
    ) -> Dict:
        """Validate truncation behavior"""

        encoded = self.tokenizer.encode(
            long_text,
            truncation=True,
            max_length=max_length,
            return_tensors="pt"
        )

        decoded = self.tokenizer.decode(encoded[0])

        return {
            "original_length": len(long_text),
            "encoded_length": encoded.shape[1],
            "was_truncated": len(self.tokenizer.encode(long_text)) > max_length,
            "decoded_sample": decoded[:200] + "..."
        }

    def validate_padding(
        self,
        texts: List[str],
        padding: str = "max_length",
        max_length: int = 512
    ) -> Dict:
        """Validate padding behavior"""

        encoded = self.tokenizer(
            texts,
            padding=padding,
            max_length=max_length,
            truncation=True,
            return_tensors="pt"
        )

        return {
            "input_ids_shape": encoded["input_ids"].shape,
            "attention_mask_shape": encoded["attention_mask"].shape,
            "all_same_length": len(set(len(ids) for ids in encoded["input_ids"])) == 1
        }

    def run_all_validations(self) -> Dict:
        """Run all validation checks"""

        results = {
            "special_tokens": self.validate_special_tokens(),
            "chat_template": self.validate_chat_template([
                {"role": "user", "content": "Hello!"},
                {"role": "assistant", "content": "Hi there!"}
            ]),
            "truncation": self.validate_truncation("x " * 10000),
            "padding": self.validate_padding(["short", "a bit longer text"])
        }

        return results
```

---

## 5. Tokenizer Integration

### 5.1 Chat Templates

```python
from transformers import PreTrainedTokenizerFast
from tokenizers import Tokenizer

def add_chat_template(
    tokenizer_path: str,
    output_path: str,
    template_style: str = "llama3"
):
    """Add chat template to tokenizer"""

    # Load tokenizer
    tokenizer = PreTrainedTokenizerFast(
        tokenizer_file=tokenizer_path
    )

    # Define chat templates
    CHAT_TEMPLATES = {
        "llama3": """{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>

' + message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>

' }}{% endif %}""",

        "chatml": """{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}""",

        "mistral": """{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}""",

        "simple": """{% for message in messages %}{{ message['role'] }}: {{ message['content'] }}\n{% endfor %}{% if add_generation_prompt %}assistant: {% endif %}"""
    }

    # Set template
    if template_style not in CHAT_TEMPLATES:
        raise ValueError(f"Unknown template style: {template_style}")

    tokenizer.chat_template = CHAT_TEMPLATES[template_style]

    # Add special tokens if needed
    special_tokens = {
        "llama3": {
            "bos_token": "<|begin_of_text|>",
            "eos_token": "<|end_of_text|>",
            "pad_token": "<|pad|>",
        },
        "chatml": {
            "bos_token": "<|im_start|>",
            "eos_token": "<|im_end|>",
        },
        "mistral": {
            "bos_token": "<s>",
            "eos_token": "</s>",
        }
    }

    if template_style in special_tokens:
        for key, value in special_tokens[template_style].items():
            setattr(tokenizer, key, value)

    # Save
    tokenizer.save_pretrained(output_path)

    return tokenizer

# Usage
tokenizer = add_chat_template(
    "my_tokenizer.json",
    "my_tokenizer_with_chat/",
    template_style="llama3"
)

# Test chat template
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "What is machine learning?"},
]

formatted = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
print(formatted)
```

### 5.2 Adding Special Tokens

```python
from transformers import AutoTokenizer, AddedToken

def add_special_tokens_to_tokenizer(
    tokenizer_path: str,
    new_tokens: List[str],
    output_path: str
) -> int:
    """Add special tokens and resize model embeddings"""

    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)

    # Create AddedToken objects with proper settings
    tokens_to_add = []
    for token in new_tokens:
        added_token = AddedToken(
            content=token,
            single_word=False,
            lstrip=False,
            rstrip=False,
            special=True,  # Mark as special token
            normalized=False
        )
        tokens_to_add.append(added_token)

    # Add tokens
    num_added = tokenizer.add_special_tokens({
        "additional_special_tokens": tokens_to_add
    })

    print(f"Added {num_added} special tokens")
    print(f"New vocab size: {len(tokenizer)}")

    # Save updated tokenizer
    tokenizer.save_pretrained(output_path)

    return num_added

# Example: Add tool-use tokens
new_tokens = [
    "<|tool_call|>",
    "<|tool_result|>",
    "<|code|>",
    "<|/code|>",
    "<|thinking|>",
    "<|/thinking|>",
]

num_added = add_special_tokens_to_tokenizer(
    "base_tokenizer/",
    new_tokens,
    "tokenizer_with_tools/"
)

# When loading model, resize embeddings:
# model.resize_token_embeddings(len(tokenizer))
```

### 5.3 Tokenizer Configuration for HuggingFace

```python
import json
from pathlib import Path

def create_hf_tokenizer_config(
    output_dir: str,
    vocab_size: int,
    model_max_length: int = 4096,
    chat_template: str = None,
    special_tokens: dict = None
):
    """Create HuggingFace tokenizer configuration files"""

    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # Default special tokens
    if special_tokens is None:
        special_tokens = {
            "bos_token": "<|begin_of_text|>",
            "eos_token": "<|end_of_text|>",
            "unk_token": "<|unk|>",
            "pad_token": "<|pad|>",
        }

    # tokenizer_config.json
    tokenizer_config = {
        "add_bos_token": True,
        "add_eos_token": False,
        "bos_token": special_tokens["bos_token"],
        "eos_token": special_tokens["eos_token"],
        "pad_token": special_tokens["pad_token"],
        "unk_token": special_tokens["unk_token"],
        "model_max_length": model_max_length,
        "padding_side": "right",
        "tokenizer_class": "PreTrainedTokenizerFast",
        "clean_up_tokenization_spaces": False,
    }

    if chat_template:
        tokenizer_config["chat_template"] = chat_template

    with open(output_path / "tokenizer_config.json", "w") as f:
        json.dump(tokenizer_config, f, indent=2)

    # special_tokens_map.json
    special_tokens_map = {
        "bos_token": special_tokens["bos_token"],
        "eos_token": special_tokens["eos_token"],
        "pad_token": special_tokens["pad_token"],
        "unk_token": special_tokens["unk_token"],
    }

    with open(output_path / "special_tokens_map.json", "w") as f:
        json.dump(special_tokens_map, f, indent=2)

    print(f"Created tokenizer config in {output_dir}")

# Usage
create_hf_tokenizer_config(
    output_dir="my_tokenizer/",
    vocab_size=32000,
    model_max_length=8192,
    chat_template=CHAT_TEMPLATES["llama3"]
)
```

---

## 6. Common Tokenizers Reference

### 6.1 Popular Model Tokenizers

| Model | Tokenizer Type | Vocab Size | Special Features |
|-------|---------------|------------|------------------|
| GPT-4 | Byte-level BPE (cl100k_base) | 100,277 | Improved multilingual, code |
| GPT-3.5 | Byte-level BPE (cl100k_base) | 100,277 | Same as GPT-4 |
| GPT-2 | Byte-level BPE (gpt2) | 50,257 | Original byte-level BPE |
| Llama 2 | SentencePiece BPE | 32,000 | Byte fallback |
| Llama 3 | Byte-level BPE (tiktoken) | 128,256 | Extended for multilingual |
| Mistral | SentencePiece BPE | 32,000 | Similar to Llama 2 |
| Qwen 2 | Byte-level BPE | 151,936 | Excellent CJK support |
| Gemma | SentencePiece | 256,128 | Very large vocabulary |
| BERT | WordPiece | 30,522 | ## subword prefix |
| T5 | SentencePiece Unigram | 32,100 | ‚ñÅ prefix for spaces |

### 6.2 Loading Popular Tokenizers

```python
# tiktoken (OpenAI)
import tiktoken

# GPT-4 tokenizer
enc = tiktoken.get_encoding("cl100k_base")
tokens = enc.encode("Hello, world!")
text = enc.decode(tokens)

# GPT-3 tokenizer
enc_gpt3 = tiktoken.get_encoding("p50k_base")

# HuggingFace Transformers
from transformers import AutoTokenizer

# Llama 3
llama_tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B")

# Mistral
mistral_tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")

# Qwen
qwen_tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2-7B")

# BERT
bert_tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
```

---

## 7. Troubleshooting

### Common Issues

| Issue | Symptoms | Solution |
|-------|----------|----------|
| OOV/UNK tokens | Many [UNK] in output | Use byte-level BPE or increase character coverage |
| Long sequences | High token count | Increase vocab size or train on domain data |
| Slow encoding | High latency | Use Rust tokenizers, batch encoding |
| Broken Unicode | Garbled text after decode | Check normalization settings, use byte fallback |
| Missing special tokens | Chat template fails | Add tokens before training or post-hoc |
| Inconsistent splits | Same word different tokens | Ensure deterministic tokenization (no sampling) |

### Debug Commands

```python
# Inspect tokenization
def debug_tokenization(tokenizer, text: str):
    """Debug tokenization of text"""

    # Get tokens
    if hasattr(tokenizer, 'encode'):
        encoding = tokenizer.encode(text)
        if hasattr(encoding, 'ids'):
            ids = encoding.ids
            tokens = encoding.tokens
        else:
            ids = encoding
            tokens = [tokenizer.decode([i]) for i in ids]
    else:
        ids = tokenizer.encode(text)
        tokens = tokenizer.convert_ids_to_tokens(ids)

    print(f"Input: {repr(text)}")
    print(f"Token count: {len(ids)}")
    print(f"Tokens: {tokens}")
    print(f"IDs: {ids}")

    # Check roundtrip
    if hasattr(tokenizer, 'decode'):
        decoded = tokenizer.decode(ids)
        print(f"Decoded: {repr(decoded)}")
        print(f"Roundtrip OK: {text == decoded}")

# Check vocabulary
def inspect_vocabulary(tokenizer, pattern: str = None):
    """Inspect vocabulary entries"""

    if hasattr(tokenizer, 'get_vocab'):
        vocab = tokenizer.get_vocab()
    else:
        vocab = {tokenizer.convert_ids_to_tokens(i): i for i in range(len(tokenizer))}

    if pattern:
        import re
        matching = {k: v for k, v in vocab.items() if re.search(pattern, k)}
        print(f"Tokens matching '{pattern}': {len(matching)}")
        for token, id in list(matching.items())[:20]:
            print(f"  {id}: {repr(token)}")
    else:
        print(f"Vocabulary size: {len(vocab)}")
        print(f"Sample tokens: {list(vocab.items())[:10]}")
```

---

## Glossary

| Term | Definition |
|------|------------|
| **BPE** | Byte-Pair Encoding - merges frequent character pairs iteratively |
| **Fertility** | Number of tokens per word/character |
| **OOV** | Out-of-vocabulary - words not in tokenizer vocabulary |
| **SentencePiece** | Language-agnostic tokenizer that treats spaces as characters |
| **Subword** | Token that is part of a word |
| **UNK** | Unknown token placeholder |
| **Vocabulary** | Set of all possible tokens |
| **WordPiece** | Subword algorithm maximizing likelihood |

---

## References

- [HuggingFace Tokenizers Documentation](https://huggingface.co/docs/tokenizers)
- [SentencePiece GitHub](https://github.com/google/sentencepiece)
- [tiktoken (OpenAI)](https://github.com/openai/tiktoken)
- [Summary of Tokenizers](https://huggingface.co/docs/transformers/en/tokenizer_summary)
- [BPE Paper](https://arxiv.org/abs/1508.07909)
- [SentencePiece Paper](https://arxiv.org/abs/1808.06226)
