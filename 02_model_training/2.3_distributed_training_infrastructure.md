# Document 2.3: Distributed Training Infrastructure Guide

## Document Information
- **Version:** 1.0
- **Last Updated:** December 2025
- **Owner:** ML Infrastructure / Platform Team
- **Category:** Model Training (Pre-training)

---

## Executive Summary

Training modern LLMs requires distributing computation across multiple GPUs and often multiple nodes. This guide covers distributed training fundamentals, parallelism strategies (data, tensor, pipeline, expert), framework selection (DeepSpeed, FSDP, Megatron-LM), hardware configuration, and optimization techniques. Proper distributed training setup can reduce training time from months to weeks while maximizing hardware utilization.

**Key Outcomes:**
- Understand parallelism strategies and when to use each
- Configure distributed training frameworks correctly
- Optimize throughput (tokens/second) and GPU utilization
- Implement fault tolerance and checkpointing

---

## Prerequisites

### Required Knowledge
- PyTorch fundamentals (tensors, modules, optimizers)
- GPU programming basics (CUDA concepts)
- Networking fundamentals (TCP/IP, bandwidth)
- Linux system administration

### Infrastructure Requirements
- Multi-GPU server (8× A100/H100 typical)
- High-bandwidth interconnect (NVLink, InfiniBand)
- Shared filesystem or fast object storage
- Container orchestration (Kubernetes or Slurm)

### Tool Installation
```bash
# PyTorch with CUDA
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# DeepSpeed
pip install deepspeed

# FSDP is built into PyTorch
# Nothing additional needed

# For Megatron-LM
git clone https://github.com/NVIDIA/Megatron-LM.git
cd Megatron-LM
pip install -e .

# Monitoring
pip install wandb tensorboard

# NCCL debugging
export NCCL_DEBUG=INFO
export NCCL_DEBUG_SUBSYS=ALL
```

---

## 1. Distributed Training Fundamentals

### 1.1 Why Distributed Training

```
┌─────────────────────────────────────────────────────────────┐
│            Why Distributed Training?                         │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  Model Size Growth:                                         │
│  ┌─────────────────────────────────────────────────────┐    │
│  │ GPT-2 (2019):    1.5B params  →  ~3 GB              │    │
│  │ GPT-3 (2020):    175B params  →  ~350 GB            │    │
│  │ GPT-4 (2023):    ~1.8T params →  ~3.6 TB            │    │
│  │ Llama 3.1 405B:  405B params  →  ~810 GB            │    │
│  └─────────────────────────────────────────────────────┘    │
│                                                              │
│  Single GPU Memory:                                          │
│  ┌─────────────────────────────────────────────────────┐    │
│  │ A100 80GB:   Can fit ~35B params (FP16, inference)  │    │
│  │ H100 80GB:   Similar capacity                       │    │
│  │ Training:    Need 4-20x more memory                 │    │
│  │              (optimizer states, gradients, activations) │ │
│  └─────────────────────────────────────────────────────┘    │
│                                                              │
│  Compute Requirements:                                       │
│  ┌─────────────────────────────────────────────────────┐    │
│  │ Llama 2 70B: ~1.7M GPU-hours (A100)                 │    │
│  │ Single GPU: ~200 years                               │    │
│  │ 2000 GPUs:  ~35 days                                 │    │
│  └─────────────────────────────────────────────────────┘    │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### 1.2 Parallelism Strategies Overview

```
┌─────────────────────────────────────────────────────────────┐
│              Parallelism Strategies                          │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  DATA PARALLELISM (DP)                                      │
│  ┌───────────────────────────────────────────────────────┐  │
│  │  Each GPU has full model copy, different data batch    │  │
│  │                                                        │  │
│  │  GPU 0: [Model] ← Batch 0                             │  │
│  │  GPU 1: [Model] ← Batch 1    → Sync gradients        │  │
│  │  GPU 2: [Model] ← Batch 2                             │  │
│  │  GPU 3: [Model] ← Batch 3                             │  │
│  │                                                        │  │
│  │  ✓ Simple, works for most models                      │  │
│  │  ✗ Model must fit on single GPU                       │  │
│  └───────────────────────────────────────────────────────┘  │
│                                                              │
│  TENSOR PARALLELISM (TP)                                    │
│  ┌───────────────────────────────────────────────────────┐  │
│  │  Split individual layers across GPUs                   │  │
│  │                                                        │  │
│  │  Layer: [W₀|W₁|W₂|W₃] → Split columns                 │  │
│  │  GPU 0: [W₀]  GPU 1: [W₁]  GPU 2: [W₂]  GPU 3: [W₃]  │  │
│  │                                                        │  │
│  │  ✓ Reduces per-GPU memory                              │  │
│  │  ✓ Low latency (within node)                           │  │
│  │  ✗ High communication overhead                         │  │
│  │  ✗ Requires NVLink for efficiency                      │  │
│  └───────────────────────────────────────────────────────┘  │
│                                                              │
│  PIPELINE PARALLELISM (PP)                                  │
│  ┌───────────────────────────────────────────────────────┐  │
│  │  Split model layers across GPUs                        │  │
│  │                                                        │  │
│  │  GPU 0: Layers 0-7   → GPU 1: Layers 8-15             │  │
│  │  GPU 2: Layers 16-23 → GPU 3: Layers 24-31            │  │
│  │                                                        │  │
│  │  ✓ Works across nodes                                  │  │
│  │  ✓ Lower communication than TP                         │  │
│  │  ✗ Pipeline bubbles (idle time)                        │  │
│  │  ✗ Complex scheduling                                  │  │
│  └───────────────────────────────────────────────────────┘  │
│                                                              │
│  ZERO/FSDP (Sharded Data Parallel)                          │
│  ┌───────────────────────────────────────────────────────┐  │
│  │  Shard optimizer states, gradients, and/or parameters │  │
│  │                                                        │  │
│  │  ZeRO-1: Shard optimizer states only                  │  │
│  │  ZeRO-2: + Shard gradients                            │  │
│  │  ZeRO-3: + Shard parameters (= FSDP)                  │  │
│  │                                                        │  │
│  │  ✓ Near-linear memory scaling                          │  │
│  │  ✓ Relatively simple to use                            │  │
│  │  ✗ Higher communication than pure DP                   │  │
│  └───────────────────────────────────────────────────────┘  │
│                                                              │
│  3D PARALLELISM (DP + TP + PP)                              │
│  ┌───────────────────────────────────────────────────────┐  │
│  │  Combine all strategies for maximum scale              │  │
│  │                                                        │  │
│  │  Example: 512 GPUs = 8 TP × 8 PP × 8 DP               │  │
│  │  - TP within node (8 GPUs with NVLink)                │  │
│  │  - PP across nodes in groups                          │  │
│  │  - DP across PP groups                                │  │
│  │                                                        │  │
│  │  ✓ Required for 100B+ models                           │  │
│  │  ✗ Complex configuration                               │  │
│  └───────────────────────────────────────────────────────┘  │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### 1.3 When to Use Each Strategy

| Model Size | Recommended Strategy | Notes |
|-----------|---------------------|-------|
| <1B params | DDP (Data Parallel) | Fits on single GPU |
| 1-10B params | FSDP or ZeRO-3 | Shard to fit memory |
| 10-70B params | FSDP + TP | TP within node, FSDP across |
| 70-400B params | TP + PP + DP | Full 3D parallelism |
| >400B params | 3D + Expert Parallel | Add MoE for efficiency |

---

## 2. Framework Selection

### 2.1 PyTorch FSDP (Fully Sharded Data Parallel)

```python
import torch
import torch.distributed as dist
from torch.distributed.fsdp import (
    FullyShardedDataParallel as FSDP,
    ShardingStrategy,
    MixedPrecision,
    BackwardPrefetch,
    CPUOffload,
)
from torch.distributed.fsdp.wrap import (
    transformer_auto_wrap_policy,
    size_based_auto_wrap_policy,
)
from functools import partial

def setup_fsdp_training(
    model,
    rank: int,
    world_size: int,
    mixed_precision: bool = True,
    cpu_offload: bool = False
):
    """Configure FSDP for training"""

    # Initialize distributed
    dist.init_process_group(
        backend="nccl",
        rank=rank,
        world_size=world_size
    )
    torch.cuda.set_device(rank)

    # Mixed precision config
    if mixed_precision:
        mp_policy = MixedPrecision(
            param_dtype=torch.bfloat16,
            reduce_dtype=torch.bfloat16,
            buffer_dtype=torch.bfloat16,
        )
    else:
        mp_policy = None

    # CPU offload config
    if cpu_offload:
        offload_policy = CPUOffload(offload_params=True)
    else:
        offload_policy = None

    # Auto-wrap policy for transformers
    from transformers.models.llama.modeling_llama import LlamaDecoderLayer

    auto_wrap_policy = partial(
        transformer_auto_wrap_policy,
        transformer_layer_cls={LlamaDecoderLayer}
    )

    # Wrap model with FSDP
    model = FSDP(
        model,
        sharding_strategy=ShardingStrategy.FULL_SHARD,  # ZeRO-3
        auto_wrap_policy=auto_wrap_policy,
        mixed_precision=mp_policy,
        cpu_offload=offload_policy,
        backward_prefetch=BackwardPrefetch.BACKWARD_PRE,
        device_id=torch.cuda.current_device(),
        limit_all_gathers=True,  # Reduce memory spikes
    )

    return model

# Sharding strategies explained
SHARDING_STRATEGIES = {
    "FULL_SHARD": "ZeRO-3: Shard params, gradients, optimizer states",
    "SHARD_GRAD_OP": "ZeRO-2: Shard gradients and optimizer states only",
    "NO_SHARD": "DDP: No sharding, just gradient sync",
    "HYBRID_SHARD": "Shard within node, replicate across nodes"
}

# Example training loop with FSDP
def train_fsdp(model, dataloader, optimizer, rank):
    model.train()

    for batch in dataloader:
        batch = {k: v.to(rank) for k, v in batch.items()}

        # Forward pass
        outputs = model(**batch)
        loss = outputs.loss

        # Backward pass
        loss.backward()

        # Gradient clipping
        model.clip_grad_norm_(1.0)

        # Update
        optimizer.step()
        optimizer.zero_grad()

        if rank == 0:
            print(f"Loss: {loss.item():.4f}")
```

### 2.2 DeepSpeed ZeRO

```python
import deepspeed
import torch
from typing import Dict, Any

# DeepSpeed configuration
DEEPSPEED_CONFIG_ZERO3 = {
    "train_batch_size": 256,  # Global batch size
    "train_micro_batch_size_per_gpu": 4,  # Per-GPU batch
    "gradient_accumulation_steps": 8,  # 256 / (8 GPUs * 4)

    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 1e-4,
            "betas": [0.9, 0.95],
            "eps": 1e-8,
            "weight_decay": 0.1
        }
    },

    "scheduler": {
        "type": "WarmupDecayLR",
        "params": {
            "warmup_min_lr": 0,
            "warmup_max_lr": 1e-4,
            "warmup_num_steps": 2000,
            "total_num_steps": 100000
        }
    },

    "fp16": {
        "enabled": False  # Use bf16 instead
    },

    "bf16": {
        "enabled": True
    },

    "zero_optimization": {
        "stage": 3,  # ZeRO-3

        # Partition parameters
        "stage3_param_persistence_threshold": 1e4,

        # Prefetching
        "stage3_prefetch_bucket_size": 5e8,
        "stage3_max_live_parameters": 1e9,
        "stage3_max_reuse_distance": 1e9,

        # Memory efficiency
        "reduce_bucket_size": 5e8,
        "allgather_bucket_size": 5e8,
        "overlap_comm": True,
        "contiguous_gradients": True,

        # CPU offload (for very large models)
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": True,
            "fast_init": True
        },
        "offload_param": {
            "device": "none"  # or "cpu" for extreme memory saving
        }
    },

    "gradient_clipping": 1.0,

    "activation_checkpointing": {
        "partition_activations": True,
        "cpu_checkpointing": False,
        "contiguous_memory_optimization": True,
        "number_checkpoints": None,
        "synchronize_checkpoint_boundary": False,
        "profile": False
    },

    "wall_clock_breakdown": False,
    "tensorboard": {
        "enabled": True,
        "output_path": "./logs/tensorboard"
    }
}

def setup_deepspeed_training(
    model,
    config: Dict[str, Any],
    training_data
):
    """Initialize DeepSpeed training"""

    # Initialize model engine
    model_engine, optimizer, _, lr_scheduler = deepspeed.initialize(
        model=model,
        config=config,
        model_parameters=model.parameters(),
        training_data=training_data
    )

    return model_engine, optimizer, lr_scheduler

def train_deepspeed(model_engine, dataloader):
    """Training loop with DeepSpeed"""

    model_engine.train()

    for step, batch in enumerate(dataloader):
        # Move to GPU
        batch = {k: v.to(model_engine.device) for k, v in batch.items()}

        # Forward pass
        outputs = model_engine(**batch)
        loss = outputs.loss

        # Backward pass (DeepSpeed handles gradient sync)
        model_engine.backward(loss)

        # Step (includes gradient clipping, optimizer step, lr scheduling)
        model_engine.step()

        if model_engine.global_rank == 0 and step % 100 == 0:
            print(f"Step {step}, Loss: {loss.item():.4f}")

# Launch script
"""
deepspeed --num_gpus=8 train.py \
    --deepspeed_config ds_config.json \
    --model_name_or_path meta-llama/Llama-2-7b
"""
```

### 2.3 Megatron-LM

```python
"""
Megatron-LM Configuration for Large Model Training

Megatron-LM provides tensor and pipeline parallelism
optimized for NVIDIA hardware.
"""

# Megatron-LM command line configuration
MEGATRON_ARGS = {
    # Model architecture
    "--num-layers": 32,
    "--hidden-size": 4096,
    "--num-attention-heads": 32,
    "--seq-length": 4096,
    "--max-position-embeddings": 4096,

    # Parallelism
    "--tensor-model-parallel-size": 8,  # TP: within node
    "--pipeline-model-parallel-size": 4,  # PP: across nodes
    # Effective world size: 8 * 4 = 32 GPUs per replica
    # With DP over remaining GPUs

    # Training
    "--micro-batch-size": 1,
    "--global-batch-size": 512,

    # Precision
    "--bf16": True,

    # Optimization
    "--optimizer": "adam",
    "--lr": 1e-4,
    "--min-lr": 1e-6,
    "--lr-warmup-fraction": 0.01,
    "--lr-decay-iters": 100000,

    # Regularization
    "--weight-decay": 0.1,
    "--clip-grad": 1.0,

    # FlashAttention
    "--use-flash-attn": True,

    # Activation checkpointing
    "--recompute-activations": True,
    "--recompute-granularity": "selective",

    # Data
    "--data-path": "data/tokenized_data",
    "--split": "98,1,1",  # train/val/test
    "--tokenizer-type": "HFTokenizer",
    "--tokenizer-model": "meta-llama/Llama-2-7b",

    # Checkpointing
    "--save": "./checkpoints",
    "--load": "./checkpoints",
    "--save-interval": 1000,

    # Logging
    "--log-interval": 10,
    "--eval-interval": 500,
    "--tensorboard-dir": "./logs/tensorboard"
}

# Convert to command line
def get_megatron_command():
    cmd = "python pretrain_gpt.py"
    for key, value in MEGATRON_ARGS.items():
        if isinstance(value, bool):
            if value:
                cmd += f" {key}"
        else:
            cmd += f" {key} {value}"
    return cmd

# Megatron-DeepSpeed integration
MEGATRON_DEEPSPEED_CONFIG = {
    "train_batch_size": 512,
    "train_micro_batch_size_per_gpu": 1,

    "zero_optimization": {
        "stage": 1,  # ZeRO-1 with Megatron's TP/PP
    },

    "bf16": {
        "enabled": True
    },

    "pipeline": {
        "pipe_partitioned": True,
        "grad_partitioned": True
    }
}
```

### 2.4 Framework Comparison

| Feature | FSDP | DeepSpeed | Megatron-LM |
|---------|------|-----------|-------------|
| **Sharding** | ZeRO-3 equivalent | ZeRO 1/2/3 | Via DeepSpeed |
| **Tensor Parallel** | Manual | Via Megatron | Native |
| **Pipeline Parallel** | No | Yes | Yes |
| **Ease of Use** | High | Medium | Low |
| **Integration** | PyTorch native | Plugin | Standalone |
| **Performance** | Good | Excellent | Best (NVIDIA) |
| **Best For** | 1-70B models | 7-200B models | 100B+ models |

---

## 3. Hardware Configuration

### 3.1 GPU Selection

```
┌─────────────────────────────────────────────────────────────┐
│                  GPU Comparison (2024)                       │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  NVIDIA A100                                                │
│  ├── Memory: 40GB / 80GB HBM2e                              │
│  ├── FP16 TFLOPs: 312                                        │
│  ├── BF16 TFLOPs: 312                                        │
│  ├── Memory BW: 2 TB/s                                       │
│  ├── NVLink: 600 GB/s (12 links)                            │
│  └── Best for: Production training, good price/perf         │
│                                                              │
│  NVIDIA H100                                                │
│  ├── Memory: 80GB HBM3                                       │
│  ├── FP16 TFLOPs: 990                                        │
│  ├── BF16 TFLOPs: 990                                        │
│  ├── FP8 TFLOPs: 1979                                        │
│  ├── Memory BW: 3.35 TB/s                                    │
│  ├── NVLink: 900 GB/s (18 links)                            │
│  └── Best for: Cutting-edge training, fastest available     │
│                                                              │
│  NVIDIA B200 (2024+)                                        │
│  ├── Memory: 192GB HBM3e                                     │
│  ├── FP8 TFLOPs: 4500                                        │
│  ├── Memory BW: 8 TB/s                                       │
│  └── Best for: Future very large models                     │
│                                                              │
│  Selection Guidelines:                                       │
│  ├── Budget-constrained: A100 40GB                          │
│  ├── Standard training: A100 80GB                           │
│  ├── Maximum performance: H100 80GB                          │
│  └── Large models (70B+): H100 required for reasonable time │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### 3.2 Node Topology

```python
"""
Optimal node configuration for different model sizes
"""

NODE_CONFIGURATIONS = {
    "8x_a100_40gb": {
        "gpus": 8,
        "memory_per_gpu": 40,
        "total_memory": 320,
        "nvlink": True,
        "nvlink_bandwidth": "600 GB/s",
        "max_model_params": "~30B (with FSDP)",
        "typical_use": "7B-13B models"
    },

    "8x_a100_80gb": {
        "gpus": 8,
        "memory_per_gpu": 80,
        "total_memory": 640,
        "nvlink": True,
        "nvlink_bandwidth": "600 GB/s",
        "max_model_params": "~70B (with FSDP)",
        "typical_use": "7B-70B models"
    },

    "8x_h100_80gb": {
        "gpus": 8,
        "memory_per_gpu": 80,
        "total_memory": 640,
        "nvlink": True,
        "nvlink_bandwidth": "900 GB/s",
        "max_model_params": "~100B (with optimizations)",
        "typical_use": "7B-100B models, fastest training"
    },

    "dgx_superpod": {
        "nodes": 32,
        "gpus_per_node": 8,
        "total_gpus": 256,
        "interconnect": "InfiniBand HDR (200 Gb/s)",
        "max_model_params": "~1T with 3D parallelism",
        "typical_use": "100B-500B models"
    }
}

def calculate_memory_requirements(
    params_billions: float,
    batch_size: int = 1,
    seq_length: int = 2048,
    mixed_precision: bool = True
) -> dict:
    """Calculate GPU memory requirements for training"""

    params = params_billions * 1e9

    # Parameter memory
    param_bytes = 2 if mixed_precision else 4
    param_memory = params * param_bytes

    # Optimizer states (Adam: momentum + variance)
    # Always FP32 for stability
    optimizer_memory = params * 4 * 2  # 2 states

    # Gradients
    gradient_memory = params * (2 if mixed_precision else 4)

    # Activations (rough estimate)
    # ~12 bytes per parameter per token in sequence
    activation_memory = 12 * params * seq_length / 1000  # Per 1000 params

    # Total per GPU (no sharding)
    total_no_shard = param_memory + optimizer_memory + gradient_memory + activation_memory

    return {
        "params_gb": param_memory / 1e9,
        "optimizer_gb": optimizer_memory / 1e9,
        "gradients_gb": gradient_memory / 1e9,
        "activations_gb": activation_memory / 1e9,
        "total_no_shard_gb": total_no_shard / 1e9,
        "min_gpus_fsdp": int(total_no_shard / (80 * 1e9)) + 1  # 80GB GPUs
    }

# Example calculations
for size in [7, 13, 70, 405]:
    reqs = calculate_memory_requirements(size)
    print(f"\n{size}B model:")
    print(f"  Total (no shard): {reqs['total_no_shard_gb']:.1f} GB")
    print(f"  Min GPUs (FSDP): {reqs['min_gpus_fsdp']}")
```

### 3.3 Network Configuration

```bash
#!/bin/bash
# Network optimization for distributed training

# Check InfiniBand status
ibstat
ibdev2netdev

# NCCL environment variables
export NCCL_DEBUG=INFO
export NCCL_IB_DISABLE=0  # Enable InfiniBand
export NCCL_IB_GID_INDEX=3
export NCCL_IB_TIMEOUT=23
export NCCL_IB_RETRY_CNT=7

# For multi-node training
export MASTER_ADDR=node-0
export MASTER_PORT=29500
export WORLD_SIZE=16  # Total GPUs across all nodes
export RANK=0  # Rank of current process

# RDMA settings
export NCCL_NET_GDR_LEVEL=5  # GPUDirect RDMA
export NCCL_NET_GDR_READ=1

# Socket settings (fallback if IB unavailable)
export NCCL_SOCKET_IFNAME=eth0
export NCCL_SOCKET_NTHREADS=4

# Check NCCL performance
# Run all-reduce benchmark
# nccl-tests/build/all_reduce_perf -b 8 -e 128M -f 2 -g 8
```

---

## 4. Tensor Parallelism

### 4.1 Column and Row Parallel Linear

```python
import torch
import torch.nn as nn
import torch.distributed as dist
from typing import Optional

class ColumnParallelLinear(nn.Module):
    """
    Linear layer with column parallelism.

    Splits output features across GPUs:
    Y = X @ W where W is [in, out/tp_size] on each GPU

    Requires all-gather on forward for full output.
    """

    def __init__(
        self,
        in_features: int,
        out_features: int,
        bias: bool = True,
        gather_output: bool = True,
        init_method = None,
        tp_group = None
    ):
        super().__init__()

        self.in_features = in_features
        self.out_features = out_features
        self.gather_output = gather_output
        self.tp_group = tp_group

        # Get tensor parallel size
        self.tp_size = dist.get_world_size(tp_group) if tp_group else 1
        self.tp_rank = dist.get_rank(tp_group) if tp_group else 0

        # Partition output features
        assert out_features % self.tp_size == 0
        self.out_features_per_partition = out_features // self.tp_size

        # Create weight and bias
        self.weight = nn.Parameter(torch.empty(
            self.out_features_per_partition,
            in_features
        ))

        if bias:
            self.bias = nn.Parameter(torch.empty(self.out_features_per_partition))
        else:
            self.register_parameter('bias', None)

        # Initialize
        self._init_weights(init_method)

    def _init_weights(self, init_method):
        if init_method is not None:
            init_method(self.weight)
        else:
            nn.init.kaiming_uniform_(self.weight)

        if self.bias is not None:
            nn.init.zeros_(self.bias)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Local matmul: [B, S, in] @ [in, out/tp] = [B, S, out/tp]
        output = nn.functional.linear(x, self.weight, self.bias)

        if self.gather_output:
            # All-gather to get full output
            output = _gather_along_last_dim(output, self.tp_group)

        return output

class RowParallelLinear(nn.Module):
    """
    Linear layer with row parallelism.

    Splits input features across GPUs:
    Y = X @ W where X is [batch, seq, in/tp_size] on each GPU

    Requires reduce-scatter on forward to combine outputs.
    """

    def __init__(
        self,
        in_features: int,
        out_features: int,
        bias: bool = True,
        input_is_parallel: bool = False,
        init_method = None,
        tp_group = None
    ):
        super().__init__()

        self.in_features = in_features
        self.out_features = out_features
        self.input_is_parallel = input_is_parallel
        self.tp_group = tp_group

        # Get tensor parallel size
        self.tp_size = dist.get_world_size(tp_group) if tp_group else 1
        self.tp_rank = dist.get_rank(tp_group) if tp_group else 0

        # Partition input features
        assert in_features % self.tp_size == 0
        self.in_features_per_partition = in_features // self.tp_size

        # Create weight
        self.weight = nn.Parameter(torch.empty(
            out_features,
            self.in_features_per_partition
        ))

        if bias:
            self.bias = nn.Parameter(torch.empty(out_features))
        else:
            self.register_parameter('bias', None)

        self._init_weights(init_method)

    def _init_weights(self, init_method):
        if init_method is not None:
            init_method(self.weight)
        else:
            nn.init.kaiming_uniform_(self.weight)

        if self.bias is not None:
            nn.init.zeros_(self.bias)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if not self.input_is_parallel:
            # Split input along last dimension
            x = _split_along_last_dim(x, self.tp_group)

        # Local matmul
        output = nn.functional.linear(x, self.weight)

        # All-reduce to combine partial sums
        if self.tp_size > 1:
            dist.all_reduce(output, group=self.tp_group)

        # Add bias after reduce
        if self.bias is not None:
            output = output + self.bias

        return output

def _gather_along_last_dim(x: torch.Tensor, group) -> torch.Tensor:
    """All-gather tensors along the last dimension"""
    world_size = dist.get_world_size(group)
    if world_size == 1:
        return x

    # Gather
    tensor_list = [torch.empty_like(x) for _ in range(world_size)]
    dist.all_gather(tensor_list, x, group=group)

    # Concatenate along last dimension
    return torch.cat(tensor_list, dim=-1)

def _split_along_last_dim(x: torch.Tensor, group) -> torch.Tensor:
    """Split tensor along the last dimension"""
    world_size = dist.get_world_size(group)
    rank = dist.get_rank(group)

    if world_size == 1:
        return x

    # Split
    chunks = torch.chunk(x, world_size, dim=-1)
    return chunks[rank].contiguous()
```

### 4.2 Tensor Parallel Attention

```python
class TensorParallelAttention(nn.Module):
    """
    Self-attention with tensor parallelism.

    Q, K, V projections use column parallelism.
    Output projection uses row parallelism.
    """

    def __init__(
        self,
        hidden_size: int,
        num_heads: int,
        num_kv_heads: int,
        tp_group = None
    ):
        super().__init__()

        self.tp_size = dist.get_world_size(tp_group) if tp_group else 1
        self.tp_rank = dist.get_rank(tp_group) if tp_group else 0

        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.num_kv_heads = num_kv_heads
        self.head_dim = hidden_size // num_heads

        # Partition heads across TP ranks
        assert num_heads % self.tp_size == 0
        assert num_kv_heads % self.tp_size == 0

        self.num_heads_per_partition = num_heads // self.tp_size
        self.num_kv_heads_per_partition = num_kv_heads // self.tp_size

        # Q projection (column parallel)
        self.q_proj = ColumnParallelLinear(
            hidden_size,
            num_heads * self.head_dim,
            bias=False,
            gather_output=False,  # Keep partitioned
            tp_group=tp_group
        )

        # K, V projections (column parallel)
        self.k_proj = ColumnParallelLinear(
            hidden_size,
            num_kv_heads * self.head_dim,
            bias=False,
            gather_output=False,
            tp_group=tp_group
        )
        self.v_proj = ColumnParallelLinear(
            hidden_size,
            num_kv_heads * self.head_dim,
            bias=False,
            gather_output=False,
            tp_group=tp_group
        )

        # Output projection (row parallel)
        self.o_proj = RowParallelLinear(
            num_heads * self.head_dim,
            hidden_size,
            bias=False,
            input_is_parallel=True,  # Input already partitioned
            tp_group=tp_group
        )

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.Tensor] = None
    ) -> torch.Tensor:

        batch_size, seq_len, _ = hidden_states.shape

        # Project Q, K, V (column parallel)
        query = self.q_proj(hidden_states)  # [B, S, heads_per_partition * head_dim]
        key = self.k_proj(hidden_states)
        value = self.v_proj(hidden_states)

        # Reshape for attention
        query = query.view(batch_size, seq_len, self.num_heads_per_partition, self.head_dim)
        key = key.view(batch_size, seq_len, self.num_kv_heads_per_partition, self.head_dim)
        value = value.view(batch_size, seq_len, self.num_kv_heads_per_partition, self.head_dim)

        # Apply RoPE, attention, etc. (local computation)
        # ... (same as non-TP attention)

        # Reshape output
        attn_output = attn_output.reshape(batch_size, seq_len, -1)

        # Output projection (row parallel, includes all-reduce)
        output = self.o_proj(attn_output)

        return output
```

---

## 5. Pipeline Parallelism

### 5.1 Pipeline Schedules

```
┌─────────────────────────────────────────────────────────────┐
│              Pipeline Parallelism Schedules                  │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  GPipe Schedule (Simple, but high bubble)                   │
│  ┌─────────────────────────────────────────────────────┐    │
│  │ GPU 0: [F1][F2][F3][F4]    [B4][B3][B2][B1]         │    │
│  │ GPU 1:    [F1][F2][F3][F4]    [B4][B3][B2][B1]      │    │
│  │ GPU 2:       [F1][F2][F3][F4]    [B4][B3][B2][B1]   │    │
│  │ GPU 3:          [F1][F2][F3][F4]    [B4][B3][B2][B1]│    │
│  │                                                      │    │
│  │ Bubble ratio: (P-1)/M where P=stages, M=microbatches│    │
│  └─────────────────────────────────────────────────────┘    │
│                                                              │
│  1F1B Schedule (Interleaved, lower bubble)                  │
│  ┌─────────────────────────────────────────────────────┐    │
│  │ GPU 0: [F1][F2][F3][F4][B1][F5][B2][F6][B3]...      │    │
│  │ GPU 1:    [F1][F2][F3][B1][F4][B2][F5][B3]...       │    │
│  │ GPU 2:       [F1][F2][B1][F3][B2][F4][B3]...        │    │
│  │ GPU 3:          [F1][B1][F2][B2][F3][B3]...         │    │
│  │                                                      │    │
│  │ Lower memory (1 activation per microbatch)          │    │
│  └─────────────────────────────────────────────────────┘    │
│                                                              │
│  Interleaved 1F1B (Megatron-LM, lowest bubble)              │
│  ┌─────────────────────────────────────────────────────┐    │
│  │ Each GPU has multiple non-contiguous stages         │    │
│  │ GPU 0: Stages 0, 4, 8, 12                           │    │
│  │ GPU 1: Stages 1, 5, 9, 13                           │    │
│  │ GPU 2: Stages 2, 6, 10, 14                          │    │
│  │ GPU 3: Stages 3, 7, 11, 15                          │    │
│  │                                                      │    │
│  │ Bubble ratio: (P-1)/(M*V) where V=virtual stages    │    │
│  └─────────────────────────────────────────────────────┘    │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### 5.2 DeepSpeed Pipeline Implementation

```python
import deepspeed
from deepspeed.pipe import PipelineModule, LayerSpec

class GPTBlock(nn.Module):
    """Single transformer block for pipeline"""

    def __init__(self, hidden_size, num_heads, ffn_size):
        super().__init__()
        self.attention = SelfAttention(hidden_size, num_heads)
        self.ffn = FeedForward(hidden_size, ffn_size)
        self.norm1 = nn.LayerNorm(hidden_size)
        self.norm2 = nn.LayerNorm(hidden_size)

    def forward(self, hidden_states):
        # Unpack if tuple (from previous stage)
        if isinstance(hidden_states, tuple):
            hidden_states = hidden_states[0]

        # Pre-norm attention
        residual = hidden_states
        hidden_states = self.norm1(hidden_states)
        hidden_states = self.attention(hidden_states)
        hidden_states = residual + hidden_states

        # Pre-norm FFN
        residual = hidden_states
        hidden_states = self.norm2(hidden_states)
        hidden_states = self.ffn(hidden_states)
        hidden_states = residual + hidden_states

        return hidden_states

def create_pipeline_model(
    num_layers: int,
    hidden_size: int,
    num_heads: int,
    ffn_size: int,
    vocab_size: int,
    num_stages: int
):
    """Create a pipeline-parallel model"""

    # Define layer specs
    layers = [
        # Embedding layer
        LayerSpec(
            nn.Embedding,
            vocab_size,
            hidden_size
        ),
    ]

    # Transformer blocks
    for _ in range(num_layers):
        layers.append(
            LayerSpec(
                GPTBlock,
                hidden_size,
                num_heads,
                ffn_size
            )
        )

    # Final norm and LM head
    layers.extend([
        LayerSpec(nn.LayerNorm, hidden_size),
        LayerSpec(nn.Linear, hidden_size, vocab_size),
    ])

    # Create pipeline module
    model = PipelineModule(
        layers=layers,
        num_stages=num_stages,
        partition_method='uniform',  # or 'parameters' for balanced
        loss_fn=nn.CrossEntropyLoss(),
        activation_checkpoint_interval=1,  # Checkpoint every layer
    )

    return model

# DeepSpeed pipeline config
PIPELINE_CONFIG = {
    "train_batch_size": 256,
    "train_micro_batch_size_per_gpu": 1,

    "pipeline": {
        "pipe_partitioned": True,
        "grad_partitioned": True,
    },

    "zero_optimization": {
        "stage": 1,  # ZeRO-1 with pipeline
    },

    "bf16": {"enabled": True},

    "steps_per_print": 100,
}
```

---

## 6. Optimization Techniques

### 6.1 Gradient Accumulation

```python
def train_with_gradient_accumulation(
    model,
    dataloader,
    optimizer,
    accumulation_steps: int = 8
):
    """
    Gradient accumulation to simulate larger batch sizes.

    Effective batch = micro_batch * accumulation_steps * world_size
    """

    model.train()

    for step, batch in enumerate(dataloader):
        batch = {k: v.cuda() for k, v in batch.items()}

        # Forward pass
        outputs = model(**batch)
        loss = outputs.loss

        # Scale loss by accumulation steps
        loss = loss / accumulation_steps

        # Backward pass (accumulates gradients)
        loss.backward()

        # Only step after accumulation_steps
        if (step + 1) % accumulation_steps == 0:
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

            # Optimizer step
            optimizer.step()
            optimizer.zero_grad()

            print(f"Step {step + 1}, Loss: {loss.item() * accumulation_steps:.4f}")
```

### 6.2 Mixed Precision Training

```python
from torch.cuda.amp import autocast, GradScaler

def train_mixed_precision(model, dataloader, optimizer):
    """Training with automatic mixed precision (AMP)"""

    scaler = GradScaler()

    for batch in dataloader:
        batch = {k: v.cuda() for k, v in batch.items()}

        optimizer.zero_grad()

        # Forward pass in mixed precision
        with autocast(dtype=torch.bfloat16):
            outputs = model(**batch)
            loss = outputs.loss

        # Backward pass with scaling
        scaler.scale(loss).backward()

        # Unscale and clip gradients
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        # Optimizer step with scaler
        scaler.step(optimizer)
        scaler.update()

# BFloat16 is preferred for LLM training
# - No loss scaling needed
# - Same dynamic range as FP32
# - Good numerical stability

# FP16 tips:
# - Use loss scaling (GradScaler)
# - Watch for NaN/Inf in gradients
# - Some ops need FP32 (softmax, layer norm)
```

### 6.3 Activation Checkpointing

```python
from torch.utils.checkpoint import checkpoint, checkpoint_sequential

class CheckpointedTransformerBlock(nn.Module):
    """Transformer block with activation checkpointing"""

    def __init__(self, hidden_size, num_heads, ffn_size, use_checkpoint=True):
        super().__init__()
        self.use_checkpoint = use_checkpoint
        self.attention = SelfAttention(hidden_size, num_heads)
        self.ffn = FeedForward(hidden_size, ffn_size)
        self.norm1 = nn.LayerNorm(hidden_size)
        self.norm2 = nn.LayerNorm(hidden_size)

    def _attention_block(self, x):
        residual = x
        x = self.norm1(x)
        x = self.attention(x)
        return residual + x

    def _ffn_block(self, x):
        residual = x
        x = self.norm2(x)
        x = self.ffn(x)
        return residual + x

    def forward(self, hidden_states):
        if self.use_checkpoint and self.training:
            # Checkpoint attention block
            hidden_states = checkpoint(
                self._attention_block,
                hidden_states,
                use_reentrant=False
            )
            # Checkpoint FFN block
            hidden_states = checkpoint(
                self._ffn_block,
                hidden_states,
                use_reentrant=False
            )
        else:
            hidden_states = self._attention_block(hidden_states)
            hidden_states = self._ffn_block(hidden_states)

        return hidden_states

# Memory savings from checkpointing:
# Without: O(num_layers * hidden_size * seq_length * batch)
# With: O(num_layers * hidden_size + seq_length * batch)
# Trade-off: ~33% more compute for recomputation
```

### 6.4 FlashAttention

```python
def flash_attention_forward(
    query: torch.Tensor,  # [B, S, H, D]
    key: torch.Tensor,
    value: torch.Tensor,
    causal: bool = True,
    dropout_p: float = 0.0
) -> torch.Tensor:
    """
    FlashAttention-2 forward pass.

    Memory: O(N) instead of O(N²)
    Speed: 2-4x faster than standard attention
    """
    try:
        from flash_attn import flash_attn_func

        output = flash_attn_func(
            query,
            key,
            value,
            dropout_p=dropout_p,
            causal=causal,
        )
        return output

    except ImportError:
        # Fallback to PyTorch SDPA
        import torch.nn.functional as F

        query = query.transpose(1, 2)  # [B, H, S, D]
        key = key.transpose(1, 2)
        value = value.transpose(1, 2)

        output = F.scaled_dot_product_attention(
            query, key, value,
            dropout_p=dropout_p,
            is_causal=causal
        )

        return output.transpose(1, 2)

# Installation
"""
pip install flash-attn --no-build-isolation

# Or for specific CUDA version:
pip install flash-attn==2.5.0 --no-build-isolation
"""
```

### 6.5 Efficient Data Loading

```python
from torch.utils.data import DataLoader, IterableDataset
import webdataset as wds

class StreamingDataset(IterableDataset):
    """Efficient streaming dataset for large-scale training"""

    def __init__(
        self,
        data_paths: list,
        tokenizer,
        seq_length: int = 2048,
        shuffle_buffer: int = 10000
    ):
        self.data_paths = data_paths
        self.tokenizer = tokenizer
        self.seq_length = seq_length
        self.shuffle_buffer = shuffle_buffer

    def __iter__(self):
        # WebDataset for efficient streaming
        dataset = (
            wds.WebDataset(self.data_paths)
            .shuffle(self.shuffle_buffer)
            .decode()
            .map(self._process_sample)
        )

        for sample in dataset:
            yield sample

    def _process_sample(self, sample):
        text = sample.get('txt', '')
        tokens = self.tokenizer.encode(text)

        # Truncate or pad to seq_length
        if len(tokens) > self.seq_length:
            tokens = tokens[:self.seq_length]
        else:
            tokens = tokens + [self.tokenizer.pad_token_id] * (self.seq_length - len(tokens))

        return {
            'input_ids': torch.tensor(tokens[:-1]),
            'labels': torch.tensor(tokens[1:])
        }

def create_distributed_dataloader(
    dataset,
    batch_size: int,
    world_size: int,
    rank: int,
    num_workers: int = 4
):
    """Create dataloader for distributed training"""

    from torch.utils.data.distributed import DistributedSampler

    sampler = DistributedSampler(
        dataset,
        num_replicas=world_size,
        rank=rank,
        shuffle=True
    )

    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        sampler=sampler,
        num_workers=num_workers,
        pin_memory=True,
        prefetch_factor=2
    )

    return dataloader
```

---

## 7. Fault Tolerance & Checkpointing

### 7.1 Distributed Checkpointing

```python
import torch.distributed.checkpoint as dcp
from torch.distributed.checkpoint.state_dict import (
    get_model_state_dict,
    get_optimizer_state_dict,
    set_model_state_dict,
    set_optimizer_state_dict,
    StateDictOptions
)

class DistributedCheckpointer:
    """Efficient distributed checkpointing for large models"""

    def __init__(
        self,
        checkpoint_dir: str,
        save_interval: int = 1000,
        keep_last_n: int = 3
    ):
        self.checkpoint_dir = checkpoint_dir
        self.save_interval = save_interval
        self.keep_last_n = keep_last_n
        self.saved_checkpoints = []

    def save(
        self,
        model,
        optimizer,
        step: int,
        loss: float = None,
        extra_state: dict = None
    ):
        """Save distributed checkpoint"""

        checkpoint_path = f"{self.checkpoint_dir}/step_{step}"

        # Get sharded state dicts
        model_state = get_model_state_dict(
            model,
            options=StateDictOptions(full_state_dict=False)
        )

        optim_state = get_optimizer_state_dict(
            model,
            optimizer,
            options=StateDictOptions(full_state_dict=False)
        )

        state_dict = {
            "model": model_state,
            "optimizer": optim_state,
            "step": step,
            "loss": loss,
        }

        if extra_state:
            state_dict.update(extra_state)

        # Save with distributed checkpoint
        dcp.save(
            state_dict=state_dict,
            storage_writer=dcp.FileSystemWriter(checkpoint_path)
        )

        # Track and clean up old checkpoints
        self.saved_checkpoints.append(checkpoint_path)
        self._cleanup_old_checkpoints()

        if dist.get_rank() == 0:
            print(f"Saved checkpoint at step {step}")

    def load(
        self,
        model,
        optimizer,
        checkpoint_path: str = None
    ) -> dict:
        """Load distributed checkpoint"""

        if checkpoint_path is None:
            # Find latest checkpoint
            checkpoint_path = self._find_latest_checkpoint()

        if checkpoint_path is None:
            return {"step": 0}

        # Load state dict
        state_dict = {
            "model": get_model_state_dict(model),
            "optimizer": get_optimizer_state_dict(model, optimizer),
        }

        dcp.load(
            state_dict=state_dict,
            storage_reader=dcp.FileSystemReader(checkpoint_path)
        )

        # Apply to model and optimizer
        set_model_state_dict(model, state_dict["model"])
        set_optimizer_state_dict(model, optimizer, state_dict["optimizer"])

        return state_dict

    def _cleanup_old_checkpoints(self):
        """Remove old checkpoints, keep last N"""
        while len(self.saved_checkpoints) > self.keep_last_n:
            old_checkpoint = self.saved_checkpoints.pop(0)
            if dist.get_rank() == 0:
                import shutil
                shutil.rmtree(old_checkpoint, ignore_errors=True)

    def _find_latest_checkpoint(self) -> str:
        """Find the latest checkpoint in directory"""
        import os
        import re

        if not os.path.exists(self.checkpoint_dir):
            return None

        checkpoints = []
        for name in os.listdir(self.checkpoint_dir):
            match = re.match(r'step_(\d+)', name)
            if match:
                step = int(match.group(1))
                checkpoints.append((step, os.path.join(self.checkpoint_dir, name)))

        if not checkpoints:
            return None

        return max(checkpoints, key=lambda x: x[0])[1]

# Usage
checkpointer = DistributedCheckpointer(
    checkpoint_dir="./checkpoints",
    save_interval=1000,
    keep_last_n=3
)

# In training loop
for step, batch in enumerate(dataloader):
    # ... training step ...

    if step % checkpointer.save_interval == 0:
        checkpointer.save(model, optimizer, step, loss=loss.item())
```

### 7.2 Elastic Training

```python
import torch.distributed.elastic as elastic
from torch.distributed.elastic.multiprocessing.errors import record

@record
def main(args):
    """
    Elastic training with automatic recovery from failures.

    Launch with:
    torchrun --nnodes=1:4 --nproc_per_node=8 \
             --rdzv_backend=c10d --rdzv_endpoint=$MASTER_ADDR:$MASTER_PORT \
             train.py
    """

    # Initialize distributed
    dist.init_process_group(backend="nccl")

    rank = dist.get_rank()
    world_size = dist.get_world_size()

    # Setup model with FSDP
    model = create_model()
    model = FSDP(model)

    # Create checkpointer
    checkpointer = DistributedCheckpointer("./checkpoints")

    # Load latest checkpoint if exists
    state = checkpointer.load(model, optimizer)
    start_step = state.get("step", 0)

    # Training loop with error handling
    try:
        for step in range(start_step, max_steps):
            batch = next(dataloader)
            loss = train_step(model, batch, optimizer)

            # Save checkpoint periodically
            if step % 1000 == 0:
                checkpointer.save(model, optimizer, step, loss=loss.item())

    except Exception as e:
        # Save emergency checkpoint on failure
        checkpointer.save(model, optimizer, step, extra_state={"error": str(e)})
        raise

if __name__ == "__main__":
    main(parse_args())

# Launch command for elastic training
"""
# Single node, 8 GPUs, can scale from 1 to 4 nodes
torchrun \
    --nnodes=1:4 \
    --nproc_per_node=8 \
    --rdzv_id=job123 \
    --rdzv_backend=c10d \
    --rdzv_endpoint=master-node:29500 \
    train.py --config config.yaml
"""
```

---

## 8. Performance Tuning

### 8.1 Throughput Benchmarking

```python
import time
from dataclasses import dataclass
from typing import Optional

@dataclass
class ThroughputMetrics:
    tokens_per_second: float
    samples_per_second: float
    mfu: float  # Model FLOPs Utilization
    gpu_memory_used_gb: float
    gpu_utilization_percent: float

def benchmark_throughput(
    model,
    dataloader,
    num_steps: int = 100,
    warmup_steps: int = 10,
    seq_length: int = 2048,
    batch_size: int = 4,
    params_billions: float = 7.0
) -> ThroughputMetrics:
    """Benchmark training throughput"""

    model.train()

    # Warmup
    for i, batch in enumerate(dataloader):
        if i >= warmup_steps:
            break
        batch = {k: v.cuda() for k, v in batch.items()}
        outputs = model(**batch)
        outputs.loss.backward()
        model.zero_grad()

    torch.cuda.synchronize()

    # Benchmark
    start_time = time.perf_counter()
    total_tokens = 0

    for i, batch in enumerate(dataloader):
        if i >= num_steps:
            break

        batch = {k: v.cuda() for k, v in batch.items()}
        outputs = model(**batch)
        outputs.loss.backward()
        model.zero_grad()

        total_tokens += batch_size * seq_length

    torch.cuda.synchronize()
    elapsed = time.perf_counter() - start_time

    # Calculate metrics
    tokens_per_second = total_tokens / elapsed
    samples_per_second = (num_steps * batch_size) / elapsed

    # MFU calculation
    # FLOPs per forward pass ≈ 2 * params * tokens
    # FLOPs per training step ≈ 6 * params * tokens (forward + backward)
    flops_per_step = 6 * params_billions * 1e9 * seq_length * batch_size
    total_flops = flops_per_step * num_steps
    achieved_flops = total_flops / elapsed

    # Theoretical peak (H100: ~990 TFLOPs BF16)
    peak_flops = 990e12  # Adjust for your GPU
    mfu = achieved_flops / peak_flops

    # Memory and utilization
    gpu_memory = torch.cuda.max_memory_allocated() / 1e9

    return ThroughputMetrics(
        tokens_per_second=tokens_per_second,
        samples_per_second=samples_per_second,
        mfu=mfu,
        gpu_memory_used_gb=gpu_memory,
        gpu_utilization_percent=mfu * 100
    )

# Target MFU values:
# 30-40%: Acceptable
# 40-50%: Good
# 50-60%: Excellent
# >60%: World-class (rare)
```

### 8.2 Profiling

```python
import torch.profiler as profiler

def profile_training(
    model,
    dataloader,
    num_steps: int = 20,
    output_dir: str = "./profiler_output"
):
    """Profile training with PyTorch Profiler"""

    with profiler.profile(
        activities=[
            profiler.ProfilerActivity.CPU,
            profiler.ProfilerActivity.CUDA,
        ],
        schedule=profiler.schedule(
            wait=2,    # Skip first 2 steps
            warmup=3,  # Warmup for 3 steps
            active=10, # Profile 10 steps
            repeat=1
        ),
        on_trace_ready=profiler.tensorboard_trace_handler(output_dir),
        record_shapes=True,
        profile_memory=True,
        with_stack=True
    ) as prof:

        for step, batch in enumerate(dataloader):
            if step >= num_steps:
                break

            batch = {k: v.cuda() for k, v in batch.items()}

            outputs = model(**batch)
            outputs.loss.backward()
            model.zero_grad()

            prof.step()

    print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=20))

    # View with:
    # tensorboard --logdir=./profiler_output
```

### 8.3 Memory Optimization Checklist

```python
def optimize_memory_usage():
    """Memory optimization techniques checklist"""

    optimizations = {
        "Gradient Checkpointing": {
            "savings": "50-70% activation memory",
            "cost": "30-40% more compute",
            "code": "model.gradient_checkpointing_enable()"
        },

        "Mixed Precision (BF16)": {
            "savings": "50% parameter + gradient memory",
            "cost": "Minimal accuracy impact",
            "code": "torch.autocast(device_type='cuda', dtype=torch.bfloat16)"
        },

        "FSDP/ZeRO-3": {
            "savings": "Linear with GPU count",
            "cost": "Communication overhead",
            "code": "FSDP(model, sharding_strategy=ShardingStrategy.FULL_SHARD)"
        },

        "CPU Offload": {
            "savings": "Optimizer states to CPU",
            "cost": "CPU-GPU transfer overhead",
            "code": "CPUOffload(offload_params=True)"
        },

        "Reduce Batch Size + Accumulation": {
            "savings": "Linear with batch size reduction",
            "cost": "More iterations",
            "code": "accumulation_steps = global_batch // micro_batch"
        },

        "FlashAttention": {
            "savings": "O(N²) → O(N) attention memory",
            "cost": "None (faster too)",
            "code": "flash_attn_func(q, k, v, causal=True)"
        },

        "Efficient Attention (GQA)": {
            "savings": "KV cache reduction",
            "cost": "Slight quality impact",
            "code": "num_kv_heads < num_heads"
        }
    }

    return optimizations
```

---

## 9. Cluster Configuration

### 9.1 Kubernetes Setup

```yaml
# kubernetes/training-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: llm-training
spec:
  parallelism: 4  # Number of pods (nodes)
  completions: 4
  template:
    spec:
      containers:
      - name: training
        image: nvcr.io/nvidia/pytorch:24.01-py3
        command:
        - torchrun
        - --nnodes=4
        - --nproc_per_node=8
        - --rdzv_id=job-123
        - --rdzv_backend=c10d
        - --rdzv_endpoint=$(MASTER_ADDR):29500
        - train.py
        - --config=config.yaml
        resources:
          limits:
            nvidia.com/gpu: 8
            memory: 1000Gi
        volumeMounts:
        - name: data
          mountPath: /data
        - name: checkpoints
          mountPath: /checkpoints
        env:
        - name: MASTER_ADDR
          value: "llm-training-0.llm-training"
        - name: NCCL_DEBUG
          value: "INFO"
        - name: NCCL_IB_DISABLE
          value: "0"
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: training-data
      - name: checkpoints
        persistentVolumeClaim:
          claimName: checkpoints
      restartPolicy: OnFailure
---
apiVersion: v1
kind: Service
metadata:
  name: llm-training
spec:
  clusterIP: None
  selector:
    job-name: llm-training
  ports:
  - port: 29500
    name: rdzv
```

### 9.2 Slurm Configuration

```bash
#!/bin/bash
#SBATCH --job-name=llm-training
#SBATCH --nodes=4
#SBATCH --ntasks-per-node=8
#SBATCH --gpus-per-node=8
#SBATCH --cpus-per-task=12
#SBATCH --mem=1000G
#SBATCH --time=7-00:00:00
#SBATCH --partition=gpu
#SBATCH --output=logs/%j.out
#SBATCH --error=logs/%j.err

# Environment
module load cuda/12.1
source /path/to/venv/bin/activate

# NCCL settings
export NCCL_DEBUG=INFO
export NCCL_IB_DISABLE=0

# Master address
export MASTER_ADDR=$(scontrol show hostname $SLURM_NODELIST | head -n 1)
export MASTER_PORT=29500

# Launch training
srun --ntasks-per-node=1 bash -c '
    torchrun \
        --nnodes=$SLURM_NNODES \
        --nproc_per_node=8 \
        --rdzv_id=$SLURM_JOB_ID \
        --rdzv_backend=c10d \
        --rdzv_endpoint=$MASTER_ADDR:$MASTER_PORT \
        train.py --config config.yaml
'
```

---

## 10. Troubleshooting

### Common Issues

| Issue | Symptoms | Solution |
|-------|----------|----------|
| NCCL timeout | Hang during init | Check network, increase timeout |
| OOM | CUDA out of memory | Reduce batch size, enable checkpointing |
| Loss NaN | Training diverges | Reduce LR, check data, use FP32 for sensitive ops |
| Slow throughput | Low MFU | Profile, check communication bottleneck |
| Gradient explosion | Very large gradients | Add gradient clipping, reduce LR |

### Debug Commands

```bash
# Check GPU status
nvidia-smi
nvidia-smi topo -m  # Check NVLink topology

# Check InfiniBand
ibstat
ibdev2netdev

# NCCL debug
export NCCL_DEBUG=INFO
export NCCL_DEBUG_SUBSYS=ALL

# Check distributed setup
python -c "import torch.distributed as dist; dist.init_process_group('nccl'); print(f'Rank: {dist.get_rank()}, World: {dist.get_world_size()}')"

# Memory debugging
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True python train.py
```

---

## Glossary

| Term | Definition |
|------|------------|
| **DP** | Data Parallelism - replicate model, split data |
| **DDP** | Distributed Data Parallel - PyTorch's DP implementation |
| **FSDP** | Fully Sharded Data Parallel - sharded DP |
| **MFU** | Model FLOPs Utilization - efficiency metric |
| **NCCL** | NVIDIA Collective Communications Library |
| **NVLink** | High-bandwidth GPU interconnect |
| **PP** | Pipeline Parallelism - split layers across GPUs |
| **TP** | Tensor Parallelism - split layers within GPU |
| **ZeRO** | Zero Redundancy Optimizer - DeepSpeed sharding |

---

## References

- [PyTorch FSDP Tutorial](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html)
- [DeepSpeed Documentation](https://www.deepspeed.ai/tutorials/)
- [Megatron-LM GitHub](https://github.com/NVIDIA/Megatron-LM)
- [FlashAttention Paper](https://arxiv.org/abs/2205.14135)
- [ZeRO Paper](https://arxiv.org/abs/1910.02054)
- [Efficient Large-Scale Training](https://lilianweng.github.io/posts/2021-09-25-train-large/)
