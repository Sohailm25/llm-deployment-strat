> **Navigation** | [← 12.3 Documentation](12.3_developer_documentation_guide.md) | [13.1 Incident Response →](../13_operations_reliability/13.1_incident_response_guide.md)
>
> | | |
> |---|---|
> | **Prerequisites** | Product development lifecycle &#124; Analytics |
> | **Related** | [10.3 Model Quality](../10_monitoring_observability/10.3_model_quality_monitoring_guide.md) &#124; [5.4 Human Evaluation](../05_evaluation_testing/5.4_human_evaluation_protocol.md) |
> | **Next** | [13.1 Incident Response](../13_operations_reliability/13.1_incident_response_guide.md) |

# 12.4 User Feedback and Iteration Guide

## Document Information
- **Version**: 1.0
- **Last Updated**: 2024
- **Owner**: Product & Developer Experience Team
- **Classification**: Internal

## Purpose and Scope

This guide provides comprehensive frameworks for collecting, analyzing, and acting on user feedback for LLM platforms. Effective feedback loops accelerate product improvement, increase user satisfaction, and drive informed decision-making.

## Prerequisites

- Understanding of product development lifecycle
- Familiarity with analytics platforms
- Access to feedback collection systems
- Knowledge of ML/LLM evaluation concepts

---

## 1. Feedback Collection Infrastructure

### 1.1 Multi-Channel Feedback Collection

```python
"""
Multi-channel user feedback collection system.
"""

from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Dict, List, Optional, Any, Callable
import json
import hashlib
import asyncio
from collections import defaultdict


class FeedbackChannel(Enum):
    """Channels for collecting feedback."""
    IN_APP = "in_app"
    API = "api"
    EMAIL = "email"
    SUPPORT = "support"
    SOCIAL = "social"
    SURVEY = "survey"
    INTERVIEW = "interview"
    COMMUNITY = "community"
    ANALYTICS = "analytics"
    IMPLICIT = "implicit"


class FeedbackType(Enum):
    """Types of feedback."""
    BUG_REPORT = "bug_report"
    FEATURE_REQUEST = "feature_request"
    USABILITY = "usability"
    PERFORMANCE = "performance"
    DOCUMENTATION = "documentation"
    QUALITY = "quality"
    PRICING = "pricing"
    GENERAL = "general"
    NPS = "nps"
    CSAT = "csat"
    CES = "ces"


class FeedbackSentiment(Enum):
    """Sentiment classification."""
    VERY_POSITIVE = "very_positive"
    POSITIVE = "positive"
    NEUTRAL = "neutral"
    NEGATIVE = "negative"
    VERY_NEGATIVE = "very_negative"


@dataclass
class FeedbackItem:
    """A single feedback item."""
    id: str
    user_id: Optional[str]
    channel: FeedbackChannel
    feedback_type: FeedbackType
    content: str
    sentiment: Optional[FeedbackSentiment] = None
    rating: Optional[int] = None  # 1-5 or 0-10 depending on type
    tags: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)
    created_at: datetime = field(default_factory=datetime.utcnow)
    context: Dict[str, Any] = field(default_factory=dict)
    response: Optional[str] = None
    resolved: bool = False

    def to_dict(self) -> Dict:
        """Convert to dictionary."""
        return {
            "id": self.id,
            "user_id": self.user_id,
            "channel": self.channel.value,
            "feedback_type": self.feedback_type.value,
            "content": self.content,
            "sentiment": self.sentiment.value if self.sentiment else None,
            "rating": self.rating,
            "tags": self.tags,
            "metadata": self.metadata,
            "created_at": self.created_at.isoformat(),
            "context": self.context,
            "resolved": self.resolved
        }


class FeedbackCollector(ABC):
    """Abstract base for feedback collectors."""

    @abstractmethod
    async def collect(self) -> List[FeedbackItem]:
        """Collect feedback from this channel."""
        pass

    @abstractmethod
    def get_channel(self) -> FeedbackChannel:
        """Get the channel this collector handles."""
        pass


class InAppFeedbackCollector(FeedbackCollector):
    """
    Collect feedback from in-app widgets and forms.
    """

    def __init__(self, api_endpoint: str, api_key: str):
        self.api_endpoint = api_endpoint
        self.api_key = api_key
        self.feedback_queue: List[Dict] = []

    def get_channel(self) -> FeedbackChannel:
        return FeedbackChannel.IN_APP

    async def collect(self) -> List[FeedbackItem]:
        """Collect queued in-app feedback."""
        items = []

        for raw_feedback in self.feedback_queue:
            item = self._parse_feedback(raw_feedback)
            if item:
                items.append(item)

        self.feedback_queue.clear()
        return items

    def submit_feedback(
        self,
        content: str,
        feedback_type: str,
        user_id: Optional[str] = None,
        rating: Optional[int] = None,
        context: Optional[Dict] = None
    ) -> str:
        """Submit in-app feedback."""
        feedback_id = self._generate_id(content, user_id)

        self.feedback_queue.append({
            "id": feedback_id,
            "content": content,
            "type": feedback_type,
            "user_id": user_id,
            "rating": rating,
            "context": context or {},
            "timestamp": datetime.utcnow().isoformat()
        })

        return feedback_id

    def _parse_feedback(self, raw: Dict) -> Optional[FeedbackItem]:
        """Parse raw feedback into FeedbackItem."""
        try:
            feedback_type = FeedbackType(raw.get("type", "general"))
        except ValueError:
            feedback_type = FeedbackType.GENERAL

        return FeedbackItem(
            id=raw["id"],
            user_id=raw.get("user_id"),
            channel=FeedbackChannel.IN_APP,
            feedback_type=feedback_type,
            content=raw["content"],
            rating=raw.get("rating"),
            context=raw.get("context", {}),
            created_at=datetime.fromisoformat(raw["timestamp"])
        )

    def _generate_id(self, content: str, user_id: Optional[str]) -> str:
        """Generate unique feedback ID."""
        data = f"{content}{user_id}{datetime.utcnow().isoformat()}"
        return hashlib.md5(data.encode()).hexdigest()[:12]


class APIFeedbackCollector(FeedbackCollector):
    """
    Collect feedback from API response ratings.
    """

    def __init__(self):
        self.feedback_buffer: List[Dict] = []

    def get_channel(self) -> FeedbackChannel:
        return FeedbackChannel.API

    async def collect(self) -> List[FeedbackItem]:
        """Collect buffered API feedback."""
        items = []

        for raw in self.feedback_buffer:
            items.append(FeedbackItem(
                id=raw["id"],
                user_id=raw.get("user_id"),
                channel=FeedbackChannel.API,
                feedback_type=FeedbackType.QUALITY,
                content=raw.get("comment", ""),
                rating=raw.get("rating"),
                context={
                    "request_id": raw.get("request_id"),
                    "model": raw.get("model"),
                    "endpoint": raw.get("endpoint")
                },
                created_at=datetime.fromisoformat(raw["timestamp"])
            ))

        self.feedback_buffer.clear()
        return items

    def record_response_feedback(
        self,
        request_id: str,
        rating: int,
        user_id: Optional[str] = None,
        comment: Optional[str] = None,
        model: Optional[str] = None
    ) -> None:
        """Record feedback for an API response."""
        self.feedback_buffer.append({
            "id": f"api-{request_id}-{datetime.utcnow().timestamp()}",
            "request_id": request_id,
            "user_id": user_id,
            "rating": rating,
            "comment": comment,
            "model": model,
            "timestamp": datetime.utcnow().isoformat()
        })


class ImplicitFeedbackCollector(FeedbackCollector):
    """
    Collect implicit feedback from user behavior.
    """

    def __init__(self):
        self.events: List[Dict] = []

    def get_channel(self) -> FeedbackChannel:
        return FeedbackChannel.IMPLICIT

    async def collect(self) -> List[FeedbackItem]:
        """Convert behavioral events to feedback items."""
        items = []

        # Aggregate events by user session
        sessions = self._aggregate_by_session()

        for session_id, session_events in sessions.items():
            feedback = self._analyze_session(session_id, session_events)
            if feedback:
                items.append(feedback)

        self.events.clear()
        return items

    def track_event(
        self,
        event_type: str,
        user_id: Optional[str] = None,
        session_id: Optional[str] = None,
        properties: Optional[Dict] = None
    ) -> None:
        """Track a user behavioral event."""
        self.events.append({
            "type": event_type,
            "user_id": user_id,
            "session_id": session_id,
            "properties": properties or {},
            "timestamp": datetime.utcnow().isoformat()
        })

    def _aggregate_by_session(self) -> Dict[str, List[Dict]]:
        """Aggregate events by session."""
        sessions = defaultdict(list)

        for event in self.events:
            session_id = event.get("session_id") or event.get("user_id") or "anonymous"
            sessions[session_id].append(event)

        return sessions

    def _analyze_session(
        self,
        session_id: str,
        events: List[Dict]
    ) -> Optional[FeedbackItem]:
        """Analyze session events to derive feedback."""
        if not events:
            return None

        # Calculate engagement metrics
        engagement_score = self._calculate_engagement(events)

        # Determine implicit sentiment based on behavior
        sentiment = self._derive_sentiment(events, engagement_score)

        return FeedbackItem(
            id=f"implicit-{session_id}-{datetime.utcnow().timestamp()}",
            user_id=events[0].get("user_id"),
            channel=FeedbackChannel.IMPLICIT,
            feedback_type=FeedbackType.USABILITY,
            content=f"Implicit feedback from {len(events)} events",
            sentiment=sentiment,
            metadata={
                "session_id": session_id,
                "event_count": len(events),
                "engagement_score": engagement_score
            },
            context={
                "events": [e["type"] for e in events]
            }
        )

    def _calculate_engagement(self, events: List[Dict]) -> float:
        """Calculate engagement score from events."""
        positive_events = {"completion", "copy", "share", "save", "upvote"}
        negative_events = {"abandon", "error", "retry", "downvote"}

        positive_count = sum(
            1 for e in events if e["type"] in positive_events
        )
        negative_count = sum(
            1 for e in events if e["type"] in negative_events
        )

        total = positive_count + negative_count
        if total == 0:
            return 0.5

        return positive_count / total

    def _derive_sentiment(
        self,
        events: List[Dict],
        engagement_score: float
    ) -> FeedbackSentiment:
        """Derive sentiment from behavioral patterns."""
        if engagement_score >= 0.8:
            return FeedbackSentiment.VERY_POSITIVE
        elif engagement_score >= 0.6:
            return FeedbackSentiment.POSITIVE
        elif engagement_score >= 0.4:
            return FeedbackSentiment.NEUTRAL
        elif engagement_score >= 0.2:
            return FeedbackSentiment.NEGATIVE
        else:
            return FeedbackSentiment.VERY_NEGATIVE


class FeedbackAggregator:
    """
    Aggregate feedback from multiple channels.
    """

    def __init__(self):
        self.collectors: Dict[FeedbackChannel, FeedbackCollector] = {}
        self.feedback_store: List[FeedbackItem] = []
        self.webhooks: List[Callable] = []

    def register_collector(self, collector: FeedbackCollector) -> None:
        """Register a feedback collector."""
        self.collectors[collector.get_channel()] = collector

    def register_webhook(self, callback: Callable[[FeedbackItem], None]) -> None:
        """Register a webhook for new feedback."""
        self.webhooks.append(callback)

    async def collect_all(self) -> List[FeedbackItem]:
        """Collect feedback from all registered channels."""
        all_feedback = []

        for channel, collector in self.collectors.items():
            try:
                feedback = await collector.collect()
                all_feedback.extend(feedback)

                # Trigger webhooks
                for item in feedback:
                    for webhook in self.webhooks:
                        try:
                            webhook(item)
                        except Exception as e:
                            print(f"Webhook error: {e}")

            except Exception as e:
                print(f"Error collecting from {channel}: {e}")

        self.feedback_store.extend(all_feedback)
        return all_feedback

    def get_feedback(
        self,
        channel: Optional[FeedbackChannel] = None,
        feedback_type: Optional[FeedbackType] = None,
        since: Optional[datetime] = None
    ) -> List[FeedbackItem]:
        """Get feedback with optional filters."""
        results = self.feedback_store

        if channel:
            results = [f for f in results if f.channel == channel]

        if feedback_type:
            results = [f for f in results if f.feedback_type == feedback_type]

        if since:
            results = [f for f in results if f.created_at >= since]

        return results
```

### 1.2 Survey and NPS Collection

```python
"""
Survey and NPS (Net Promoter Score) collection system.
"""

from dataclasses import dataclass, field
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from enum import Enum
import json
import random
import string


class SurveyType(Enum):
    """Types of surveys."""
    NPS = "nps"
    CSAT = "csat"
    CES = "ces"
    CUSTOM = "custom"
    ONBOARDING = "onboarding"
    FEATURE = "feature"
    CHURN = "churn"


class QuestionType(Enum):
    """Types of survey questions."""
    RATING = "rating"
    SCALE = "scale"
    MULTIPLE_CHOICE = "multiple_choice"
    CHECKBOX = "checkbox"
    TEXT = "text"
    YES_NO = "yes_no"


@dataclass
class SurveyQuestion:
    """A survey question."""
    id: str
    question_type: QuestionType
    text: str
    required: bool = True
    options: List[str] = field(default_factory=list)
    scale_min: int = 0
    scale_max: int = 10
    scale_labels: Dict[int, str] = field(default_factory=dict)
    conditional_on: Optional[str] = None
    conditional_values: List[Any] = field(default_factory=list)


@dataclass
class Survey:
    """A survey definition."""
    id: str
    survey_type: SurveyType
    title: str
    description: str
    questions: List[SurveyQuestion]
    targeting: Dict[str, Any] = field(default_factory=dict)
    active: bool = True
    created_at: datetime = field(default_factory=datetime.utcnow)
    expires_at: Optional[datetime] = None


@dataclass
class SurveyResponse:
    """A survey response."""
    id: str
    survey_id: str
    user_id: Optional[str]
    answers: Dict[str, Any]
    completed: bool
    started_at: datetime
    completed_at: Optional[datetime] = None
    metadata: Dict[str, Any] = field(default_factory=dict)


class SurveyManager:
    """
    Manage surveys and collect responses.
    """

    def __init__(self):
        self.surveys: Dict[str, Survey] = {}
        self.responses: Dict[str, List[SurveyResponse]] = {}
        self.user_survey_history: Dict[str, List[str]] = {}

    def create_nps_survey(
        self,
        title: str = "How likely are you to recommend us?",
        follow_up: bool = True
    ) -> Survey:
        """Create a standard NPS survey."""
        questions = [
            SurveyQuestion(
                id="nps_score",
                question_type=QuestionType.SCALE,
                text="How likely are you to recommend our platform to a colleague or friend?",
                scale_min=0,
                scale_max=10,
                scale_labels={
                    0: "Not at all likely",
                    5: "Neutral",
                    10: "Extremely likely"
                }
            )
        ]

        if follow_up:
            questions.extend([
                SurveyQuestion(
                    id="nps_reason",
                    question_type=QuestionType.TEXT,
                    text="What's the primary reason for your score?",
                    required=False
                ),
                SurveyQuestion(
                    id="nps_improvement",
                    question_type=QuestionType.TEXT,
                    text="What could we do to improve your experience?",
                    required=False,
                    conditional_on="nps_score",
                    conditional_values=list(range(0, 7))  # Only for detractors
                )
            ])

        survey = Survey(
            id=self._generate_id(),
            survey_type=SurveyType.NPS,
            title=title,
            description="Net Promoter Score survey",
            questions=questions
        )

        self.surveys[survey.id] = survey
        return survey

    def create_csat_survey(
        self,
        context: str = "your recent experience"
    ) -> Survey:
        """Create a CSAT (Customer Satisfaction) survey."""
        survey = Survey(
            id=self._generate_id(),
            survey_type=SurveyType.CSAT,
            title="Customer Satisfaction Survey",
            description=f"Rate your satisfaction with {context}",
            questions=[
                SurveyQuestion(
                    id="csat_score",
                    question_type=QuestionType.RATING,
                    text=f"How satisfied are you with {context}?",
                    scale_min=1,
                    scale_max=5,
                    scale_labels={
                        1: "Very Dissatisfied",
                        2: "Dissatisfied",
                        3: "Neutral",
                        4: "Satisfied",
                        5: "Very Satisfied"
                    }
                ),
                SurveyQuestion(
                    id="csat_feedback",
                    question_type=QuestionType.TEXT,
                    text="Any additional feedback?",
                    required=False
                )
            ]
        )

        self.surveys[survey.id] = survey
        return survey

    def create_ces_survey(
        self,
        task: str = "completing your task"
    ) -> Survey:
        """Create a CES (Customer Effort Score) survey."""
        survey = Survey(
            id=self._generate_id(),
            survey_type=SurveyType.CES,
            title="Effort Survey",
            description=f"Rate the ease of {task}",
            questions=[
                SurveyQuestion(
                    id="ces_score",
                    question_type=QuestionType.SCALE,
                    text=f"How easy was {task}?",
                    scale_min=1,
                    scale_max=7,
                    scale_labels={
                        1: "Extremely Difficult",
                        4: "Neither Easy nor Difficult",
                        7: "Extremely Easy"
                    }
                ),
                SurveyQuestion(
                    id="ces_blockers",
                    question_type=QuestionType.TEXT,
                    text="What made this difficult?",
                    required=False,
                    conditional_on="ces_score",
                    conditional_values=[1, 2, 3]
                )
            ]
        )

        self.surveys[survey.id] = survey
        return survey

    def create_custom_survey(
        self,
        title: str,
        description: str,
        questions: List[Dict]
    ) -> Survey:
        """Create a custom survey."""
        parsed_questions = []

        for q in questions:
            parsed_questions.append(SurveyQuestion(
                id=q.get("id", self._generate_id()),
                question_type=QuestionType(q["type"]),
                text=q["text"],
                required=q.get("required", True),
                options=q.get("options", []),
                scale_min=q.get("scale_min", 0),
                scale_max=q.get("scale_max", 10),
                scale_labels=q.get("scale_labels", {}),
                conditional_on=q.get("conditional_on"),
                conditional_values=q.get("conditional_values", [])
            ))

        survey = Survey(
            id=self._generate_id(),
            survey_type=SurveyType.CUSTOM,
            title=title,
            description=description,
            questions=parsed_questions
        )

        self.surveys[survey.id] = survey
        return survey

    def should_show_survey(
        self,
        user_id: str,
        survey_id: str,
        cooldown_days: int = 30
    ) -> bool:
        """Determine if a survey should be shown to a user."""
        survey = self.surveys.get(survey_id)
        if not survey or not survey.active:
            return False

        if survey.expires_at and datetime.utcnow() > survey.expires_at:
            return False

        # Check cooldown
        user_history = self.user_survey_history.get(user_id, [])
        if survey_id in user_history:
            survey_responses = self.responses.get(survey_id, [])
            user_responses = [
                r for r in survey_responses
                if r.user_id == user_id
            ]

            if user_responses:
                latest = max(r.started_at for r in user_responses)
                if datetime.utcnow() - latest < timedelta(days=cooldown_days):
                    return False

        # Check targeting criteria
        return self._check_targeting(user_id, survey.targeting)

    def start_survey(
        self,
        survey_id: str,
        user_id: Optional[str] = None,
        metadata: Optional[Dict] = None
    ) -> Optional[SurveyResponse]:
        """Start a survey response."""
        survey = self.surveys.get(survey_id)
        if not survey:
            return None

        response = SurveyResponse(
            id=self._generate_id(),
            survey_id=survey_id,
            user_id=user_id,
            answers={},
            completed=False,
            started_at=datetime.utcnow(),
            metadata=metadata or {}
        )

        if survey_id not in self.responses:
            self.responses[survey_id] = []
        self.responses[survey_id].append(response)

        if user_id:
            if user_id not in self.user_survey_history:
                self.user_survey_history[user_id] = []
            self.user_survey_history[user_id].append(survey_id)

        return response

    def submit_answer(
        self,
        response_id: str,
        question_id: str,
        answer: Any
    ) -> bool:
        """Submit an answer to a survey question."""
        for survey_responses in self.responses.values():
            for response in survey_responses:
                if response.id == response_id:
                    response.answers[question_id] = answer
                    return True

        return False

    def complete_survey(self, response_id: str) -> bool:
        """Mark a survey response as complete."""
        for survey_responses in self.responses.values():
            for response in survey_responses:
                if response.id == response_id:
                    response.completed = True
                    response.completed_at = datetime.utcnow()
                    return True

        return False

    def calculate_nps(self, survey_id: str) -> Dict[str, Any]:
        """Calculate NPS score for a survey."""
        responses = self.responses.get(survey_id, [])
        completed = [r for r in responses if r.completed]

        if not completed:
            return {"nps": None, "responses": 0}

        scores = [r.answers.get("nps_score", 0) for r in completed]

        promoters = len([s for s in scores if s >= 9])
        passives = len([s for s in scores if 7 <= s <= 8])
        detractors = len([s for s in scores if s <= 6])

        total = len(scores)
        nps = ((promoters - detractors) / total) * 100

        return {
            "nps": round(nps, 1),
            "promoters": promoters,
            "passives": passives,
            "detractors": detractors,
            "total_responses": total,
            "promoter_percentage": round(promoters / total * 100, 1),
            "detractor_percentage": round(detractors / total * 100, 1)
        }

    def calculate_csat(self, survey_id: str) -> Dict[str, Any]:
        """Calculate CSAT score for a survey."""
        responses = self.responses.get(survey_id, [])
        completed = [r for r in responses if r.completed]

        if not completed:
            return {"csat": None, "responses": 0}

        scores = [r.answers.get("csat_score", 0) for r in completed]

        # CSAT = (satisfied + very satisfied) / total
        satisfied = len([s for s in scores if s >= 4])
        csat = (satisfied / len(scores)) * 100

        return {
            "csat": round(csat, 1),
            "average_score": round(sum(scores) / len(scores), 2),
            "total_responses": len(scores),
            "score_distribution": {
                i: scores.count(i) for i in range(1, 6)
            }
        }

    def _check_targeting(
        self,
        user_id: str,
        targeting: Dict[str, Any]
    ) -> bool:
        """Check if user matches targeting criteria."""
        # Simplified - would check user attributes against targeting rules
        return True

    def _generate_id(self) -> str:
        """Generate a unique ID."""
        return ''.join(random.choices(string.ascii_lowercase + string.digits, k=12))
```

---

## 2. Feedback Analysis and Processing

### 2.1 Sentiment Analysis and Classification

```python
"""
Feedback sentiment analysis and automatic classification.
"""

from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any, Tuple
from enum import Enum
import re
from collections import defaultdict


@dataclass
class AnalyzedFeedback:
    """Feedback with analysis results."""
    original: 'FeedbackItem'
    sentiment: FeedbackSentiment
    sentiment_score: float  # -1 to 1
    categories: List[str]
    entities: List[Dict[str, str]]
    urgency: str  # low, medium, high, critical
    actionable: bool
    suggested_actions: List[str]


class FeedbackAnalyzer:
    """
    Analyze feedback for sentiment, categories, and insights.
    """

    def __init__(self):
        self.category_keywords = self._load_category_keywords()
        self.sentiment_lexicon = self._load_sentiment_lexicon()
        self.urgency_indicators = self._load_urgency_indicators()

    def analyze(self, feedback: 'FeedbackItem') -> AnalyzedFeedback:
        """Analyze a feedback item."""
        content = feedback.content.lower()

        # Sentiment analysis
        sentiment, sentiment_score = self._analyze_sentiment(content)

        # Category classification
        categories = self._classify_categories(content)

        # Entity extraction
        entities = self._extract_entities(content)

        # Urgency assessment
        urgency = self._assess_urgency(content, sentiment_score)

        # Determine actionability
        actionable = self._is_actionable(content, categories)

        # Generate suggested actions
        suggested_actions = self._suggest_actions(
            categories, sentiment, urgency
        )

        return AnalyzedFeedback(
            original=feedback,
            sentiment=sentiment,
            sentiment_score=sentiment_score,
            categories=categories,
            entities=entities,
            urgency=urgency,
            actionable=actionable,
            suggested_actions=suggested_actions
        )

    def batch_analyze(
        self,
        feedback_items: List['FeedbackItem']
    ) -> List[AnalyzedFeedback]:
        """Analyze multiple feedback items."""
        return [self.analyze(item) for item in feedback_items]

    def _analyze_sentiment(
        self,
        text: str
    ) -> Tuple[FeedbackSentiment, float]:
        """Analyze sentiment of text."""
        words = text.split()
        positive_count = 0
        negative_count = 0

        for word in words:
            if word in self.sentiment_lexicon["positive"]:
                positive_count += 1
            elif word in self.sentiment_lexicon["negative"]:
                negative_count += 1

        total = positive_count + negative_count
        if total == 0:
            return FeedbackSentiment.NEUTRAL, 0.0

        score = (positive_count - negative_count) / total

        if score >= 0.5:
            sentiment = FeedbackSentiment.VERY_POSITIVE
        elif score >= 0.2:
            sentiment = FeedbackSentiment.POSITIVE
        elif score <= -0.5:
            sentiment = FeedbackSentiment.VERY_NEGATIVE
        elif score <= -0.2:
            sentiment = FeedbackSentiment.NEGATIVE
        else:
            sentiment = FeedbackSentiment.NEUTRAL

        return sentiment, score

    def _classify_categories(self, text: str) -> List[str]:
        """Classify feedback into categories."""
        categories = []

        for category, keywords in self.category_keywords.items():
            for keyword in keywords:
                if keyword in text:
                    if category not in categories:
                        categories.append(category)
                    break

        if not categories:
            categories.append("general")

        return categories

    def _extract_entities(self, text: str) -> List[Dict[str, str]]:
        """Extract entities from feedback text."""
        entities = []

        # Extract model names
        model_patterns = [
            r'gpt-\d+(?:\.\d+)?(?:-turbo)?',
            r'claude-\d+(?:\.\d+)?',
            r'llama-?\d+',
            r'gemini-?(?:pro|ultra)?'
        ]

        for pattern in model_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                entities.append({
                    "type": "model",
                    "value": match
                })

        # Extract feature references
        feature_patterns = [
            r'(?:the\s+)?(\w+)\s+feature',
            r'(?:the\s+)?(\w+)\s+api',
            r'(?:the\s+)?(\w+)\s+endpoint'
        ]

        for pattern in feature_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                entities.append({
                    "type": "feature",
                    "value": match
                })

        # Extract error codes
        error_patterns = [
            r'error\s*(?:code)?:?\s*(\w+)',
            r'(\d{3})\s+(?:error|status)'
        ]

        for pattern in error_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                entities.append({
                    "type": "error",
                    "value": match
                })

        return entities

    def _assess_urgency(self, text: str, sentiment_score: float) -> str:
        """Assess urgency of feedback."""
        urgency_score = 0

        # Check for urgency indicators
        for indicator, weight in self.urgency_indicators.items():
            if indicator in text:
                urgency_score += weight

        # Factor in sentiment
        if sentiment_score < -0.5:
            urgency_score += 2

        # Check for specific urgent phrases
        urgent_phrases = [
            "production", "down", "broken", "urgent", "asap",
            "critical", "blocking", "cannot", "stopped working"
        ]

        for phrase in urgent_phrases:
            if phrase in text:
                urgency_score += 3

        if urgency_score >= 6:
            return "critical"
        elif urgency_score >= 4:
            return "high"
        elif urgency_score >= 2:
            return "medium"
        else:
            return "low"

    def _is_actionable(
        self,
        text: str,
        categories: List[str]
    ) -> bool:
        """Determine if feedback is actionable."""
        actionable_categories = {
            "bug_report", "feature_request", "documentation",
            "performance", "usability"
        }

        if any(cat in actionable_categories for cat in categories):
            return True

        # Check for specific actionable patterns
        actionable_patterns = [
            r'(?:please|could you|can you|would be nice)',
            r'(?:should|needs to|must|has to)',
            r'(?:doesn\'t work|not working|broken|bug)',
            r'(?:add|implement|create|build|fix)'
        ]

        for pattern in actionable_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                return True

        return False

    def _suggest_actions(
        self,
        categories: List[str],
        sentiment: FeedbackSentiment,
        urgency: str
    ) -> List[str]:
        """Suggest actions based on analysis."""
        actions = []

        if urgency in ["critical", "high"]:
            actions.append("Prioritize for immediate review")

        if "bug_report" in categories:
            actions.append("Create bug ticket")
            actions.append("Investigate and reproduce issue")

        if "feature_request" in categories:
            actions.append("Add to feature backlog")
            actions.append("Evaluate against product roadmap")

        if "documentation" in categories:
            actions.append("Review and update documentation")

        if "performance" in categories:
            actions.append("Investigate performance metrics")

        if sentiment in [FeedbackSentiment.NEGATIVE, FeedbackSentiment.VERY_NEGATIVE]:
            actions.append("Consider reaching out to user")

        return actions

    def _load_category_keywords(self) -> Dict[str, List[str]]:
        """Load category classification keywords."""
        return {
            "bug_report": [
                "bug", "error", "broken", "crash", "doesn't work",
                "not working", "issue", "problem", "fail"
            ],
            "feature_request": [
                "feature", "add", "implement", "would be nice",
                "suggestion", "request", "wish", "need"
            ],
            "documentation": [
                "docs", "documentation", "guide", "tutorial",
                "example", "unclear", "confusing"
            ],
            "performance": [
                "slow", "latency", "timeout", "performance",
                "speed", "fast", "quick"
            ],
            "usability": [
                "confusing", "difficult", "hard to", "intuitive",
                "user experience", "ux", "ui"
            ],
            "pricing": [
                "price", "cost", "expensive", "billing",
                "subscription", "plan", "tier"
            ],
            "quality": [
                "quality", "accurate", "wrong", "incorrect",
                "hallucination", "output"
            ]
        }

    def _load_sentiment_lexicon(self) -> Dict[str, set]:
        """Load sentiment lexicon."""
        return {
            "positive": {
                "great", "excellent", "amazing", "awesome", "love",
                "fantastic", "wonderful", "perfect", "good", "best",
                "helpful", "useful", "easy", "fast", "reliable",
                "impressed", "satisfied", "happy", "thank"
            },
            "negative": {
                "bad", "terrible", "awful", "horrible", "hate",
                "worst", "poor", "disappointing", "frustrating",
                "broken", "useless", "slow", "difficult", "confusing",
                "annoying", "angry", "unhappy", "disappointed"
            }
        }

    def _load_urgency_indicators(self) -> Dict[str, int]:
        """Load urgency indicator weights."""
        return {
            "urgent": 3,
            "asap": 3,
            "critical": 3,
            "production": 2,
            "blocking": 2,
            "cannot": 1,
            "broken": 1,
            "down": 2,
            "emergency": 3
        }
```

### 2.2 Feedback Aggregation and Trends

```python
"""
Feedback aggregation and trend analysis.
"""

from dataclasses import dataclass, field
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from collections import defaultdict
import statistics


@dataclass
class FeedbackTrend:
    """A trend in feedback data."""
    metric: str
    direction: str  # increasing, decreasing, stable
    change_percentage: float
    period: str
    current_value: float
    previous_value: float
    data_points: List[Tuple[datetime, float]]


@dataclass
class FeedbackSummary:
    """Summary of feedback over a period."""
    period_start: datetime
    period_end: datetime
    total_count: int
    by_channel: Dict[str, int]
    by_type: Dict[str, int]
    by_sentiment: Dict[str, int]
    average_rating: Optional[float]
    nps_score: Optional[float]
    top_categories: List[Tuple[str, int]]
    top_issues: List[Dict[str, Any]]
    trends: List[FeedbackTrend]


class FeedbackAggregationEngine:
    """
    Aggregate and analyze feedback trends.
    """

    def __init__(self):
        self.feedback_history: List[AnalyzedFeedback] = []

    def add_feedback(self, feedback: AnalyzedFeedback) -> None:
        """Add analyzed feedback to history."""
        self.feedback_history.append(feedback)

    def generate_summary(
        self,
        start_date: datetime,
        end_date: datetime
    ) -> FeedbackSummary:
        """Generate a summary of feedback for a period."""
        period_feedback = [
            f for f in self.feedback_history
            if start_date <= f.original.created_at <= end_date
        ]

        # Count by channel
        by_channel = defaultdict(int)
        for f in period_feedback:
            by_channel[f.original.channel.value] += 1

        # Count by type
        by_type = defaultdict(int)
        for f in period_feedback:
            by_type[f.original.feedback_type.value] += 1

        # Count by sentiment
        by_sentiment = defaultdict(int)
        for f in period_feedback:
            by_sentiment[f.sentiment.value] += 1

        # Calculate average rating
        ratings = [
            f.original.rating for f in period_feedback
            if f.original.rating is not None
        ]
        average_rating = statistics.mean(ratings) if ratings else None

        # Calculate NPS if available
        nps_score = self._calculate_period_nps(period_feedback)

        # Get top categories
        category_counts = defaultdict(int)
        for f in period_feedback:
            for cat in f.categories:
                category_counts[cat] += 1

        top_categories = sorted(
            category_counts.items(),
            key=lambda x: x[1],
            reverse=True
        )[:5]

        # Identify top issues
        top_issues = self._identify_top_issues(period_feedback)

        # Calculate trends
        trends = self._calculate_trends(start_date, end_date)

        return FeedbackSummary(
            period_start=start_date,
            period_end=end_date,
            total_count=len(period_feedback),
            by_channel=dict(by_channel),
            by_type=dict(by_type),
            by_sentiment=dict(by_sentiment),
            average_rating=average_rating,
            nps_score=nps_score,
            top_categories=top_categories,
            top_issues=top_issues,
            trends=trends
        )

    def get_volume_trend(
        self,
        period_days: int = 30,
        granularity: str = "day"
    ) -> List[Dict[str, Any]]:
        """Get feedback volume trend over time."""
        end_date = datetime.utcnow()
        start_date = end_date - timedelta(days=period_days)

        period_feedback = [
            f for f in self.feedback_history
            if start_date <= f.original.created_at <= end_date
        ]

        # Group by granularity
        if granularity == "day":
            delta = timedelta(days=1)
        elif granularity == "week":
            delta = timedelta(weeks=1)
        else:
            delta = timedelta(hours=1)

        data_points = []
        current = start_date

        while current < end_date:
            next_period = current + delta
            period_count = len([
                f for f in period_feedback
                if current <= f.original.created_at < next_period
            ])

            data_points.append({
                "date": current.isoformat(),
                "count": period_count
            })

            current = next_period

        return data_points

    def get_sentiment_trend(
        self,
        period_days: int = 30
    ) -> List[Dict[str, Any]]:
        """Get sentiment trend over time."""
        end_date = datetime.utcnow()
        start_date = end_date - timedelta(days=period_days)

        data_points = []
        current = start_date
        delta = timedelta(days=1)

        while current < end_date:
            next_period = current + delta
            period_feedback = [
                f for f in self.feedback_history
                if current <= f.original.created_at < next_period
            ]

            if period_feedback:
                avg_sentiment = statistics.mean([
                    f.sentiment_score for f in period_feedback
                ])
            else:
                avg_sentiment = 0

            data_points.append({
                "date": current.isoformat(),
                "average_sentiment": avg_sentiment,
                "count": len(period_feedback)
            })

            current = next_period

        return data_points

    def get_category_distribution(
        self,
        period_days: int = 30
    ) -> Dict[str, int]:
        """Get distribution of feedback categories."""
        end_date = datetime.utcnow()
        start_date = end_date - timedelta(days=period_days)

        period_feedback = [
            f for f in self.feedback_history
            if start_date <= f.original.created_at <= end_date
        ]

        category_counts = defaultdict(int)
        for f in period_feedback:
            for cat in f.categories:
                category_counts[cat] += 1

        return dict(category_counts)

    def identify_emerging_issues(
        self,
        lookback_days: int = 7,
        threshold: float = 2.0
    ) -> List[Dict[str, Any]]:
        """Identify emerging issues based on recent trends."""
        now = datetime.utcnow()
        recent_start = now - timedelta(days=lookback_days)
        baseline_start = recent_start - timedelta(days=lookback_days * 2)

        # Get recent and baseline feedback
        recent = [
            f for f in self.feedback_history
            if recent_start <= f.original.created_at <= now
        ]

        baseline = [
            f for f in self.feedback_history
            if baseline_start <= f.original.created_at < recent_start
        ]

        # Compare category frequencies
        recent_categories = defaultdict(int)
        for f in recent:
            for cat in f.categories:
                recent_categories[cat] += 1

        baseline_categories = defaultdict(int)
        for f in baseline:
            for cat in f.categories:
                baseline_categories[cat] += 1

        emerging = []
        for cat, count in recent_categories.items():
            baseline_count = baseline_categories.get(cat, 1)
            ratio = count / baseline_count

            if ratio >= threshold:
                emerging.append({
                    "category": cat,
                    "recent_count": count,
                    "baseline_count": baseline_count,
                    "increase_ratio": ratio,
                    "sample_feedback": [
                        f.original.content[:100]
                        for f in recent if cat in f.categories
                    ][:3]
                })

        return sorted(emerging, key=lambda x: x["increase_ratio"], reverse=True)

    def _calculate_period_nps(
        self,
        feedback: List[AnalyzedFeedback]
    ) -> Optional[float]:
        """Calculate NPS for feedback with ratings."""
        nps_ratings = [
            f.original.rating for f in feedback
            if f.original.rating is not None and f.original.rating <= 10
        ]

        if not nps_ratings:
            return None

        promoters = len([r for r in nps_ratings if r >= 9])
        detractors = len([r for r in nps_ratings if r <= 6])

        return ((promoters - detractors) / len(nps_ratings)) * 100

    def _identify_top_issues(
        self,
        feedback: List[AnalyzedFeedback]
    ) -> List[Dict[str, Any]]:
        """Identify top issues from feedback."""
        # Group by entities and categories
        issue_groups = defaultdict(list)

        for f in feedback:
            if f.urgency in ["high", "critical"] or f.sentiment_score < -0.3:
                key = tuple(sorted(f.categories))
                issue_groups[key].append(f)

        top_issues = []
        for categories, items in issue_groups.items():
            if len(items) >= 2:  # Minimum threshold
                top_issues.append({
                    "categories": list(categories),
                    "count": len(items),
                    "average_sentiment": statistics.mean([
                        i.sentiment_score for i in items
                    ]),
                    "sample": items[0].original.content[:200]
                })

        return sorted(top_issues, key=lambda x: x["count"], reverse=True)[:5]

    def _calculate_trends(
        self,
        start_date: datetime,
        end_date: datetime
    ) -> List[FeedbackTrend]:
        """Calculate trends for the period."""
        period_length = (end_date - start_date).days
        previous_start = start_date - timedelta(days=period_length)

        current_feedback = [
            f for f in self.feedback_history
            if start_date <= f.original.created_at <= end_date
        ]

        previous_feedback = [
            f for f in self.feedback_history
            if previous_start <= f.original.created_at < start_date
        ]

        trends = []

        # Volume trend
        current_volume = len(current_feedback)
        previous_volume = len(previous_feedback) or 1

        volume_change = ((current_volume - previous_volume) / previous_volume) * 100

        trends.append(FeedbackTrend(
            metric="volume",
            direction="increasing" if volume_change > 5 else "decreasing" if volume_change < -5 else "stable",
            change_percentage=volume_change,
            period=f"{period_length} days",
            current_value=current_volume,
            previous_value=previous_volume,
            data_points=[]
        ))

        # Sentiment trend
        current_sentiment = statistics.mean([
            f.sentiment_score for f in current_feedback
        ]) if current_feedback else 0

        previous_sentiment = statistics.mean([
            f.sentiment_score for f in previous_feedback
        ]) if previous_feedback else 0

        sentiment_change = (current_sentiment - previous_sentiment) * 100

        trends.append(FeedbackTrend(
            metric="sentiment",
            direction="increasing" if sentiment_change > 5 else "decreasing" if sentiment_change < -5 else "stable",
            change_percentage=sentiment_change,
            period=f"{period_length} days",
            current_value=current_sentiment,
            previous_value=previous_sentiment,
            data_points=[]
        ))

        return trends
```

---

## 3. Feedback-Driven Development

### 3.1 Feedback to Action Pipeline

```python
"""
Convert feedback into actionable development items.
"""

from dataclasses import dataclass, field
from datetime import datetime
from typing import Dict, List, Optional, Any
from enum import Enum
import uuid


class ActionType(Enum):
    """Types of actions derived from feedback."""
    BUG_FIX = "bug_fix"
    FEATURE = "feature"
    DOCUMENTATION = "documentation"
    INVESTIGATION = "investigation"
    OUTREACH = "outreach"
    MONITORING = "monitoring"
    NO_ACTION = "no_action"


class ActionPriority(Enum):
    """Priority levels for actions."""
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"


@dataclass
class ActionItem:
    """An action item derived from feedback."""
    id: str
    action_type: ActionType
    priority: ActionPriority
    title: str
    description: str
    source_feedback_ids: List[str]
    assignee: Optional[str] = None
    status: str = "open"
    created_at: datetime = field(default_factory=datetime.utcnow)
    due_date: Optional[datetime] = None
    tags: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class FeedbackActionRule:
    """Rule for converting feedback to actions."""
    name: str
    conditions: Dict[str, Any]
    action_type: ActionType
    priority_mapping: Dict[str, ActionPriority]
    auto_assign_to: Optional[str] = None
    tags: List[str] = field(default_factory=list)


class FeedbackToActionPipeline:
    """
    Pipeline for converting feedback into actionable items.
    """

    def __init__(self):
        self.rules: List[FeedbackActionRule] = []
        self.actions: Dict[str, ActionItem] = {}
        self.feedback_action_map: Dict[str, str] = {}

        self._load_default_rules()

    def _load_default_rules(self) -> None:
        """Load default feedback-to-action rules."""
        self.rules = [
            FeedbackActionRule(
                name="Critical Bug",
                conditions={
                    "categories": ["bug_report"],
                    "urgency": ["critical"]
                },
                action_type=ActionType.BUG_FIX,
                priority_mapping={
                    "critical": ActionPriority.CRITICAL,
                    "high": ActionPriority.HIGH
                },
                tags=["bug", "critical"]
            ),
            FeedbackActionRule(
                name="High Priority Bug",
                conditions={
                    "categories": ["bug_report"],
                    "urgency": ["high"]
                },
                action_type=ActionType.BUG_FIX,
                priority_mapping={
                    "high": ActionPriority.HIGH,
                    "medium": ActionPriority.MEDIUM
                },
                tags=["bug"]
            ),
            FeedbackActionRule(
                name="Feature Request",
                conditions={
                    "categories": ["feature_request"],
                    "actionable": True
                },
                action_type=ActionType.FEATURE,
                priority_mapping={
                    "high": ActionPriority.MEDIUM,
                    "medium": ActionPriority.LOW
                },
                tags=["feature", "request"]
            ),
            FeedbackActionRule(
                name="Documentation Update",
                conditions={
                    "categories": ["documentation"]
                },
                action_type=ActionType.DOCUMENTATION,
                priority_mapping={
                    "high": ActionPriority.MEDIUM,
                    "medium": ActionPriority.LOW,
                    "low": ActionPriority.LOW
                },
                tags=["docs"]
            ),
            FeedbackActionRule(
                name="Unhappy Customer Outreach",
                conditions={
                    "sentiment": ["very_negative", "negative"],
                    "has_user_id": True
                },
                action_type=ActionType.OUTREACH,
                priority_mapping={
                    "critical": ActionPriority.HIGH,
                    "high": ActionPriority.HIGH,
                    "medium": ActionPriority.MEDIUM
                },
                tags=["outreach", "customer-success"]
            ),
            FeedbackActionRule(
                name="Performance Investigation",
                conditions={
                    "categories": ["performance"]
                },
                action_type=ActionType.INVESTIGATION,
                priority_mapping={
                    "critical": ActionPriority.HIGH,
                    "high": ActionPriority.MEDIUM,
                    "medium": ActionPriority.LOW
                },
                tags=["performance", "investigation"]
            )
        ]

    def add_rule(self, rule: FeedbackActionRule) -> None:
        """Add a custom rule."""
        self.rules.append(rule)

    def process_feedback(
        self,
        feedback: AnalyzedFeedback
    ) -> Optional[ActionItem]:
        """Process feedback and create action item if applicable."""
        # Check if already processed
        if feedback.original.id in self.feedback_action_map:
            return self.actions.get(self.feedback_action_map[feedback.original.id])

        # Find matching rule
        matching_rule = self._find_matching_rule(feedback)

        if not matching_rule:
            return None

        # Create action item
        action = self._create_action(feedback, matching_rule)
        self.actions[action.id] = action
        self.feedback_action_map[feedback.original.id] = action.id

        return action

    def process_batch(
        self,
        feedback_items: List[AnalyzedFeedback]
    ) -> List[ActionItem]:
        """Process multiple feedback items."""
        actions = []

        for feedback in feedback_items:
            action = self.process_feedback(feedback)
            if action:
                actions.append(action)

        # Deduplicate similar actions
        return self._deduplicate_actions(actions)

    def _find_matching_rule(
        self,
        feedback: AnalyzedFeedback
    ) -> Optional[FeedbackActionRule]:
        """Find the first matching rule for feedback."""
        for rule in self.rules:
            if self._matches_conditions(feedback, rule.conditions):
                return rule

        return None

    def _matches_conditions(
        self,
        feedback: AnalyzedFeedback,
        conditions: Dict[str, Any]
    ) -> bool:
        """Check if feedback matches rule conditions."""
        for condition_key, condition_value in conditions.items():
            if condition_key == "categories":
                if not any(cat in feedback.categories for cat in condition_value):
                    return False

            elif condition_key == "urgency":
                if feedback.urgency not in condition_value:
                    return False

            elif condition_key == "sentiment":
                if feedback.sentiment.value not in condition_value:
                    return False

            elif condition_key == "actionable":
                if feedback.actionable != condition_value:
                    return False

            elif condition_key == "has_user_id":
                if (feedback.original.user_id is not None) != condition_value:
                    return False

        return True

    def _create_action(
        self,
        feedback: AnalyzedFeedback,
        rule: FeedbackActionRule
    ) -> ActionItem:
        """Create an action item from feedback and rule."""
        priority = rule.priority_mapping.get(
            feedback.urgency,
            ActionPriority.MEDIUM
        )

        title = self._generate_title(feedback, rule)
        description = self._generate_description(feedback)

        return ActionItem(
            id=str(uuid.uuid4())[:8],
            action_type=rule.action_type,
            priority=priority,
            title=title,
            description=description,
            source_feedback_ids=[feedback.original.id],
            assignee=rule.auto_assign_to,
            tags=rule.tags + feedback.categories
        )

    def _generate_title(
        self,
        feedback: AnalyzedFeedback,
        rule: FeedbackActionRule
    ) -> str:
        """Generate action title from feedback."""
        content = feedback.original.content

        # Extract key phrase (first sentence or 100 chars)
        key_phrase = content.split('.')[0][:100]

        type_prefix = {
            ActionType.BUG_FIX: "[Bug]",
            ActionType.FEATURE: "[Feature]",
            ActionType.DOCUMENTATION: "[Docs]",
            ActionType.INVESTIGATION: "[Investigate]",
            ActionType.OUTREACH: "[Outreach]",
            ActionType.MONITORING: "[Monitor]"
        }

        prefix = type_prefix.get(rule.action_type, "")
        return f"{prefix} {key_phrase}"

    def _generate_description(self, feedback: AnalyzedFeedback) -> str:
        """Generate action description from feedback."""
        description = f"""## Original Feedback

{feedback.original.content}

## Analysis

- **Sentiment**: {feedback.sentiment.value} ({feedback.sentiment_score:.2f})
- **Categories**: {', '.join(feedback.categories)}
- **Urgency**: {feedback.urgency}

## Suggested Actions

"""
        for action in feedback.suggested_actions:
            description += f"- {action}\n"

        if feedback.entities:
            description += "\n## Related Entities\n\n"
            for entity in feedback.entities:
                description += f"- {entity['type']}: {entity['value']}\n"

        return description

    def _deduplicate_actions(
        self,
        actions: List[ActionItem]
    ) -> List[ActionItem]:
        """Deduplicate similar actions by merging them."""
        # Group by action type and similar titles
        groups: Dict[str, List[ActionItem]] = defaultdict(list)

        for action in actions:
            key = f"{action.action_type.value}-{'-'.join(sorted(action.tags[:2]))}"
            groups[key].append(action)

        deduplicated = []

        for key, group in groups.items():
            if len(group) == 1:
                deduplicated.append(group[0])
            else:
                # Merge similar actions
                merged = self._merge_actions(group)
                deduplicated.append(merged)

        return deduplicated

    def _merge_actions(self, actions: List[ActionItem]) -> ActionItem:
        """Merge multiple similar actions into one."""
        # Use highest priority
        priorities = [a.priority for a in actions]
        highest_priority = min(priorities, key=lambda p: list(ActionPriority).index(p))

        # Combine feedback IDs
        all_feedback_ids = []
        for a in actions:
            all_feedback_ids.extend(a.source_feedback_ids)

        # Combine tags
        all_tags = set()
        for a in actions:
            all_tags.update(a.tags)

        return ActionItem(
            id=str(uuid.uuid4())[:8],
            action_type=actions[0].action_type,
            priority=highest_priority,
            title=f"{actions[0].title} (+{len(actions)-1} similar)",
            description=actions[0].description + f"\n\n---\n\n*Merged from {len(actions)} similar feedback items*",
            source_feedback_ids=all_feedback_ids,
            tags=list(all_tags),
            metadata={"merged_count": len(actions)}
        )


class ActionTracker:
    """
    Track action items through their lifecycle.
    """

    def __init__(self):
        self.actions: Dict[str, ActionItem] = {}
        self.history: List[Dict] = []

    def add_action(self, action: ActionItem) -> None:
        """Add an action to track."""
        self.actions[action.id] = action
        self._log_event(action.id, "created", {})

    def assign(self, action_id: str, assignee: str) -> None:
        """Assign an action to someone."""
        if action_id in self.actions:
            self.actions[action_id].assignee = assignee
            self._log_event(action_id, "assigned", {"assignee": assignee})

    def update_status(self, action_id: str, status: str) -> None:
        """Update action status."""
        if action_id in self.actions:
            old_status = self.actions[action_id].status
            self.actions[action_id].status = status
            self._log_event(action_id, "status_changed", {
                "old_status": old_status,
                "new_status": status
            })

    def complete(self, action_id: str, resolution: str = "") -> None:
        """Mark an action as complete."""
        if action_id in self.actions:
            self.actions[action_id].status = "completed"
            self._log_event(action_id, "completed", {"resolution": resolution})

    def get_open_actions(
        self,
        priority: Optional[ActionPriority] = None
    ) -> List[ActionItem]:
        """Get open actions, optionally filtered by priority."""
        open_actions = [
            a for a in self.actions.values()
            if a.status not in ["completed", "closed", "wontfix"]
        ]

        if priority:
            open_actions = [a for a in open_actions if a.priority == priority]

        return sorted(
            open_actions,
            key=lambda a: list(ActionPriority).index(a.priority)
        )

    def get_metrics(self) -> Dict[str, Any]:
        """Get action tracking metrics."""
        total = len(self.actions)
        by_status = defaultdict(int)
        by_priority = defaultdict(int)
        by_type = defaultdict(int)

        for action in self.actions.values():
            by_status[action.status] += 1
            by_priority[action.priority.value] += 1
            by_type[action.action_type.value] += 1

        return {
            "total": total,
            "by_status": dict(by_status),
            "by_priority": dict(by_priority),
            "by_type": dict(by_type),
            "open_count": total - by_status.get("completed", 0) - by_status.get("closed", 0)
        }

    def _log_event(
        self,
        action_id: str,
        event_type: str,
        data: Dict
    ) -> None:
        """Log an action event."""
        self.history.append({
            "action_id": action_id,
            "event_type": event_type,
            "data": data,
            "timestamp": datetime.utcnow().isoformat()
        })
```

### 3.2 Feedback Loop Closure

```python
"""
Close the feedback loop by communicating with users.
"""

from dataclasses import dataclass, field
from datetime import datetime
from typing import Dict, List, Optional, Any, Callable
from enum import Enum


class CommunicationType(Enum):
    """Types of feedback communications."""
    ACKNOWLEDGMENT = "acknowledgment"
    STATUS_UPDATE = "status_update"
    RESOLUTION = "resolution"
    FOLLOW_UP = "follow_up"
    THANK_YOU = "thank_you"


@dataclass
class FeedbackCommunication:
    """A communication sent to a feedback submitter."""
    id: str
    feedback_id: str
    user_id: str
    communication_type: CommunicationType
    subject: str
    content: str
    sent_at: Optional[datetime] = None
    channel: str = "email"
    metadata: Dict[str, Any] = field(default_factory=dict)


class FeedbackLoopCloser:
    """
    Manage closing the feedback loop with users.
    """

    def __init__(self):
        self.templates: Dict[str, str] = self._load_templates()
        self.communications: List[FeedbackCommunication] = []
        self.send_handlers: Dict[str, Callable] = {}

    def _load_templates(self) -> Dict[str, str]:
        """Load communication templates."""
        return {
            "acknowledgment": """
Hi {user_name},

Thank you for your feedback! We've received your {feedback_type} and our team is reviewing it.

Your feedback ID is: {feedback_id}

We appreciate you taking the time to help us improve.

Best regards,
The {product_name} Team
""",
            "status_update": """
Hi {user_name},

We wanted to update you on your feedback ({feedback_id}).

Status: {status}

{details}

Thank you for your patience.

Best regards,
The {product_name} Team
""",
            "resolution": """
Hi {user_name},

Great news! Your feedback ({feedback_id}) has been addressed.

{resolution_details}

{action_taken}

Thank you for helping us improve {product_name}!

Best regards,
The {product_name} Team
""",
            "follow_up": """
Hi {user_name},

Following up on your recent feedback ({feedback_id}).

{follow_up_message}

If you have any additional thoughts or questions, please don't hesitate to reach out.

Best regards,
The {product_name} Team
""",
            "thank_you": """
Hi {user_name},

Thank you for being part of our community and sharing your thoughts with us.

Your feedback about {feedback_topic} has been incredibly valuable.

{personalized_message}

We're committed to continuously improving based on feedback like yours.

Best regards,
The {product_name} Team
"""
        }

    def register_send_handler(
        self,
        channel: str,
        handler: Callable[[FeedbackCommunication], bool]
    ) -> None:
        """Register a handler for sending communications."""
        self.send_handlers[channel] = handler

    def send_acknowledgment(
        self,
        feedback: 'FeedbackItem',
        product_name: str = "Our Platform"
    ) -> FeedbackCommunication:
        """Send acknowledgment for received feedback."""
        if not feedback.user_id:
            return None

        content = self.templates["acknowledgment"].format(
            user_name=self._get_user_name(feedback.user_id),
            feedback_type=feedback.feedback_type.value.replace("_", " "),
            feedback_id=feedback.id,
            product_name=product_name
        )

        comm = FeedbackCommunication(
            id=f"ack-{feedback.id}",
            feedback_id=feedback.id,
            user_id=feedback.user_id,
            communication_type=CommunicationType.ACKNOWLEDGMENT,
            subject=f"We received your feedback - {feedback.id}",
            content=content
        )

        self._send(comm)
        return comm

    def send_status_update(
        self,
        feedback_id: str,
        user_id: str,
        status: str,
        details: str,
        product_name: str = "Our Platform"
    ) -> FeedbackCommunication:
        """Send status update for feedback."""
        content = self.templates["status_update"].format(
            user_name=self._get_user_name(user_id),
            feedback_id=feedback_id,
            status=status,
            details=details,
            product_name=product_name
        )

        comm = FeedbackCommunication(
            id=f"update-{feedback_id}-{datetime.utcnow().timestamp()}",
            feedback_id=feedback_id,
            user_id=user_id,
            communication_type=CommunicationType.STATUS_UPDATE,
            subject=f"Update on your feedback - {feedback_id}",
            content=content
        )

        self._send(comm)
        return comm

    def send_resolution(
        self,
        feedback_id: str,
        user_id: str,
        resolution_details: str,
        action_taken: str,
        product_name: str = "Our Platform"
    ) -> FeedbackCommunication:
        """Send resolution notification for feedback."""
        content = self.templates["resolution"].format(
            user_name=self._get_user_name(user_id),
            feedback_id=feedback_id,
            resolution_details=resolution_details,
            action_taken=action_taken,
            product_name=product_name
        )

        comm = FeedbackCommunication(
            id=f"resolved-{feedback_id}",
            feedback_id=feedback_id,
            user_id=user_id,
            communication_type=CommunicationType.RESOLUTION,
            subject=f"Your feedback has been addressed - {feedback_id}",
            content=content
        )

        self._send(comm)
        return comm

    def send_bulk_update(
        self,
        feedback_ids: List[str],
        user_ids: List[str],
        update_type: str,
        message: str
    ) -> List[FeedbackCommunication]:
        """Send bulk updates for multiple feedback items."""
        communications = []

        for feedback_id, user_id in zip(feedback_ids, user_ids):
            if user_id:
                comm = self.send_status_update(
                    feedback_id=feedback_id,
                    user_id=user_id,
                    status=update_type,
                    details=message
                )
                if comm:
                    communications.append(comm)

        return communications

    def _send(self, comm: FeedbackCommunication) -> bool:
        """Send a communication through the appropriate channel."""
        handler = self.send_handlers.get(comm.channel)

        if handler:
            success = handler(comm)
            if success:
                comm.sent_at = datetime.utcnow()
                self.communications.append(comm)
                return True

        return False

    def _get_user_name(self, user_id: str) -> str:
        """Get user name from user ID (placeholder)."""
        # In production, would look up user details
        return "there"

    def get_communication_history(
        self,
        feedback_id: Optional[str] = None,
        user_id: Optional[str] = None
    ) -> List[FeedbackCommunication]:
        """Get communication history."""
        results = self.communications

        if feedback_id:
            results = [c for c in results if c.feedback_id == feedback_id]

        if user_id:
            results = [c for c in results if c.user_id == user_id]

        return results


class FeedbackResponseTracker:
    """
    Track and analyze feedback response metrics.
    """

    def __init__(self):
        self.response_times: List[Dict] = []

    def record_response(
        self,
        feedback_id: str,
        feedback_created: datetime,
        response_type: str
    ) -> None:
        """Record a response to feedback."""
        response_time = (datetime.utcnow() - feedback_created).total_seconds()

        self.response_times.append({
            "feedback_id": feedback_id,
            "response_type": response_type,
            "response_time_seconds": response_time,
            "timestamp": datetime.utcnow().isoformat()
        })

    def get_average_response_time(
        self,
        response_type: Optional[str] = None
    ) -> float:
        """Get average response time in hours."""
        records = self.response_times

        if response_type:
            records = [r for r in records if r["response_type"] == response_type]

        if not records:
            return 0.0

        avg_seconds = sum(r["response_time_seconds"] for r in records) / len(records)
        return avg_seconds / 3600  # Convert to hours

    def get_response_metrics(self) -> Dict[str, Any]:
        """Get comprehensive response metrics."""
        if not self.response_times:
            return {"no_data": True}

        response_seconds = [r["response_time_seconds"] for r in self.response_times]

        return {
            "total_responses": len(self.response_times),
            "average_response_time_hours": self.get_average_response_time(),
            "median_response_time_hours": sorted(response_seconds)[len(response_seconds)//2] / 3600,
            "min_response_time_hours": min(response_seconds) / 3600,
            "max_response_time_hours": max(response_seconds) / 3600,
            "responses_under_24h": len([r for r in response_seconds if r < 86400]),
            "responses_under_1h": len([r for r in response_seconds if r < 3600])
        }
```

---

## 4. LLM-Specific Feedback Mechanisms

### 4.1 Response Quality Feedback

```python
"""
LLM response quality feedback collection and analysis.
"""

from dataclasses import dataclass, field
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
from enum import Enum
import hashlib


class ResponseQualityDimension(Enum):
    """Dimensions of response quality."""
    ACCURACY = "accuracy"
    RELEVANCE = "relevance"
    COMPLETENESS = "completeness"
    CLARITY = "clarity"
    HELPFULNESS = "helpfulness"
    SAFETY = "safety"
    FORMATTING = "formatting"


@dataclass
class ResponseFeedback:
    """Feedback on an LLM response."""
    id: str
    request_id: str
    user_id: Optional[str]
    model: str
    prompt_hash: str
    response_hash: str
    overall_rating: int  # 1-5
    dimension_ratings: Dict[ResponseQualityDimension, int] = field(default_factory=dict)
    feedback_text: Optional[str] = None
    preferred_response: Optional[str] = None
    issues: List[str] = field(default_factory=list)
    created_at: datetime = field(default_factory=datetime.utcnow)
    context: Dict[str, Any] = field(default_factory=dict)


class ResponseQualityTracker:
    """
    Track and analyze LLM response quality.
    """

    def __init__(self):
        self.feedback: List[ResponseFeedback] = []
        self.model_metrics: Dict[str, Dict] = {}

    def record_thumbs_feedback(
        self,
        request_id: str,
        is_positive: bool,
        user_id: Optional[str] = None,
        model: str = "unknown",
        prompt: str = "",
        response: str = ""
    ) -> ResponseFeedback:
        """Record simple thumbs up/down feedback."""
        feedback = ResponseFeedback(
            id=f"fb-{request_id}",
            request_id=request_id,
            user_id=user_id,
            model=model,
            prompt_hash=self._hash_content(prompt),
            response_hash=self._hash_content(response),
            overall_rating=5 if is_positive else 1
        )

        self.feedback.append(feedback)
        self._update_model_metrics(feedback)

        return feedback

    def record_detailed_feedback(
        self,
        request_id: str,
        overall_rating: int,
        dimension_ratings: Dict[str, int],
        user_id: Optional[str] = None,
        model: str = "unknown",
        prompt: str = "",
        response: str = "",
        feedback_text: Optional[str] = None,
        preferred_response: Optional[str] = None,
        issues: Optional[List[str]] = None
    ) -> ResponseFeedback:
        """Record detailed multi-dimensional feedback."""
        parsed_dimensions = {
            ResponseQualityDimension(k): v
            for k, v in dimension_ratings.items()
        }

        feedback = ResponseFeedback(
            id=f"fb-{request_id}",
            request_id=request_id,
            user_id=user_id,
            model=model,
            prompt_hash=self._hash_content(prompt),
            response_hash=self._hash_content(response),
            overall_rating=overall_rating,
            dimension_ratings=parsed_dimensions,
            feedback_text=feedback_text,
            preferred_response=preferred_response,
            issues=issues or []
        )

        self.feedback.append(feedback)
        self._update_model_metrics(feedback)

        return feedback

    def record_comparison_feedback(
        self,
        request_id: str,
        prompt: str,
        response_a: str,
        response_b: str,
        model_a: str,
        model_b: str,
        preferred: str,  # "a", "b", or "tie"
        user_id: Optional[str] = None,
        reason: Optional[str] = None
    ) -> Dict[str, ResponseFeedback]:
        """Record A/B comparison feedback."""
        # Create feedback for both responses
        feedback_a = ResponseFeedback(
            id=f"fb-{request_id}-a",
            request_id=request_id,
            user_id=user_id,
            model=model_a,
            prompt_hash=self._hash_content(prompt),
            response_hash=self._hash_content(response_a),
            overall_rating=5 if preferred == "a" else (3 if preferred == "tie" else 1),
            context={"comparison_id": request_id, "position": "a", "preferred": preferred}
        )

        feedback_b = ResponseFeedback(
            id=f"fb-{request_id}-b",
            request_id=request_id,
            user_id=user_id,
            model=model_b,
            prompt_hash=self._hash_content(prompt),
            response_hash=self._hash_content(response_b),
            overall_rating=5 if preferred == "b" else (3 if preferred == "tie" else 1),
            context={"comparison_id": request_id, "position": "b", "preferred": preferred}
        )

        self.feedback.extend([feedback_a, feedback_b])
        self._update_model_metrics(feedback_a)
        self._update_model_metrics(feedback_b)

        return {"a": feedback_a, "b": feedback_b}

    def get_model_quality_metrics(
        self,
        model: str
    ) -> Dict[str, Any]:
        """Get quality metrics for a specific model."""
        model_feedback = [f for f in self.feedback if f.model == model]

        if not model_feedback:
            return {"model": model, "no_data": True}

        ratings = [f.overall_rating for f in model_feedback]
        avg_rating = sum(ratings) / len(ratings)

        # Calculate dimension averages
        dimension_totals = {dim: [] for dim in ResponseQualityDimension}
        for f in model_feedback:
            for dim, rating in f.dimension_ratings.items():
                dimension_totals[dim].append(rating)

        dimension_averages = {
            dim.value: sum(ratings) / len(ratings) if ratings else None
            for dim, ratings in dimension_totals.items()
        }

        # Calculate issue frequency
        issue_counts = {}
        for f in model_feedback:
            for issue in f.issues:
                issue_counts[issue] = issue_counts.get(issue, 0) + 1

        return {
            "model": model,
            "total_feedback": len(model_feedback),
            "average_rating": round(avg_rating, 2),
            "rating_distribution": {
                i: ratings.count(i) for i in range(1, 6)
            },
            "dimension_averages": dimension_averages,
            "top_issues": sorted(
                issue_counts.items(),
                key=lambda x: x[1],
                reverse=True
            )[:5],
            "positive_rate": len([r for r in ratings if r >= 4]) / len(ratings)
        }

    def get_comparison_win_rates(self) -> Dict[str, Dict[str, float]]:
        """Calculate win rates from comparison feedback."""
        comparisons = [
            f for f in self.feedback
            if "comparison_id" in f.context
        ]

        # Group by comparison
        comparison_groups = {}
        for f in comparisons:
            comp_id = f.context["comparison_id"]
            if comp_id not in comparison_groups:
                comparison_groups[comp_id] = {}
            comparison_groups[comp_id][f.context["position"]] = f

        # Calculate win rates
        matchups = {}

        for comp_id, group in comparison_groups.items():
            if "a" in group and "b" in group:
                model_a = group["a"].model
                model_b = group["b"].model
                preferred = group["a"].context.get("preferred")

                key = f"{model_a}_vs_{model_b}"
                if key not in matchups:
                    matchups[key] = {"wins_a": 0, "wins_b": 0, "ties": 0, "total": 0}

                matchups[key]["total"] += 1
                if preferred == "a":
                    matchups[key]["wins_a"] += 1
                elif preferred == "b":
                    matchups[key]["wins_b"] += 1
                else:
                    matchups[key]["ties"] += 1

        # Convert to win rates
        win_rates = {}
        for matchup, counts in matchups.items():
            models = matchup.split("_vs_")
            win_rates[matchup] = {
                models[0]: counts["wins_a"] / counts["total"],
                models[1]: counts["wins_b"] / counts["total"],
                "ties": counts["ties"] / counts["total"],
                "total_comparisons": counts["total"]
            }

        return win_rates

    def identify_problem_patterns(
        self,
        min_occurrences: int = 3
    ) -> List[Dict[str, Any]]:
        """Identify patterns in negative feedback."""
        negative_feedback = [f for f in self.feedback if f.overall_rating <= 2]

        # Group by prompt similarity
        prompt_groups = {}
        for f in negative_feedback:
            prompt_hash = f.prompt_hash
            if prompt_hash not in prompt_groups:
                prompt_groups[prompt_hash] = []
            prompt_groups[prompt_hash].append(f)

        patterns = []
        for prompt_hash, group in prompt_groups.items():
            if len(group) >= min_occurrences:
                # Analyze common issues
                all_issues = []
                for f in group:
                    all_issues.extend(f.issues)

                issue_freq = {}
                for issue in all_issues:
                    issue_freq[issue] = issue_freq.get(issue, 0) + 1

                patterns.append({
                    "prompt_hash": prompt_hash,
                    "occurrence_count": len(group),
                    "models_affected": list(set(f.model for f in group)),
                    "common_issues": sorted(
                        issue_freq.items(),
                        key=lambda x: x[1],
                        reverse=True
                    ),
                    "sample_feedback": group[0].feedback_text
                })

        return sorted(patterns, key=lambda x: x["occurrence_count"], reverse=True)

    def _hash_content(self, content: str) -> str:
        """Hash content for deduplication."""
        return hashlib.md5(content.encode()).hexdigest()[:12]

    def _update_model_metrics(self, feedback: ResponseFeedback) -> None:
        """Update cached model metrics."""
        model = feedback.model
        if model not in self.model_metrics:
            self.model_metrics[model] = {
                "count": 0,
                "total_rating": 0
            }

        self.model_metrics[model]["count"] += 1
        self.model_metrics[model]["total_rating"] += feedback.overall_rating
```

### 4.2 Training Data Collection from Feedback

```python
"""
Collect training data from user feedback for model improvement.
"""

from dataclasses import dataclass, field
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
import json


@dataclass
class TrainingExample:
    """A training example derived from feedback."""
    id: str
    prompt: str
    preferred_response: str
    rejected_response: Optional[str] = None
    source_feedback_id: str = ""
    quality_score: float = 0.0
    tags: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)
    created_at: datetime = field(default_factory=datetime.utcnow)
    reviewed: bool = False
    approved: bool = False


class TrainingDataCollector:
    """
    Collect and curate training data from user feedback.
    """

    def __init__(self):
        self.examples: List[TrainingExample] = []
        self.quality_threshold: float = 0.7
        self.review_queue: List[str] = []

    def collect_from_feedback(
        self,
        feedback: ResponseFeedback,
        original_prompt: str,
        original_response: str
    ) -> Optional[TrainingExample]:
        """Collect training example from feedback."""
        # Only collect from high-quality feedback
        if feedback.overall_rating < 4 and not feedback.preferred_response:
            return None

        # Calculate quality score
        quality_score = self._calculate_quality_score(feedback)

        if quality_score < self.quality_threshold:
            return None

        example = TrainingExample(
            id=f"train-{feedback.id}",
            prompt=original_prompt,
            preferred_response=feedback.preferred_response or original_response,
            rejected_response=original_response if feedback.preferred_response else None,
            source_feedback_id=feedback.id,
            quality_score=quality_score,
            tags=feedback.issues,
            metadata={
                "model": feedback.model,
                "user_id": feedback.user_id,
                "rating": feedback.overall_rating
            }
        )

        self.examples.append(example)

        # Add to review queue if quality is borderline
        if quality_score < 0.9:
            self.review_queue.append(example.id)

        return example

    def collect_from_comparison(
        self,
        comparison_feedback: Dict[str, ResponseFeedback],
        prompt: str,
        response_a: str,
        response_b: str
    ) -> Optional[TrainingExample]:
        """Collect training example from comparison feedback."""
        fb_a = comparison_feedback.get("a")
        fb_b = comparison_feedback.get("b")

        if not fb_a or not fb_b:
            return None

        preferred = fb_a.context.get("preferred")

        if preferred == "tie":
            return None

        if preferred == "a":
            preferred_response = response_a
            rejected_response = response_b
            source_feedback = fb_a
        else:
            preferred_response = response_b
            rejected_response = response_a
            source_feedback = fb_b

        example = TrainingExample(
            id=f"train-comp-{fb_a.request_id}",
            prompt=prompt,
            preferred_response=preferred_response,
            rejected_response=rejected_response,
            source_feedback_id=fb_a.request_id,
            quality_score=0.9,  # Comparisons are generally high quality
            tags=["comparison"],
            metadata={
                "comparison_type": "preference",
                "preferred_model": source_feedback.model
            }
        )

        self.examples.append(example)
        return example

    def review_example(
        self,
        example_id: str,
        approved: bool,
        reviewer_notes: Optional[str] = None
    ) -> None:
        """Review a training example."""
        for example in self.examples:
            if example.id == example_id:
                example.reviewed = True
                example.approved = approved
                example.metadata["reviewer_notes"] = reviewer_notes

                if example_id in self.review_queue:
                    self.review_queue.remove(example_id)

                break

    def export_training_data(
        self,
        format: str = "jsonl",
        only_approved: bool = True
    ) -> str:
        """Export training data in specified format."""
        examples_to_export = self.examples

        if only_approved:
            examples_to_export = [
                e for e in examples_to_export
                if e.approved or (not e.reviewed and e.quality_score >= 0.9)
            ]

        if format == "jsonl":
            return self._export_jsonl(examples_to_export)
        elif format == "dpo":
            return self._export_dpo_format(examples_to_export)
        elif format == "sft":
            return self._export_sft_format(examples_to_export)
        else:
            raise ValueError(f"Unknown format: {format}")

    def get_collection_stats(self) -> Dict[str, Any]:
        """Get statistics about collected training data."""
        total = len(self.examples)
        reviewed = len([e for e in self.examples if e.reviewed])
        approved = len([e for e in self.examples if e.approved])

        quality_scores = [e.quality_score for e in self.examples]

        return {
            "total_examples": total,
            "reviewed": reviewed,
            "approved": approved,
            "pending_review": len(self.review_queue),
            "average_quality_score": sum(quality_scores) / len(quality_scores) if quality_scores else 0,
            "with_rejected_response": len([e for e in self.examples if e.rejected_response]),
            "by_tag": self._count_by_tag()
        }

    def _calculate_quality_score(self, feedback: ResponseFeedback) -> float:
        """Calculate quality score for feedback."""
        score = 0.0

        # Base score from rating
        score += (feedback.overall_rating - 1) / 4 * 0.4

        # Bonus for detailed feedback
        if feedback.feedback_text and len(feedback.feedback_text) > 50:
            score += 0.1

        # Bonus for preferred response
        if feedback.preferred_response:
            score += 0.2

        # Bonus for dimension ratings
        if feedback.dimension_ratings:
            score += 0.1

        # Bonus for identified user
        if feedback.user_id:
            score += 0.1

        # Penalty for very short feedback
        if feedback.feedback_text and len(feedback.feedback_text) < 10:
            score -= 0.1

        return min(1.0, max(0.0, score))

    def _export_jsonl(self, examples: List[TrainingExample]) -> str:
        """Export as JSONL."""
        lines = []
        for example in examples:
            lines.append(json.dumps({
                "prompt": example.prompt,
                "response": example.preferred_response,
                "rejected": example.rejected_response,
                "quality_score": example.quality_score
            }))
        return "\n".join(lines)

    def _export_dpo_format(self, examples: List[TrainingExample]) -> str:
        """Export in DPO (Direct Preference Optimization) format."""
        dpo_examples = [
            e for e in examples if e.rejected_response
        ]

        lines = []
        for example in dpo_examples:
            lines.append(json.dumps({
                "prompt": example.prompt,
                "chosen": example.preferred_response,
                "rejected": example.rejected_response
            }))
        return "\n".join(lines)

    def _export_sft_format(self, examples: List[TrainingExample]) -> str:
        """Export in SFT (Supervised Fine-Tuning) format."""
        lines = []
        for example in examples:
            lines.append(json.dumps({
                "messages": [
                    {"role": "user", "content": example.prompt},
                    {"role": "assistant", "content": example.preferred_response}
                ]
            }))
        return "\n".join(lines)

    def _count_by_tag(self) -> Dict[str, int]:
        """Count examples by tag."""
        tag_counts = {}
        for example in self.examples:
            for tag in example.tags:
                tag_counts[tag] = tag_counts.get(tag, 0) + 1
        return tag_counts
```

---

## 5. Feedback Reporting and Dashboards

### 5.1 Executive Dashboard

```python
"""
Executive feedback dashboard components.
"""

from dataclasses import dataclass, field
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any


@dataclass
class DashboardMetric:
    """A metric for dashboard display."""
    name: str
    current_value: float
    previous_value: float
    unit: str
    trend: str  # up, down, stable
    change_percentage: float
    target: Optional[float] = None
    status: str = "normal"  # normal, warning, critical


@dataclass
class DashboardReport:
    """Complete dashboard report."""
    generated_at: datetime
    period_start: datetime
    period_end: datetime
    metrics: List[DashboardMetric]
    charts: Dict[str, Any]
    highlights: List[str]
    alerts: List[Dict[str, Any]]
    recommendations: List[str]


class FeedbackDashboard:
    """
    Generate executive feedback dashboards.
    """

    def __init__(
        self,
        aggregator: FeedbackAggregationEngine,
        survey_manager: SurveyManager
    ):
        self.aggregator = aggregator
        self.survey_manager = survey_manager

    def generate_executive_report(
        self,
        period_days: int = 30
    ) -> DashboardReport:
        """Generate executive dashboard report."""
        end_date = datetime.utcnow()
        start_date = end_date - timedelta(days=period_days)
        previous_start = start_date - timedelta(days=period_days)

        # Get summaries for both periods
        current_summary = self.aggregator.generate_summary(start_date, end_date)
        previous_summary = self.aggregator.generate_summary(previous_start, start_date)

        # Calculate metrics
        metrics = self._calculate_metrics(current_summary, previous_summary)

        # Generate charts data
        charts = self._generate_charts(period_days)

        # Generate highlights
        highlights = self._generate_highlights(current_summary, previous_summary)

        # Check for alerts
        alerts = self._check_alerts(current_summary)

        # Generate recommendations
        recommendations = self._generate_recommendations(current_summary, alerts)

        return DashboardReport(
            generated_at=datetime.utcnow(),
            period_start=start_date,
            period_end=end_date,
            metrics=metrics,
            charts=charts,
            highlights=highlights,
            alerts=alerts,
            recommendations=recommendations
        )

    def _calculate_metrics(
        self,
        current: FeedbackSummary,
        previous: FeedbackSummary
    ) -> List[DashboardMetric]:
        """Calculate dashboard metrics."""
        metrics = []

        # Total feedback volume
        volume_change = self._calculate_change(
            current.total_count,
            previous.total_count
        )
        metrics.append(DashboardMetric(
            name="Total Feedback",
            current_value=current.total_count,
            previous_value=previous.total_count,
            unit="items",
            trend=volume_change["trend"],
            change_percentage=volume_change["percentage"]
        ))

        # NPS Score
        if current.nps_score is not None:
            nps_change = self._calculate_change(
                current.nps_score,
                previous.nps_score or 0
            )
            metrics.append(DashboardMetric(
                name="NPS Score",
                current_value=current.nps_score,
                previous_value=previous.nps_score or 0,
                unit="points",
                trend=nps_change["trend"],
                change_percentage=nps_change["percentage"],
                target=50,
                status=self._get_nps_status(current.nps_score)
            ))

        # Average Rating
        if current.average_rating is not None:
            rating_change = self._calculate_change(
                current.average_rating,
                previous.average_rating or 0
            )
            metrics.append(DashboardMetric(
                name="Average Rating",
                current_value=current.average_rating,
                previous_value=previous.average_rating or 0,
                unit="/ 5",
                trend=rating_change["trend"],
                change_percentage=rating_change["percentage"],
                target=4.0,
                status=self._get_rating_status(current.average_rating)
            ))

        # Sentiment distribution
        positive = current.by_sentiment.get("positive", 0) + current.by_sentiment.get("very_positive", 0)
        negative = current.by_sentiment.get("negative", 0) + current.by_sentiment.get("very_negative", 0)
        total_sentiment = positive + negative + current.by_sentiment.get("neutral", 0)

        if total_sentiment > 0:
            positive_rate = (positive / total_sentiment) * 100
            prev_positive = previous.by_sentiment.get("positive", 0) + previous.by_sentiment.get("very_positive", 0)
            prev_total = sum(previous.by_sentiment.values()) or 1
            prev_rate = (prev_positive / prev_total) * 100

            sentiment_change = self._calculate_change(positive_rate, prev_rate)

            metrics.append(DashboardMetric(
                name="Positive Sentiment Rate",
                current_value=positive_rate,
                previous_value=prev_rate,
                unit="%",
                trend=sentiment_change["trend"],
                change_percentage=sentiment_change["percentage"],
                target=70.0,
                status="normal" if positive_rate >= 60 else "warning" if positive_rate >= 40 else "critical"
            ))

        return metrics

    def _generate_charts(self, period_days: int) -> Dict[str, Any]:
        """Generate chart data."""
        return {
            "volume_trend": self.aggregator.get_volume_trend(period_days),
            "sentiment_trend": self.aggregator.get_sentiment_trend(period_days),
            "category_distribution": self.aggregator.get_category_distribution(period_days)
        }

    def _generate_highlights(
        self,
        current: FeedbackSummary,
        previous: FeedbackSummary
    ) -> List[str]:
        """Generate key highlights."""
        highlights = []

        # Volume highlight
        volume_change = ((current.total_count - previous.total_count) / (previous.total_count or 1)) * 100
        if abs(volume_change) > 10:
            direction = "increased" if volume_change > 0 else "decreased"
            highlights.append(
                f"Feedback volume {direction} by {abs(volume_change):.1f}% compared to the previous period"
            )

        # NPS highlight
        if current.nps_score is not None:
            if current.nps_score >= 50:
                highlights.append(f"NPS score of {current.nps_score:.0f} indicates strong user satisfaction")
            elif current.nps_score < 0:
                highlights.append(f"NPS score of {current.nps_score:.0f} requires immediate attention")

        # Top category highlight
        if current.top_categories:
            top_cat, count = current.top_categories[0]
            highlights.append(f"'{top_cat}' was the most common feedback category with {count} mentions")

        # Emerging issues
        emerging = self.aggregator.identify_emerging_issues()
        if emerging:
            top_emerging = emerging[0]
            highlights.append(
                f"Emerging issue: '{top_emerging['category']}' increased {top_emerging['increase_ratio']:.1f}x"
            )

        return highlights

    def _check_alerts(self, summary: FeedbackSummary) -> List[Dict[str, Any]]:
        """Check for alert conditions."""
        alerts = []

        # NPS alert
        if summary.nps_score is not None and summary.nps_score < 0:
            alerts.append({
                "severity": "critical",
                "type": "nps",
                "message": f"NPS score is negative ({summary.nps_score:.0f})",
                "recommendation": "Review recent feedback for common issues"
            })

        # High negative sentiment
        negative = summary.by_sentiment.get("negative", 0) + summary.by_sentiment.get("very_negative", 0)
        total = sum(summary.by_sentiment.values()) or 1
        negative_rate = negative / total

        if negative_rate > 0.4:
            alerts.append({
                "severity": "warning",
                "type": "sentiment",
                "message": f"High negative sentiment rate ({negative_rate*100:.0f}%)",
                "recommendation": "Investigate common complaints"
            })

        # Feedback spike
        for trend in summary.trends:
            if trend.metric == "volume" and trend.change_percentage > 50:
                alerts.append({
                    "severity": "info",
                    "type": "volume",
                    "message": f"Significant increase in feedback volume ({trend.change_percentage:.0f}%)",
                    "recommendation": "Review new feedback for patterns"
                })

        return alerts

    def _generate_recommendations(
        self,
        summary: FeedbackSummary,
        alerts: List[Dict]
    ) -> List[str]:
        """Generate actionable recommendations."""
        recommendations = []

        # Based on alerts
        for alert in alerts:
            if alert["severity"] == "critical":
                recommendations.append(f"URGENT: {alert['recommendation']}")
            elif alert["severity"] == "warning":
                recommendations.append(alert['recommendation'])

        # Based on top categories
        for category, count in summary.top_categories[:3]:
            if category == "bug_report":
                recommendations.append("Review and prioritize bug reports in the backlog")
            elif category == "feature_request":
                recommendations.append("Consider feature requests for roadmap planning")
            elif category == "documentation":
                recommendations.append("Update documentation based on user feedback")
            elif category == "performance":
                recommendations.append("Investigate performance concerns and optimize")

        # Based on top issues
        if summary.top_issues:
            recommendations.append(
                f"Address the top issue affecting {summary.top_issues[0]['count']} users"
            )

        return recommendations[:5]  # Limit to top 5

    def _calculate_change(
        self,
        current: float,
        previous: float
    ) -> Dict[str, Any]:
        """Calculate change between periods."""
        if previous == 0:
            percentage = 100 if current > 0 else 0
        else:
            percentage = ((current - previous) / previous) * 100

        if percentage > 5:
            trend = "up"
        elif percentage < -5:
            trend = "down"
        else:
            trend = "stable"

        return {
            "percentage": percentage,
            "trend": trend
        }

    def _get_nps_status(self, nps: float) -> str:
        """Get status based on NPS score."""
        if nps >= 50:
            return "normal"
        elif nps >= 0:
            return "warning"
        else:
            return "critical"

    def _get_rating_status(self, rating: float) -> str:
        """Get status based on rating."""
        if rating >= 4.0:
            return "normal"
        elif rating >= 3.0:
            return "warning"
        else:
            return "critical"

    def export_report_html(self, report: DashboardReport) -> str:
        """Export report as HTML."""
        html = f"""
<!DOCTYPE html>
<html>
<head>
    <title>Feedback Dashboard Report</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 40px; }}
        .metric {{ display: inline-block; padding: 20px; margin: 10px; border: 1px solid #ddd; border-radius: 8px; }}
        .metric.critical {{ border-color: #ff4444; background: #fff0f0; }}
        .metric.warning {{ border-color: #ffaa00; background: #fffaf0; }}
        .trend-up {{ color: green; }}
        .trend-down {{ color: red; }}
        .alert {{ padding: 15px; margin: 10px 0; border-radius: 4px; }}
        .alert.critical {{ background: #ffebee; border-left: 4px solid #f44336; }}
        .alert.warning {{ background: #fff3e0; border-left: 4px solid #ff9800; }}
        .alert.info {{ background: #e3f2fd; border-left: 4px solid #2196f3; }}
    </style>
</head>
<body>
    <h1>Feedback Dashboard Report</h1>
    <p>Period: {report.period_start.strftime('%Y-%m-%d')} to {report.period_end.strftime('%Y-%m-%d')}</p>

    <h2>Key Metrics</h2>
    <div class="metrics">
"""
        for metric in report.metrics:
            trend_class = f"trend-{metric.trend}" if metric.trend != "stable" else ""
            status_class = metric.status

            html += f"""
        <div class="metric {status_class}">
            <h3>{metric.name}</h3>
            <p class="value">{metric.current_value:.1f} {metric.unit}</p>
            <p class="{trend_class}">
                {'+' if metric.change_percentage > 0 else ''}{metric.change_percentage:.1f}% vs previous
            </p>
        </div>
"""

        html += """
    </div>

    <h2>Alerts</h2>
"""
        for alert in report.alerts:
            html += f"""
    <div class="alert {alert['severity']}">
        <strong>{alert['type'].upper()}</strong>: {alert['message']}
        <br><em>Recommendation: {alert['recommendation']}</em>
    </div>
"""

        html += """
    <h2>Key Highlights</h2>
    <ul>
"""
        for highlight in report.highlights:
            html += f"        <li>{highlight}</li>\n"

        html += """
    </ul>

    <h2>Recommendations</h2>
    <ol>
"""
        for rec in report.recommendations:
            html += f"        <li>{rec}</li>\n"

        html += """
    </ol>
</body>
</html>
"""
        return html
```

---

## 6. Best Practices and Patterns

### 6.1 Feedback Collection Best Practices

```yaml
# Feedback Collection Best Practices

collection_timing:
  in_context:
    description: "Collect feedback when the experience is fresh"
    examples:
      - "Thumbs up/down immediately after response"
      - "Quick survey after completing a task"
      - "NPS at natural break points"

  delayed:
    description: "Allow time for reflection"
    examples:
      - "Weekly satisfaction surveys"
      - "Monthly feature feedback"
      - "Quarterly relationship surveys"

feedback_types:
  quantitative:
    - ratings: "1-5 star ratings, thumbs up/down"
    - scales: "NPS (0-10), CSAT (1-5), CES (1-7)"
    - metrics: "Completion rates, time on task"

  qualitative:
    - open_text: "Free-form feedback"
    - guided: "Structured follow-up questions"
    - contextual: "Feedback with context (session, user)"

user_experience:
  minimize_friction:
    - "Keep surveys short (< 3 minutes)"
    - "Use progressive disclosure"
    - "Make feedback optional but visible"
    - "Respect user time and attention"

  incentivize_thoughtfully:
    - "Thank users for feedback"
    - "Show impact of their feedback"
    - "Consider small rewards for detailed feedback"
    - "Avoid over-incentivizing (quality > quantity)"

  close_the_loop:
    - "Acknowledge receipt of feedback"
    - "Provide status updates when possible"
    - "Communicate when feedback leads to changes"

sampling_strategy:
  representative:
    - "Sample across user segments"
    - "Include both active and less active users"
    - "Balance heavy users with casual users"

  targeted:
    - "Focus on specific features or experiences"
    - "Target users of new features"
    - "Follow up on support interactions"

privacy_and_trust:
  transparency:
    - "Explain how feedback will be used"
    - "Allow anonymous feedback"
    - "Be clear about data retention"

  consent:
    - "Obtain clear consent for feedback collection"
    - "Make opt-out easy"
    - "Respect user preferences"
```

### 6.2 Feedback Analysis Patterns

```python
"""
Common patterns for feedback analysis.
"""

from typing import Dict, List, Any
from datetime import datetime, timedelta


class FeedbackAnalysisPatterns:
    """
    Common patterns for analyzing feedback data.
    """

    @staticmethod
    def cohort_analysis(
        feedback: List[Dict],
        cohort_field: str,
        metric: str
    ) -> Dict[str, Any]:
        """
        Analyze feedback by user cohorts.

        Example:
            cohort_analysis(feedback, "signup_month", "nps_score")
        """
        cohorts = {}

        for item in feedback:
            cohort = item.get(cohort_field, "unknown")
            if cohort not in cohorts:
                cohorts[cohort] = []
            cohorts[cohort].append(item.get(metric, 0))

        return {
            cohort: {
                "count": len(values),
                "average": sum(values) / len(values) if values else 0,
                "min": min(values) if values else 0,
                "max": max(values) if values else 0
            }
            for cohort, values in cohorts.items()
        }

    @staticmethod
    def funnel_analysis(
        events: List[Dict],
        funnel_steps: List[str]
    ) -> Dict[str, Any]:
        """
        Analyze user funnel progression.

        Example:
            funnel_analysis(events, ["view", "interact", "complete", "feedback"])
        """
        step_counts = {step: 0 for step in funnel_steps}
        users_by_step = {step: set() for step in funnel_steps}

        for event in events:
            step = event.get("step")
            user_id = event.get("user_id")

            if step in step_counts:
                step_counts[step] += 1
                if user_id:
                    users_by_step[step].add(user_id)

        # Calculate conversion rates
        conversions = {}
        for i in range(len(funnel_steps) - 1):
            current = len(users_by_step[funnel_steps[i]])
            next_step = len(users_by_step[funnel_steps[i + 1]])

            conversions[f"{funnel_steps[i]}_to_{funnel_steps[i+1]}"] = (
                next_step / current if current > 0 else 0
            )

        return {
            "step_counts": step_counts,
            "unique_users": {k: len(v) for k, v in users_by_step.items()},
            "conversion_rates": conversions
        }

    @staticmethod
    def sentiment_journey_mapping(
        feedback_timeline: List[Dict]
    ) -> Dict[str, Any]:
        """
        Map sentiment changes over user journey.

        Example:
            sentiment_journey_mapping(user_feedback_timeline)
        """
        journey_stages = ["onboarding", "first_value", "regular_use", "power_user"]
        stage_sentiments = {stage: [] for stage in journey_stages}

        for item in feedback_timeline:
            stage = item.get("journey_stage", "unknown")
            sentiment = item.get("sentiment_score", 0)

            if stage in stage_sentiments:
                stage_sentiments[stage].append(sentiment)

        return {
            stage: {
                "average_sentiment": sum(scores) / len(scores) if scores else 0,
                "count": len(scores),
                "trend": "positive" if sum(scores) / len(scores) > 0 else "negative"
            }
            for stage, scores in stage_sentiments.items()
        }

    @staticmethod
    def feature_impact_analysis(
        feedback_before: List[Dict],
        feedback_after: List[Dict],
        metrics: List[str]
    ) -> Dict[str, Any]:
        """
        Analyze impact of feature changes on feedback metrics.

        Example:
            feature_impact_analysis(before, after, ["satisfaction", "ease_of_use"])
        """
        results = {}

        for metric in metrics:
            before_values = [f.get(metric, 0) for f in feedback_before]
            after_values = [f.get(metric, 0) for f in feedback_after]

            before_avg = sum(before_values) / len(before_values) if before_values else 0
            after_avg = sum(after_values) / len(after_values) if after_values else 0

            change = after_avg - before_avg
            change_pct = (change / before_avg * 100) if before_avg else 0

            results[metric] = {
                "before": before_avg,
                "after": after_avg,
                "change": change,
                "change_percentage": change_pct,
                "significant": abs(change_pct) > 10  # Simplified significance
            }

        return results
```

---

## Summary

This User Feedback and Iteration Guide provides comprehensive frameworks for:

1. **Feedback Collection Infrastructure** - Multi-channel collection with in-app, API, and implicit feedback mechanisms

2. **Survey Management** - NPS, CSAT, CES, and custom surveys with targeting and analysis

3. **Feedback Analysis** - Sentiment analysis, classification, and trend identification

4. **Feedback-Driven Development** - Converting feedback to actions with prioritization and tracking

5. **Loop Closure** - Communicating with users about their feedback status

6. **LLM-Specific Feedback** - Response quality tracking and training data collection

7. **Reporting and Dashboards** - Executive dashboards with metrics, alerts, and recommendations

Key principles:
- **Multi-channel**: Collect feedback from all user touchpoints
- **Timely**: Capture feedback when experiences are fresh
- **Actionable**: Convert feedback into concrete improvements
- **Closed-loop**: Always communicate back to users
- **Data-driven**: Use analytics to identify patterns and priorities

---

> **Navigation**
> [← 12.3 Documentation](12.3_developer_documentation_guide.md) | **[Index](../README.md#15-repository-structure)** | [13.1 Incident Response →](../13_operations_reliability/13.1_incident_response_guide.md)
