> **Navigation** | [← 12.1 Prompt Engineering](12.1_prompt_engineering_guide.md) | [12.3 Developer Documentation →](12.3_developer_documentation_guide.md)
>
> | | |
> |---|---|
> | **Prerequisites** | [9.3 API Design](../09_inference_serving/9.3_api_design_llm_services_guide.md) &#124; Multiple languages &#124; Async programming |
> | **Related** | [12.3 Documentation](12.3_developer_documentation_guide.md) &#124; [9.3 API Design](../09_inference_serving/9.3_api_design_llm_services_guide.md) |
> | **Next** | [12.3 Developer Documentation](12.3_developer_documentation_guide.md) |

# Document 12.2: SDK & Client Library Guide

## Purpose

Designing and implementing client libraries for LLM services that provide excellent developer experience, robust error handling, and seamless integration across multiple programming languages.

## Prerequisites

- Understanding of LLM API patterns (9.3)
- Familiarity with multiple programming languages (Python, TypeScript, Go)
- Experience with SDK design principles
- Knowledge of async programming patterns

## Table of Contents

1. [SDK Design Principles](#1-sdk-design-principles)
2. [Core Features](#2-core-features)
3. [Language-Specific SDKs](#3-language-specific-sdks)
4. [SDK Distribution](#4-sdk-distribution)
5. [Testing & Quality](#5-testing--quality)
6. [Implementation](#6-implementation)

---

## 1. SDK Design Principles

### 1.1 Design Philosophy

```python
"""
Core design principles for LLM SDKs.
"""
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any, TypeVar, Generic
from enum import Enum


class SDKDesignPrinciple(Enum):
    """Core design principles."""
    INTUITIVE = "intuitive"           # Easy to discover and use
    CONSISTENT = "consistent"         # Consistent patterns across features
    RESILIENT = "resilient"           # Handles failures gracefully
    OBSERVABLE = "observable"         # Provides visibility into operations
    EXTENSIBLE = "extensible"         # Easy to extend without modifying


@dataclass
class SDKConfiguration:
    """Base configuration for SDK clients."""
    # Authentication
    api_key: Optional[str] = None
    access_token: Optional[str] = None

    # Endpoint
    base_url: str = "https://api.example.com"
    api_version: str = "v1"

    # Timeouts (in seconds)
    connect_timeout: float = 10.0
    read_timeout: float = 60.0
    write_timeout: float = 60.0

    # Retry configuration
    max_retries: int = 3
    retry_delay: float = 1.0
    retry_backoff_factor: float = 2.0
    retry_on_status: List[int] = field(default_factory=lambda: [429, 500, 502, 503, 504])

    # Limits
    max_concurrent_requests: int = 100

    # Observability
    enable_logging: bool = True
    log_level: str = "INFO"
    enable_metrics: bool = True

    # Features
    enable_streaming: bool = True
    enable_caching: bool = False
    cache_ttl: int = 300


class SDKError(Exception):
    """Base SDK error."""
    def __init__(
        self,
        message: str,
        code: Optional[str] = None,
        status_code: Optional[int] = None,
        request_id: Optional[str] = None,
        details: Optional[Dict] = None
    ):
        super().__init__(message)
        self.message = message
        self.code = code
        self.status_code = status_code
        self.request_id = request_id
        self.details = details or {}

    def __str__(self):
        parts = [self.message]
        if self.code:
            parts.append(f"Code: {self.code}")
        if self.status_code:
            parts.append(f"Status: {self.status_code}")
        if self.request_id:
            parts.append(f"Request ID: {self.request_id}")
        return " | ".join(parts)


class AuthenticationError(SDKError):
    """Authentication failed."""
    pass


class RateLimitError(SDKError):
    """Rate limit exceeded."""
    def __init__(self, message: str, retry_after: Optional[float] = None, **kwargs):
        super().__init__(message, **kwargs)
        self.retry_after = retry_after


class ValidationError(SDKError):
    """Request validation failed."""
    pass


class ServerError(SDKError):
    """Server-side error."""
    pass


class TimeoutError(SDKError):
    """Request timed out."""
    pass


class ContentFilterError(SDKError):
    """Content was filtered by safety systems."""
    pass
```

### 1.2 Interface Design

```python
"""
Interface design patterns for intuitive SDK usage.
"""
from typing import Dict, List, Optional, Any, Union, Iterator, AsyncIterator
from dataclasses import dataclass, field
from enum import Enum
import json


class Role(Enum):
    """Message roles."""
    SYSTEM = "system"
    USER = "user"
    ASSISTANT = "assistant"
    TOOL = "tool"


@dataclass
class Message:
    """Chat message."""
    role: Role
    content: str
    name: Optional[str] = None
    tool_calls: Optional[List[Dict]] = None
    tool_call_id: Optional[str] = None

    def to_dict(self) -> Dict:
        d = {"role": self.role.value, "content": self.content}
        if self.name:
            d["name"] = self.name
        if self.tool_calls:
            d["tool_calls"] = self.tool_calls
        if self.tool_call_id:
            d["tool_call_id"] = self.tool_call_id
        return d


@dataclass
class Tool:
    """Tool definition for function calling."""
    name: str
    description: str
    parameters: Dict[str, Any]

    def to_dict(self) -> Dict:
        return {
            "type": "function",
            "function": {
                "name": self.name,
                "description": self.description,
                "parameters": self.parameters
            }
        }


@dataclass
class Usage:
    """Token usage information."""
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int


@dataclass
class Choice:
    """Response choice."""
    index: int
    message: Message
    finish_reason: str


@dataclass
class ChatCompletion:
    """Chat completion response."""
    id: str
    model: str
    choices: List[Choice]
    usage: Usage
    created: int

    def __str__(self) -> str:
        if self.choices:
            return self.choices[0].message.content
        return ""


@dataclass
class ChatCompletionChunk:
    """Streaming chat completion chunk."""
    id: str
    model: str
    delta_content: Optional[str]
    delta_tool_calls: Optional[List[Dict]]
    finish_reason: Optional[str]


# Fluent builder pattern
class ChatCompletionBuilder:
    """
    Fluent builder for chat completions.
    Provides intuitive, discoverable API.
    """

    def __init__(self, client: 'LLMClient'):
        self._client = client
        self._model: str = ""
        self._messages: List[Message] = []
        self._temperature: Optional[float] = None
        self._max_tokens: Optional[int] = None
        self._top_p: Optional[float] = None
        self._tools: Optional[List[Tool]] = None
        self._tool_choice: Optional[str] = None
        self._response_format: Optional[Dict] = None
        self._stream: bool = False

    def model(self, model: str) -> 'ChatCompletionBuilder':
        """Set the model."""
        self._model = model
        return self

    def system(self, content: str) -> 'ChatCompletionBuilder':
        """Add system message."""
        self._messages.append(Message(role=Role.SYSTEM, content=content))
        return self

    def user(self, content: str) -> 'ChatCompletionBuilder':
        """Add user message."""
        self._messages.append(Message(role=Role.USER, content=content))
        return self

    def assistant(self, content: str) -> 'ChatCompletionBuilder':
        """Add assistant message."""
        self._messages.append(Message(role=Role.ASSISTANT, content=content))
        return self

    def messages(self, messages: List[Message]) -> 'ChatCompletionBuilder':
        """Set all messages."""
        self._messages = messages
        return self

    def temperature(self, temp: float) -> 'ChatCompletionBuilder':
        """Set temperature (0-2)."""
        if not 0 <= temp <= 2:
            raise ValueError("Temperature must be between 0 and 2")
        self._temperature = temp
        return self

    def max_tokens(self, tokens: int) -> 'ChatCompletionBuilder':
        """Set max tokens."""
        if tokens <= 0:
            raise ValueError("max_tokens must be positive")
        self._max_tokens = tokens
        return self

    def top_p(self, p: float) -> 'ChatCompletionBuilder':
        """Set top_p (0-1)."""
        if not 0 <= p <= 1:
            raise ValueError("top_p must be between 0 and 1")
        self._top_p = p
        return self

    def tools(self, tools: List[Tool]) -> 'ChatCompletionBuilder':
        """Add tools for function calling."""
        self._tools = tools
        return self

    def tool_choice(self, choice: str) -> 'ChatCompletionBuilder':
        """Set tool choice: 'auto', 'none', or specific tool."""
        self._tool_choice = choice
        return self

    def json_response(self) -> 'ChatCompletionBuilder':
        """Request JSON response format."""
        self._response_format = {"type": "json_object"}
        return self

    def stream(self) -> 'ChatCompletionBuilder':
        """Enable streaming."""
        self._stream = True
        return self

    def build(self) -> Dict:
        """Build the request payload."""
        if not self._model:
            raise ValueError("Model is required")
        if not self._messages:
            raise ValueError("At least one message is required")

        payload = {
            "model": self._model,
            "messages": [m.to_dict() for m in self._messages],
            "stream": self._stream
        }

        if self._temperature is not None:
            payload["temperature"] = self._temperature
        if self._max_tokens is not None:
            payload["max_tokens"] = self._max_tokens
        if self._top_p is not None:
            payload["top_p"] = self._top_p
        if self._tools:
            payload["tools"] = [t.to_dict() for t in self._tools]
        if self._tool_choice:
            payload["tool_choice"] = self._tool_choice
        if self._response_format:
            payload["response_format"] = self._response_format

        return payload

    async def execute(self) -> ChatCompletion:
        """Execute the request."""
        return await self._client.chat_completions(self.build())

    async def execute_stream(self) -> AsyncIterator[ChatCompletionChunk]:
        """Execute with streaming."""
        self._stream = True
        async for chunk in self._client.chat_completions_stream(self.build()):
            yield chunk
```

---

## 2. Core Features

### 2.1 Request Building and Validation

```python
"""
Request building with validation.
"""
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any, Callable
from enum import Enum
import re


class ParameterType(Enum):
    """Parameter types for validation."""
    STRING = "string"
    INTEGER = "integer"
    FLOAT = "float"
    BOOLEAN = "boolean"
    LIST = "list"
    DICT = "dict"
    ENUM = "enum"


@dataclass
class ParameterSpec:
    """Specification for a parameter."""
    name: str
    param_type: ParameterType
    required: bool = False
    default: Any = None
    min_value: Optional[float] = None
    max_value: Optional[float] = None
    min_length: Optional[int] = None
    max_length: Optional[int] = None
    pattern: Optional[str] = None
    enum_values: Optional[List[Any]] = None
    custom_validator: Optional[Callable[[Any], bool]] = None
    description: str = ""


class RequestValidator:
    """Validates request parameters before sending."""

    def __init__(self, specs: List[ParameterSpec]):
        self.specs = {s.name: s for s in specs}

    def validate(self, params: Dict[str, Any]) -> tuple[bool, List[str]]:
        """
        Validate parameters.

        Returns: (is_valid, list of error messages)
        """
        errors = []

        # Check required parameters
        for name, spec in self.specs.items():
            if spec.required and name not in params:
                errors.append(f"Missing required parameter: {name}")

        # Validate each parameter
        for name, value in params.items():
            if name not in self.specs:
                continue  # Unknown parameters are passed through

            spec = self.specs[name]
            param_errors = self._validate_parameter(spec, value)
            errors.extend(param_errors)

        return len(errors) == 0, errors

    def _validate_parameter(self, spec: ParameterSpec, value: Any) -> List[str]:
        """Validate a single parameter."""
        errors = []

        # Type check
        if not self._check_type(spec.param_type, value):
            errors.append(f"{spec.name}: expected {spec.param_type.value}, got {type(value).__name__}")
            return errors  # Skip other validations if type is wrong

        # Range check for numbers
        if spec.param_type in [ParameterType.INTEGER, ParameterType.FLOAT]:
            if spec.min_value is not None and value < spec.min_value:
                errors.append(f"{spec.name}: must be >= {spec.min_value}")
            if spec.max_value is not None and value > spec.max_value:
                errors.append(f"{spec.name}: must be <= {spec.max_value}")

        # Length check for strings and lists
        if spec.param_type in [ParameterType.STRING, ParameterType.LIST]:
            if spec.min_length is not None and len(value) < spec.min_length:
                errors.append(f"{spec.name}: length must be >= {spec.min_length}")
            if spec.max_length is not None and len(value) > spec.max_length:
                errors.append(f"{spec.name}: length must be <= {spec.max_length}")

        # Pattern check for strings
        if spec.param_type == ParameterType.STRING and spec.pattern:
            if not re.match(spec.pattern, value):
                errors.append(f"{spec.name}: does not match pattern {spec.pattern}")

        # Enum check
        if spec.param_type == ParameterType.ENUM and spec.enum_values:
            if value not in spec.enum_values:
                errors.append(f"{spec.name}: must be one of {spec.enum_values}")

        # Custom validation
        if spec.custom_validator:
            if not spec.custom_validator(value):
                errors.append(f"{spec.name}: failed custom validation")

        return errors

    def _check_type(self, param_type: ParameterType, value: Any) -> bool:
        """Check if value matches expected type."""
        type_map = {
            ParameterType.STRING: str,
            ParameterType.INTEGER: int,
            ParameterType.FLOAT: (int, float),
            ParameterType.BOOLEAN: bool,
            ParameterType.LIST: list,
            ParameterType.DICT: dict,
            ParameterType.ENUM: (str, int)
        }
        expected = type_map.get(param_type)
        return isinstance(value, expected)


# Chat completions validation
CHAT_COMPLETION_SPECS = [
    ParameterSpec("model", ParameterType.STRING, required=True),
    ParameterSpec("messages", ParameterType.LIST, required=True, min_length=1),
    ParameterSpec("temperature", ParameterType.FLOAT, min_value=0.0, max_value=2.0),
    ParameterSpec("max_tokens", ParameterType.INTEGER, min_value=1),
    ParameterSpec("top_p", ParameterType.FLOAT, min_value=0.0, max_value=1.0),
    ParameterSpec("n", ParameterType.INTEGER, min_value=1, max_value=10),
    ParameterSpec("stream", ParameterType.BOOLEAN),
    ParameterSpec("stop", ParameterType.LIST, max_length=4)
]
```

### 2.2 Response Handling

```python
"""
Response handling with streaming support.
"""
from typing import Dict, List, Optional, Any, Iterator, AsyncIterator
from dataclasses import dataclass
import json
import asyncio


@dataclass
class APIResponse:
    """Wrapper for API responses."""
    status_code: int
    headers: Dict[str, str]
    data: Any
    request_id: Optional[str] = None
    latency_ms: Optional[float] = None

    @property
    def is_success(self) -> bool:
        return 200 <= self.status_code < 300


class ResponseParser:
    """Parses API responses into typed objects."""

    @staticmethod
    def parse_chat_completion(data: Dict) -> ChatCompletion:
        """Parse chat completion response."""
        choices = []
        for choice_data in data.get("choices", []):
            message_data = choice_data.get("message", {})
            message = Message(
                role=Role(message_data.get("role", "assistant")),
                content=message_data.get("content", ""),
                tool_calls=message_data.get("tool_calls")
            )
            choice = Choice(
                index=choice_data.get("index", 0),
                message=message,
                finish_reason=choice_data.get("finish_reason", "")
            )
            choices.append(choice)

        usage_data = data.get("usage", {})
        usage = Usage(
            prompt_tokens=usage_data.get("prompt_tokens", 0),
            completion_tokens=usage_data.get("completion_tokens", 0),
            total_tokens=usage_data.get("total_tokens", 0)
        )

        return ChatCompletion(
            id=data.get("id", ""),
            model=data.get("model", ""),
            choices=choices,
            usage=usage,
            created=data.get("created", 0)
        )

    @staticmethod
    def parse_streaming_chunk(line: str) -> Optional[ChatCompletionChunk]:
        """Parse a streaming chunk."""
        if not line.startswith("data: "):
            return None

        data_str = line[6:]  # Remove "data: " prefix
        if data_str == "[DONE]":
            return None

        try:
            data = json.loads(data_str)
            delta = data.get("choices", [{}])[0].get("delta", {})

            return ChatCompletionChunk(
                id=data.get("id", ""),
                model=data.get("model", ""),
                delta_content=delta.get("content"),
                delta_tool_calls=delta.get("tool_calls"),
                finish_reason=data.get("choices", [{}])[0].get("finish_reason")
            )
        except json.JSONDecodeError:
            return None


class StreamingResponse:
    """Handles streaming responses."""

    def __init__(self, response_iterator: AsyncIterator[bytes]):
        self._iterator = response_iterator
        self._buffer = ""
        self._chunks: List[ChatCompletionChunk] = []

    async def __aiter__(self) -> AsyncIterator[ChatCompletionChunk]:
        """Iterate over streaming chunks."""
        async for data in self._iterator:
            self._buffer += data.decode("utf-8")

            while "\n" in self._buffer:
                line, self._buffer = self._buffer.split("\n", 1)
                line = line.strip()

                if not line:
                    continue

                chunk = ResponseParser.parse_streaming_chunk(line)
                if chunk:
                    self._chunks.append(chunk)
                    yield chunk

    async def collect(self) -> str:
        """Collect all chunks into complete response."""
        content_parts = []
        async for chunk in self:
            if chunk.delta_content:
                content_parts.append(chunk.delta_content)
        return "".join(content_parts)

    @property
    def chunks(self) -> List[ChatCompletionChunk]:
        """Get all received chunks."""
        return self._chunks
```

### 2.3 Resilience Features

```python
"""
Retry logic, circuit breakers, and error handling.
"""
from typing import Optional, Callable, Any, List, Type
from dataclasses import dataclass
from enum import Enum
import asyncio
import random
import time


class RetryStrategy(Enum):
    """Retry strategies."""
    EXPONENTIAL = "exponential"
    LINEAR = "linear"
    FIXED = "fixed"


@dataclass
class RetryConfig:
    """Retry configuration."""
    max_retries: int = 3
    initial_delay: float = 1.0
    max_delay: float = 60.0
    backoff_factor: float = 2.0
    strategy: RetryStrategy = RetryStrategy.EXPONENTIAL
    jitter: bool = True
    retry_on_exceptions: List[Type[Exception]] = None
    retry_on_status_codes: List[int] = None

    def __post_init__(self):
        if self.retry_on_exceptions is None:
            self.retry_on_exceptions = [TimeoutError, ServerError]
        if self.retry_on_status_codes is None:
            self.retry_on_status_codes = [429, 500, 502, 503, 504]


class RetryHandler:
    """Handles retry logic with backoff."""

    def __init__(self, config: RetryConfig):
        self.config = config

    def calculate_delay(self, attempt: int) -> float:
        """Calculate delay for given attempt."""
        if self.config.strategy == RetryStrategy.EXPONENTIAL:
            delay = self.config.initial_delay * (self.config.backoff_factor ** attempt)
        elif self.config.strategy == RetryStrategy.LINEAR:
            delay = self.config.initial_delay * (attempt + 1)
        else:  # FIXED
            delay = self.config.initial_delay

        # Apply max delay cap
        delay = min(delay, self.config.max_delay)

        # Add jitter
        if self.config.jitter:
            delay = delay * (0.5 + random.random())

        return delay

    def should_retry(
        self,
        attempt: int,
        exception: Optional[Exception] = None,
        status_code: Optional[int] = None
    ) -> bool:
        """Determine if request should be retried."""
        if attempt >= self.config.max_retries:
            return False

        if exception:
            for exc_type in self.config.retry_on_exceptions:
                if isinstance(exception, exc_type):
                    return True

        if status_code and status_code in self.config.retry_on_status_codes:
            return True

        return False

    async def execute_with_retry(
        self,
        func: Callable,
        *args,
        **kwargs
    ) -> Any:
        """Execute function with retry logic."""
        last_exception = None

        for attempt in range(self.config.max_retries + 1):
            try:
                return await func(*args, **kwargs)
            except Exception as e:
                last_exception = e

                if not self.should_retry(attempt, exception=e):
                    raise

                delay = self.calculate_delay(attempt)
                await asyncio.sleep(delay)

        raise last_exception


class CircuitBreakerState(Enum):
    """Circuit breaker states."""
    CLOSED = "closed"      # Normal operation
    OPEN = "open"          # Failing, reject requests
    HALF_OPEN = "half_open"  # Testing if recovered


@dataclass
class CircuitBreakerConfig:
    """Circuit breaker configuration."""
    failure_threshold: int = 5
    success_threshold: int = 2
    timeout: float = 60.0  # Seconds before trying again


class CircuitBreaker:
    """Circuit breaker for fault tolerance."""

    def __init__(self, config: CircuitBreakerConfig):
        self.config = config
        self.state = CircuitBreakerState.CLOSED
        self.failure_count = 0
        self.success_count = 0
        self.last_failure_time: Optional[float] = None

    def record_success(self):
        """Record a successful call."""
        if self.state == CircuitBreakerState.HALF_OPEN:
            self.success_count += 1
            if self.success_count >= self.config.success_threshold:
                self._close()
        elif self.state == CircuitBreakerState.CLOSED:
            self.failure_count = 0

    def record_failure(self):
        """Record a failed call."""
        self.failure_count += 1
        self.last_failure_time = time.time()

        if self.state == CircuitBreakerState.HALF_OPEN:
            self._open()
        elif self.failure_count >= self.config.failure_threshold:
            self._open()

    def can_execute(self) -> bool:
        """Check if request can proceed."""
        if self.state == CircuitBreakerState.CLOSED:
            return True
        elif self.state == CircuitBreakerState.OPEN:
            if self._timeout_elapsed():
                self._half_open()
                return True
            return False
        else:  # HALF_OPEN
            return True

    def _open(self):
        self.state = CircuitBreakerState.OPEN
        self.success_count = 0

    def _close(self):
        self.state = CircuitBreakerState.CLOSED
        self.failure_count = 0
        self.success_count = 0

    def _half_open(self):
        self.state = CircuitBreakerState.HALF_OPEN
        self.success_count = 0

    def _timeout_elapsed(self) -> bool:
        if self.last_failure_time is None:
            return True
        return time.time() - self.last_failure_time >= self.config.timeout

    async def execute(self, func: Callable, *args, **kwargs) -> Any:
        """Execute function with circuit breaker protection."""
        if not self.can_execute():
            raise SDKError(
                "Circuit breaker is open",
                code="CIRCUIT_OPEN"
            )

        try:
            result = await func(*args, **kwargs)
            self.record_success()
            return result
        except Exception as e:
            self.record_failure()
            raise
```

---

## 3. Language-Specific SDKs

### 3.1 Python SDK

```python
"""
Complete Python SDK implementation.
"""
from typing import Dict, List, Optional, Any, Union, AsyncIterator
import httpx
import asyncio
from contextlib import asynccontextmanager
import logging


class LLMClient:
    """
    Python SDK for LLM API.

    Usage:
        client = LLMClient(api_key="your-key")

        # Simple usage
        response = await client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": "Hello!"}]
        )

        # Fluent builder
        response = await (
            client.chat.completions
            .model("gpt-4")
            .system("You are a helpful assistant")
            .user("Hello!")
            .execute()
        )

        # Streaming
        async for chunk in client.chat.completions.create_stream(...):
            print(chunk.delta_content, end="")
    """

    def __init__(
        self,
        api_key: Optional[str] = None,
        base_url: str = "https://api.example.com/v1",
        timeout: float = 60.0,
        max_retries: int = 3,
        **kwargs
    ):
        self.api_key = api_key
        self.base_url = base_url.rstrip("/")
        self.timeout = timeout

        self._config = SDKConfiguration(
            api_key=api_key,
            base_url=base_url,
            read_timeout=timeout,
            max_retries=max_retries,
            **kwargs
        )

        self._http_client: Optional[httpx.AsyncClient] = None
        self._retry_handler = RetryHandler(RetryConfig(max_retries=max_retries))
        self._circuit_breaker = CircuitBreaker(CircuitBreakerConfig())

        # Initialize resource namespaces
        self.chat = ChatNamespace(self)
        self.embeddings = EmbeddingsNamespace(self)
        self.models = ModelsNamespace(self)

        self._logger = logging.getLogger(__name__)

    async def __aenter__(self) -> 'LLMClient':
        """Async context manager entry."""
        await self._ensure_client()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit."""
        await self.close()

    async def _ensure_client(self):
        """Ensure HTTP client is initialized."""
        if self._http_client is None:
            self._http_client = httpx.AsyncClient(
                base_url=self.base_url,
                timeout=httpx.Timeout(self.timeout),
                headers=self._default_headers()
            )

    async def close(self):
        """Close the client."""
        if self._http_client:
            await self._http_client.aclose()
            self._http_client = None

    def _default_headers(self) -> Dict[str, str]:
        """Get default headers."""
        headers = {
            "Content-Type": "application/json",
            "User-Agent": "llm-python-sdk/1.0.0"
        }
        if self.api_key:
            headers["Authorization"] = f"Bearer {self.api_key}"
        return headers

    async def _request(
        self,
        method: str,
        path: str,
        **kwargs
    ) -> APIResponse:
        """Make an API request with retries and circuit breaker."""
        await self._ensure_client()

        async def do_request():
            response = await self._http_client.request(method, path, **kwargs)

            # Parse error responses
            if not 200 <= response.status_code < 300:
                self._handle_error_response(response)

            return APIResponse(
                status_code=response.status_code,
                headers=dict(response.headers),
                data=response.json() if response.content else None,
                request_id=response.headers.get("x-request-id")
            )

        # Execute with circuit breaker and retry
        return await self._circuit_breaker.execute(
            self._retry_handler.execute_with_retry,
            do_request
        )

    async def _request_stream(
        self,
        method: str,
        path: str,
        **kwargs
    ) -> AsyncIterator[bytes]:
        """Make a streaming request."""
        await self._ensure_client()

        async with self._http_client.stream(method, path, **kwargs) as response:
            if not 200 <= response.status_code < 300:
                content = await response.aread()
                self._handle_error_response(response)

            async for chunk in response.aiter_bytes():
                yield chunk

    def _handle_error_response(self, response: httpx.Response):
        """Handle error response."""
        try:
            error_data = response.json()
        except:
            error_data = {"message": response.text}

        message = error_data.get("error", {}).get("message", "Unknown error")
        code = error_data.get("error", {}).get("code")
        request_id = response.headers.get("x-request-id")

        if response.status_code == 401:
            raise AuthenticationError(message, code=code, request_id=request_id)
        elif response.status_code == 429:
            retry_after = float(response.headers.get("retry-after", 60))
            raise RateLimitError(
                message, code=code, request_id=request_id, retry_after=retry_after
            )
        elif response.status_code == 400:
            raise ValidationError(message, code=code, request_id=request_id)
        elif response.status_code >= 500:
            raise ServerError(
                message, code=code, status_code=response.status_code, request_id=request_id
            )
        else:
            raise SDKError(
                message, code=code, status_code=response.status_code, request_id=request_id
            )


class ChatNamespace:
    """Chat API namespace."""

    def __init__(self, client: LLMClient):
        self._client = client
        self.completions = CompletionsResource(client)


class CompletionsResource:
    """Chat completions resource."""

    def __init__(self, client: LLMClient):
        self._client = client
        self._validator = RequestValidator(CHAT_COMPLETION_SPECS)

    async def create(
        self,
        model: str,
        messages: List[Dict[str, str]],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        top_p: Optional[float] = None,
        n: int = 1,
        stream: bool = False,
        stop: Optional[List[str]] = None,
        tools: Optional[List[Dict]] = None,
        tool_choice: Optional[str] = None,
        response_format: Optional[Dict] = None,
        **kwargs
    ) -> ChatCompletion:
        """Create a chat completion."""
        payload = {
            "model": model,
            "messages": messages,
            "n": n,
            "stream": stream
        }

        if temperature is not None:
            payload["temperature"] = temperature
        if max_tokens is not None:
            payload["max_tokens"] = max_tokens
        if top_p is not None:
            payload["top_p"] = top_p
        if stop is not None:
            payload["stop"] = stop
        if tools is not None:
            payload["tools"] = tools
        if tool_choice is not None:
            payload["tool_choice"] = tool_choice
        if response_format is not None:
            payload["response_format"] = response_format

        payload.update(kwargs)

        # Validate
        is_valid, errors = self._validator.validate(payload)
        if not is_valid:
            raise ValidationError(f"Validation failed: {'; '.join(errors)}")

        response = await self._client._request("POST", "/chat/completions", json=payload)
        return ResponseParser.parse_chat_completion(response.data)

    async def create_stream(
        self,
        model: str,
        messages: List[Dict[str, str]],
        **kwargs
    ) -> AsyncIterator[ChatCompletionChunk]:
        """Create a streaming chat completion."""
        payload = {
            "model": model,
            "messages": messages,
            "stream": True,
            **kwargs
        }

        stream = self._client._request_stream("POST", "/chat/completions", json=payload)
        async for chunk in StreamingResponse(stream):
            yield chunk

    def builder(self) -> ChatCompletionBuilder:
        """Get a fluent builder."""
        return ChatCompletionBuilder(self._client)

    # Convenience: allow direct builder-style access
    def model(self, model: str) -> ChatCompletionBuilder:
        """Start building with model."""
        return self.builder().model(model)


class EmbeddingsNamespace:
    """Embeddings API namespace."""

    def __init__(self, client: LLMClient):
        self._client = client

    async def create(
        self,
        model: str,
        input: Union[str, List[str]],
        encoding_format: str = "float",
        dimensions: Optional[int] = None
    ) -> Dict:
        """Create embeddings."""
        payload = {
            "model": model,
            "input": input if isinstance(input, list) else [input],
            "encoding_format": encoding_format
        }
        if dimensions:
            payload["dimensions"] = dimensions

        response = await self._client._request("POST", "/embeddings", json=payload)
        return response.data


class ModelsNamespace:
    """Models API namespace."""

    def __init__(self, client: LLMClient):
        self._client = client

    async def list(self) -> List[Dict]:
        """List available models."""
        response = await self._client._request("GET", "/models")
        return response.data.get("data", [])

    async def get(self, model_id: str) -> Dict:
        """Get model details."""
        response = await self._client._request("GET", f"/models/{model_id}")
        return response.data
```

### 3.2 TypeScript SDK

```typescript
/**
 * TypeScript SDK for LLM API.
 */

// Types
export interface Message {
  role: 'system' | 'user' | 'assistant' | 'tool';
  content: string;
  name?: string;
  toolCalls?: ToolCall[];
  toolCallId?: string;
}

export interface ToolCall {
  id: string;
  type: 'function';
  function: {
    name: string;
    arguments: string;
  };
}

export interface Tool {
  type: 'function';
  function: {
    name: string;
    description: string;
    parameters: Record<string, unknown>;
  };
}

export interface ChatCompletionRequest {
  model: string;
  messages: Message[];
  temperature?: number;
  maxTokens?: number;
  topP?: number;
  n?: number;
  stream?: boolean;
  stop?: string[];
  tools?: Tool[];
  toolChoice?: string | { type: 'function'; function: { name: string } };
  responseFormat?: { type: 'json_object' };
}

export interface ChatCompletion {
  id: string;
  model: string;
  choices: Array<{
    index: number;
    message: Message;
    finishReason: string;
  }>;
  usage: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
  created: number;
}

export interface ChatCompletionChunk {
  id: string;
  model: string;
  choices: Array<{
    index: number;
    delta: Partial<Message>;
    finishReason: string | null;
  }>;
}

// Configuration
export interface ClientConfig {
  apiKey?: string;
  baseUrl?: string;
  timeout?: number;
  maxRetries?: number;
  headers?: Record<string, string>;
}

// Errors
export class LLMError extends Error {
  constructor(
    message: string,
    public code?: string,
    public statusCode?: number,
    public requestId?: string
  ) {
    super(message);
    this.name = 'LLMError';
  }
}

export class AuthenticationError extends LLMError {
  constructor(message: string, requestId?: string) {
    super(message, 'AUTHENTICATION_ERROR', 401, requestId);
    this.name = 'AuthenticationError';
  }
}

export class RateLimitError extends LLMError {
  constructor(
    message: string,
    public retryAfter?: number,
    requestId?: string
  ) {
    super(message, 'RATE_LIMIT_ERROR', 429, requestId);
    this.name = 'RateLimitError';
  }
}

// Client
export class LLMClient {
  private config: Required<ClientConfig>;
  private abortController: AbortController | null = null;

  constructor(config: ClientConfig = {}) {
    this.config = {
      apiKey: config.apiKey ?? process.env.LLM_API_KEY ?? '',
      baseUrl: config.baseUrl ?? 'https://api.example.com/v1',
      timeout: config.timeout ?? 60000,
      maxRetries: config.maxRetries ?? 3,
      headers: config.headers ?? {},
    };

    this.chat = {
      completions: {
        create: this.createChatCompletion.bind(this),
        stream: this.streamChatCompletion.bind(this),
      },
    };

    this.embeddings = {
      create: this.createEmbedding.bind(this),
    };
  }

  // Namespaces
  chat: {
    completions: {
      create: (request: ChatCompletionRequest) => Promise<ChatCompletion>;
      stream: (
        request: ChatCompletionRequest
      ) => AsyncGenerator<ChatCompletionChunk>;
    };
  };

  embeddings: {
    create: (request: EmbeddingRequest) => Promise<EmbeddingResponse>;
  };

  private async request<T>(
    method: string,
    path: string,
    body?: unknown
  ): Promise<T> {
    const url = `${this.config.baseUrl}${path}`;
    const headers: Record<string, string> = {
      'Content-Type': 'application/json',
      ...this.config.headers,
    };

    if (this.config.apiKey) {
      headers['Authorization'] = `Bearer ${this.config.apiKey}`;
    }

    let lastError: Error | null = null;

    for (let attempt = 0; attempt <= this.config.maxRetries; attempt++) {
      try {
        const response = await fetch(url, {
          method,
          headers,
          body: body ? JSON.stringify(body) : undefined,
          signal: AbortSignal.timeout(this.config.timeout),
        });

        if (!response.ok) {
          await this.handleErrorResponse(response);
        }

        return (await response.json()) as T;
      } catch (error) {
        lastError = error as Error;

        if (this.shouldRetry(error, attempt)) {
          await this.delay(this.calculateDelay(attempt));
          continue;
        }

        throw error;
      }
    }

    throw lastError;
  }

  private async *streamRequest(
    path: string,
    body: unknown
  ): AsyncGenerator<string> {
    const url = `${this.config.baseUrl}${path}`;
    const headers: Record<string, string> = {
      'Content-Type': 'application/json',
      ...this.config.headers,
    };

    if (this.config.apiKey) {
      headers['Authorization'] = `Bearer ${this.config.apiKey}`;
    }

    const response = await fetch(url, {
      method: 'POST',
      headers,
      body: JSON.stringify(body),
    });

    if (!response.ok) {
      await this.handleErrorResponse(response);
    }

    const reader = response.body?.getReader();
    if (!reader) throw new Error('No response body');

    const decoder = new TextDecoder();
    let buffer = '';

    while (true) {
      const { done, value } = await reader.read();
      if (done) break;

      buffer += decoder.decode(value, { stream: true });

      const lines = buffer.split('\n');
      buffer = lines.pop() ?? '';

      for (const line of lines) {
        const trimmed = line.trim();
        if (trimmed.startsWith('data: ')) {
          yield trimmed.slice(6);
        }
      }
    }
  }

  private async handleErrorResponse(response: Response): Promise<never> {
    const requestId = response.headers.get('x-request-id') ?? undefined;
    let errorData: { error?: { message?: string; code?: string } } = {};

    try {
      errorData = await response.json();
    } catch {
      // Ignore JSON parse errors
    }

    const message = errorData.error?.message ?? 'Unknown error';
    const code = errorData.error?.code;

    if (response.status === 401) {
      throw new AuthenticationError(message, requestId);
    } else if (response.status === 429) {
      const retryAfter = parseInt(
        response.headers.get('retry-after') ?? '60',
        10
      );
      throw new RateLimitError(message, retryAfter, requestId);
    } else {
      throw new LLMError(message, code, response.status, requestId);
    }
  }

  private shouldRetry(error: unknown, attempt: number): boolean {
    if (attempt >= this.config.maxRetries) return false;

    if (error instanceof RateLimitError) return true;
    if (error instanceof LLMError && (error.statusCode ?? 0) >= 500)
      return true;

    return false;
  }

  private calculateDelay(attempt: number): number {
    const baseDelay = 1000;
    const delay = baseDelay * Math.pow(2, attempt);
    const jitter = Math.random() * 0.3 * delay;
    return Math.min(delay + jitter, 60000);
  }

  private delay(ms: number): Promise<void> {
    return new Promise((resolve) => setTimeout(resolve, ms));
  }

  // API Methods
  private async createChatCompletion(
    request: ChatCompletionRequest
  ): Promise<ChatCompletion> {
    return this.request<ChatCompletion>('POST', '/chat/completions', {
      ...request,
      stream: false,
    });
  }

  private async *streamChatCompletion(
    request: ChatCompletionRequest
  ): AsyncGenerator<ChatCompletionChunk> {
    for await (const data of this.streamRequest('/chat/completions', {
      ...request,
      stream: true,
    })) {
      if (data === '[DONE]') break;

      try {
        const chunk = JSON.parse(data) as ChatCompletionChunk;
        yield chunk;
      } catch {
        // Skip malformed chunks
      }
    }
  }
}

// Fluent builder
export class ChatCompletionBuilder {
  private request: Partial<ChatCompletionRequest> = {};
  private client: LLMClient;

  constructor(client: LLMClient) {
    this.client = client;
  }

  model(model: string): this {
    this.request.model = model;
    return this;
  }

  messages(messages: Message[]): this {
    this.request.messages = messages;
    return this;
  }

  system(content: string): this {
    this.request.messages = this.request.messages ?? [];
    this.request.messages.push({ role: 'system', content });
    return this;
  }

  user(content: string): this {
    this.request.messages = this.request.messages ?? [];
    this.request.messages.push({ role: 'user', content });
    return this;
  }

  temperature(temp: number): this {
    this.request.temperature = temp;
    return this;
  }

  maxTokens(tokens: number): this {
    this.request.maxTokens = tokens;
    return this;
  }

  async execute(): Promise<ChatCompletion> {
    if (!this.request.model || !this.request.messages) {
      throw new Error('Model and messages are required');
    }
    return this.client.chat.completions.create(
      this.request as ChatCompletionRequest
    );
  }
}
```

---

## 4. SDK Distribution

### 4.1 Package Configuration

```python
# pyproject.toml for Python SDK
"""
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "llm-sdk"
version = "1.0.0"
description = "Official SDK for LLM API"
readme = "README.md"
license = "MIT"
requires-python = ">=3.8"
authors = [
    { name = "LLM Team", email = "sdk@example.com" }
]
classifiers = [
    "Development Status :: 5 - Production/Stable",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.8",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Typing :: Typed",
]
dependencies = [
    "httpx>=0.25.0",
    "pydantic>=2.0.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0.0",
    "pytest-asyncio>=0.21.0",
    "pytest-cov>=4.0.0",
    "mypy>=1.0.0",
    "ruff>=0.1.0",
    "respx>=0.20.0",
]

[project.urls]
Documentation = "https://docs.example.com/sdk/python"
Repository = "https://github.com/example/llm-python-sdk"
Changelog = "https://github.com/example/llm-python-sdk/blob/main/CHANGELOG.md"

[tool.hatch.build.targets.wheel]
packages = ["src/llm_sdk"]

[tool.mypy]
strict = true
python_version = "3.8"

[tool.ruff]
select = ["E", "F", "I", "B", "C4", "UP"]
line-length = 100
target-version = "py38"

[tool.pytest.ini_options]
asyncio_mode = "auto"
testpaths = ["tests"]
"""
```

```json
// package.json for TypeScript SDK
{
  "name": "@example/llm-sdk",
  "version": "1.0.0",
  "description": "Official SDK for LLM API",
  "main": "dist/index.js",
  "module": "dist/index.mjs",
  "types": "dist/index.d.ts",
  "exports": {
    ".": {
      "import": "./dist/index.mjs",
      "require": "./dist/index.js",
      "types": "./dist/index.d.ts"
    }
  },
  "files": ["dist", "README.md", "LICENSE"],
  "scripts": {
    "build": "tsup src/index.ts --format cjs,esm --dts",
    "test": "vitest run",
    "test:watch": "vitest",
    "lint": "eslint src --ext .ts",
    "typecheck": "tsc --noEmit",
    "prepublishOnly": "npm run build"
  },
  "keywords": ["llm", "ai", "sdk", "api"],
  "author": "LLM Team <sdk@example.com>",
  "license": "MIT",
  "repository": {
    "type": "git",
    "url": "https://github.com/example/llm-typescript-sdk"
  },
  "devDependencies": {
    "@types/node": "^20.0.0",
    "eslint": "^8.0.0",
    "tsup": "^8.0.0",
    "typescript": "^5.0.0",
    "vitest": "^1.0.0"
  },
  "engines": {
    "node": ">=18.0.0"
  }
}
```

### 4.2 Versioning Strategy

```python
"""
SDK versioning utilities.
"""
from dataclasses import dataclass
from typing import Tuple
import re


@dataclass
class SemanticVersion:
    """Semantic version representation."""
    major: int
    minor: int
    patch: int
    prerelease: str = ""
    build: str = ""

    @classmethod
    def parse(cls, version_string: str) -> 'SemanticVersion':
        """Parse a semantic version string."""
        pattern = r'^(\d+)\.(\d+)\.(\d+)(?:-([a-zA-Z0-9.]+))?(?:\+([a-zA-Z0-9.]+))?$'
        match = re.match(pattern, version_string)
        if not match:
            raise ValueError(f"Invalid version string: {version_string}")

        return cls(
            major=int(match.group(1)),
            minor=int(match.group(2)),
            patch=int(match.group(3)),
            prerelease=match.group(4) or "",
            build=match.group(5) or ""
        )

    def __str__(self) -> str:
        version = f"{self.major}.{self.minor}.{self.patch}"
        if self.prerelease:
            version += f"-{self.prerelease}"
        if self.build:
            version += f"+{self.build}"
        return version

    def bump_major(self) -> 'SemanticVersion':
        return SemanticVersion(self.major + 1, 0, 0)

    def bump_minor(self) -> 'SemanticVersion':
        return SemanticVersion(self.major, self.minor + 1, 0)

    def bump_patch(self) -> 'SemanticVersion':
        return SemanticVersion(self.major, self.minor, self.patch + 1)


class SDKVersioning:
    """
    Versioning guidelines for SDK releases.

    Major version: Breaking API changes
    - Removed methods or classes
    - Changed method signatures
    - Changed default behavior

    Minor version: New features (backwards compatible)
    - New methods or classes
    - New optional parameters
    - New configuration options

    Patch version: Bug fixes
    - Bug fixes
    - Documentation updates
    - Internal refactoring (no API changes)
    """

    CHANGELOG_TEMPLATE = """
# Changelog

All notable changes to this SDK will be documented in this file.

## [{version}] - {date}

### Added
{added}

### Changed
{changed}

### Fixed
{fixed}

### Deprecated
{deprecated}

### Removed
{removed}

### Security
{security}
"""

    @staticmethod
    def generate_changelog_entry(
        version: str,
        date: str,
        added: list = None,
        changed: list = None,
        fixed: list = None,
        deprecated: list = None,
        removed: list = None,
        security: list = None
    ) -> str:
        """Generate a changelog entry."""
        def format_list(items):
            if not items:
                return "- None\n"
            return "\n".join(f"- {item}" for item in items) + "\n"

        return SDKVersioning.CHANGELOG_TEMPLATE.format(
            version=version,
            date=date,
            added=format_list(added),
            changed=format_list(changed),
            fixed=format_list(fixed),
            deprecated=format_list(deprecated),
            removed=format_list(removed),
            security=format_list(security)
        )
```

---

## 5. Testing & Quality

### 5.1 SDK Testing Framework

```python
"""
Testing framework for SDK.
"""
import pytest
import respx
from httpx import Response
from llm_sdk import LLMClient, ChatCompletion, AuthenticationError, RateLimitError


class TestChatCompletions:
    """Test chat completions API."""

    @pytest.fixture
    def client(self):
        return LLMClient(api_key="test-key", base_url="https://api.test.com/v1")

    @respx.mock
    @pytest.mark.asyncio
    async def test_create_chat_completion(self, client):
        """Test basic chat completion."""
        mock_response = {
            "id": "chatcmpl-123",
            "model": "gpt-4",
            "choices": [{
                "index": 0,
                "message": {"role": "assistant", "content": "Hello!"},
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": 10,
                "completion_tokens": 5,
                "total_tokens": 15
            },
            "created": 1234567890
        }

        respx.post("https://api.test.com/v1/chat/completions").mock(
            return_value=Response(200, json=mock_response)
        )

        async with client:
            response = await client.chat.completions.create(
                model="gpt-4",
                messages=[{"role": "user", "content": "Hi"}]
            )

        assert isinstance(response, ChatCompletion)
        assert response.choices[0].message.content == "Hello!"
        assert response.usage.total_tokens == 15

    @respx.mock
    @pytest.mark.asyncio
    async def test_authentication_error(self, client):
        """Test authentication error handling."""
        respx.post("https://api.test.com/v1/chat/completions").mock(
            return_value=Response(
                401,
                json={"error": {"message": "Invalid API key"}}
            )
        )

        async with client:
            with pytest.raises(AuthenticationError) as exc:
                await client.chat.completions.create(
                    model="gpt-4",
                    messages=[{"role": "user", "content": "Hi"}]
                )

        assert "Invalid API key" in str(exc.value)

    @respx.mock
    @pytest.mark.asyncio
    async def test_rate_limit_error(self, client):
        """Test rate limit error handling."""
        respx.post("https://api.test.com/v1/chat/completions").mock(
            return_value=Response(
                429,
                json={"error": {"message": "Rate limit exceeded"}},
                headers={"retry-after": "30"}
            )
        )

        async with client:
            with pytest.raises(RateLimitError) as exc:
                await client.chat.completions.create(
                    model="gpt-4",
                    messages=[{"role": "user", "content": "Hi"}]
                )

        assert exc.value.retry_after == 30

    @respx.mock
    @pytest.mark.asyncio
    async def test_streaming(self, client):
        """Test streaming response."""
        chunks = [
            b'data: {"id":"1","choices":[{"delta":{"content":"Hello"}}]}\n\n',
            b'data: {"id":"1","choices":[{"delta":{"content":" World"}}]}\n\n',
            b'data: [DONE]\n\n'
        ]

        respx.post("https://api.test.com/v1/chat/completions").mock(
            return_value=Response(200, content=b"".join(chunks))
        )

        async with client:
            content_parts = []
            async for chunk in client.chat.completions.create_stream(
                model="gpt-4",
                messages=[{"role": "user", "content": "Hi"}]
            ):
                if chunk.delta_content:
                    content_parts.append(chunk.delta_content)

        assert "".join(content_parts) == "Hello World"


class TestRetryBehavior:
    """Test retry and resilience behavior."""

    @respx.mock
    @pytest.mark.asyncio
    async def test_retry_on_server_error(self):
        """Test retry on 500 errors."""
        client = LLMClient(api_key="test", base_url="https://api.test.com/v1", max_retries=2)

        call_count = 0

        def mock_handler(request):
            nonlocal call_count
            call_count += 1
            if call_count < 3:
                return Response(500, json={"error": {"message": "Server error"}})
            return Response(200, json={
                "id": "1",
                "model": "gpt-4",
                "choices": [{"message": {"content": "OK"}, "finish_reason": "stop", "index": 0}],
                "usage": {"prompt_tokens": 1, "completion_tokens": 1, "total_tokens": 2},
                "created": 1
            })

        respx.post("https://api.test.com/v1/chat/completions").mock(side_effect=mock_handler)

        async with client:
            response = await client.chat.completions.create(
                model="gpt-4",
                messages=[{"role": "user", "content": "Hi"}]
            )

        assert call_count == 3
        assert response.choices[0].message.content == "OK"
```

---

## 6. Implementation

### 6.1 Complete SDK Example

```python
"""
Complete SDK usage example.
"""
import asyncio
from llm_sdk import LLMClient, Tool


async def main():
    # Initialize client
    client = LLMClient(
        api_key="your-api-key",
        base_url="https://api.example.com/v1",
        timeout=30.0,
        max_retries=3
    )

    async with client:
        # Basic completion
        response = await client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": "What is Python?"}
            ],
            temperature=0.7,
            max_tokens=500
        )
        print(f"Response: {response.choices[0].message.content}")
        print(f"Tokens used: {response.usage.total_tokens}")

        # Fluent builder style
        response = await (
            client.chat.completions
            .model("gpt-4")
            .system("You are a coding expert.")
            .user("Write a Python function to reverse a string")
            .temperature(0.0)
            .max_tokens(200)
            .execute()
        )
        print(f"\nCode:\n{response.choices[0].message.content}")

        # Streaming
        print("\nStreaming response: ", end="")
        async for chunk in client.chat.completions.create_stream(
            model="gpt-4",
            messages=[{"role": "user", "content": "Count from 1 to 5"}]
        ):
            if chunk.delta_content:
                print(chunk.delta_content, end="", flush=True)
        print()

        # Function calling
        tools = [
            Tool(
                name="get_weather",
                description="Get weather for a location",
                parameters={
                    "type": "object",
                    "properties": {
                        "location": {"type": "string", "description": "City name"}
                    },
                    "required": ["location"]
                }
            )
        ]

        response = await client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": "What's the weather in Tokyo?"}],
            tools=[t.to_dict() for t in tools],
            tool_choice="auto"
        )

        if response.choices[0].message.tool_calls:
            print(f"\nTool calls: {response.choices[0].message.tool_calls}")


if __name__ == "__main__":
    asyncio.run(main())
```

---

## Appendices

### Appendix A: Python SDK Template

```python
# src/llm_sdk/__init__.py
"""LLM SDK - Official Python client library."""

from .client import LLMClient
from .types import (
    Message,
    Role,
    Tool,
    ChatCompletion,
    ChatCompletionChunk,
    Usage,
)
from .errors import (
    SDKError,
    AuthenticationError,
    RateLimitError,
    ValidationError,
    ServerError,
    TimeoutError,
)
from .config import SDKConfiguration

__version__ = "1.0.0"
__all__ = [
    "LLMClient",
    "Message",
    "Role",
    "Tool",
    "ChatCompletion",
    "ChatCompletionChunk",
    "Usage",
    "SDKError",
    "AuthenticationError",
    "RateLimitError",
    "ValidationError",
    "ServerError",
    "TimeoutError",
    "SDKConfiguration",
]
```

### Appendix B: TypeScript SDK Template

```typescript
// src/index.ts
export { LLMClient } from './client';
export { ChatCompletionBuilder } from './builders';
export type {
  Message,
  Tool,
  ChatCompletion,
  ChatCompletionChunk,
  ChatCompletionRequest,
  EmbeddingRequest,
  EmbeddingResponse,
  ClientConfig,
} from './types';
export {
  LLMError,
  AuthenticationError,
  RateLimitError,
  ValidationError,
} from './errors';
```

### Appendix C: API Client Generator Configs

```yaml
# openapi-generator config
generatorName: python
outputDir: generated/python
packageName: llm_sdk
additionalProperties:
  projectName: llm-sdk
  packageVersion: 1.0.0
  library: httpx
  generateSourceCodeOnly: true
  useOneOfDiscriminatorLookup: true

typeMappings:
  DateTime: datetime

importMappings:
  datetime: datetime
```

### Appendix D: Testing Best Practices

```markdown
# SDK Testing Best Practices

## Unit Tests
- Mock all HTTP calls
- Test error handling paths
- Test parameter validation
- Test type conversions

## Integration Tests
- Use test API keys
- Test against staging environment
- Test rate limiting behavior
- Test timeout handling

## End-to-End Tests
- Full request/response cycle
- Streaming responses
- Function calling
- Multi-turn conversations

## Performance Tests
- Connection pooling efficiency
- Memory usage with large responses
- Streaming memory usage
- Concurrent request handling
```

---

## References

1. OpenAI Python SDK Design
2. Stripe API Client Best Practices
3. Google Cloud Client Libraries Guidelines
4. Azure SDK Design Guidelines
5. httpx Documentation
