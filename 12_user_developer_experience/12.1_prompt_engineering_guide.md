> **Navigation** | [← 11.5 Access Control](../11_security_governance/11.5_access_control_authentication_guide.md) | [12.2 SDK Design →](12.2_sdk_client_library_guide.md)
>
> | | |
> |---|---|
> | **Prerequisites** | LLM fundamentals &#124; API usage experience |
> | **Related** | [7.6 Advanced RAG](../07_rag_pipeline/7.6_advanced_rag_patterns_guide.md) &#124; [5.3 LLM-as-Judge](../05_evaluation_testing/5.3_llm_as_judge_evaluation.md) |
> | **Next** | [12.2 SDK & Client Library](12.2_sdk_client_library_guide.md) |

# Document 12.1: Prompt Engineering Guide

## Purpose

Best practices for designing effective prompts for LLM applications, covering fundamental techniques through advanced patterns for production systems.

## Prerequisites

- Basic understanding of LLM capabilities and limitations
- Familiarity with LLM API usage
- Experience with at least one LLM application

## Table of Contents

1. [Prompt Engineering Fundamentals](#1-prompt-engineering-fundamentals)
2. [Prompt Techniques](#2-prompt-techniques)
3. [Prompt Templates](#3-prompt-templates)
4. [Prompt Optimization](#4-prompt-optimization)
5. [Production Prompts](#5-production-prompts)
6. [Implementation](#6-implementation)

---

## 1. Prompt Engineering Fundamentals

### 1.1 Prompt Anatomy

```python
"""
Core prompt structures and components.
"""
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any, Union
from enum import Enum
from datetime import datetime
import json


class MessageRole(Enum):
    """Standard message roles in chat-based LLMs."""
    SYSTEM = "system"
    USER = "user"
    ASSISTANT = "assistant"
    TOOL = "tool"
    FUNCTION = "function"


@dataclass
class Message:
    """Single message in a conversation."""
    role: MessageRole
    content: str
    name: Optional[str] = None  # For tool/function messages
    metadata: Dict = field(default_factory=dict)


@dataclass
class PromptTemplate:
    """Template for constructing prompts."""
    template_id: str
    name: str
    version: str

    system_template: str
    user_template: str
    assistant_prefix: Optional[str] = None  # For prefilling

    # Template variables
    variables: List[str] = field(default_factory=list)
    defaults: Dict[str, Any] = field(default_factory=dict)

    # Metadata
    description: str = ""
    examples: List[Dict] = field(default_factory=list)
    tags: List[str] = field(default_factory=list)

    created_at: datetime = field(default_factory=datetime.utcnow)
    updated_at: datetime = field(default_factory=datetime.utcnow)


class PromptBuilder:
    """
    Builds prompts from templates and variables.
    """

    def __init__(self):
        self.templates: Dict[str, PromptTemplate] = {}

    def register_template(self, template: PromptTemplate) -> None:
        """Register a prompt template."""
        self.templates[template.template_id] = template

    def build(
        self,
        template_id: str,
        variables: Dict[str, Any],
        conversation_history: Optional[List[Message]] = None
    ) -> List[Dict[str, str]]:
        """Build a prompt from a template."""
        template = self.templates.get(template_id)
        if not template:
            raise ValueError(f"Template {template_id} not found")

        # Merge defaults with provided variables
        all_vars = {**template.defaults, **variables}

        # Validate required variables
        missing = [v for v in template.variables if v not in all_vars]
        if missing:
            raise ValueError(f"Missing required variables: {missing}")

        messages = []

        # System message
        system_content = self._render(template.system_template, all_vars)
        messages.append({
            "role": "system",
            "content": system_content
        })

        # Add conversation history if provided
        if conversation_history:
            for msg in conversation_history:
                messages.append({
                    "role": msg.role.value,
                    "content": msg.content
                })

        # User message
        user_content = self._render(template.user_template, all_vars)
        messages.append({
            "role": "user",
            "content": user_content
        })

        # Assistant prefix (for prefilling/steering)
        if template.assistant_prefix:
            prefix = self._render(template.assistant_prefix, all_vars)
            messages.append({
                "role": "assistant",
                "content": prefix
            })

        return messages

    def _render(self, template: str, variables: Dict[str, Any]) -> str:
        """Render template with variables."""
        result = template
        for key, value in variables.items():
            placeholder = f"{{{key}}}"
            if isinstance(value, (list, dict)):
                value = json.dumps(value, indent=2)
            result = result.replace(placeholder, str(value))
        return result


# Core prompt patterns
class PromptPatterns:
    """Common prompt patterns and structures."""

    @staticmethod
    def basic_instruction(task: str, context: Optional[str] = None) -> str:
        """Basic instruction pattern."""
        prompt = f"Task: {task}\n"
        if context:
            prompt += f"\nContext:\n{context}\n"
        prompt += "\nPlease complete the task."
        return prompt

    @staticmethod
    def role_based(role: str, task: str, constraints: Optional[List[str]] = None) -> str:
        """Role-based instruction pattern."""
        prompt = f"You are {role}.\n\n"
        prompt += f"Your task is to: {task}\n"
        if constraints:
            prompt += "\nConstraints:\n"
            for c in constraints:
                prompt += f"- {c}\n"
        return prompt

    @staticmethod
    def structured_output(
        task: str,
        output_format: Dict[str, str],
        example: Optional[Dict] = None
    ) -> str:
        """Structured output pattern."""
        prompt = f"{task}\n\n"
        prompt += "Respond in the following JSON format:\n"
        prompt += "```json\n"
        prompt += json.dumps(output_format, indent=2)
        prompt += "\n```\n"
        if example:
            prompt += "\nExample:\n```json\n"
            prompt += json.dumps(example, indent=2)
            prompt += "\n```"
        return prompt

    @staticmethod
    def few_shot(
        task_description: str,
        examples: List[Dict[str, str]],
        query: str
    ) -> str:
        """Few-shot learning pattern."""
        prompt = f"{task_description}\n\n"
        prompt += "Examples:\n\n"
        for i, ex in enumerate(examples, 1):
            prompt += f"Example {i}:\n"
            prompt += f"Input: {ex['input']}\n"
            prompt += f"Output: {ex['output']}\n\n"
        prompt += f"Now process this:\nInput: {query}\nOutput:"
        return prompt
```

### 1.2 Prompt Components

```python
"""
Modular prompt components for building complex prompts.
"""
from typing import List, Dict, Optional, Callable
from dataclasses import dataclass
from abc import ABC, abstractmethod


class PromptComponent(ABC):
    """Base class for prompt components."""

    @abstractmethod
    def render(self, context: Dict) -> str:
        """Render the component with context."""
        pass


class RoleComponent(PromptComponent):
    """Defines the AI's role and persona."""

    def __init__(
        self,
        role: str,
        expertise: List[str] = None,
        personality_traits: List[str] = None,
        communication_style: str = None
    ):
        self.role = role
        self.expertise = expertise or []
        self.personality_traits = personality_traits or []
        self.communication_style = communication_style

    def render(self, context: Dict) -> str:
        prompt = f"You are {self.role}."

        if self.expertise:
            prompt += f" You have expertise in: {', '.join(self.expertise)}."

        if self.personality_traits:
            prompt += f" You are: {', '.join(self.personality_traits)}."

        if self.communication_style:
            prompt += f" Communicate in a {self.communication_style} manner."

        return prompt


class ContextComponent(PromptComponent):
    """Provides context information."""

    def __init__(
        self,
        header: str = "Context",
        max_length: Optional[int] = None,
        formatter: Optional[Callable] = None
    ):
        self.header = header
        self.max_length = max_length
        self.formatter = formatter or self._default_formatter

    def _default_formatter(self, context_items: List[Dict]) -> str:
        return "\n".join(
            f"- {item.get('title', '')}: {item.get('content', '')}"
            for item in context_items
        )

    def render(self, context: Dict) -> str:
        context_data = context.get("context_items", [])
        if not context_data:
            return ""

        formatted = self.formatter(context_data)

        if self.max_length and len(formatted) > self.max_length:
            formatted = formatted[:self.max_length] + "..."

        return f"{self.header}:\n{formatted}"


class InstructionComponent(PromptComponent):
    """Defines the task instructions."""

    def __init__(
        self,
        primary_instruction: str,
        sub_instructions: List[str] = None,
        constraints: List[str] = None,
        priorities: List[str] = None
    ):
        self.primary_instruction = primary_instruction
        self.sub_instructions = sub_instructions or []
        self.constraints = constraints or []
        self.priorities = priorities or []

    def render(self, context: Dict) -> str:
        # Substitute variables in instruction
        instruction = self.primary_instruction
        for key, value in context.items():
            instruction = instruction.replace(f"{{{key}}}", str(value))

        prompt = f"Task: {instruction}\n"

        if self.sub_instructions:
            prompt += "\nSteps:\n"
            for i, step in enumerate(self.sub_instructions, 1):
                prompt += f"{i}. {step}\n"

        if self.constraints:
            prompt += "\nConstraints:\n"
            for c in self.constraints:
                prompt += f"- {c}\n"

        if self.priorities:
            prompt += "\nPriorities (in order):\n"
            for i, p in enumerate(self.priorities, 1):
                prompt += f"{i}. {p}\n"

        return prompt


class OutputFormatComponent(PromptComponent):
    """Defines the expected output format."""

    def __init__(
        self,
        format_type: str,  # "json", "markdown", "xml", "plain"
        schema: Optional[Dict] = None,
        example: Optional[str] = None,
        strict: bool = True
    ):
        self.format_type = format_type
        self.schema = schema
        self.example = example
        self.strict = strict

    def render(self, context: Dict) -> str:
        prompt = f"Output Format: {self.format_type.upper()}\n"

        if self.schema:
            prompt += "\nRequired structure:\n```json\n"
            prompt += json.dumps(self.schema, indent=2)
            prompt += "\n```\n"

        if self.example:
            prompt += f"\nExample output:\n```{self.format_type}\n"
            prompt += self.example
            prompt += "\n```\n"

        if self.strict:
            prompt += "\nIMPORTANT: Respond ONLY with the specified format. No additional text."

        return prompt


class ExamplesComponent(PromptComponent):
    """Provides few-shot examples."""

    def __init__(
        self,
        examples: List[Dict[str, str]],
        example_format: str = "Input: {input}\nOutput: {output}"
    ):
        self.examples = examples
        self.example_format = example_format

    def render(self, context: Dict) -> str:
        if not self.examples:
            return ""

        prompt = "Examples:\n\n"
        for i, ex in enumerate(self.examples, 1):
            prompt += f"Example {i}:\n"
            prompt += self.example_format.format(**ex)
            prompt += "\n\n"

        return prompt


class PromptAssembler:
    """Assembles prompt components into complete prompts."""

    def __init__(self):
        self.components: List[PromptComponent] = []

    def add(self, component: PromptComponent) -> 'PromptAssembler':
        """Add a component to the prompt."""
        self.components.append(component)
        return self

    def build(self, context: Dict) -> str:
        """Build the complete prompt."""
        parts = []
        for component in self.components:
            rendered = component.render(context)
            if rendered:
                parts.append(rendered)
        return "\n\n".join(parts)

    def build_messages(self, context: Dict, user_query: str) -> List[Dict]:
        """Build as chat messages."""
        system_prompt = self.build(context)
        return [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_query}
        ]
```

---

## 2. Prompt Techniques

### 2.1 Basic Techniques

```python
"""
Basic prompt engineering techniques.
"""


class BasicPromptTechniques:
    """Collection of basic prompt techniques."""

    @staticmethod
    def clear_instructions(task: str, details: List[str] = None) -> str:
        """
        Technique: Clear, specific instructions.
        Use when: Simple tasks that need precise execution.
        """
        prompt = f"Please {task}.\n"
        if details:
            prompt += "\nSpecifically:\n"
            for d in details:
                prompt += f"- {d}\n"
        return prompt

    @staticmethod
    def role_assignment(
        role: str,
        expertise: str,
        task: str
    ) -> str:
        """
        Technique: Role/persona assignment.
        Use when: Task benefits from specific expertise or perspective.
        """
        return f"""You are {role} with deep expertise in {expertise}.

Your task: {task}

Apply your specialized knowledge to provide the best possible response."""

    @staticmethod
    def output_specification(
        task: str,
        format_spec: str,
        requirements: List[str] = None
    ) -> str:
        """
        Technique: Explicit output format specification.
        Use when: Structured output is required.
        """
        prompt = f"{task}\n\n"
        prompt += f"Format your response as: {format_spec}\n"
        if requirements:
            prompt += "\nEnsure your response:\n"
            for r in requirements:
                prompt += f"- {r}\n"
        return prompt

    @staticmethod
    def delimiter_separation(
        instruction: str,
        content: str,
        delimiter: str = "###"
    ) -> str:
        """
        Technique: Clear delimiter separation.
        Use when: Need to separate instructions from content to prevent injection.
        """
        return f"""{instruction}

Content to process is enclosed between delimiters:

{delimiter}
{content}
{delimiter}

Process ONLY the content between the delimiters."""

    @staticmethod
    def step_by_step(
        task: str,
        steps: List[str]
    ) -> str:
        """
        Technique: Explicit step-by-step instructions.
        Use when: Complex multi-step tasks.
        """
        prompt = f"Complete the following task: {task}\n\n"
        prompt += "Follow these steps exactly:\n"
        for i, step in enumerate(steps, 1):
            prompt += f"\nStep {i}: {step}"
        prompt += "\n\nExecute each step in order."
        return prompt


class ZeroShotTechniques:
    """Zero-shot prompting techniques."""

    @staticmethod
    def direct(task: str) -> str:
        """Direct zero-shot prompt."""
        return task

    @staticmethod
    def instructed(task: str, instruction: str = None) -> str:
        """Zero-shot with instruction."""
        instruction = instruction or "Complete the following task:"
        return f"{instruction}\n\n{task}"

    @staticmethod
    def constrained(task: str, constraints: List[str]) -> str:
        """Zero-shot with constraints."""
        prompt = f"{task}\n\nConstraints:\n"
        for c in constraints:
            prompt += f"- {c}\n"
        return prompt


class FewShotTechniques:
    """Few-shot prompting techniques."""

    @staticmethod
    def standard(
        task_description: str,
        examples: List[Dict[str, str]],
        query: str
    ) -> str:
        """Standard few-shot with examples."""
        prompt = f"{task_description}\n\n"
        for i, ex in enumerate(examples, 1):
            prompt += f"Example {i}:\n"
            prompt += f"Input: {ex['input']}\n"
            prompt += f"Output: {ex['output']}\n\n"
        prompt += f"Now your turn:\nInput: {query}\nOutput:"
        return prompt

    @staticmethod
    def structured(
        task_description: str,
        examples: List[Dict[str, Any]],
        query: str,
        input_key: str = "input",
        output_key: str = "output"
    ) -> str:
        """Few-shot with structured examples."""
        prompt = f"{task_description}\n\n"
        prompt += "Examples:\n\n"
        for i, ex in enumerate(examples, 1):
            prompt += f"--- Example {i} ---\n"
            prompt += f"{input_key.upper()}:\n{ex[input_key]}\n\n"
            prompt += f"{output_key.upper()}:\n{ex[output_key]}\n\n"
        prompt += f"--- Your Task ---\n"
        prompt += f"{input_key.upper()}:\n{query}\n\n"
        prompt += f"{output_key.upper()}:"
        return prompt

    @staticmethod
    def chain_of_thought_examples(
        task_description: str,
        examples: List[Dict[str, str]],  # Each has input, reasoning, output
        query: str
    ) -> str:
        """Few-shot with chain-of-thought examples."""
        prompt = f"{task_description}\n\n"
        prompt += "Examples showing the reasoning process:\n\n"
        for i, ex in enumerate(examples, 1):
            prompt += f"Example {i}:\n"
            prompt += f"Input: {ex['input']}\n"
            prompt += f"Reasoning: {ex['reasoning']}\n"
            prompt += f"Output: {ex['output']}\n\n"
        prompt += f"Now solve:\nInput: {query}\nReasoning:"
        return prompt
```

### 2.2 Advanced Techniques

```python
"""
Advanced prompt engineering techniques.
"""


class ChainOfThought:
    """Chain-of-thought prompting techniques."""

    @staticmethod
    def basic(task: str) -> str:
        """
        Basic chain-of-thought.
        Simply appends "Let's think step by step."
        """
        return f"{task}\n\nLet's think step by step."

    @staticmethod
    def structured(task: str, thought_prefix: str = "Reasoning:") -> str:
        """Structured chain-of-thought with explicit sections."""
        return f"""{task}

{thought_prefix}
1. First, I need to understand...
2. Then, I should consider...
3. Finally, I can conclude...

Based on this reasoning, my answer is:"""

    @staticmethod
    def step_back(task: str, abstraction_prompt: str = None) -> str:
        """
        Step-back prompting.
        First asks for high-level principles, then applies them.
        """
        abstraction = abstraction_prompt or "What are the key principles or concepts relevant to this problem?"
        return f"""Before solving this problem, let's step back.

Problem: {task}

{abstraction}

Now, using these principles, solve the problem step by step."""

    @staticmethod
    def self_consistency(task: str, num_paths: int = 3) -> str:
        """
        Self-consistency prompting.
        Generates multiple reasoning paths.
        Note: Requires post-processing to aggregate answers.
        """
        return f"""{task}

Generate {num_paths} different ways to solve this problem.

For each approach:
1. Explain your reasoning
2. Show the steps
3. Provide the final answer

Then indicate which answer appears most frequently and is most likely correct."""


class TreeOfThoughts:
    """Tree of Thoughts prompting."""

    @staticmethod
    def basic(task: str, num_branches: int = 3) -> str:
        """Basic Tree of Thoughts prompt."""
        return f"""Task: {task}

Let's explore this systematically using a tree of thoughts approach.

Step 1: Generate {num_branches} initial approaches
Step 2: For each approach, evaluate its promise (rate 1-10)
Step 3: Expand the most promising approach with {num_branches} sub-approaches
Step 4: Evaluate and select the best path
Step 5: Execute the selected approach

Begin:"""

    @staticmethod
    def deliberate(
        task: str,
        evaluation_criteria: List[str]
    ) -> str:
        """Tree of Thoughts with deliberate evaluation."""
        criteria_str = "\n".join(f"- {c}" for c in evaluation_criteria)
        return f"""Task: {task}

Approach this using deliberate reasoning:

1. PROPOSE: Generate 3 distinct approaches to solve this
2. EVALUATE: For each approach, assess against these criteria:
{criteria_str}
3. SELECT: Choose the best approach based on evaluation
4. EXPAND: Break down the selected approach into detailed steps
5. EXECUTE: Carry out the steps and provide the solution

Show all your work."""


class ReAct:
    """ReAct (Reasoning + Acting) prompting."""

    @staticmethod
    def basic(task: str, available_actions: List[str]) -> str:
        """Basic ReAct prompt."""
        actions_str = ", ".join(available_actions)
        return f"""You are an assistant that reasons and takes actions to complete tasks.

Available actions: {actions_str}

Task: {task}

Use this format:
Thought: [Your reasoning about what to do]
Action: [One of: {actions_str}]
Action Input: [Input for the action]
Observation: [Result of the action - this will be provided]
... (repeat Thought/Action/Observation as needed)
Thought: I have gathered enough information
Final Answer: [Your final response]

Begin!

Thought:"""

    @staticmethod
    def with_examples(
        task: str,
        available_actions: List[str],
        examples: List[Dict]
    ) -> str:
        """ReAct with examples."""
        actions_str = ", ".join(available_actions)

        prompt = f"""You are an assistant that reasons and takes actions.

Available actions: {actions_str}

Format:
Thought: [reasoning]
Action: [action name]
Action Input: [input]
Observation: [result]
... (repeat as needed)
Final Answer: [response]

Examples:
"""
        for ex in examples:
            prompt += f"\nTask: {ex['task']}\n{ex['trace']}\n"

        prompt += f"\nNow solve:\nTask: {task}\n\nThought:"
        return prompt


class Reflection:
    """Self-reflection prompting techniques."""

    @staticmethod
    def basic(task: str) -> str:
        """Basic reflection prompt."""
        return f"""{task}

After providing your answer, reflect on it:
1. What assumptions did I make?
2. What could be wrong with my reasoning?
3. Is there a better approach?
4. What's my confidence level (1-10)?

If reflection reveals issues, revise your answer."""

    @staticmethod
    def iterative(task: str, max_iterations: int = 3) -> str:
        """Iterative refinement through reflection."""
        return f"""Task: {task}

Complete this task using iterative refinement:

ITERATION 1:
- Draft: [Your initial solution]
- Self-Critique: [What's wrong or could be better?]
- Score (1-10): [Rate your draft]

ITERATION 2:
- Refined Draft: [Improved solution based on critique]
- Self-Critique: [Remaining issues?]
- Score (1-10): [Rate improvement]

(Continue up to {max_iterations} iterations until score >= 8)

FINAL ANSWER: [Your best solution]"""

    @staticmethod
    def constitutional(task: str, principles: List[str]) -> str:
        """Constitutional AI-style reflection with principles."""
        principles_str = "\n".join(f"- {p}" for p in principles)
        return f"""{task}

After generating your response, evaluate it against these principles:
{principles_str}

For each principle:
1. Does the response follow this principle? (Yes/No)
2. If No, how should it be revised?

Provide your original response, then the evaluation, then a revised response if needed.

ORIGINAL RESPONSE:
[Your response]

PRINCIPLE EVALUATION:
[Evaluation for each principle]

REVISED RESPONSE (if needed):
[Improved response]"""


class PromptChaining:
    """Techniques for chaining multiple prompts."""

    @staticmethod
    def decomposition(complex_task: str, subtasks: List[str]) -> List[str]:
        """
        Decompose complex task into chained subtask prompts.
        Returns list of prompts to execute sequentially.
        """
        prompts = []

        # Initial decomposition understanding
        prompts.append(f"""Task: {complex_task}

This task will be completed in stages. Here's the plan:
{chr(10).join(f'{i+1}. {st}' for i, st in enumerate(subtasks))}

First, let's complete stage 1: {subtasks[0]}""")

        # Subsequent stages reference previous results
        for i, subtask in enumerate(subtasks[1:], 2):
            prompts.append(f"""Previous stages completed. Results: {{previous_results}}

Now complete stage {i}: {subtask}

Build on the previous results.""")

        # Final synthesis
        prompts.append(f"""All stages completed. Results from each stage: {{all_results}}

Now synthesize everything into a final, coherent response for:
{complex_task}""")

        return prompts

    @staticmethod
    def verification_chain() -> List[str]:
        """Chain that generates then verifies."""
        return [
            # Generation
            "{task}",
            # Verification
            """Review the following response for correctness and completeness:

Response: {previous_response}

Task it was meant to address: {original_task}

Evaluation:
1. Is it correct? (Yes/Partially/No)
2. Is it complete? (Yes/Partially/No)
3. Any errors or omissions?
4. Suggested improvements?

If issues found, provide a corrected version.""",
            # Final refinement
            """Original response: {original_response}
Verification feedback: {verification_feedback}

Produce a final, refined response incorporating the feedback."""
        ]
```

### 2.3 Specialized Techniques

```python
"""
Specialized prompting techniques for specific use cases.
"""


class RAGPromptTechniques:
    """Prompting techniques for RAG systems."""

    @staticmethod
    def basic_rag(query: str, context: str) -> str:
        """Basic RAG prompt."""
        return f"""Answer the question based on the provided context.

Context:
{context}

Question: {query}

Answer based on the context. If the context doesn't contain relevant information, say so."""

    @staticmethod
    def citation_rag(query: str, documents: List[Dict[str, str]]) -> str:
        """RAG with citation requirements."""
        context = ""
        for i, doc in enumerate(documents, 1):
            context += f"\n[Document {i}]\n{doc['content']}\n"

        return f"""Answer the question using the provided documents. Cite your sources.

Documents:
{context}

Question: {query}

Instructions:
1. Answer the question comprehensively
2. After each claim, cite the source as [Document N]
3. If documents don't contain relevant information, state that clearly
4. Do not make claims without citation

Answer:"""

    @staticmethod
    def multi_hop_rag(query: str, documents: List[str]) -> str:
        """RAG for multi-hop reasoning questions."""
        docs = "\n\n".join(f"Document {i+1}:\n{d}" for i, d in enumerate(documents))
        return f"""Answer the question by connecting information from multiple documents.

Documents:
{docs}

Question: {query}

Think step by step:
1. Identify relevant facts in each document
2. Connect facts across documents
3. Reason through the connections
4. Provide the final answer

Show your reasoning:"""

    @staticmethod
    def self_rag(query: str, context: str) -> str:
        """Self-RAG style with retrieval criticism."""
        return f"""Context:
{context}

Question: {query}

First, evaluate the relevance of the context:
- Is this context relevant to the question? [Relevant/Partially Relevant/Not Relevant]
- Does it contain enough information to answer? [Yes/Partially/No]

Then, answer based on your evaluation:
- If relevant and sufficient: Answer using the context
- If partially relevant: Answer what you can and note gaps
- If not relevant: State that and explain what information would be needed"""


class CodePromptTechniques:
    """Prompting techniques for code-related tasks."""

    @staticmethod
    def code_generation(
        task: str,
        language: str,
        requirements: List[str] = None
    ) -> str:
        """Generate code from natural language."""
        prompt = f"""Write {language} code to: {task}

Requirements:
"""
        if requirements:
            for r in requirements:
                prompt += f"- {r}\n"
        else:
            prompt += "- Clean, readable code\n- Include comments for complex logic\n- Handle edge cases\n"

        prompt += f"""
Provide:
1. The complete code
2. Brief explanation of how it works
3. Example usage

```{language.lower()}
"""
        return prompt

    @staticmethod
    def code_review(code: str, focus_areas: List[str] = None) -> str:
        """Review code for issues."""
        focus = focus_areas or ["bugs", "security", "performance", "style"]
        return f"""Review the following code:

```
{code}
```

Focus on: {', '.join(focus)}

For each issue found:
1. Location (line number or code section)
2. Issue type ({'/'.join(focus)})
3. Severity (Critical/High/Medium/Low)
4. Description
5. Suggested fix

Format as a structured review."""

    @staticmethod
    def code_explanation(code: str, audience: str = "junior developer") -> str:
        """Explain code."""
        return f"""Explain the following code for a {audience}:

```
{code}
```

Cover:
1. Overall purpose - what does this code do?
2. Key components and their roles
3. How data flows through the code
4. Any notable patterns or techniques used
5. Potential gotchas or things to be careful about"""

    @staticmethod
    def debugging(code: str, error: str, context: str = None) -> str:
        """Help debug code."""
        prompt = f"""Debug the following code:

```
{code}
```

Error: {error}
"""
        if context:
            prompt += f"\nAdditional context: {context}\n"

        prompt += """
Analysis:
1. What is causing this error?
2. Why does this happen?
3. How to fix it?
4. How to prevent similar issues?

Provide the corrected code."""
        return prompt


class ExtractionPromptTechniques:
    """Prompting for information extraction."""

    @staticmethod
    def entity_extraction(
        text: str,
        entity_types: List[str],
        output_format: str = "json"
    ) -> str:
        """Extract entities from text."""
        return f"""Extract the following entity types from the text:
Entity types: {', '.join(entity_types)}

Text:
{text}

Output as {output_format} with this structure:
{{
    "entity_type": ["entity1", "entity2", ...]
}}

Only extract entities explicitly mentioned. Do not infer."""

    @staticmethod
    def structured_extraction(
        text: str,
        schema: Dict[str, str]
    ) -> str:
        """Extract structured data according to schema."""
        schema_str = json.dumps(schema, indent=2)
        return f"""Extract information from the text according to this schema:

Schema:
{schema_str}

Text:
{text}

Instructions:
- Extract only information present in the text
- Use null for fields not found
- Match the exact schema structure

Output as JSON:"""

    @staticmethod
    def relationship_extraction(
        text: str,
        relationship_types: List[str]
    ) -> str:
        """Extract relationships between entities."""
        return f"""Extract relationships from the text.

Relationship types to find: {', '.join(relationship_types)}

Text:
{text}

For each relationship found, output:
{{
    "subject": "entity1",
    "relationship": "relationship_type",
    "object": "entity2",
    "evidence": "quote from text"
}}

Output as a JSON array of relationships:"""
```

---

## 3. Prompt Templates

### 3.1 Template Library

```python
"""
Production-ready prompt template library.
"""
from typing import Dict, List, Optional
from dataclasses import dataclass
from enum import Enum


class TaskType(Enum):
    """Types of tasks for template selection."""
    QUESTION_ANSWERING = "qa"
    SUMMARIZATION = "summarization"
    CLASSIFICATION = "classification"
    EXTRACTION = "extraction"
    GENERATION = "generation"
    CONVERSATION = "conversation"
    CODE = "code"
    ANALYSIS = "analysis"


@dataclass
class TemplateLibraryEntry:
    """Entry in the template library."""
    template_id: str
    task_type: TaskType
    name: str
    description: str
    system_prompt: str
    user_template: str
    variables: List[str]
    examples: List[Dict]
    best_practices: List[str]


class PromptTemplateLibrary:
    """Library of production-ready prompt templates."""

    TEMPLATES = {
        # Question Answering Templates
        "qa_basic": TemplateLibraryEntry(
            template_id="qa_basic",
            task_type=TaskType.QUESTION_ANSWERING,
            name="Basic Q&A",
            description="Simple question answering without context",
            system_prompt="""You are a helpful assistant that answers questions accurately and concisely.

Guidelines:
- Be direct and factual
- If unsure, say so
- Cite sources when possible""",
            user_template="{question}",
            variables=["question"],
            examples=[
                {"question": "What is the capital of France?", "answer": "Paris"}
            ],
            best_practices=[
                "Keep questions specific",
                "Provide context when needed"
            ]
        ),

        "qa_rag": TemplateLibraryEntry(
            template_id="qa_rag",
            task_type=TaskType.QUESTION_ANSWERING,
            name="RAG Q&A with Context",
            description="Question answering grounded in provided context",
            system_prompt="""You are a precise assistant that answers questions based solely on provided context.

Instructions:
- Only use information from the context
- Quote relevant parts when answering
- If the context doesn't contain the answer, say "I cannot find this information in the provided context"
- Never make up information""",
            user_template="""Context:
{context}

Question: {question}

Answer based on the context above:""",
            variables=["context", "question"],
            examples=[],
            best_practices=[
                "Ensure context is relevant",
                "Chunk context appropriately",
                "Include source references"
            ]
        ),

        # Summarization Templates
        "summarize_basic": TemplateLibraryEntry(
            template_id="summarize_basic",
            task_type=TaskType.SUMMARIZATION,
            name="Basic Summarization",
            description="Summarize text to specified length",
            system_prompt="""You are an expert summarizer. Create clear, accurate summaries that capture key points.

Guidelines:
- Preserve the most important information
- Maintain factual accuracy
- Use clear, concise language
- Keep the original meaning intact""",
            user_template="""Summarize the following text in {length}:

Text:
{text}

Summary:""",
            variables=["text", "length"],
            examples=[],
            best_practices=[
                "Specify desired length clearly",
                "Use '2-3 sentences' or 'bullet points'",
                "For long texts, consider hierarchical summarization"
            ]
        ),

        "summarize_structured": TemplateLibraryEntry(
            template_id="summarize_structured",
            task_type=TaskType.SUMMARIZATION,
            name="Structured Summary",
            description="Summarize with specific structure",
            system_prompt="""You are an expert at creating structured summaries. Extract key information into organized sections.""",
            user_template="""Create a structured summary of the following text:

Text:
{text}

Use this structure:
- Main Topic: [one sentence]
- Key Points: [bullet list of 3-5 main points]
- Conclusions: [main conclusions or takeaways]
- Action Items: [if any, otherwise "None"]""",
            variables=["text"],
            examples=[],
            best_practices=[
                "Use consistent structure",
                "Adapt sections to content type"
            ]
        ),

        # Classification Templates
        "classify_basic": TemplateLibraryEntry(
            template_id="classify_basic",
            task_type=TaskType.CLASSIFICATION,
            name="Basic Classification",
            description="Classify text into predefined categories",
            system_prompt="""You are a precise classifier. Categorize text into exactly one of the provided categories.

Rules:
- Choose only from the provided categories
- Be consistent in classification
- If uncertain, choose the most likely category""",
            user_template="""Classify the following text into one of these categories: {categories}

Text: {text}

Category:""",
            variables=["text", "categories"],
            examples=[],
            best_practices=[
                "List categories clearly",
                "Use mutually exclusive categories",
                "Consider adding 'Other' category"
            ]
        ),

        "classify_with_reasoning": TemplateLibraryEntry(
            template_id="classify_with_reasoning",
            task_type=TaskType.CLASSIFICATION,
            name="Classification with Reasoning",
            description="Classify with explanation",
            system_prompt="""You are an expert classifier. Categorize text and explain your reasoning.""",
            user_template="""Classify the following text and explain your reasoning.

Categories: {categories}

Text: {text}

Respond in this format:
Category: [selected category]
Confidence: [High/Medium/Low]
Reasoning: [brief explanation of why this category was chosen]""",
            variables=["text", "categories"],
            examples=[],
            best_practices=[
                "Confidence helps with threshold decisions",
                "Reasoning aids debugging and trust"
            ]
        ),

        # Extraction Templates
        "extract_entities": TemplateLibraryEntry(
            template_id="extract_entities",
            task_type=TaskType.EXTRACTION,
            name="Entity Extraction",
            description="Extract specific entities from text",
            system_prompt="""You are a precise information extractor. Extract only the requested entities from text.

Rules:
- Only extract explicitly mentioned entities
- Do not infer or assume
- Use exact text when possible
- Return empty list if no entities found""",
            user_template="""Extract {entity_types} from the following text:

Text: {text}

Return as JSON:
{{"entity_type": ["entity1", "entity2", ...]}}""",
            variables=["text", "entity_types"],
            examples=[],
            best_practices=[
                "Be specific about entity types",
                "Provide examples for ambiguous cases"
            ]
        ),

        # Conversation Templates
        "conversation_assistant": TemplateLibraryEntry(
            template_id="conversation_assistant",
            task_type=TaskType.CONVERSATION,
            name="Conversational Assistant",
            description="Multi-turn conversation assistant",
            system_prompt="""You are a helpful, friendly assistant engaged in a conversation.

Guidelines:
- Maintain context from previous messages
- Be concise but thorough
- Ask clarifying questions when needed
- Admit when you don't know something
- Stay on topic but allow natural conversation flow""",
            user_template="{message}",
            variables=["message"],
            examples=[],
            best_practices=[
                "Maintain conversation history",
                "Implement appropriate context window management",
                "Consider conversation summarization for long chats"
            ]
        ),

        # Code Templates
        "code_generate": TemplateLibraryEntry(
            template_id="code_generate",
            task_type=TaskType.CODE,
            name="Code Generation",
            description="Generate code from description",
            system_prompt="""You are an expert programmer. Write clean, efficient, well-documented code.

Standards:
- Follow language best practices
- Include error handling
- Add comments for complex logic
- Use meaningful variable names
- Consider edge cases""",
            user_template="""Write {language} code to: {task}

Requirements:
{requirements}

Provide:
1. The complete code
2. Brief usage example
3. Any important notes

```{language}
""",
            variables=["language", "task", "requirements"],
            examples=[],
            best_practices=[
                "Specify language version if relevant",
                "Include specific requirements",
                "Request tests if needed"
            ]
        ),

        # Analysis Templates
        "analyze_sentiment": TemplateLibraryEntry(
            template_id="analyze_sentiment",
            task_type=TaskType.ANALYSIS,
            name="Sentiment Analysis",
            description="Analyze sentiment of text",
            system_prompt="""You are a sentiment analysis expert. Analyze text for emotional tone and sentiment.""",
            user_template="""Analyze the sentiment of the following text:

Text: {text}

Provide:
1. Overall sentiment: [Positive/Negative/Neutral/Mixed]
2. Confidence: [0-100%]
3. Key indicators: [words/phrases that indicate the sentiment]
4. Aspect sentiments (if applicable): [sentiment by topic/aspect]""",
            variables=["text"],
            examples=[],
            best_practices=[
                "Consider context and domain",
                "Handle sarcasm and irony"
            ]
        )
    }

    @classmethod
    def get_template(cls, template_id: str) -> Optional[TemplateLibraryEntry]:
        """Get a template by ID."""
        return cls.TEMPLATES.get(template_id)

    @classmethod
    def list_templates(cls, task_type: Optional[TaskType] = None) -> List[Dict]:
        """List available templates."""
        templates = []
        for tid, template in cls.TEMPLATES.items():
            if task_type is None or template.task_type == task_type:
                templates.append({
                    "id": tid,
                    "name": template.name,
                    "task_type": template.task_type.value,
                    "description": template.description
                })
        return templates

    @classmethod
    def render(cls, template_id: str, variables: Dict) -> List[Dict[str, str]]:
        """Render a template with variables."""
        template = cls.get_template(template_id)
        if not template:
            raise ValueError(f"Template {template_id} not found")

        # Check required variables
        missing = [v for v in template.variables if v not in variables]
        if missing:
            raise ValueError(f"Missing required variables: {missing}")

        # Render templates
        system = template.system_prompt
        user = template.user_template

        for key, value in variables.items():
            if isinstance(value, list):
                value = ", ".join(str(v) for v in value)
            system = system.replace(f"{{{key}}}", str(value))
            user = user.replace(f"{{{key}}}", str(value))

        return [
            {"role": "system", "content": system},
            {"role": "user", "content": user}
        ]
```

---

## 4. Prompt Optimization

### 4.1 A/B Testing Prompts

```python
"""
A/B testing framework for prompt optimization.
"""
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Callable, Any
from datetime import datetime
from enum import Enum
import random
import hashlib
import statistics


class ExperimentStatus(Enum):
    """Status of a prompt experiment."""
    DRAFT = "draft"
    RUNNING = "running"
    PAUSED = "paused"
    COMPLETED = "completed"
    ANALYZED = "analyzed"


@dataclass
class PromptVariant:
    """A variant in a prompt experiment."""
    variant_id: str
    name: str
    system_prompt: str
    user_template: str
    weight: float = 1.0  # For weighted random assignment
    metadata: Dict = field(default_factory=dict)


@dataclass
class ExperimentResult:
    """Result of a single experiment trial."""
    variant_id: str
    request_id: str
    user_id: Optional[str]
    timestamp: datetime

    # Metrics
    latency_ms: float
    tokens_used: int
    response: str

    # Quality metrics (to be filled by evaluation)
    quality_score: Optional[float] = None
    relevance_score: Optional[float] = None
    user_feedback: Optional[int] = None  # e.g., 1-5 rating
    task_success: Optional[bool] = None

    metadata: Dict = field(default_factory=dict)


@dataclass
class PromptExperiment:
    """Prompt A/B experiment definition."""
    experiment_id: str
    name: str
    description: str

    variants: List[PromptVariant]
    status: ExperimentStatus = ExperimentStatus.DRAFT

    # Configuration
    traffic_percentage: float = 100.0  # % of traffic in experiment
    min_samples_per_variant: int = 100
    max_samples: Optional[int] = None

    # Results
    results: List[ExperimentResult] = field(default_factory=list)

    created_at: datetime = field(default_factory=datetime.utcnow)
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None


class PromptExperimentManager:
    """
    Manages prompt A/B testing experiments.
    """

    def __init__(self):
        self.experiments: Dict[str, PromptExperiment] = {}

    def create_experiment(
        self,
        name: str,
        description: str,
        variants: List[PromptVariant],
        traffic_percentage: float = 100.0,
        min_samples: int = 100
    ) -> PromptExperiment:
        """Create a new prompt experiment."""
        exp_id = f"exp_{datetime.utcnow().strftime('%Y%m%d%H%M%S')}"

        experiment = PromptExperiment(
            experiment_id=exp_id,
            name=name,
            description=description,
            variants=variants,
            traffic_percentage=traffic_percentage,
            min_samples_per_variant=min_samples
        )

        self.experiments[exp_id] = experiment
        return experiment

    def start_experiment(self, experiment_id: str) -> bool:
        """Start an experiment."""
        exp = self.experiments.get(experiment_id)
        if not exp:
            return False

        if exp.status != ExperimentStatus.DRAFT:
            return False

        exp.status = ExperimentStatus.RUNNING
        exp.started_at = datetime.utcnow()
        return True

    def assign_variant(
        self,
        experiment_id: str,
        user_id: Optional[str] = None,
        request_id: Optional[str] = None
    ) -> Optional[PromptVariant]:
        """Assign a variant for a request."""
        exp = self.experiments.get(experiment_id)
        if not exp or exp.status != ExperimentStatus.RUNNING:
            return None

        # Check if request should be in experiment
        if random.random() * 100 > exp.traffic_percentage:
            return None  # Not in experiment

        # Consistent assignment for same user
        if user_id:
            hash_input = f"{experiment_id}:{user_id}"
            hash_value = int(hashlib.md5(hash_input.encode()).hexdigest(), 16)
            variant_index = hash_value % len(exp.variants)
            return exp.variants[variant_index]

        # Weighted random assignment
        total_weight = sum(v.weight for v in exp.variants)
        r = random.random() * total_weight
        cumulative = 0
        for variant in exp.variants:
            cumulative += variant.weight
            if r <= cumulative:
                return variant

        return exp.variants[-1]

    def record_result(
        self,
        experiment_id: str,
        variant_id: str,
        request_id: str,
        latency_ms: float,
        tokens_used: int,
        response: str,
        user_id: Optional[str] = None,
        **metrics
    ) -> ExperimentResult:
        """Record a result from an experiment trial."""
        exp = self.experiments.get(experiment_id)
        if not exp:
            raise ValueError(f"Experiment {experiment_id} not found")

        result = ExperimentResult(
            variant_id=variant_id,
            request_id=request_id,
            user_id=user_id,
            timestamp=datetime.utcnow(),
            latency_ms=latency_ms,
            tokens_used=tokens_used,
            response=response,
            **{k: v for k, v in metrics.items()
               if k in ['quality_score', 'relevance_score', 'user_feedback', 'task_success']}
        )

        exp.results.append(result)

        # Check if experiment should complete
        self._check_completion(exp)

        return result

    def _check_completion(self, exp: PromptExperiment):
        """Check if experiment should be marked complete."""
        if exp.status != ExperimentStatus.RUNNING:
            return

        # Check min samples per variant
        variant_counts = {}
        for result in exp.results:
            variant_counts[result.variant_id] = variant_counts.get(result.variant_id, 0) + 1

        all_sufficient = all(
            variant_counts.get(v.variant_id, 0) >= exp.min_samples_per_variant
            for v in exp.variants
        )

        # Check max samples
        max_reached = exp.max_samples and len(exp.results) >= exp.max_samples

        if all_sufficient or max_reached:
            exp.status = ExperimentStatus.COMPLETED
            exp.completed_at = datetime.utcnow()

    def analyze_experiment(self, experiment_id: str) -> Dict:
        """Analyze experiment results."""
        exp = self.experiments.get(experiment_id)
        if not exp:
            raise ValueError(f"Experiment {experiment_id} not found")

        # Group results by variant
        variant_results: Dict[str, List[ExperimentResult]] = {}
        for result in exp.results:
            if result.variant_id not in variant_results:
                variant_results[result.variant_id] = []
            variant_results[result.variant_id].append(result)

        analysis = {
            "experiment_id": experiment_id,
            "name": exp.name,
            "status": exp.status.value,
            "total_samples": len(exp.results),
            "variants": {}
        }

        for variant in exp.variants:
            results = variant_results.get(variant.variant_id, [])
            if not results:
                continue

            latencies = [r.latency_ms for r in results]
            tokens = [r.tokens_used for r in results]
            quality_scores = [r.quality_score for r in results if r.quality_score is not None]
            feedback = [r.user_feedback for r in results if r.user_feedback is not None]
            success_rate = sum(1 for r in results if r.task_success) / len(results) if results else 0

            analysis["variants"][variant.variant_id] = {
                "name": variant.name,
                "samples": len(results),
                "latency": {
                    "mean": statistics.mean(latencies),
                    "median": statistics.median(latencies),
                    "p95": sorted(latencies)[int(len(latencies) * 0.95)] if latencies else 0
                },
                "tokens": {
                    "mean": statistics.mean(tokens),
                    "total": sum(tokens)
                },
                "quality": {
                    "mean": statistics.mean(quality_scores) if quality_scores else None,
                    "samples": len(quality_scores)
                },
                "feedback": {
                    "mean": statistics.mean(feedback) if feedback else None,
                    "samples": len(feedback)
                },
                "success_rate": success_rate
            }

        # Determine winner
        if exp.status == ExperimentStatus.COMPLETED:
            analysis["recommendation"] = self._determine_winner(analysis["variants"])

        exp.status = ExperimentStatus.ANALYZED
        return analysis

    def _determine_winner(self, variant_stats: Dict) -> Dict:
        """Determine the winning variant."""
        if not variant_stats:
            return {"winner": None, "reason": "No data"}

        # Score variants (simple weighted scoring)
        scores = {}
        for vid, stats in variant_stats.items():
            score = 0

            # Quality (weight: 40%)
            if stats["quality"]["mean"]:
                score += stats["quality"]["mean"] * 0.4 * 20  # Normalize to 0-100

            # User feedback (weight: 30%)
            if stats["feedback"]["mean"]:
                score += stats["feedback"]["mean"] * 0.3 * 20  # Assume 1-5 scale

            # Success rate (weight: 20%)
            score += stats["success_rate"] * 0.2 * 100

            # Efficiency - lower tokens is better (weight: 10%)
            max_tokens = max(s["tokens"]["mean"] for s in variant_stats.values())
            if max_tokens > 0:
                efficiency = 1 - (stats["tokens"]["mean"] / max_tokens)
                score += efficiency * 0.1 * 100

            scores[vid] = score

        winner = max(scores.items(), key=lambda x: x[1])
        return {
            "winner": winner[0],
            "score": winner[1],
            "scores": scores,
            "reason": "Based on quality, feedback, success rate, and efficiency"
        }
```

### 4.2 Automatic Prompt Optimization

```python
"""
Automatic prompt optimization techniques.
"""
from typing import Dict, List, Optional, Callable, Tuple
from dataclasses import dataclass
import random


@dataclass
class OptimizationConfig:
    """Configuration for prompt optimization."""
    max_iterations: int = 10
    population_size: int = 5
    mutation_rate: float = 0.3
    elite_size: int = 1
    early_stopping_threshold: float = 0.95


class PromptOptimizer:
    """
    Automatic prompt optimization using evolutionary algorithms.
    """

    def __init__(
        self,
        evaluator: Callable[[str, List[Dict]], float],
        config: OptimizationConfig = None
    ):
        """
        Args:
            evaluator: Function that takes (prompt, test_cases) and returns score 0-1
            config: Optimization configuration
        """
        self.evaluator = evaluator
        self.config = config or OptimizationConfig()

    def optimize(
        self,
        initial_prompt: str,
        test_cases: List[Dict],
        mutation_strategies: List[Callable[[str], str]] = None
    ) -> Tuple[str, float, List[Dict]]:
        """
        Optimize a prompt using evolutionary algorithm.

        Returns: (best_prompt, best_score, optimization_history)
        """
        strategies = mutation_strategies or self._default_strategies()
        history = []

        # Initialize population
        population = [initial_prompt]
        for _ in range(self.config.population_size - 1):
            mutated = self._mutate(initial_prompt, strategies)
            population.append(mutated)

        best_prompt = initial_prompt
        best_score = 0

        for iteration in range(self.config.max_iterations):
            # Evaluate population
            scores = []
            for prompt in population:
                score = self.evaluator(prompt, test_cases)
                scores.append((prompt, score))

            # Sort by score
            scores.sort(key=lambda x: x[1], reverse=True)

            # Update best
            if scores[0][1] > best_score:
                best_score = scores[0][1]
                best_prompt = scores[0][0]

            history.append({
                "iteration": iteration,
                "best_score": best_score,
                "avg_score": sum(s[1] for s in scores) / len(scores),
                "best_prompt_preview": best_prompt[:100] + "..."
            })

            # Early stopping
            if best_score >= self.config.early_stopping_threshold:
                break

            # Selection and reproduction
            new_population = []

            # Keep elite
            for i in range(self.config.elite_size):
                new_population.append(scores[i][0])

            # Generate new population through mutation and crossover
            while len(new_population) < self.config.population_size:
                # Select parent (tournament selection)
                parent = self._tournament_select(scores)

                # Mutate with probability
                if random.random() < self.config.mutation_rate:
                    child = self._mutate(parent, strategies)
                else:
                    child = parent

                new_population.append(child)

            population = new_population

        return best_prompt, best_score, history

    def _tournament_select(
        self,
        scored_population: List[Tuple[str, float]],
        tournament_size: int = 3
    ) -> str:
        """Tournament selection."""
        tournament = random.sample(scored_population, min(tournament_size, len(scored_population)))
        winner = max(tournament, key=lambda x: x[1])
        return winner[0]

    def _mutate(
        self,
        prompt: str,
        strategies: List[Callable[[str], str]]
    ) -> str:
        """Apply random mutation strategy."""
        strategy = random.choice(strategies)
        return strategy(prompt)

    def _default_strategies(self) -> List[Callable[[str], str]]:
        """Default mutation strategies."""
        return [
            self._add_emphasis,
            self._add_step_by_step,
            self._add_constraints,
            self._rephrase_instruction,
            self._add_example_request,
            self._add_output_format
        ]

    def _add_emphasis(self, prompt: str) -> str:
        """Add emphasis to key instructions."""
        emphases = [
            "IMPORTANT: ",
            "Note: ",
            "Remember: ",
            "Critical: "
        ]
        sentences = prompt.split(". ")
        if len(sentences) > 1:
            idx = random.randint(0, len(sentences) - 1)
            sentences[idx] = random.choice(emphases) + sentences[idx]
        return ". ".join(sentences)

    def _add_step_by_step(self, prompt: str) -> str:
        """Add step-by-step instruction."""
        additions = [
            "\n\nLet's approach this step by step.",
            "\n\nThink through this carefully.",
            "\n\nBreak this down into steps."
        ]
        return prompt + random.choice(additions)

    def _add_constraints(self, prompt: str) -> str:
        """Add constraints."""
        constraints = [
            "\n\nBe concise.",
            "\n\nBe specific and detailed.",
            "\n\nFocus on accuracy.",
            "\n\nAvoid speculation."
        ]
        return prompt + random.choice(constraints)

    def _rephrase_instruction(self, prompt: str) -> str:
        """Rephrase using different instruction style."""
        rephrases = [
            ("Please ", ""),
            ("You should ", ""),
            ("", "Please "),
            ("Provide ", "Generate "),
            ("Create ", "Produce ")
        ]
        old, new = random.choice(rephrases)
        return prompt.replace(old, new, 1)

    def _add_example_request(self, prompt: str) -> str:
        """Request example format."""
        return prompt + "\n\nFirst, briefly describe your approach."

    def _add_output_format(self, prompt: str) -> str:
        """Add output format instruction."""
        formats = [
            "\n\nFormat your response clearly.",
            "\n\nStructure your answer with clear sections.",
            "\n\nUse bullet points where appropriate."
        ]
        return prompt + random.choice(formats)


class PromptRefiner:
    """
    Uses LLM to refine prompts based on failures.
    """

    def __init__(self, llm_client: Any):
        self.llm = llm_client

    async def refine_prompt(
        self,
        original_prompt: str,
        failures: List[Dict],
        successful_examples: List[Dict] = None
    ) -> str:
        """Refine prompt based on failure analysis."""
        refinement_prompt = f"""You are a prompt engineering expert. Analyze this prompt and its failures to create an improved version.

ORIGINAL PROMPT:
{original_prompt}

FAILURES (cases where the prompt didn't work well):
{self._format_failures(failures)}

{self._format_successes(successful_examples) if successful_examples else ""}

TASK:
1. Analyze what went wrong in each failure case
2. Identify patterns in the failures
3. Create an improved prompt that addresses these issues
4. Explain your improvements

Provide the improved prompt between <improved_prompt> tags."""

        response = await self.llm.complete(refinement_prompt)

        # Extract improved prompt
        import re
        match = re.search(r'<improved_prompt>(.*?)</improved_prompt>', response, re.DOTALL)
        if match:
            return match.group(1).strip()
        return original_prompt

    def _format_failures(self, failures: List[Dict]) -> str:
        result = ""
        for i, f in enumerate(failures, 1):
            result += f"\nFailure {i}:\n"
            result += f"  Input: {f.get('input', 'N/A')}\n"
            result += f"  Expected: {f.get('expected', 'N/A')}\n"
            result += f"  Actual: {f.get('actual', 'N/A')}\n"
            result += f"  Issue: {f.get('issue', 'N/A')}\n"
        return result

    def _format_successes(self, successes: List[Dict]) -> str:
        if not successes:
            return ""
        result = "\nSUCCESSFUL EXAMPLES (for reference):\n"
        for i, s in enumerate(successes[:3], 1):
            result += f"\nSuccess {i}:\n"
            result += f"  Input: {s.get('input', 'N/A')}\n"
            result += f"  Output: {s.get('output', 'N/A')}\n"
        return result
```

---

## 5. Production Prompts

### 5.1 Version Control and Management

```python
"""
Production prompt management system.
"""
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any
from datetime import datetime
from enum import Enum
import hashlib
import json


class PromptStatus(Enum):
    """Status of a prompt in production."""
    DRAFT = "draft"
    REVIEW = "review"
    APPROVED = "approved"
    ACTIVE = "active"
    DEPRECATED = "deprecated"
    ARCHIVED = "archived"


@dataclass
class PromptVersion:
    """A version of a prompt."""
    version_id: str
    version_number: str  # Semantic versioning
    system_prompt: str
    user_template: str

    # Metadata
    created_at: datetime
    created_by: str
    change_description: str

    # Testing
    test_results: Optional[Dict] = None
    approved_by: Optional[str] = None
    approved_at: Optional[datetime] = None

    # Hash for integrity
    content_hash: str = ""

    def __post_init__(self):
        if not self.content_hash:
            self.content_hash = self._compute_hash()

    def _compute_hash(self) -> str:
        content = f"{self.system_prompt}|{self.user_template}"
        return hashlib.sha256(content.encode()).hexdigest()[:16]


@dataclass
class ManagedPrompt:
    """A managed prompt with version history."""
    prompt_id: str
    name: str
    description: str
    task_type: str

    # Versions
    versions: List[PromptVersion] = field(default_factory=list)
    active_version: Optional[str] = None  # version_id of active version

    # Configuration
    variables: List[str] = field(default_factory=list)
    defaults: Dict[str, Any] = field(default_factory=dict)

    # Metadata
    created_at: datetime = field(default_factory=datetime.utcnow)
    updated_at: datetime = field(default_factory=datetime.utcnow)
    owner: str = ""
    tags: List[str] = field(default_factory=list)


class PromptRegistry:
    """
    Production prompt registry with versioning.
    """

    def __init__(self, storage_backend: Any = None):
        self.storage = storage_backend or {}
        self.prompts: Dict[str, ManagedPrompt] = {}

    def create_prompt(
        self,
        name: str,
        description: str,
        task_type: str,
        system_prompt: str,
        user_template: str,
        created_by: str,
        variables: List[str] = None,
        defaults: Dict = None
    ) -> ManagedPrompt:
        """Create a new managed prompt."""
        prompt_id = f"prompt_{datetime.utcnow().strftime('%Y%m%d%H%M%S')}"

        # Create initial version
        version = PromptVersion(
            version_id=f"{prompt_id}_v1.0.0",
            version_number="1.0.0",
            system_prompt=system_prompt,
            user_template=user_template,
            created_at=datetime.utcnow(),
            created_by=created_by,
            change_description="Initial version"
        )

        prompt = ManagedPrompt(
            prompt_id=prompt_id,
            name=name,
            description=description,
            task_type=task_type,
            versions=[version],
            variables=variables or [],
            defaults=defaults or {},
            owner=created_by
        )

        self.prompts[prompt_id] = prompt
        return prompt

    def update_prompt(
        self,
        prompt_id: str,
        system_prompt: str,
        user_template: str,
        updated_by: str,
        change_description: str,
        version_bump: str = "patch"  # "major", "minor", "patch"
    ) -> PromptVersion:
        """Create a new version of a prompt."""
        prompt = self.prompts.get(prompt_id)
        if not prompt:
            raise ValueError(f"Prompt {prompt_id} not found")

        # Calculate new version number
        current_version = prompt.versions[-1].version_number
        new_version = self._bump_version(current_version, version_bump)

        version = PromptVersion(
            version_id=f"{prompt_id}_v{new_version}",
            version_number=new_version,
            system_prompt=system_prompt,
            user_template=user_template,
            created_at=datetime.utcnow(),
            created_by=updated_by,
            change_description=change_description
        )

        prompt.versions.append(version)
        prompt.updated_at = datetime.utcnow()

        return version

    def _bump_version(self, current: str, bump_type: str) -> str:
        """Bump version number."""
        parts = [int(p) for p in current.split(".")]
        if bump_type == "major":
            parts[0] += 1
            parts[1] = 0
            parts[2] = 0
        elif bump_type == "minor":
            parts[1] += 1
            parts[2] = 0
        else:  # patch
            parts[2] += 1
        return ".".join(str(p) for p in parts)

    def approve_version(
        self,
        prompt_id: str,
        version_id: str,
        approved_by: str,
        test_results: Dict = None
    ) -> bool:
        """Approve a prompt version."""
        prompt = self.prompts.get(prompt_id)
        if not prompt:
            return False

        for version in prompt.versions:
            if version.version_id == version_id:
                version.approved_by = approved_by
                version.approved_at = datetime.utcnow()
                version.test_results = test_results
                return True

        return False

    def activate_version(
        self,
        prompt_id: str,
        version_id: str
    ) -> bool:
        """Activate a specific version."""
        prompt = self.prompts.get(prompt_id)
        if not prompt:
            return False

        # Verify version exists and is approved
        version = next((v for v in prompt.versions if v.version_id == version_id), None)
        if not version or not version.approved_by:
            return False

        prompt.active_version = version_id
        return True

    def get_active_prompt(self, prompt_id: str) -> Optional[Dict]:
        """Get the active version of a prompt."""
        prompt = self.prompts.get(prompt_id)
        if not prompt or not prompt.active_version:
            return None

        version = next(
            (v for v in prompt.versions if v.version_id == prompt.active_version),
            None
        )
        if not version:
            return None

        return {
            "prompt_id": prompt_id,
            "version_id": version.version_id,
            "version_number": version.version_number,
            "system_prompt": version.system_prompt,
            "user_template": version.user_template,
            "variables": prompt.variables,
            "defaults": prompt.defaults,
            "content_hash": version.content_hash
        }

    def render_prompt(
        self,
        prompt_id: str,
        variables: Dict,
        version_id: Optional[str] = None
    ) -> List[Dict[str, str]]:
        """Render a prompt with variables."""
        prompt = self.prompts.get(prompt_id)
        if not prompt:
            raise ValueError(f"Prompt {prompt_id} not found")

        # Get version
        if version_id:
            version = next((v for v in prompt.versions if v.version_id == version_id), None)
        else:
            version = next(
                (v for v in prompt.versions if v.version_id == prompt.active_version),
                None
            )

        if not version:
            raise ValueError("No version found")

        # Merge defaults with provided variables
        all_vars = {**prompt.defaults, **variables}

        # Validate required variables
        missing = [v for v in prompt.variables if v not in all_vars]
        if missing:
            raise ValueError(f"Missing required variables: {missing}")

        # Render
        system = version.system_prompt
        user = version.user_template

        for key, value in all_vars.items():
            if isinstance(value, (list, dict)):
                value = json.dumps(value)
            system = system.replace(f"{{{key}}}", str(value))
            user = user.replace(f"{{{key}}}", str(value))

        return [
            {"role": "system", "content": system},
            {"role": "user", "content": user}
        ]

    def list_prompts(
        self,
        task_type: Optional[str] = None,
        owner: Optional[str] = None
    ) -> List[Dict]:
        """List prompts with optional filters."""
        results = []
        for prompt in self.prompts.values():
            if task_type and prompt.task_type != task_type:
                continue
            if owner and prompt.owner != owner:
                continue

            active_version = next(
                (v for v in prompt.versions if v.version_id == prompt.active_version),
                None
            )

            results.append({
                "prompt_id": prompt.prompt_id,
                "name": prompt.name,
                "task_type": prompt.task_type,
                "active_version": active_version.version_number if active_version else None,
                "total_versions": len(prompt.versions),
                "owner": prompt.owner,
                "updated_at": prompt.updated_at.isoformat()
            })

        return results


class PromptTestRunner:
    """
    Runs tests on prompts before deployment.
    """

    def __init__(self, llm_client: Any):
        self.llm = llm_client

    async def run_tests(
        self,
        prompt: Dict,
        test_cases: List[Dict],
        evaluators: List[Callable] = None
    ) -> Dict:
        """Run test cases against a prompt."""
        results = {
            "total": len(test_cases),
            "passed": 0,
            "failed": 0,
            "details": []
        }

        for test in test_cases:
            # Prepare prompt with test input
            messages = [
                {"role": "system", "content": prompt["system_prompt"]},
                {"role": "user", "content": prompt["user_template"].format(**test["input"])}
            ]

            # Run inference
            response = await self.llm.chat(messages)

            # Evaluate
            passed = True
            evaluations = []

            if evaluators:
                for evaluator in evaluators:
                    eval_result = evaluator(response, test.get("expected"))
                    evaluations.append(eval_result)
                    if not eval_result.get("passed", True):
                        passed = False
            else:
                # Simple exact match if no evaluators
                if test.get("expected"):
                    passed = response.strip() == test["expected"].strip()

            if passed:
                results["passed"] += 1
            else:
                results["failed"] += 1

            results["details"].append({
                "input": test["input"],
                "expected": test.get("expected"),
                "actual": response,
                "passed": passed,
                "evaluations": evaluations
            })

        results["pass_rate"] = results["passed"] / results["total"] if results["total"] > 0 else 0
        return results
```

---

## 6. Implementation

### 6.1 Complete Prompt Engineering System

```python
"""
Complete prompt engineering system implementation.
"""


class PromptEngineeringSystem:
    """
    Complete system for prompt engineering, testing, and deployment.
    """

    def __init__(self, llm_client: Any, config: Dict = None):
        self.llm = llm_client
        self.config = config or {}

        # Initialize components
        self.registry = PromptRegistry()
        self.template_library = PromptTemplateLibrary()
        self.experiment_manager = PromptExperimentManager()
        self.test_runner = PromptTestRunner(llm_client)

    async def create_and_test_prompt(
        self,
        name: str,
        task_type: str,
        system_prompt: str,
        user_template: str,
        test_cases: List[Dict],
        created_by: str
    ) -> Dict:
        """Create a prompt and run initial tests."""
        # Create prompt
        prompt = self.registry.create_prompt(
            name=name,
            description=f"Prompt for {task_type}",
            task_type=task_type,
            system_prompt=system_prompt,
            user_template=user_template,
            created_by=created_by
        )

        # Run tests
        active = self.registry.get_active_prompt(prompt.prompt_id)
        if not active:
            # Activate first version for testing
            self.registry.activate_version(
                prompt.prompt_id,
                prompt.versions[0].version_id
            )
            active = self.registry.get_active_prompt(prompt.prompt_id)

        test_results = await self.test_runner.run_tests(
            {
                "system_prompt": active["system_prompt"],
                "user_template": active["user_template"]
            },
            test_cases
        )

        return {
            "prompt": prompt,
            "test_results": test_results,
            "status": "passed" if test_results["pass_rate"] >= 0.8 else "needs_improvement"
        }

    async def optimize_prompt(
        self,
        prompt_id: str,
        test_cases: List[Dict],
        max_iterations: int = 5
    ) -> Dict:
        """Optimize a prompt using automatic techniques."""
        active = self.registry.get_active_prompt(prompt_id)
        if not active:
            raise ValueError(f"No active prompt for {prompt_id}")

        # Create evaluator
        async def evaluate(prompt_text: str, tests: List[Dict]) -> float:
            results = await self.test_runner.run_tests(
                {"system_prompt": active["system_prompt"], "user_template": prompt_text},
                tests
            )
            return results["pass_rate"]

        # Run optimization
        optimizer = PromptOptimizer(
            evaluator=lambda p, t: evaluate(p, t),  # Simplified for sync
            config=OptimizationConfig(max_iterations=max_iterations)
        )

        best_prompt, best_score, history = optimizer.optimize(
            active["user_template"],
            test_cases
        )

        return {
            "original_score": history[0]["best_score"] if history else 0,
            "optimized_score": best_score,
            "optimized_template": best_prompt,
            "improvement": best_score - (history[0]["best_score"] if history else 0),
            "history": history
        }

    async def run_ab_test(
        self,
        prompt_id: str,
        variant_templates: Dict[str, str],
        traffic_percentage: float = 50
    ) -> str:
        """Set up A/B test for prompt variants."""
        active = self.registry.get_active_prompt(prompt_id)
        if not active:
            raise ValueError(f"No active prompt for {prompt_id}")

        # Create variants
        variants = [
            PromptVariant(
                variant_id="control",
                name="Control",
                system_prompt=active["system_prompt"],
                user_template=active["user_template"]
            )
        ]

        for name, template in variant_templates.items():
            variants.append(PromptVariant(
                variant_id=name,
                name=name.title(),
                system_prompt=active["system_prompt"],
                user_template=template
            ))

        # Create experiment
        experiment = self.experiment_manager.create_experiment(
            name=f"A/B Test for {prompt_id}",
            description="Comparing prompt variants",
            variants=variants,
            traffic_percentage=traffic_percentage
        )

        self.experiment_manager.start_experiment(experiment.experiment_id)

        return experiment.experiment_id


def example_usage():
    """Example of using the prompt engineering system."""

    # Example: Creating a customer support prompt

    system_prompt = """You are a helpful customer support assistant for TechCorp.

Guidelines:
- Be friendly and professional
- Provide accurate information about our products
- If you don't know something, say so and offer to escalate
- Keep responses concise but complete"""

    user_template = """Customer inquiry: {inquiry}

Customer context:
- Name: {customer_name}
- Account type: {account_type}
- Previous interactions: {interaction_count}

Please provide a helpful response."""

    test_cases = [
        {
            "input": {
                "inquiry": "How do I reset my password?",
                "customer_name": "John",
                "account_type": "Premium",
                "interaction_count": 5
            },
            "expected_contains": ["password", "reset"]
        },
        {
            "input": {
                "inquiry": "I want a refund",
                "customer_name": "Jane",
                "account_type": "Basic",
                "interaction_count": 1
            },
            "expected_contains": ["refund", "policy"]
        }
    ]

    # Initialize system (with mock client for example)
    # system = PromptEngineeringSystem(llm_client)

    # Create and test prompt
    # result = await system.create_and_test_prompt(
    #     name="Customer Support",
    #     task_type="conversation",
    #     system_prompt=system_prompt,
    #     user_template=user_template,
    #     test_cases=test_cases,
    #     created_by="ml-team"
    # )

    print("Prompt engineering system initialized")
    print(f"System prompt length: {len(system_prompt)}")
    print(f"Test cases: {len(test_cases)}")
```

---

## Appendices

### Appendix A: Prompt Template Library

```yaml
# templates/qa_grounded.yaml
template_id: qa_grounded
name: Grounded Question Answering
version: "1.0.0"

system_prompt: |
  You answer questions based solely on provided context.

  Rules:
  - Only use information from the context
  - Quote sources when answering
  - Say "I cannot find this information" if context doesn't contain answer
  - Never make assumptions or infer beyond the context

user_template: |
  Context:
  {context}

  Question: {question}

  Answer:

variables:
  - context
  - question

best_practices:
  - Chunk context appropriately (512-1024 tokens)
  - Include document sources/titles
  - Test with adversarial questions
```

### Appendix B: Testing Framework Setup

```python
# tests/test_prompts.py
import pytest
from prompt_system import PromptTestRunner, PromptRegistry

@pytest.fixture
def registry():
    return PromptRegistry()

@pytest.fixture
def test_runner(mock_llm):
    return PromptTestRunner(mock_llm)

async def test_qa_prompt_accuracy(registry, test_runner):
    """Test Q&A prompt accuracy."""
    prompt = registry.create_prompt(
        name="Test QA",
        task_type="qa",
        system_prompt="Answer questions accurately.",
        user_template="Question: {question}",
        created_by="test"
    )

    test_cases = [
        {"input": {"question": "What is 2+2?"}, "expected": "4"}
    ]

    results = await test_runner.run_tests(
        registry.get_active_prompt(prompt.prompt_id),
        test_cases
    )

    assert results["pass_rate"] >= 0.8
```

### Appendix C: Optimization Scripts

```python
# scripts/optimize_prompt.py
import asyncio
from prompt_system import PromptEngineeringSystem

async def optimize_production_prompt(prompt_id: str):
    """Optimize a production prompt."""
    system = PromptEngineeringSystem(llm_client)

    # Load test cases
    test_cases = load_test_cases(prompt_id)

    # Optimize
    result = await system.optimize_prompt(
        prompt_id=prompt_id,
        test_cases=test_cases,
        max_iterations=10
    )

    print(f"Optimization complete:")
    print(f"  Original score: {result['original_score']:.2%}")
    print(f"  Optimized score: {result['optimized_score']:.2%}")
    print(f"  Improvement: {result['improvement']:.2%}")

    # Save optimized version if improved
    if result['improvement'] > 0.05:
        registry.update_prompt(
            prompt_id=prompt_id,
            user_template=result['optimized_template'],
            updated_by="optimization_script",
            change_description=f"Automated optimization (+{result['improvement']:.1%})"
        )

if __name__ == "__main__":
    asyncio.run(optimize_production_prompt("prompt_customer_support"))
```

### Appendix D: Best Practices Checklist

```markdown
# Prompt Engineering Best Practices

## Design
- [ ] Clear, specific instructions
- [ ] Appropriate role/persona assignment
- [ ] Explicit output format specification
- [ ] Delimiter separation for user content
- [ ] Few-shot examples for complex tasks

## Testing
- [ ] Test with diverse inputs
- [ ] Include edge cases
- [ ] Test failure modes
- [ ] Measure consistency
- [ ] Evaluate safety

## Production
- [ ] Version control all prompts
- [ ] Require approval for changes
- [ ] Monitor production performance
- [ ] A/B test significant changes
- [ ] Document all prompts

## Security
- [ ] Prevent prompt injection
- [ ] Protect system prompt
- [ ] Validate user inputs
- [ ] Filter sensitive outputs
```

---

## References

1. OpenAI Prompt Engineering Guide
2. Anthropic Prompt Engineering Best Practices
3. Chain-of-Thought Prompting (Wei et al., 2022)
4. Tree of Thoughts (Yao et al., 2023)
5. ReAct: Synergizing Reasoning and Acting (Yao et al., 2022)
