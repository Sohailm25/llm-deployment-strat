> **Navigation** | [← 14.3 GPU Infrastructure](../14_cost_capacity_management/14.3_gpu_infrastructure_optimization_guide.md) | [15.2 Data Migration →](15.2_data_migration_guide.md)
>
> | | |
> |---|---|
> | **Prerequisites** | Model serving infrastructure &#124; Model registries &#124; Evaluation metrics |
> | **Related** | [8.1 Model Registry](../08_mlops_lifecycle/8.1_model_registry_guide.md) &#124; [8.3 Model Versioning](../08_mlops_lifecycle/8.3_model_versioning_artifacts_guide.md) |
> | **Next** | [15.2 Data Migration](15.2_data_migration_guide.md) |

# 15.1 Model Migration Guide

## Document Information
- **Version**: 1.0
- **Last Updated**: 2024-01-15
- **Owner**: ML Platform Team
- **Classification**: Internal

## Purpose and Scope

This guide provides comprehensive procedures for migrating LLM models within the Multi-Cloud RAG Platform. It covers model version upgrades, provider migrations, architecture transitions, and rollback procedures.

## Prerequisites

- Understanding of model serving infrastructure
- Access to model registries and deployment systems
- Familiarity with model evaluation metrics
- Testing environment access

## 1. Model Migration Framework

### 1.1 Migration Types and Planning

```python
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Callable
from enum import Enum
from datetime import datetime, timedelta
import json


class MigrationType(Enum):
    """Types of model migrations."""
    VERSION_UPGRADE = "version_upgrade"         # Same model, new version
    PROVIDER_SWITCH = "provider_switch"         # Different provider
    ARCHITECTURE_CHANGE = "architecture_change" # Different model architecture
    QUANTIZATION = "quantization"               # Same model, different precision
    FINE_TUNED = "fine_tuned"                   # Base to fine-tuned
    CONSOLIDATION = "consolidation"             # Multiple models to one


class MigrationStrategy(Enum):
    """Migration deployment strategies."""
    BIG_BANG = "big_bang"                 # Instant cutover
    BLUE_GREEN = "blue_green"             # Parallel deployment
    CANARY = "canary"                     # Gradual traffic shift
    SHADOW = "shadow"                     # Shadow testing first
    FEATURE_FLAG = "feature_flag"         # Per-feature/user rollout


class MigrationPhase(Enum):
    """Phases of migration."""
    PLANNING = "planning"
    PREPARATION = "preparation"
    VALIDATION = "validation"
    DEPLOYMENT = "deployment"
    MONITORING = "monitoring"
    COMPLETION = "completion"
    ROLLBACK = "rollback"


@dataclass
class ModelSpec:
    """Specification of a model."""
    model_id: str
    provider: str
    model_name: str
    version: str
    architecture: str
    parameters: str  # e.g., "7B", "70B"
    context_length: int
    input_cost_per_1k: float
    output_cost_per_1k: float
    latency_p50_ms: float
    latency_p99_ms: float
    capabilities: List[str] = field(default_factory=list)


@dataclass
class MigrationPlan:
    """Complete migration plan."""
    migration_id: str
    migration_type: MigrationType
    strategy: MigrationStrategy
    source_model: ModelSpec
    target_model: ModelSpec
    created_at: datetime
    planned_start: datetime
    planned_end: datetime
    owner: str
    stakeholders: List[str]
    current_phase: MigrationPhase = MigrationPhase.PLANNING
    phases: Dict[str, Dict] = field(default_factory=dict)
    rollback_plan: Optional[Dict] = None
    success_criteria: List[Dict] = field(default_factory=list)
    risks: List[Dict] = field(default_factory=list)


class MigrationPlanner:
    """Plan and manage model migrations."""

    def __init__(self):
        self.migrations: Dict[str, MigrationPlan] = {}
        self.model_catalog: Dict[str, ModelSpec] = {}

    def register_model(self, model: ModelSpec):
        """Register a model in the catalog."""
        self.model_catalog[model.model_id] = model

    def create_migration_plan(
        self,
        source_model_id: str,
        target_model_id: str,
        migration_type: MigrationType,
        strategy: MigrationStrategy,
        owner: str,
        planned_start: datetime
    ) -> MigrationPlan:
        """Create a migration plan."""
        source = self.model_catalog.get(source_model_id)
        target = self.model_catalog.get(target_model_id)

        if not source or not target:
            raise ValueError("Source or target model not found")

        migration_id = f"MIG-{datetime.utcnow().strftime('%Y%m%d%H%M%S')}"

        plan = MigrationPlan(
            migration_id=migration_id,
            migration_type=migration_type,
            strategy=strategy,
            source_model=source,
            target_model=target,
            created_at=datetime.utcnow(),
            planned_start=planned_start,
            planned_end=planned_start + timedelta(days=self._estimate_duration(migration_type, strategy)),
            owner=owner,
            stakeholders=[],
            phases=self._create_phase_plan(migration_type, strategy),
            rollback_plan=self._create_rollback_plan(source, target),
            success_criteria=self._define_success_criteria(source, target),
            risks=self._identify_risks(migration_type, source, target)
        )

        self.migrations[migration_id] = plan
        return plan

    def _estimate_duration(
        self,
        migration_type: MigrationType,
        strategy: MigrationStrategy
    ) -> int:
        """Estimate migration duration in days."""
        base_duration = {
            MigrationType.VERSION_UPGRADE: 3,
            MigrationType.PROVIDER_SWITCH: 7,
            MigrationType.ARCHITECTURE_CHANGE: 14,
            MigrationType.QUANTIZATION: 5,
            MigrationType.FINE_TUNED: 10,
            MigrationType.CONSOLIDATION: 21
        }

        strategy_multiplier = {
            MigrationStrategy.BIG_BANG: 1.0,
            MigrationStrategy.BLUE_GREEN: 1.5,
            MigrationStrategy.CANARY: 2.0,
            MigrationStrategy.SHADOW: 2.5,
            MigrationStrategy.FEATURE_FLAG: 2.0
        }

        return int(base_duration[migration_type] * strategy_multiplier[strategy])

    def _create_phase_plan(
        self,
        migration_type: MigrationType,
        strategy: MigrationStrategy
    ) -> Dict[str, Dict]:
        """Create phase-by-phase plan."""
        return {
            "planning": {
                "tasks": [
                    "Define migration scope and objectives",
                    "Identify affected systems and dependencies",
                    "Create test plan",
                    "Define success criteria",
                    "Get stakeholder approval"
                ],
                "duration_days": 2,
                "deliverables": ["Migration plan document", "Test plan"]
            },
            "preparation": {
                "tasks": [
                    "Set up target model infrastructure",
                    "Configure monitoring and alerting",
                    "Prepare rollback procedures",
                    "Create feature flags (if applicable)",
                    "Update documentation"
                ],
                "duration_days": 3,
                "deliverables": ["Infrastructure ready", "Monitoring configured"]
            },
            "validation": {
                "tasks": [
                    "Run offline evaluation tests",
                    "Compare output quality metrics",
                    "Performance benchmarking",
                    "Shadow testing (if applicable)",
                    "Security and compliance review"
                ],
                "duration_days": 5,
                "deliverables": ["Evaluation report", "Performance benchmarks"]
            },
            "deployment": {
                "tasks": [
                    "Deploy target model",
                    "Configure traffic routing",
                    "Execute cutover (per strategy)",
                    "Monitor initial traffic",
                    "Address immediate issues"
                ],
                "duration_days": 2,
                "deliverables": ["Deployment complete", "Initial metrics"]
            },
            "monitoring": {
                "tasks": [
                    "Monitor quality metrics",
                    "Track user feedback",
                    "Analyze cost impact",
                    "Performance monitoring",
                    "Incident response (if needed)"
                ],
                "duration_days": 7,
                "deliverables": ["Monitoring report", "Issue log"]
            },
            "completion": {
                "tasks": [
                    "Final quality assessment",
                    "Cost analysis",
                    "Documentation update",
                    "Decommission old model",
                    "Post-migration review"
                ],
                "duration_days": 2,
                "deliverables": ["Final report", "Lessons learned"]
            }
        }

    def _create_rollback_plan(
        self,
        source: ModelSpec,
        target: ModelSpec
    ) -> Dict:
        """Create rollback plan."""
        return {
            "trigger_conditions": [
                "Error rate exceeds 5%",
                "Latency increases by more than 50%",
                "Quality metrics degrade by more than 10%",
                "Critical security issue discovered"
            ],
            "rollback_steps": [
                "Alert on-call team",
                "Verify rollback readiness",
                "Switch traffic to source model",
                "Verify source model is serving",
                "Notify stakeholders",
                "Document incident"
            ],
            "rollback_duration_minutes": 15,
            "source_model_retention_days": 30,
            "data_preservation": True
        }

    def _define_success_criteria(
        self,
        source: ModelSpec,
        target: ModelSpec
    ) -> List[Dict]:
        """Define success criteria for migration."""
        return [
            {
                "metric": "error_rate",
                "condition": "less_than",
                "threshold": 0.01,
                "measurement_period_hours": 24
            },
            {
                "metric": "latency_p50",
                "condition": "less_than",
                "threshold": target.latency_p50_ms * 1.1,  # 10% tolerance
                "measurement_period_hours": 24
            },
            {
                "metric": "latency_p99",
                "condition": "less_than",
                "threshold": target.latency_p99_ms * 1.2,  # 20% tolerance
                "measurement_period_hours": 24
            },
            {
                "metric": "quality_score",
                "condition": "greater_than",
                "threshold": 0.95,  # Relative to source
                "measurement_period_hours": 48
            },
            {
                "metric": "user_satisfaction",
                "condition": "greater_than",
                "threshold": 0.9,  # No more than 10% degradation
                "measurement_period_hours": 72
            }
        ]

    def _identify_risks(
        self,
        migration_type: MigrationType,
        source: ModelSpec,
        target: ModelSpec
    ) -> List[Dict]:
        """Identify migration risks."""
        risks = [
            {
                "risk": "Output quality degradation",
                "likelihood": "medium",
                "impact": "high",
                "mitigation": "Extensive testing, canary deployment"
            },
            {
                "risk": "Performance regression",
                "likelihood": "medium",
                "impact": "high",
                "mitigation": "Benchmarking, gradual rollout"
            },
            {
                "risk": "Compatibility issues",
                "likelihood": "low",
                "impact": "medium",
                "mitigation": "API compatibility testing"
            }
        ]

        # Add type-specific risks
        if migration_type == MigrationType.PROVIDER_SWITCH:
            risks.append({
                "risk": "API behavior differences",
                "likelihood": "high",
                "impact": "medium",
                "mitigation": "Adapter layer, thorough testing"
            })

        if migration_type == MigrationType.ARCHITECTURE_CHANGE:
            risks.append({
                "risk": "Significant behavior changes",
                "likelihood": "high",
                "impact": "high",
                "mitigation": "Extensive evaluation, longer canary"
            })

        if target.context_length != source.context_length:
            risks.append({
                "risk": "Context length mismatch",
                "likelihood": "medium",
                "impact": "medium",
                "mitigation": "Update chunking, test long documents"
            })

        return risks

    def update_phase(
        self,
        migration_id: str,
        new_phase: MigrationPhase
    ) -> MigrationPlan:
        """Update migration to new phase."""
        plan = self.migrations.get(migration_id)
        if not plan:
            raise ValueError(f"Migration {migration_id} not found")

        plan.current_phase = new_phase
        return plan

    def generate_migration_report(
        self,
        migration_id: str
    ) -> str:
        """Generate migration status report."""
        plan = self.migrations.get(migration_id)
        if not plan:
            return "Migration not found"

        report = f"""
# Model Migration Report

## Migration ID: {plan.migration_id}

### Overview
- **Type**: {plan.migration_type.value}
- **Strategy**: {plan.strategy.value}
- **Current Phase**: {plan.current_phase.value}
- **Owner**: {plan.owner}

### Models
| Attribute | Source | Target |
|-----------|--------|--------|
| Provider | {plan.source_model.provider} | {plan.target_model.provider} |
| Model | {plan.source_model.model_name} | {plan.target_model.model_name} |
| Version | {plan.source_model.version} | {plan.target_model.version} |
| Parameters | {plan.source_model.parameters} | {plan.target_model.parameters} |
| Context | {plan.source_model.context_length} | {plan.target_model.context_length} |

### Timeline
- **Planned Start**: {plan.planned_start.strftime('%Y-%m-%d')}
- **Planned End**: {plan.planned_end.strftime('%Y-%m-%d')}

### Success Criteria
"""
        for criteria in plan.success_criteria:
            report += f"- {criteria['metric']}: {criteria['condition']} {criteria['threshold']}\n"

        report += "\n### Risks\n"
        for risk in plan.risks:
            report += f"- **{risk['risk']}** ({risk['likelihood']} likelihood, {risk['impact']} impact)\n"
            report += f"  - Mitigation: {risk['mitigation']}\n"

        return report
```

### 1.2 Model Evaluation Framework

```python
from dataclasses import dataclass
from typing import Dict, List, Optional, Callable, Any
from datetime import datetime
import json
import numpy as np


@dataclass
class EvaluationDataset:
    """Dataset for model evaluation."""
    name: str
    samples: List[Dict]
    ground_truth_key: str
    input_key: str
    metadata: Dict = field(default_factory=dict)


@dataclass
class EvaluationResult:
    """Result of a model evaluation."""
    model_id: str
    dataset_name: str
    metrics: Dict[str, float]
    samples_evaluated: int
    evaluation_time_seconds: float
    evaluated_at: datetime
    details: List[Dict] = field(default_factory=list)


class ModelEvaluator:
    """Evaluate models for migration comparison."""

    def __init__(self):
        self.metrics: Dict[str, Callable] = {}
        self.datasets: Dict[str, EvaluationDataset] = {}
        self._register_default_metrics()

    def _register_default_metrics(self):
        """Register default evaluation metrics."""
        self.metrics["exact_match"] = self._exact_match
        self.metrics["f1_score"] = self._f1_score
        self.metrics["semantic_similarity"] = self._semantic_similarity
        self.metrics["rouge_l"] = self._rouge_l
        self.metrics["response_length_ratio"] = self._response_length_ratio

    def register_metric(self, name: str, func: Callable):
        """Register a custom metric."""
        self.metrics[name] = func

    def add_dataset(self, dataset: EvaluationDataset):
        """Add an evaluation dataset."""
        self.datasets[dataset.name] = dataset

    def evaluate_model(
        self,
        model_id: str,
        model_fn: Callable[[str], str],
        dataset_name: str,
        metrics_to_compute: List[str]
    ) -> EvaluationResult:
        """Evaluate a model on a dataset."""
        dataset = self.datasets.get(dataset_name)
        if not dataset:
            raise ValueError(f"Dataset {dataset_name} not found")

        start_time = datetime.utcnow()
        results = []
        metric_scores: Dict[str, List[float]] = {m: [] for m in metrics_to_compute}

        for sample in dataset.samples:
            input_text = sample[dataset.input_key]
            ground_truth = sample[dataset.ground_truth_key]

            # Get model prediction
            prediction = model_fn(input_text)

            # Compute metrics
            sample_result = {
                "input": input_text[:100] + "..." if len(input_text) > 100 else input_text,
                "ground_truth": ground_truth[:100] + "..." if len(ground_truth) > 100 else ground_truth,
                "prediction": prediction[:100] + "..." if len(prediction) > 100 else prediction,
                "metrics": {}
            }

            for metric_name in metrics_to_compute:
                if metric_name in self.metrics:
                    score = self.metrics[metric_name](prediction, ground_truth)
                    metric_scores[metric_name].append(score)
                    sample_result["metrics"][metric_name] = score

            results.append(sample_result)

        # Aggregate metrics
        aggregated_metrics = {
            name: np.mean(scores) for name, scores in metric_scores.items()
        }

        end_time = datetime.utcnow()

        return EvaluationResult(
            model_id=model_id,
            dataset_name=dataset_name,
            metrics=aggregated_metrics,
            samples_evaluated=len(dataset.samples),
            evaluation_time_seconds=(end_time - start_time).total_seconds(),
            evaluated_at=end_time,
            details=results
        )

    def _exact_match(self, prediction: str, ground_truth: str) -> float:
        """Exact match metric."""
        return 1.0 if prediction.strip().lower() == ground_truth.strip().lower() else 0.0

    def _f1_score(self, prediction: str, ground_truth: str) -> float:
        """Token-level F1 score."""
        pred_tokens = set(prediction.lower().split())
        truth_tokens = set(ground_truth.lower().split())

        if not pred_tokens or not truth_tokens:
            return 0.0

        common = pred_tokens & truth_tokens
        precision = len(common) / len(pred_tokens)
        recall = len(common) / len(truth_tokens)

        if precision + recall == 0:
            return 0.0

        return 2 * precision * recall / (precision + recall)

    def _semantic_similarity(self, prediction: str, ground_truth: str) -> float:
        """Semantic similarity using embeddings (simplified)."""
        # In production, would use actual embeddings
        # Simplified: word overlap ratio
        pred_words = set(prediction.lower().split())
        truth_words = set(ground_truth.lower().split())

        if not pred_words or not truth_words:
            return 0.0

        intersection = pred_words & truth_words
        union = pred_words | truth_words

        return len(intersection) / len(union)

    def _rouge_l(self, prediction: str, ground_truth: str) -> float:
        """ROUGE-L score (simplified LCS-based)."""
        pred_tokens = prediction.lower().split()
        truth_tokens = ground_truth.lower().split()

        if not pred_tokens or not truth_tokens:
            return 0.0

        # Simple LCS implementation
        m, n = len(pred_tokens), len(truth_tokens)
        dp = [[0] * (n + 1) for _ in range(m + 1)]

        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if pred_tokens[i-1] == truth_tokens[j-1]:
                    dp[i][j] = dp[i-1][j-1] + 1
                else:
                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])

        lcs_length = dp[m][n]
        precision = lcs_length / m
        recall = lcs_length / n

        if precision + recall == 0:
            return 0.0

        return 2 * precision * recall / (precision + recall)

    def _response_length_ratio(self, prediction: str, ground_truth: str) -> float:
        """Response length ratio (penalize too short or too long)."""
        pred_len = len(prediction.split())
        truth_len = len(ground_truth.split())

        if truth_len == 0:
            return 0.0

        ratio = pred_len / truth_len

        # Score: 1.0 for perfect match, decreases for deviation
        if ratio > 1:
            return min(1.0, 2 - ratio)  # Penalize longer
        else:
            return ratio  # Penalize shorter


class MigrationComparator:
    """Compare source and target models for migration."""

    def __init__(self, evaluator: ModelEvaluator):
        self.evaluator = evaluator

    def compare_models(
        self,
        source_model_fn: Callable[[str], str],
        target_model_fn: Callable[[str], str],
        source_model_id: str,
        target_model_id: str,
        datasets: List[str],
        metrics: List[str]
    ) -> Dict:
        """Compare two models across datasets."""
        comparisons = []

        for dataset_name in datasets:
            source_result = self.evaluator.evaluate_model(
                source_model_id, source_model_fn, dataset_name, metrics
            )
            target_result = self.evaluator.evaluate_model(
                target_model_id, target_model_fn, dataset_name, metrics
            )

            comparison = {
                "dataset": dataset_name,
                "metrics": {}
            }

            for metric in metrics:
                source_score = source_result.metrics.get(metric, 0)
                target_score = target_result.metrics.get(metric, 0)
                diff = target_score - source_score
                relative_change = (diff / source_score * 100) if source_score else 0

                comparison["metrics"][metric] = {
                    "source": source_score,
                    "target": target_score,
                    "difference": diff,
                    "relative_change_percent": relative_change,
                    "improved": diff > 0
                }

            comparisons.append(comparison)

        # Overall summary
        overall_metrics = {}
        for metric in metrics:
            source_avg = np.mean([
                c["metrics"][metric]["source"] for c in comparisons
            ])
            target_avg = np.mean([
                c["metrics"][metric]["target"] for c in comparisons
            ])
            overall_metrics[metric] = {
                "source_average": source_avg,
                "target_average": target_avg,
                "improvement": target_avg - source_avg
            }

        return {
            "source_model": source_model_id,
            "target_model": target_model_id,
            "datasets_evaluated": len(datasets),
            "comparisons": comparisons,
            "overall_metrics": overall_metrics,
            "migration_recommended": self._recommend_migration(overall_metrics)
        }

    def _recommend_migration(self, overall_metrics: Dict) -> Dict:
        """Recommend whether to proceed with migration."""
        improvements = 0
        regressions = 0
        significant_regressions = 0

        for metric, data in overall_metrics.items():
            if data["improvement"] > 0:
                improvements += 1
            elif data["improvement"] < -0.05:  # More than 5% regression
                significant_regressions += 1
            elif data["improvement"] < 0:
                regressions += 1

        if significant_regressions > 0:
            recommendation = "not_recommended"
            reason = f"{significant_regressions} metrics showed significant regression"
        elif regressions > improvements:
            recommendation = "caution"
            reason = "More metrics regressed than improved"
        else:
            recommendation = "recommended"
            reason = "Overall improvement or stable performance"

        return {
            "recommendation": recommendation,
            "reason": reason,
            "improvements": improvements,
            "regressions": regressions,
            "significant_regressions": significant_regressions
        }
```

## 2. Migration Execution

### 2.1 Traffic Routing and Canary Deployment

```python
from dataclasses import dataclass
from typing import Dict, List, Optional, Callable
from datetime import datetime, timedelta
import random
import asyncio


@dataclass
class TrafficRule:
    """Traffic routing rule."""
    rule_id: str
    target_model: str
    weight: float  # 0.0 to 1.0
    conditions: Dict = field(default_factory=dict)
    priority: int = 0


@dataclass
class CanaryConfig:
    """Canary deployment configuration."""
    initial_weight: float = 0.01  # 1%
    max_weight: float = 1.0
    increment: float = 0.05
    increment_interval_minutes: int = 30
    auto_rollback_threshold: Dict = field(default_factory=dict)


class TrafficRouter:
    """Route traffic between models during migration."""

    def __init__(self):
        self.rules: List[TrafficRule] = []
        self.request_log: List[Dict] = []

    def add_rule(self, rule: TrafficRule):
        """Add a routing rule."""
        self.rules.append(rule)
        self.rules.sort(key=lambda r: r.priority, reverse=True)

    def update_weight(self, rule_id: str, new_weight: float):
        """Update weight for a rule."""
        for rule in self.rules:
            if rule.rule_id == rule_id:
                rule.weight = new_weight
                break

    def route_request(
        self,
        request_id: str,
        user_id: Optional[str] = None,
        features: Dict = None
    ) -> str:
        """Route a request to a model."""
        features = features or {}

        for rule in self.rules:
            # Check conditions
            if self._matches_conditions(rule.conditions, user_id, features):
                # Apply weight
                if random.random() < rule.weight:
                    self._log_routing(request_id, rule.target_model, rule.rule_id)
                    return rule.target_model

        # Default to first rule's target
        default_model = self.rules[0].target_model if self.rules else "default"
        self._log_routing(request_id, default_model, "default")
        return default_model

    def _matches_conditions(
        self,
        conditions: Dict,
        user_id: Optional[str],
        features: Dict
    ) -> bool:
        """Check if request matches rule conditions."""
        if not conditions:
            return True

        # User whitelist
        if "user_whitelist" in conditions:
            if user_id not in conditions["user_whitelist"]:
                return False

        # Feature flags
        if "features" in conditions:
            for key, value in conditions["features"].items():
                if features.get(key) != value:
                    return False

        return True

    def _log_routing(
        self,
        request_id: str,
        model: str,
        rule_id: str
    ):
        """Log routing decision."""
        self.request_log.append({
            "request_id": request_id,
            "model": model,
            "rule_id": rule_id,
            "timestamp": datetime.utcnow().isoformat()
        })

    def get_traffic_distribution(
        self,
        since: datetime
    ) -> Dict[str, int]:
        """Get traffic distribution since a time."""
        distribution: Dict[str, int] = {}

        for log in self.request_log:
            log_time = datetime.fromisoformat(log["timestamp"])
            if log_time >= since:
                model = log["model"]
                distribution[model] = distribution.get(model, 0) + 1

        return distribution


class CanaryDeployer:
    """Manage canary deployments for model migration."""

    def __init__(
        self,
        router: TrafficRouter,
        config: CanaryConfig
    ):
        self.router = router
        self.config = config
        self.canary_rule_id: Optional[str] = None
        self.canary_started: Optional[datetime] = None
        self.current_weight: float = 0.0
        self.metrics_history: List[Dict] = []

    def start_canary(
        self,
        source_model: str,
        target_model: str
    ):
        """Start canary deployment."""
        # Add rule for target (canary)
        self.canary_rule_id = f"canary-{datetime.utcnow().strftime('%Y%m%d%H%M%S')}"
        self.router.add_rule(TrafficRule(
            rule_id=self.canary_rule_id,
            target_model=target_model,
            weight=self.config.initial_weight,
            priority=10
        ))

        self.current_weight = self.config.initial_weight
        self.canary_started = datetime.utcnow()

    def check_and_progress(
        self,
        current_metrics: Dict
    ) -> Dict:
        """Check metrics and potentially progress canary."""
        if not self.canary_rule_id:
            return {"status": "not_started"}

        # Check rollback conditions
        if self._should_rollback(current_metrics):
            self.rollback()
            return {
                "status": "rolled_back",
                "reason": "Metrics exceeded rollback threshold"
            }

        # Check if ready to progress
        time_since_last = datetime.utcnow() - (self.canary_started or datetime.utcnow())
        if time_since_last.total_seconds() >= self.config.increment_interval_minutes * 60:
            if self.current_weight < self.config.max_weight:
                self._progress_canary()
                return {
                    "status": "progressed",
                    "new_weight": self.current_weight
                }

        return {
            "status": "monitoring",
            "current_weight": self.current_weight,
            "time_to_next_check_minutes": max(
                0,
                self.config.increment_interval_minutes -
                time_since_last.total_seconds() / 60
            )
        }

    def _should_rollback(self, metrics: Dict) -> bool:
        """Check if rollback is needed based on metrics."""
        for metric, threshold in self.config.auto_rollback_threshold.items():
            current_value = metrics.get(metric)
            if current_value is not None:
                if "max" in threshold and current_value > threshold["max"]:
                    return True
                if "min" in threshold and current_value < threshold["min"]:
                    return True
        return False

    def _progress_canary(self):
        """Progress canary to next weight level."""
        new_weight = min(
            self.current_weight + self.config.increment,
            self.config.max_weight
        )
        self.router.update_weight(self.canary_rule_id, new_weight)
        self.current_weight = new_weight

        self.metrics_history.append({
            "timestamp": datetime.utcnow().isoformat(),
            "weight": new_weight,
            "action": "progress"
        })

    def rollback(self):
        """Rollback canary deployment."""
        if self.canary_rule_id:
            self.router.update_weight(self.canary_rule_id, 0.0)
            self.current_weight = 0.0

            self.metrics_history.append({
                "timestamp": datetime.utcnow().isoformat(),
                "weight": 0.0,
                "action": "rollback"
            })

    def complete(self):
        """Complete canary deployment (promote to 100%)."""
        if self.canary_rule_id:
            self.router.update_weight(self.canary_rule_id, 1.0)
            self.current_weight = 1.0

            self.metrics_history.append({
                "timestamp": datetime.utcnow().isoformat(),
                "weight": 1.0,
                "action": "complete"
            })

    def get_status(self) -> Dict:
        """Get current canary status."""
        return {
            "rule_id": self.canary_rule_id,
            "started_at": self.canary_started.isoformat() if self.canary_started else None,
            "current_weight": self.current_weight,
            "config": {
                "initial_weight": self.config.initial_weight,
                "max_weight": self.config.max_weight,
                "increment": self.config.increment,
                "interval_minutes": self.config.increment_interval_minutes
            },
            "history": self.metrics_history[-10:]  # Last 10 events
        }
```

### 2.2 Shadow Testing

```python
from dataclasses import dataclass
from typing import Dict, List, Callable, Optional
from datetime import datetime
import asyncio
import json


@dataclass
class ShadowTestResult:
    """Result of a shadow test comparison."""
    request_id: str
    input: str
    source_output: str
    target_output: str
    source_latency_ms: float
    target_latency_ms: float
    output_match: bool
    semantic_similarity: float
    timestamp: datetime


class ShadowTester:
    """Run shadow tests comparing source and target models."""

    def __init__(
        self,
        source_model_fn: Callable[[str], str],
        target_model_fn: Callable[[str], str],
        similarity_fn: Callable[[str, str], float]
    ):
        self.source_model = source_model_fn
        self.target_model = target_model_fn
        self.similarity_fn = similarity_fn
        self.results: List[ShadowTestResult] = []

    async def shadow_request(
        self,
        request_id: str,
        input_text: str
    ) -> ShadowTestResult:
        """Run shadow test for a single request."""
        # Run both models concurrently
        source_start = datetime.utcnow()
        source_output = self.source_model(input_text)
        source_latency = (datetime.utcnow() - source_start).total_seconds() * 1000

        target_start = datetime.utcnow()
        target_output = self.target_model(input_text)
        target_latency = (datetime.utcnow() - target_start).total_seconds() * 1000

        # Compare outputs
        output_match = source_output.strip() == target_output.strip()
        similarity = self.similarity_fn(source_output, target_output)

        result = ShadowTestResult(
            request_id=request_id,
            input=input_text,
            source_output=source_output,
            target_output=target_output,
            source_latency_ms=source_latency,
            target_latency_ms=target_latency,
            output_match=output_match,
            semantic_similarity=similarity,
            timestamp=datetime.utcnow()
        )

        self.results.append(result)
        return result

    def get_summary(self) -> Dict:
        """Get summary of shadow test results."""
        if not self.results:
            return {"error": "No results yet"}

        exact_matches = sum(1 for r in self.results if r.output_match)
        avg_similarity = sum(r.semantic_similarity for r in self.results) / len(self.results)
        avg_source_latency = sum(r.source_latency_ms for r in self.results) / len(self.results)
        avg_target_latency = sum(r.target_latency_ms for r in self.results) / len(self.results)

        # Find significant differences
        significant_diffs = [
            r for r in self.results
            if r.semantic_similarity < 0.8
        ]

        return {
            "total_requests": len(self.results),
            "exact_match_rate": exact_matches / len(self.results),
            "average_similarity": avg_similarity,
            "source_avg_latency_ms": avg_source_latency,
            "target_avg_latency_ms": avg_target_latency,
            "latency_difference_ms": avg_target_latency - avg_source_latency,
            "latency_change_percent": (avg_target_latency - avg_source_latency) / avg_source_latency * 100,
            "significant_differences": len(significant_diffs),
            "recommendation": self._get_recommendation(avg_similarity, avg_target_latency - avg_source_latency)
        }

    def _get_recommendation(
        self,
        avg_similarity: float,
        latency_diff: float
    ) -> str:
        """Get recommendation based on shadow test results."""
        if avg_similarity < 0.85:
            return "NOT_READY - Significant output differences detected"
        if latency_diff > 200:  # 200ms slower
            return "CAUTION - Target model is significantly slower"
        if avg_similarity >= 0.95:
            return "READY - Outputs are highly similar"
        return "CONDITIONAL - Review differences before proceeding"

    def get_difference_report(self) -> str:
        """Generate report of significant differences."""
        report = "# Shadow Test Difference Report\n\n"

        low_similarity = [r for r in self.results if r.semantic_similarity < 0.9]
        low_similarity.sort(key=lambda r: r.semantic_similarity)

        report += f"## Summary\n"
        report += f"- Total requests: {len(self.results)}\n"
        report += f"- Low similarity cases: {len(low_similarity)}\n\n"

        report += "## Cases with Low Similarity\n\n"
        for result in low_similarity[:20]:  # Top 20 differences
            report += f"### Request: {result.request_id}\n"
            report += f"- Similarity: {result.semantic_similarity:.2%}\n"
            report += f"- Input: {result.input[:200]}...\n"
            report += f"- Source output: {result.source_output[:200]}...\n"
            report += f"- Target output: {result.target_output[:200]}...\n\n"

        return report


class ABTestManager:
    """Manage A/B tests during migration."""

    def __init__(self):
        self.experiments: Dict[str, Dict] = {}

    def create_experiment(
        self,
        experiment_id: str,
        source_model: str,
        target_model: str,
        traffic_split: float = 0.5,
        duration_hours: int = 24
    ):
        """Create an A/B experiment."""
        self.experiments[experiment_id] = {
            "source_model": source_model,
            "target_model": target_model,
            "traffic_split": traffic_split,
            "started_at": datetime.utcnow(),
            "ends_at": datetime.utcnow() + timedelta(hours=duration_hours),
            "source_metrics": [],
            "target_metrics": [],
            "status": "running"
        }

    def record_metric(
        self,
        experiment_id: str,
        model: str,
        metric_name: str,
        value: float
    ):
        """Record a metric for an experiment."""
        exp = self.experiments.get(experiment_id)
        if not exp:
            return

        metric_data = {
            "name": metric_name,
            "value": value,
            "timestamp": datetime.utcnow().isoformat()
        }

        if model == exp["source_model"]:
            exp["source_metrics"].append(metric_data)
        elif model == exp["target_model"]:
            exp["target_metrics"].append(metric_data)

    def get_experiment_results(
        self,
        experiment_id: str
    ) -> Dict:
        """Get results of an experiment."""
        exp = self.experiments.get(experiment_id)
        if not exp:
            return {"error": "Experiment not found"}

        # Aggregate metrics
        source_aggregated = self._aggregate_metrics(exp["source_metrics"])
        target_aggregated = self._aggregate_metrics(exp["target_metrics"])

        # Statistical comparison
        comparisons = {}
        for metric_name in source_aggregated:
            if metric_name in target_aggregated:
                source_val = source_aggregated[metric_name]["mean"]
                target_val = target_aggregated[metric_name]["mean"]
                diff = target_val - source_val
                comparisons[metric_name] = {
                    "source": source_val,
                    "target": target_val,
                    "difference": diff,
                    "percent_change": (diff / source_val * 100) if source_val else 0,
                    "winner": "target" if diff > 0 else "source" if diff < 0 else "tie"
                }

        return {
            "experiment_id": experiment_id,
            "status": exp["status"],
            "started_at": exp["started_at"].isoformat(),
            "source_model": exp["source_model"],
            "target_model": exp["target_model"],
            "traffic_split": exp["traffic_split"],
            "source_samples": len(exp["source_metrics"]),
            "target_samples": len(exp["target_metrics"]),
            "comparisons": comparisons
        }

    def _aggregate_metrics(self, metrics: List[Dict]) -> Dict:
        """Aggregate metrics by name."""
        by_name: Dict[str, List[float]] = {}
        for m in metrics:
            name = m["name"]
            if name not in by_name:
                by_name[name] = []
            by_name[name].append(m["value"])

        return {
            name: {
                "mean": sum(values) / len(values),
                "min": min(values),
                "max": max(values),
                "count": len(values)
            }
            for name, values in by_name.items()
        }
```

## 3. Provider Migration

### 3.1 Provider Adapter Layer

```python
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Dict, List, Optional, Any
from datetime import datetime


@dataclass
class ModelRequest:
    """Standardized model request."""
    prompt: str
    max_tokens: int
    temperature: float = 0.7
    top_p: float = 1.0
    stop_sequences: List[str] = field(default_factory=list)
    system_prompt: Optional[str] = None


@dataclass
class ModelResponse:
    """Standardized model response."""
    text: str
    input_tokens: int
    output_tokens: int
    finish_reason: str
    latency_ms: float
    model_id: str
    provider: str


class ProviderAdapter(ABC):
    """Abstract adapter for LLM providers."""

    @abstractmethod
    def generate(self, request: ModelRequest) -> ModelResponse:
        """Generate response from the model."""
        pass

    @abstractmethod
    def get_model_info(self) -> Dict:
        """Get model information."""
        pass

    @abstractmethod
    def health_check(self) -> bool:
        """Check provider health."""
        pass


class OpenAIAdapter(ProviderAdapter):
    """Adapter for OpenAI models."""

    def __init__(self, api_key: str, model: str = "gpt-4"):
        self.api_key = api_key
        self.model = model
        import openai
        self.client = openai.OpenAI(api_key=api_key)

    def generate(self, request: ModelRequest) -> ModelResponse:
        start_time = datetime.utcnow()

        messages = []
        if request.system_prompt:
            messages.append({"role": "system", "content": request.system_prompt})
        messages.append({"role": "user", "content": request.prompt})

        response = self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            max_tokens=request.max_tokens,
            temperature=request.temperature,
            top_p=request.top_p,
            stop=request.stop_sequences or None
        )

        latency_ms = (datetime.utcnow() - start_time).total_seconds() * 1000

        return ModelResponse(
            text=response.choices[0].message.content,
            input_tokens=response.usage.prompt_tokens,
            output_tokens=response.usage.completion_tokens,
            finish_reason=response.choices[0].finish_reason,
            latency_ms=latency_ms,
            model_id=self.model,
            provider="openai"
        )

    def get_model_info(self) -> Dict:
        return {
            "provider": "openai",
            "model": self.model,
            "capabilities": ["chat", "completion", "function_calling"]
        }

    def health_check(self) -> bool:
        try:
            self.client.models.retrieve(self.model)
            return True
        except Exception:
            return False


class AnthropicAdapter(ProviderAdapter):
    """Adapter for Anthropic models."""

    def __init__(self, api_key: str, model: str = "claude-3-sonnet-20240229"):
        self.api_key = api_key
        self.model = model
        import anthropic
        self.client = anthropic.Anthropic(api_key=api_key)

    def generate(self, request: ModelRequest) -> ModelResponse:
        start_time = datetime.utcnow()

        response = self.client.messages.create(
            model=self.model,
            max_tokens=request.max_tokens,
            system=request.system_prompt or "",
            messages=[{"role": "user", "content": request.prompt}],
            temperature=request.temperature,
            top_p=request.top_p,
            stop_sequences=request.stop_sequences or None
        )

        latency_ms = (datetime.utcnow() - start_time).total_seconds() * 1000

        return ModelResponse(
            text=response.content[0].text,
            input_tokens=response.usage.input_tokens,
            output_tokens=response.usage.output_tokens,
            finish_reason=response.stop_reason,
            latency_ms=latency_ms,
            model_id=self.model,
            provider="anthropic"
        )

    def get_model_info(self) -> Dict:
        return {
            "provider": "anthropic",
            "model": self.model,
            "capabilities": ["chat", "completion", "vision"]
        }

    def health_check(self) -> bool:
        try:
            # Simple API check
            self.client.messages.create(
                model=self.model,
                max_tokens=1,
                messages=[{"role": "user", "content": "hi"}]
            )
            return True
        except Exception:
            return False


class ProviderMigrationManager:
    """Manage migration between providers."""

    def __init__(self):
        self.adapters: Dict[str, ProviderAdapter] = {}

    def register_adapter(self, name: str, adapter: ProviderAdapter):
        """Register a provider adapter."""
        self.adapters[name] = adapter

    def test_compatibility(
        self,
        source_name: str,
        target_name: str,
        test_prompts: List[str]
    ) -> Dict:
        """Test compatibility between providers."""
        source = self.adapters.get(source_name)
        target = self.adapters.get(target_name)

        if not source or not target:
            return {"error": "Adapter not found"}

        results = []
        for prompt in test_prompts:
            request = ModelRequest(prompt=prompt, max_tokens=500)

            try:
                source_response = source.generate(request)
                target_response = target.generate(request)

                results.append({
                    "prompt": prompt[:100] + "...",
                    "source_success": True,
                    "target_success": True,
                    "source_tokens": source_response.output_tokens,
                    "target_tokens": target_response.output_tokens,
                    "source_latency": source_response.latency_ms,
                    "target_latency": target_response.latency_ms
                })
            except Exception as e:
                results.append({
                    "prompt": prompt[:100] + "...",
                    "error": str(e)
                })

        successful = [r for r in results if r.get("source_success") and r.get("target_success")]

        return {
            "source": source_name,
            "target": target_name,
            "total_tests": len(test_prompts),
            "successful_tests": len(successful),
            "average_latency_diff": sum(
                r["target_latency"] - r["source_latency"]
                for r in successful
            ) / len(successful) if successful else 0,
            "results": results
        }
```

## 4. Rollback Procedures

### 4.1 Rollback Manager

```python
from dataclasses import dataclass
from typing import Dict, List, Optional, Callable
from datetime import datetime, timedelta
from enum import Enum


class RollbackTrigger(Enum):
    """Triggers for automatic rollback."""
    ERROR_RATE = "error_rate"
    LATENCY = "latency"
    QUALITY = "quality"
    MANUAL = "manual"


@dataclass
class RollbackEvent:
    """Record of a rollback event."""
    event_id: str
    migration_id: str
    trigger: RollbackTrigger
    trigger_value: float
    threshold: float
    initiated_by: str
    initiated_at: datetime
    completed_at: Optional[datetime] = None
    success: bool = False
    notes: str = ""


class RollbackManager:
    """Manage rollback procedures for model migrations."""

    def __init__(
        self,
        router: TrafficRouter,
        monitoring: 'MonitoringService'
    ):
        self.router = router
        self.monitoring = monitoring
        self.rollback_history: List[RollbackEvent] = []

        # Rollback thresholds
        self.thresholds = {
            RollbackTrigger.ERROR_RATE: 0.05,  # 5%
            RollbackTrigger.LATENCY: 2.0,       # 2x increase
            RollbackTrigger.QUALITY: 0.85       # 15% degradation
        }

    def check_rollback_conditions(
        self,
        migration_id: str,
        current_metrics: Dict
    ) -> Optional[RollbackTrigger]:
        """Check if rollback should be triggered."""
        # Check error rate
        if current_metrics.get("error_rate", 0) > self.thresholds[RollbackTrigger.ERROR_RATE]:
            return RollbackTrigger.ERROR_RATE

        # Check latency
        baseline_latency = current_metrics.get("baseline_latency_ms", 100)
        current_latency = current_metrics.get("current_latency_ms", 100)
        if current_latency > baseline_latency * self.thresholds[RollbackTrigger.LATENCY]:
            return RollbackTrigger.LATENCY

        # Check quality
        if current_metrics.get("quality_score", 1.0) < self.thresholds[RollbackTrigger.QUALITY]:
            return RollbackTrigger.QUALITY

        return None

    def execute_rollback(
        self,
        migration_id: str,
        trigger: RollbackTrigger,
        initiated_by: str
    ) -> RollbackEvent:
        """Execute rollback procedure."""
        event = RollbackEvent(
            event_id=f"RB-{datetime.utcnow().strftime('%Y%m%d%H%M%S')}",
            migration_id=migration_id,
            trigger=trigger,
            trigger_value=0,  # Would be set based on actual trigger
            threshold=self.thresholds.get(trigger, 0),
            initiated_by=initiated_by,
            initiated_at=datetime.utcnow()
        )

        try:
            # Step 1: Route all traffic to source
            self._route_to_source(migration_id)

            # Step 2: Verify source is healthy
            if not self._verify_source_health():
                raise Exception("Source model health check failed")

            # Step 3: Update feature flags
            self._disable_target_feature_flags(migration_id)

            # Step 4: Alert team
            self._send_rollback_alert(event)

            event.completed_at = datetime.utcnow()
            event.success = True
            event.notes = "Rollback completed successfully"

        except Exception as e:
            event.completed_at = datetime.utcnow()
            event.success = False
            event.notes = f"Rollback failed: {str(e)}"

        self.rollback_history.append(event)
        return event

    def _route_to_source(self, migration_id: str):
        """Route all traffic to source model."""
        # Set canary/target weight to 0
        self.router.update_weight(f"canary-{migration_id}", 0.0)

    def _verify_source_health(self) -> bool:
        """Verify source model is healthy."""
        # Would check actual health metrics
        return True

    def _disable_target_feature_flags(self, migration_id: str):
        """Disable feature flags for target model."""
        pass

    def _send_rollback_alert(self, event: RollbackEvent):
        """Send rollback alert to team."""
        pass

    def get_rollback_readiness(
        self,
        migration_id: str
    ) -> Dict:
        """Check rollback readiness."""
        checks = {
            "source_model_available": True,
            "source_model_healthy": True,
            "routing_configured": True,
            "feature_flags_available": True,
            "team_notified": True
        }

        # Would perform actual checks

        all_ready = all(checks.values())

        return {
            "migration_id": migration_id,
            "ready": all_ready,
            "checks": checks,
            "estimated_rollback_time_seconds": 30
        }

    def generate_rollback_runbook(
        self,
        migration_id: str,
        source_model: str,
        target_model: str
    ) -> str:
        """Generate rollback runbook."""
        return f"""
# Rollback Runbook - {migration_id}

## Overview
- **Source Model**: {source_model}
- **Target Model**: {target_model}
- **Generated**: {datetime.utcnow().isoformat()}

## Automatic Rollback Triggers
| Trigger | Threshold |
|---------|-----------|
| Error Rate | > {self.thresholds[RollbackTrigger.ERROR_RATE]*100}% |
| Latency | > {self.thresholds[RollbackTrigger.LATENCY]}x baseline |
| Quality Score | < {self.thresholds[RollbackTrigger.QUALITY]*100}% of baseline |

## Manual Rollback Procedure

### Step 1: Verify Issue
```
# Check current error rate
curl http://metrics.internal/api/v1/query?query=model_error_rate{{model="{target_model}"}}

# Check current latency
curl http://metrics.internal/api/v1/query?query=model_latency_p99{{model="{target_model}"}}
```

### Step 2: Execute Rollback
```
# Option A: Via CLI
model-migrate rollback --migration-id {migration_id}

# Option B: Via API
curl -X POST http://migration.internal/api/v1/rollback \\
  -H "Content-Type: application/json" \\
  -d '{{"migration_id": "{migration_id}", "reason": "manual"}}'
```

### Step 3: Verify Rollback
```
# Check traffic is routed to source
curl http://routing.internal/api/v1/routes

# Verify error rate is normal
curl http://metrics.internal/api/v1/query?query=model_error_rate{{model="{source_model}"}}
```

### Step 4: Notify Team
1. Post in #incidents Slack channel
2. Update status page if customer-facing
3. Create incident ticket

## Post-Rollback Actions
- [ ] Document root cause
- [ ] Update migration plan
- [ ] Schedule post-mortem if needed
"""
```

## 5. Post-Migration Validation

### 5.1 Validation Checklist

```python
from dataclasses import dataclass
from typing import Dict, List, Optional
from datetime import datetime


@dataclass
class ValidationCheck:
    """A validation check item."""
    check_id: str
    name: str
    description: str
    check_type: str  # automated, manual
    severity: str    # critical, high, medium, low
    passed: Optional[bool] = None
    result: str = ""
    checked_at: Optional[datetime] = None


class PostMigrationValidator:
    """Validate migration completion."""

    def __init__(self):
        self.checks = self._initialize_checks()
        self.results: Dict[str, ValidationCheck] = {}

    def _initialize_checks(self) -> List[ValidationCheck]:
        return [
            # Critical checks
            ValidationCheck(
                check_id="C001",
                name="Error Rate Normal",
                description="Error rate is within acceptable bounds",
                check_type="automated",
                severity="critical"
            ),
            ValidationCheck(
                check_id="C002",
                name="Latency Normal",
                description="P50 and P99 latency within SLA",
                check_type="automated",
                severity="critical"
            ),
            ValidationCheck(
                check_id="C003",
                name="All Traffic Migrated",
                description="100% of traffic on new model",
                check_type="automated",
                severity="critical"
            ),
            # High priority
            ValidationCheck(
                check_id="H001",
                name="Quality Metrics Stable",
                description="Quality scores match or exceed baseline",
                check_type="automated",
                severity="high"
            ),
            ValidationCheck(
                check_id="H002",
                name="Cost Within Budget",
                description="Model cost is within projected range",
                check_type="automated",
                severity="high"
            ),
            ValidationCheck(
                check_id="H003",
                name="No User Complaints",
                description="No increase in user complaints",
                check_type="manual",
                severity="high"
            ),
            # Medium priority
            ValidationCheck(
                check_id="M001",
                name="Documentation Updated",
                description="All documentation reflects new model",
                check_type="manual",
                severity="medium"
            ),
            ValidationCheck(
                check_id="M002",
                name="Monitoring Configured",
                description="All dashboards and alerts configured",
                check_type="automated",
                severity="medium"
            ),
            ValidationCheck(
                check_id="M003",
                name="Old Model Decommissioned",
                description="Old model resources cleaned up",
                check_type="manual",
                severity="medium"
            ),
            # Low priority
            ValidationCheck(
                check_id="L001",
                name="Team Notified",
                description="All stakeholders notified of completion",
                check_type="manual",
                severity="low"
            ),
            ValidationCheck(
                check_id="L002",
                name="Lessons Documented",
                description="Migration lessons learned documented",
                check_type="manual",
                severity="low"
            )
        ]

    def run_automated_checks(
        self,
        metrics: Dict
    ) -> Dict[str, ValidationCheck]:
        """Run automated validation checks."""
        for check in self.checks:
            if check.check_type != "automated":
                continue

            check.checked_at = datetime.utcnow()

            if check.check_id == "C001":
                error_rate = metrics.get("error_rate", 0)
                check.passed = error_rate < 0.01
                check.result = f"Error rate: {error_rate*100:.2f}%"

            elif check.check_id == "C002":
                p99_latency = metrics.get("latency_p99_ms", 0)
                sla_latency = metrics.get("sla_latency_ms", 1000)
                check.passed = p99_latency < sla_latency
                check.result = f"P99: {p99_latency}ms (SLA: {sla_latency}ms)"

            elif check.check_id == "C003":
                traffic_percent = metrics.get("new_model_traffic_percent", 0)
                check.passed = traffic_percent >= 99.9
                check.result = f"Traffic on new model: {traffic_percent}%"

            elif check.check_id == "H001":
                quality_score = metrics.get("quality_score", 0)
                baseline_quality = metrics.get("baseline_quality", 1)
                check.passed = quality_score >= baseline_quality * 0.95
                check.result = f"Quality: {quality_score:.2f} (baseline: {baseline_quality:.2f})"

            elif check.check_id == "H002":
                cost = metrics.get("daily_cost", 0)
                budget = metrics.get("daily_budget", 0)
                check.passed = cost <= budget * 1.1  # 10% tolerance
                check.result = f"Cost: ${cost:.2f}/day (budget: ${budget:.2f})"

            elif check.check_id == "M002":
                dashboards_ok = metrics.get("dashboards_configured", False)
                alerts_ok = metrics.get("alerts_configured", False)
                check.passed = dashboards_ok and alerts_ok
                check.result = f"Dashboards: {dashboards_ok}, Alerts: {alerts_ok}"

            self.results[check.check_id] = check

        return self.results

    def mark_manual_check(
        self,
        check_id: str,
        passed: bool,
        result: str
    ):
        """Mark a manual check as complete."""
        for check in self.checks:
            if check.check_id == check_id:
                check.passed = passed
                check.result = result
                check.checked_at = datetime.utcnow()
                self.results[check_id] = check
                break

    def get_validation_summary(self) -> Dict:
        """Get validation summary."""
        total = len(self.checks)
        completed = len([c for c in self.checks if c.checked_at])
        passed = len([c for c in self.checks if c.passed])
        failed = len([c for c in self.checks if c.passed is False])
        pending = total - completed

        critical_passed = all(
            c.passed for c in self.checks
            if c.severity == "critical" and c.checked_at
        )

        return {
            "total_checks": total,
            "completed": completed,
            "passed": passed,
            "failed": failed,
            "pending": pending,
            "critical_passed": critical_passed,
            "migration_approved": critical_passed and failed == 0 and pending == 0,
            "checks": [
                {
                    "id": c.check_id,
                    "name": c.name,
                    "severity": c.severity,
                    "passed": c.passed,
                    "result": c.result
                }
                for c in self.checks
            ]
        }

    def generate_validation_report(self) -> str:
        """Generate validation report."""
        summary = self.get_validation_summary()

        report = "# Post-Migration Validation Report\n\n"
        report += f"**Generated**: {datetime.utcnow().isoformat()}\n\n"

        report += "## Summary\n\n"
        report += f"- Total Checks: {summary['total_checks']}\n"
        report += f"- Passed: {summary['passed']}\n"
        report += f"- Failed: {summary['failed']}\n"
        report += f"- Pending: {summary['pending']}\n"
        report += f"- **Migration Approved**: {'Yes' if summary['migration_approved'] else 'No'}\n\n"

        for severity in ["critical", "high", "medium", "low"]:
            checks = [c for c in self.checks if c.severity == severity]
            if checks:
                report += f"## {severity.title()} Checks\n\n"
                for check in checks:
                    status = "PASS" if check.passed else "FAIL" if check.passed is False else "PENDING"
                    report += f"### [{status}] {check.name}\n"
                    report += f"{check.description}\n"
                    if check.result:
                        report += f"- Result: {check.result}\n"
                    report += "\n"

        return report
```

## Troubleshooting

### Common Migration Issues

| Issue | Symptoms | Resolution |
|-------|----------|------------|
| Output quality degradation | Lower quality scores | Review prompt templates, adjust parameters |
| Latency increase | Higher P99 latency | Check model size, optimize batching |
| Token limit errors | Truncated outputs | Adjust max tokens, review prompts |
| API incompatibility | Request failures | Update adapter layer, test thoroughly |
| Cost overrun | Higher than expected bills | Review pricing, optimize usage |

### Migration Checklist

- [ ] Evaluation completed on representative datasets
- [ ] Shadow testing passed
- [ ] Rollback procedure tested
- [ ] Monitoring configured
- [ ] Team notified
- [ ] Documentation updated

## Related Documentation

- [5.1 Model Serving Architecture](../05_llm_integration/5.1_model_serving_architecture_guide.md)
- [15.2 Data Migration Guide](15.2_data_migration_guide.md)
- [13.1 Incident Response Guide](../13_operations_reliability/13.1_incident_response_guide.md)

## Version History

| Version | Date | Author | Changes |
|---------|------|--------|---------|
| 1.0 | 2024-01-15 | ML Platform Team | Initial release |

---

> **Navigation**
> [← 14.3 GPU Infrastructure](../14_cost_capacity_management/14.3_gpu_infrastructure_optimization_guide.md) | **[Index](../README.md#15-repository-structure)** | [15.2 Data Migration →](15.2_data_migration_guide.md)
