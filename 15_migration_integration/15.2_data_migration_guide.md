> **Navigation** | [← 15.1 Model Migration](15.1_model_migration_guide.md) | [15.3 System Integration →](15.3_system_integration_guide.md)
>
> | | |
> |---|---|
> | **Prerequisites** | Data store access &#124; Database administration &#124; Backup/recovery |
> | **Related** | [7.1 Vector Database](../07_rag_pipeline/7.1_vector_database_guide.md) &#124; [1.4 Data Versioning](../01_data_pipeline/1.4_data_versioning_lineage.md) |
> | **Next** | [15.3 System Integration](15.3_system_integration_guide.md) |

# 15.2 Data Migration Guide

## Document Information
- **Version**: 1.0
- **Last Updated**: 2024-01-15
- **Owner**: Data Engineering Team
- **Classification**: Internal

## Purpose and Scope

This guide provides comprehensive procedures for migrating data within the Multi-Cloud RAG Platform. It covers document migration, embedding migration, vector store transitions, and database migrations with data integrity validation.

## Prerequisites

- Access to source and target data stores
- Understanding of data schemas and formats
- Database administration privileges
- Backup and recovery procedures understood

## 1. Data Migration Framework

### 1.1 Migration Types and Planning

```python
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any, Callable
from enum import Enum
from datetime import datetime, timedelta
import hashlib
import json


class DataMigrationType(Enum):
    """Types of data migrations."""
    DOCUMENT_STORE = "document_store"       # Document storage migration
    VECTOR_STORE = "vector_store"           # Embedding/vector migration
    DATABASE = "database"                    # Relational database migration
    OBJECT_STORAGE = "object_storage"       # S3/GCS/Azure blob migration
    CACHE = "cache"                         # Cache layer migration
    FULL_PLATFORM = "full_platform"         # Complete platform migration


class MigrationStrategy(Enum):
    """Migration strategies."""
    BIG_BANG = "big_bang"                   # Full cutover at once
    INCREMENTAL = "incremental"             # Batch-by-batch migration
    PARALLEL_RUN = "parallel_run"           # Both systems active
    BLUE_GREEN = "blue_green"               # Switch between environments
    STRANGLER = "strangler"                 # Gradual replacement


class DataIntegrityLevel(Enum):
    """Data integrity validation levels."""
    COUNT_ONLY = "count_only"               # Just count records
    CHECKSUM = "checksum"                   # Verify checksums
    FULL_COMPARISON = "full_comparison"     # Compare all fields
    SAMPLING = "sampling"                   # Sample-based validation


@dataclass
class DataSource:
    """Source or target data store."""
    name: str
    type: str  # postgresql, mongodb, s3, pinecone, etc.
    connection_string: str
    schema_version: str
    estimated_size_gb: float
    record_count: int
    metadata: Dict = field(default_factory=dict)


@dataclass
class MigrationPhase:
    """A phase in the migration process."""
    phase_id: str
    name: str
    description: str
    dependencies: List[str]
    estimated_duration_hours: float
    tasks: List[str]
    validation_steps: List[str]
    rollback_procedure: str


@dataclass
class DataMigrationPlan:
    """Complete data migration plan."""
    migration_id: str
    migration_type: DataMigrationType
    strategy: MigrationStrategy
    source: DataSource
    target: DataSource
    phases: List[MigrationPhase]
    created_at: datetime
    planned_start: datetime
    estimated_duration_hours: float
    owner: str
    integrity_level: DataIntegrityLevel
    downtime_window_hours: float = 0
    parallel_workers: int = 4
    batch_size: int = 1000


class MigrationPlanner:
    """Plan data migrations."""

    def __init__(self):
        self.plans: Dict[str, DataMigrationPlan] = {}

    def create_migration_plan(
        self,
        source: DataSource,
        target: DataSource,
        migration_type: DataMigrationType,
        strategy: MigrationStrategy,
        owner: str
    ) -> DataMigrationPlan:
        """Create a migration plan."""
        migration_id = f"DM-{datetime.utcnow().strftime('%Y%m%d%H%M%S')}"

        phases = self._create_phases(migration_type, strategy, source, target)
        duration = sum(p.estimated_duration_hours for p in phases)

        plan = DataMigrationPlan(
            migration_id=migration_id,
            migration_type=migration_type,
            strategy=strategy,
            source=source,
            target=target,
            phases=phases,
            created_at=datetime.utcnow(),
            planned_start=datetime.utcnow() + timedelta(days=1),
            estimated_duration_hours=duration,
            owner=owner,
            integrity_level=DataIntegrityLevel.CHECKSUM
        )

        self.plans[migration_id] = plan
        return plan

    def _create_phases(
        self,
        migration_type: DataMigrationType,
        strategy: MigrationStrategy,
        source: DataSource,
        target: DataSource
    ) -> List[MigrationPhase]:
        """Create migration phases based on type and strategy."""
        phases = [
            MigrationPhase(
                phase_id="PREP",
                name="Preparation",
                description="Prepare source and target systems",
                dependencies=[],
                estimated_duration_hours=2,
                tasks=[
                    "Validate source connectivity",
                    "Create target schema",
                    "Configure network access",
                    "Set up monitoring",
                    "Create backup of source"
                ],
                validation_steps=[
                    "Source accessible",
                    "Target schema created",
                    "Backup verified"
                ],
                rollback_procedure="Remove target schema"
            ),
            MigrationPhase(
                phase_id="INITIAL",
                name="Initial Data Load",
                description="Migrate bulk of historical data",
                dependencies=["PREP"],
                estimated_duration_hours=self._estimate_transfer_time(source),
                tasks=[
                    "Export data from source",
                    "Transform data if needed",
                    "Load data to target",
                    "Create indexes"
                ],
                validation_steps=[
                    "Record counts match",
                    "Checksums verified",
                    "Indexes created"
                ],
                rollback_procedure="Truncate target tables"
            )
        ]

        if strategy in [MigrationStrategy.INCREMENTAL, MigrationStrategy.PARALLEL_RUN]:
            phases.append(MigrationPhase(
                phase_id="CDC",
                name="Change Data Capture",
                description="Capture and apply ongoing changes",
                dependencies=["INITIAL"],
                estimated_duration_hours=4,
                tasks=[
                    "Enable CDC on source",
                    "Configure replication",
                    "Apply incremental changes",
                    "Monitor lag"
                ],
                validation_steps=[
                    "Replication lag < 1 minute",
                    "No data loss detected"
                ],
                rollback_procedure="Disable CDC, resync if needed"
            ))

        phases.extend([
            MigrationPhase(
                phase_id="VALIDATE",
                name="Validation",
                description="Comprehensive data validation",
                dependencies=["INITIAL"] if strategy == MigrationStrategy.BIG_BANG else ["CDC"],
                estimated_duration_hours=2,
                tasks=[
                    "Run integrity checks",
                    "Compare sample records",
                    "Validate relationships",
                    "Test queries"
                ],
                validation_steps=[
                    "All integrity checks pass",
                    "Sample comparison successful",
                    "Query results match"
                ],
                rollback_procedure="Document issues, plan remediation"
            ),
            MigrationPhase(
                phase_id="CUTOVER",
                name="Cutover",
                description="Switch to target system",
                dependencies=["VALIDATE"],
                estimated_duration_hours=1,
                tasks=[
                    "Stop writes to source",
                    "Final sync",
                    "Update application configs",
                    "Verify connectivity",
                    "Enable traffic to target"
                ],
                validation_steps=[
                    "Applications connected to target",
                    "No errors in logs",
                    "Performance acceptable"
                ],
                rollback_procedure="Revert application configs to source"
            ),
            MigrationPhase(
                phase_id="CLEANUP",
                name="Cleanup",
                description="Post-migration cleanup",
                dependencies=["CUTOVER"],
                estimated_duration_hours=2,
                tasks=[
                    "Monitor target performance",
                    "Archive source data",
                    "Update documentation",
                    "Decommission source (after retention)"
                ],
                validation_steps=[
                    "Target stable for 48 hours",
                    "Source archived"
                ],
                rollback_procedure="N/A - Point of no return"
            )
        ])

        return phases

    def _estimate_transfer_time(self, source: DataSource) -> float:
        """Estimate data transfer time in hours."""
        # Assume 100 MB/s transfer rate
        transfer_rate_gbh = 360  # GB per hour
        return max(1, source.estimated_size_gb / transfer_rate_gbh)

    def generate_migration_runbook(
        self,
        migration_id: str
    ) -> str:
        """Generate detailed migration runbook."""
        plan = self.plans.get(migration_id)
        if not plan:
            return "Migration plan not found"

        runbook = f"""
# Data Migration Runbook

## Migration ID: {plan.migration_id}

### Overview
- **Type**: {plan.migration_type.value}
- **Strategy**: {plan.strategy.value}
- **Source**: {plan.source.name} ({plan.source.type})
- **Target**: {plan.target.name} ({plan.target.type})
- **Estimated Duration**: {plan.estimated_duration_hours:.1f} hours
- **Owner**: {plan.owner}

### Data Volume
- Source Size: {plan.source.estimated_size_gb:.1f} GB
- Record Count: {plan.source.record_count:,}
- Batch Size: {plan.batch_size:,}
- Parallel Workers: {plan.parallel_workers}

### Pre-Migration Checklist
- [ ] Source backup completed and verified
- [ ] Target environment provisioned
- [ ] Network connectivity tested
- [ ] Monitoring and alerting configured
- [ ] Rollback procedure tested
- [ ] Stakeholders notified
- [ ] Maintenance window scheduled

"""
        for phase in plan.phases:
            runbook += f"""
## Phase: {phase.name}

**Description**: {phase.description}

**Estimated Duration**: {phase.estimated_duration_hours:.1f} hours

**Dependencies**: {', '.join(phase.dependencies) if phase.dependencies else 'None'}

### Tasks
"""
            for i, task in enumerate(phase.tasks, 1):
                runbook += f"{i}. {task}\n"

            runbook += "\n### Validation Steps\n"
            for step in phase.validation_steps:
                runbook += f"- [ ] {step}\n"

            runbook += f"\n**Rollback**: {phase.rollback_procedure}\n"

        return runbook
```

### 1.2 Data Extraction and Transformation

```python
from dataclasses import dataclass
from typing import Dict, List, Optional, Iterator, Any, Callable
from datetime import datetime
import hashlib
import json


@dataclass
class DataBatch:
    """A batch of data for migration."""
    batch_id: str
    records: List[Dict]
    source_offset: int
    record_count: int
    checksum: str
    extracted_at: datetime


@dataclass
class TransformationRule:
    """Rule for data transformation."""
    source_field: str
    target_field: str
    transformation: Callable[[Any], Any]
    default_value: Any = None
    required: bool = True


class DataExtractor:
    """Extract data from source systems."""

    def __init__(
        self,
        source: DataSource,
        batch_size: int = 1000
    ):
        self.source = source
        self.batch_size = batch_size
        self.connection = None

    def connect(self):
        """Connect to source system."""
        # Implementation depends on source type
        pass

    def disconnect(self):
        """Disconnect from source system."""
        if self.connection:
            self.connection.close()

    def get_total_count(self) -> int:
        """Get total record count."""
        # Implementation depends on source type
        return self.source.record_count

    def extract_batches(
        self,
        start_offset: int = 0,
        end_offset: Optional[int] = None
    ) -> Iterator[DataBatch]:
        """Extract data in batches."""
        offset = start_offset
        batch_num = 0

        while True:
            records = self._fetch_batch(offset)
            if not records:
                break

            checksum = self._calculate_checksum(records)

            yield DataBatch(
                batch_id=f"batch-{batch_num}",
                records=records,
                source_offset=offset,
                record_count=len(records),
                checksum=checksum,
                extracted_at=datetime.utcnow()
            )

            offset += len(records)
            batch_num += 1

            if end_offset and offset >= end_offset:
                break

    def _fetch_batch(self, offset: int) -> List[Dict]:
        """Fetch a batch of records from source."""
        # Implementation depends on source type
        return []

    def _calculate_checksum(self, records: List[Dict]) -> str:
        """Calculate checksum for batch verification."""
        content = json.dumps(records, sort_keys=True, default=str)
        return hashlib.sha256(content.encode()).hexdigest()


class DataTransformer:
    """Transform data between schemas."""

    def __init__(self):
        self.rules: List[TransformationRule] = []
        self.custom_transformers: Dict[str, Callable] = {}

    def add_rule(self, rule: TransformationRule):
        """Add a transformation rule."""
        self.rules.append(rule)

    def add_field_mapping(
        self,
        source_field: str,
        target_field: str,
        transformation: Optional[Callable] = None
    ):
        """Add simple field mapping."""
        self.rules.append(TransformationRule(
            source_field=source_field,
            target_field=target_field,
            transformation=transformation or (lambda x: x)
        ))

    def register_transformer(
        self,
        name: str,
        transformer: Callable
    ):
        """Register a custom transformer function."""
        self.custom_transformers[name] = transformer

    def transform_record(self, record: Dict) -> Dict:
        """Transform a single record."""
        transformed = {}

        for rule in self.rules:
            source_value = record.get(rule.source_field)

            if source_value is None:
                if rule.required and rule.default_value is None:
                    raise ValueError(f"Required field {rule.source_field} is missing")
                transformed[rule.target_field] = rule.default_value
            else:
                try:
                    transformed[rule.target_field] = rule.transformation(source_value)
                except Exception as e:
                    if rule.default_value is not None:
                        transformed[rule.target_field] = rule.default_value
                    else:
                        raise ValueError(f"Transformation failed for {rule.source_field}: {e}")

        return transformed

    def transform_batch(self, batch: DataBatch) -> DataBatch:
        """Transform a batch of records."""
        transformed_records = [
            self.transform_record(record)
            for record in batch.records
        ]

        checksum = hashlib.sha256(
            json.dumps(transformed_records, sort_keys=True, default=str).encode()
        ).hexdigest()

        return DataBatch(
            batch_id=batch.batch_id,
            records=transformed_records,
            source_offset=batch.source_offset,
            record_count=len(transformed_records),
            checksum=checksum,
            extracted_at=batch.extracted_at
        )

    @staticmethod
    def common_transformations() -> Dict[str, Callable]:
        """Common transformation functions."""
        return {
            "lowercase": lambda x: x.lower() if isinstance(x, str) else x,
            "uppercase": lambda x: x.upper() if isinstance(x, str) else x,
            "trim": lambda x: x.strip() if isinstance(x, str) else x,
            "to_int": lambda x: int(x) if x else 0,
            "to_float": lambda x: float(x) if x else 0.0,
            "to_bool": lambda x: bool(x),
            "to_string": lambda x: str(x) if x is not None else "",
            "parse_json": lambda x: json.loads(x) if isinstance(x, str) else x,
            "to_json": lambda x: json.dumps(x) if not isinstance(x, str) else x,
            "iso_date": lambda x: datetime.fromisoformat(x) if isinstance(x, str) else x
        }
```

### 1.3 Data Loader

```python
from dataclasses import dataclass
from typing import Dict, List, Optional
from datetime import datetime
import asyncio


@dataclass
class LoadResult:
    """Result of loading a batch."""
    batch_id: str
    records_loaded: int
    records_failed: int
    errors: List[str]
    duration_seconds: float
    loaded_at: datetime


class DataLoader:
    """Load data into target systems."""

    def __init__(
        self,
        target: DataSource,
        batch_size: int = 1000,
        parallel_workers: int = 4
    ):
        self.target = target
        self.batch_size = batch_size
        self.parallel_workers = parallel_workers
        self.connection = None
        self.load_results: List[LoadResult] = []

    def connect(self):
        """Connect to target system."""
        pass

    def disconnect(self):
        """Disconnect from target system."""
        if self.connection:
            self.connection.close()

    def load_batch(self, batch: DataBatch) -> LoadResult:
        """Load a single batch."""
        start_time = datetime.utcnow()
        loaded = 0
        failed = 0
        errors = []

        for record in batch.records:
            try:
                self._insert_record(record)
                loaded += 1
            except Exception as e:
                failed += 1
                errors.append(str(e))

        duration = (datetime.utcnow() - start_time).total_seconds()

        result = LoadResult(
            batch_id=batch.batch_id,
            records_loaded=loaded,
            records_failed=failed,
            errors=errors[:10],  # Limit error messages
            duration_seconds=duration,
            loaded_at=datetime.utcnow()
        )

        self.load_results.append(result)
        return result

    def _insert_record(self, record: Dict):
        """Insert a single record."""
        # Implementation depends on target type
        pass

    def bulk_load(
        self,
        batches: List[DataBatch],
        on_progress: Optional[Callable[[int, int], None]] = None
    ) -> Dict:
        """Load multiple batches."""
        total_loaded = 0
        total_failed = 0
        total_batches = len(batches)

        for i, batch in enumerate(batches):
            result = self.load_batch(batch)
            total_loaded += result.records_loaded
            total_failed += result.records_failed

            if on_progress:
                on_progress(i + 1, total_batches)

        return {
            "total_batches": total_batches,
            "total_records_loaded": total_loaded,
            "total_records_failed": total_failed,
            "success_rate": total_loaded / (total_loaded + total_failed) if (total_loaded + total_failed) > 0 else 0
        }

    def get_load_summary(self) -> Dict:
        """Get summary of load operations."""
        if not self.load_results:
            return {}

        total_loaded = sum(r.records_loaded for r in self.load_results)
        total_failed = sum(r.records_failed for r in self.load_results)
        total_duration = sum(r.duration_seconds for r in self.load_results)

        return {
            "batches_processed": len(self.load_results),
            "total_records_loaded": total_loaded,
            "total_records_failed": total_failed,
            "total_duration_seconds": total_duration,
            "average_batch_duration": total_duration / len(self.load_results),
            "records_per_second": total_loaded / total_duration if total_duration > 0 else 0
        }
```

## 2. Vector Store Migration

### 2.1 Embedding Migration

```python
from dataclasses import dataclass
from typing import Dict, List, Optional, Iterator
from datetime import datetime
import numpy as np


@dataclass
class EmbeddingRecord:
    """An embedding record."""
    id: str
    vector: List[float]
    metadata: Dict
    text: Optional[str] = None
    namespace: Optional[str] = None


@dataclass
class EmbeddingBatch:
    """Batch of embeddings for migration."""
    batch_id: str
    embeddings: List[EmbeddingRecord]
    dimension: int
    extracted_at: datetime


class VectorStoreMigrator:
    """Migrate embeddings between vector stores."""

    def __init__(
        self,
        source_client: 'VectorStoreClient',
        target_client: 'VectorStoreClient',
        batch_size: int = 100
    ):
        self.source = source_client
        self.target = target_client
        self.batch_size = batch_size
        self.migration_stats = {
            "vectors_migrated": 0,
            "vectors_failed": 0,
            "batches_processed": 0
        }

    def get_source_stats(self) -> Dict:
        """Get statistics from source vector store."""
        return self.source.get_stats()

    def extract_embeddings(
        self,
        namespace: Optional[str] = None,
        filter: Optional[Dict] = None
    ) -> Iterator[EmbeddingBatch]:
        """Extract embeddings from source."""
        offset = 0
        batch_num = 0

        while True:
            # Fetch batch from source
            results = self.source.fetch(
                namespace=namespace,
                filter=filter,
                limit=self.batch_size,
                offset=offset
            )

            if not results:
                break

            embeddings = [
                EmbeddingRecord(
                    id=r["id"],
                    vector=r["values"],
                    metadata=r.get("metadata", {}),
                    text=r.get("metadata", {}).get("text"),
                    namespace=namespace
                )
                for r in results
            ]

            yield EmbeddingBatch(
                batch_id=f"emb-batch-{batch_num}",
                embeddings=embeddings,
                dimension=len(embeddings[0].vector) if embeddings else 0,
                extracted_at=datetime.utcnow()
            )

            offset += len(results)
            batch_num += 1

    def migrate_batch(
        self,
        batch: EmbeddingBatch,
        target_namespace: Optional[str] = None
    ) -> Dict:
        """Migrate a batch of embeddings."""
        vectors_to_upsert = [
            {
                "id": emb.id,
                "values": emb.vector,
                "metadata": emb.metadata
            }
            for emb in batch.embeddings
        ]

        try:
            self.target.upsert(
                vectors=vectors_to_upsert,
                namespace=target_namespace
            )

            self.migration_stats["vectors_migrated"] += len(batch.embeddings)
            self.migration_stats["batches_processed"] += 1

            return {
                "batch_id": batch.batch_id,
                "status": "success",
                "vectors_migrated": len(batch.embeddings)
            }

        except Exception as e:
            self.migration_stats["vectors_failed"] += len(batch.embeddings)
            return {
                "batch_id": batch.batch_id,
                "status": "failed",
                "error": str(e)
            }

    def validate_migration(
        self,
        sample_size: int = 100
    ) -> Dict:
        """Validate migrated embeddings."""
        # Get sample IDs from source
        source_sample = self.source.fetch(limit=sample_size)
        sample_ids = [r["id"] for r in source_sample]

        # Fetch same IDs from target
        validation_results = {
            "total_checked": 0,
            "matches": 0,
            "mismatches": 0,
            "missing": 0,
            "dimension_errors": 0
        }

        for record in source_sample:
            validation_results["total_checked"] += 1

            target_record = self.target.fetch_by_id(record["id"])

            if not target_record:
                validation_results["missing"] += 1
                continue

            # Compare vectors
            source_vector = np.array(record["values"])
            target_vector = np.array(target_record["values"])

            if len(source_vector) != len(target_vector):
                validation_results["dimension_errors"] += 1
                continue

            # Check if vectors are similar (allow small floating point differences)
            if np.allclose(source_vector, target_vector, rtol=1e-5):
                validation_results["matches"] += 1
            else:
                validation_results["mismatches"] += 1

        validation_results["success_rate"] = (
            validation_results["matches"] / validation_results["total_checked"]
            if validation_results["total_checked"] > 0 else 0
        )

        return validation_results

    def run_full_migration(
        self,
        source_namespace: Optional[str] = None,
        target_namespace: Optional[str] = None,
        on_progress: Optional[Callable[[Dict], None]] = None
    ) -> Dict:
        """Run complete migration."""
        start_time = datetime.utcnow()

        for batch in self.extract_embeddings(namespace=source_namespace):
            result = self.migrate_batch(batch, target_namespace)

            if on_progress:
                on_progress({
                    "batch_id": batch.batch_id,
                    "status": result["status"],
                    "total_migrated": self.migration_stats["vectors_migrated"],
                    "total_failed": self.migration_stats["vectors_failed"]
                })

        duration = (datetime.utcnow() - start_time).total_seconds()

        # Validate
        validation = self.validate_migration()

        return {
            "migration_stats": self.migration_stats,
            "duration_seconds": duration,
            "vectors_per_second": self.migration_stats["vectors_migrated"] / duration if duration > 0 else 0,
            "validation": validation
        }


class EmbeddingReindexer:
    """Reindex embeddings with a new model."""

    def __init__(
        self,
        document_store: 'DocumentStore',
        old_embedding_model: 'EmbeddingModel',
        new_embedding_model: 'EmbeddingModel',
        vector_store: 'VectorStoreClient'
    ):
        self.document_store = document_store
        self.old_model = old_embedding_model
        self.new_model = new_embedding_model
        self.vector_store = vector_store

    def analyze_dimension_change(self) -> Dict:
        """Analyze impact of dimension change."""
        old_dim = self.old_model.dimension
        new_dim = self.new_model.dimension

        return {
            "old_dimension": old_dim,
            "new_dimension": new_dim,
            "dimension_change": new_dim - old_dim,
            "storage_impact_percent": (new_dim - old_dim) / old_dim * 100,
            "requires_index_rebuild": old_dim != new_dim
        }

    def reindex_documents(
        self,
        batch_size: int = 100,
        on_progress: Optional[Callable[[int, int], None]] = None
    ) -> Dict:
        """Reindex all documents with new embedding model."""
        total_documents = self.document_store.count()
        processed = 0
        errors = []

        offset = 0
        while offset < total_documents:
            # Fetch documents
            documents = self.document_store.fetch(
                limit=batch_size,
                offset=offset
            )

            if not documents:
                break

            # Generate new embeddings
            texts = [doc["content"] for doc in documents]
            try:
                new_embeddings = self.new_model.embed(texts)

                # Update vector store
                vectors = [
                    {
                        "id": doc["id"],
                        "values": embedding.tolist(),
                        "metadata": doc.get("metadata", {})
                    }
                    for doc, embedding in zip(documents, new_embeddings)
                ]

                self.vector_store.upsert(vectors=vectors)
                processed += len(documents)

            except Exception as e:
                errors.append({"offset": offset, "error": str(e)})

            offset += batch_size

            if on_progress:
                on_progress(processed, total_documents)

        return {
            "total_documents": total_documents,
            "processed": processed,
            "errors": errors,
            "success_rate": processed / total_documents if total_documents > 0 else 0
        }
```

## 3. Database Migration

### 3.1 PostgreSQL Migration

```python
from dataclasses import dataclass
from typing import Dict, List, Optional
from datetime import datetime
import psycopg2
from psycopg2.extras import RealDictCursor


@dataclass
class TableMigrationConfig:
    """Configuration for table migration."""
    source_table: str
    target_table: str
    columns: List[str]
    primary_key: str
    batch_size: int = 10000
    where_clause: Optional[str] = None
    transform_sql: Optional[str] = None


class PostgreSQLMigrator:
    """Migrate PostgreSQL databases."""

    def __init__(
        self,
        source_conn_string: str,
        target_conn_string: str
    ):
        self.source_conn = psycopg2.connect(source_conn_string)
        self.target_conn = psycopg2.connect(target_conn_string)

    def get_table_stats(self, table_name: str) -> Dict:
        """Get statistics for a table."""
        with self.source_conn.cursor() as cur:
            cur.execute(f"SELECT COUNT(*) FROM {table_name}")
            count = cur.fetchone()[0]

            cur.execute(f"""
                SELECT pg_size_pretty(pg_total_relation_size('{table_name}'))
            """)
            size = cur.fetchone()[0]

        return {
            "table": table_name,
            "row_count": count,
            "size": size
        }

    def create_target_schema(
        self,
        source_table: str,
        target_table: str
    ):
        """Create target table schema from source."""
        with self.source_conn.cursor() as cur:
            cur.execute(f"""
                SELECT column_name, data_type, character_maximum_length,
                       is_nullable, column_default
                FROM information_schema.columns
                WHERE table_name = '{source_table}'
                ORDER BY ordinal_position
            """)
            columns = cur.fetchall()

        # Generate CREATE TABLE statement
        column_defs = []
        for col in columns:
            col_name, data_type, max_length, nullable, default = col
            col_def = f"{col_name} {data_type}"
            if max_length:
                col_def += f"({max_length})"
            if nullable == "NO":
                col_def += " NOT NULL"
            if default:
                col_def += f" DEFAULT {default}"
            column_defs.append(col_def)

        create_sql = f"""
            CREATE TABLE IF NOT EXISTS {target_table} (
                {', '.join(column_defs)}
            )
        """

        with self.target_conn.cursor() as cur:
            cur.execute(create_sql)
        self.target_conn.commit()

    def migrate_table(
        self,
        config: TableMigrationConfig,
        on_progress: Optional[Callable[[int, int], None]] = None
    ) -> Dict:
        """Migrate a single table."""
        stats = self.get_table_stats(config.source_table)
        total_rows = stats["row_count"]

        migrated = 0
        errors = []
        start_time = datetime.utcnow()

        # Build SELECT query
        columns_str = ", ".join(config.columns)
        select_sql = f"""
            SELECT {columns_str} FROM {config.source_table}
            {f'WHERE {config.where_clause}' if config.where_clause else ''}
            ORDER BY {config.primary_key}
            LIMIT %s OFFSET %s
        """

        offset = 0
        while offset < total_rows:
            try:
                with self.source_conn.cursor(cursor_factory=RealDictCursor) as src_cur:
                    src_cur.execute(select_sql, (config.batch_size, offset))
                    rows = src_cur.fetchall()

                if not rows:
                    break

                # Insert into target
                self._insert_batch(config.target_table, config.columns, rows)
                migrated += len(rows)

            except Exception as e:
                errors.append({"offset": offset, "error": str(e)})

            offset += config.batch_size

            if on_progress:
                on_progress(migrated, total_rows)

        duration = (datetime.utcnow() - start_time).total_seconds()

        return {
            "table": config.source_table,
            "total_rows": total_rows,
            "migrated": migrated,
            "errors": errors,
            "duration_seconds": duration,
            "rows_per_second": migrated / duration if duration > 0 else 0
        }

    def _insert_batch(
        self,
        table_name: str,
        columns: List[str],
        rows: List[Dict]
    ):
        """Insert a batch of rows."""
        if not rows:
            return

        columns_str = ", ".join(columns)
        placeholders = ", ".join(["%s"] * len(columns))

        insert_sql = f"""
            INSERT INTO {table_name} ({columns_str})
            VALUES ({placeholders})
            ON CONFLICT DO NOTHING
        """

        values = [
            tuple(row[col] for col in columns)
            for row in rows
        ]

        with self.target_conn.cursor() as cur:
            cur.executemany(insert_sql, values)
        self.target_conn.commit()

    def setup_logical_replication(
        self,
        publication_name: str,
        tables: List[str]
    ) -> Dict:
        """Set up logical replication for CDC."""
        # Create publication on source
        tables_str = ", ".join(tables)
        with self.source_conn.cursor() as cur:
            cur.execute(f"""
                CREATE PUBLICATION {publication_name}
                FOR TABLE {tables_str}
            """)
        self.source_conn.commit()

        # Create subscription on target
        source_info = self._get_connection_info(self.source_conn)
        with self.target_conn.cursor() as cur:
            cur.execute(f"""
                CREATE SUBSCRIPTION {publication_name}_sub
                CONNECTION 'host={source_info['host']} port={source_info['port']} dbname={source_info['dbname']} user={source_info['user']}'
                PUBLICATION {publication_name}
            """)
        self.target_conn.commit()

        return {
            "publication": publication_name,
            "subscription": f"{publication_name}_sub",
            "tables": tables
        }

    def _get_connection_info(self, conn) -> Dict:
        """Get connection information."""
        info = conn.info
        return {
            "host": info.host,
            "port": info.port,
            "dbname": info.dbname,
            "user": info.user
        }

    def verify_data_integrity(
        self,
        table_name: str,
        sample_size: int = 1000
    ) -> Dict:
        """Verify data integrity between source and target."""
        # Count comparison
        with self.source_conn.cursor() as cur:
            cur.execute(f"SELECT COUNT(*) FROM {table_name}")
            source_count = cur.fetchone()[0]

        with self.target_conn.cursor() as cur:
            cur.execute(f"SELECT COUNT(*) FROM {table_name}")
            target_count = cur.fetchone()[0]

        # Sample comparison
        with self.source_conn.cursor(cursor_factory=RealDictCursor) as cur:
            cur.execute(f"""
                SELECT * FROM {table_name}
                ORDER BY RANDOM()
                LIMIT {sample_size}
            """)
            source_samples = cur.fetchall()

        matches = 0
        mismatches = 0

        for sample in source_samples:
            pk = sample.get("id")  # Assuming 'id' is primary key
            with self.target_conn.cursor(cursor_factory=RealDictCursor) as cur:
                cur.execute(f"SELECT * FROM {table_name} WHERE id = %s", (pk,))
                target_record = cur.fetchone()

            if target_record and dict(sample) == dict(target_record):
                matches += 1
            else:
                mismatches += 1

        return {
            "source_count": source_count,
            "target_count": target_count,
            "count_match": source_count == target_count,
            "samples_checked": len(source_samples),
            "matches": matches,
            "mismatches": mismatches,
            "integrity_score": matches / len(source_samples) if source_samples else 0
        }

    def close(self):
        """Close database connections."""
        self.source_conn.close()
        self.target_conn.close()
```

## 4. Data Validation

### 4.1 Comprehensive Validation Framework

```python
from dataclasses import dataclass
from typing import Dict, List, Optional, Callable, Any
from datetime import datetime
from enum import Enum
import hashlib
import json


class ValidationLevel(Enum):
    """Validation strictness levels."""
    BASIC = "basic"         # Count only
    STANDARD = "standard"   # Count + checksum
    STRICT = "strict"       # Full record comparison
    EXHAUSTIVE = "exhaustive"  # All records + relationships


@dataclass
class ValidationResult:
    """Result of a validation check."""
    check_name: str
    passed: bool
    source_value: Any
    target_value: Any
    difference: Optional[Any] = None
    details: str = ""


@dataclass
class ValidationReport:
    """Complete validation report."""
    migration_id: str
    validation_level: ValidationLevel
    started_at: datetime
    completed_at: datetime
    results: List[ValidationResult]
    overall_passed: bool
    confidence_score: float


class DataValidator:
    """Validate data migration integrity."""

    def __init__(
        self,
        source_accessor: Callable,
        target_accessor: Callable
    ):
        self.source = source_accessor
        self.target = target_accessor
        self.results: List[ValidationResult] = []

    def validate_count(
        self,
        entity_name: str,
        tolerance_percent: float = 0.0
    ) -> ValidationResult:
        """Validate record counts match."""
        source_count = self.source.count(entity_name)
        target_count = self.target.count(entity_name)

        diff = abs(source_count - target_count)
        diff_percent = (diff / source_count * 100) if source_count > 0 else 0

        passed = diff_percent <= tolerance_percent

        result = ValidationResult(
            check_name=f"count_{entity_name}",
            passed=passed,
            source_value=source_count,
            target_value=target_count,
            difference=diff,
            details=f"Difference: {diff} ({diff_percent:.2f}%)"
        )

        self.results.append(result)
        return result

    def validate_checksum(
        self,
        entity_name: str,
        columns: List[str]
    ) -> ValidationResult:
        """Validate data checksum matches."""
        source_data = self.source.fetch_all(entity_name, columns)
        target_data = self.target.fetch_all(entity_name, columns)

        source_checksum = self._calculate_checksum(source_data)
        target_checksum = self._calculate_checksum(target_data)

        passed = source_checksum == target_checksum

        result = ValidationResult(
            check_name=f"checksum_{entity_name}",
            passed=passed,
            source_value=source_checksum[:16] + "...",
            target_value=target_checksum[:16] + "...",
            details="Checksums match" if passed else "Checksum mismatch"
        )

        self.results.append(result)
        return result

    def validate_sample(
        self,
        entity_name: str,
        sample_size: int = 100,
        key_column: str = "id"
    ) -> ValidationResult:
        """Validate a sample of records."""
        source_sample = self.source.fetch_sample(entity_name, sample_size)

        matches = 0
        mismatches = 0
        missing = 0

        for source_record in source_sample:
            key_value = source_record[key_column]
            target_record = self.target.fetch_by_key(entity_name, key_column, key_value)

            if target_record is None:
                missing += 1
            elif self._records_match(source_record, target_record):
                matches += 1
            else:
                mismatches += 1

        passed = missing == 0 and mismatches == 0

        result = ValidationResult(
            check_name=f"sample_{entity_name}",
            passed=passed,
            source_value=sample_size,
            target_value=matches,
            difference={"missing": missing, "mismatches": mismatches},
            details=f"Matches: {matches}, Missing: {missing}, Mismatches: {mismatches}"
        )

        self.results.append(result)
        return result

    def validate_relationships(
        self,
        parent_entity: str,
        child_entity: str,
        foreign_key: str
    ) -> ValidationResult:
        """Validate referential integrity."""
        # Get all foreign key values from child
        child_fks = set(self.target.fetch_column(child_entity, foreign_key))

        # Get all primary keys from parent
        parent_pks = set(self.target.fetch_column(parent_entity, "id"))

        # Find orphans
        orphans = child_fks - parent_pks

        passed = len(orphans) == 0

        result = ValidationResult(
            check_name=f"relationship_{child_entity}_{parent_entity}",
            passed=passed,
            source_value=len(child_fks),
            target_value=len(parent_pks),
            difference=len(orphans),
            details=f"Orphan records: {len(orphans)}"
        )

        self.results.append(result)
        return result

    def validate_aggregates(
        self,
        entity_name: str,
        aggregations: Dict[str, str]  # column -> agg_function (sum, avg, min, max)
    ) -> ValidationResult:
        """Validate aggregate values match."""
        source_aggs = self.source.calculate_aggregates(entity_name, aggregations)
        target_aggs = self.target.calculate_aggregates(entity_name, aggregations)

        mismatches = []
        for col, agg_func in aggregations.items():
            source_val = source_aggs.get(col)
            target_val = target_aggs.get(col)

            if source_val != target_val:
                mismatches.append({
                    "column": col,
                    "function": agg_func,
                    "source": source_val,
                    "target": target_val
                })

        passed = len(mismatches) == 0

        result = ValidationResult(
            check_name=f"aggregates_{entity_name}",
            passed=passed,
            source_value=source_aggs,
            target_value=target_aggs,
            difference=mismatches,
            details=f"{len(mismatches)} aggregate mismatches"
        )

        self.results.append(result)
        return result

    def _calculate_checksum(self, data: List[Dict]) -> str:
        """Calculate checksum for data."""
        content = json.dumps(data, sort_keys=True, default=str)
        return hashlib.sha256(content.encode()).hexdigest()

    def _records_match(
        self,
        source: Dict,
        target: Dict
    ) -> bool:
        """Check if two records match."""
        for key, source_val in source.items():
            target_val = target.get(key)
            if source_val != target_val:
                return False
        return True

    def run_validation(
        self,
        level: ValidationLevel,
        entities: List[str]
    ) -> ValidationReport:
        """Run complete validation."""
        start_time = datetime.utcnow()
        self.results = []

        for entity in entities:
            # Always do count validation
            self.validate_count(entity)

            if level in [ValidationLevel.STANDARD, ValidationLevel.STRICT, ValidationLevel.EXHAUSTIVE]:
                self.validate_sample(entity)

            if level in [ValidationLevel.STRICT, ValidationLevel.EXHAUSTIVE]:
                # Would need column information
                pass

        end_time = datetime.utcnow()

        # Calculate overall status
        passed_count = sum(1 for r in self.results if r.passed)
        total_count = len(self.results)
        overall_passed = all(r.passed for r in self.results)
        confidence = passed_count / total_count if total_count > 0 else 0

        return ValidationReport(
            migration_id="",  # Would be set by caller
            validation_level=level,
            started_at=start_time,
            completed_at=end_time,
            results=self.results,
            overall_passed=overall_passed,
            confidence_score=confidence
        )

    def generate_report(self) -> str:
        """Generate validation report."""
        report = "# Data Validation Report\n\n"
        report += f"**Generated**: {datetime.utcnow().isoformat()}\n\n"

        passed = sum(1 for r in self.results if r.passed)
        total = len(self.results)

        report += f"## Summary\n\n"
        report += f"- Total Checks: {total}\n"
        report += f"- Passed: {passed}\n"
        report += f"- Failed: {total - passed}\n"
        report += f"- Score: {passed/total*100:.1f}%\n\n"

        report += "## Results\n\n"
        report += "| Check | Status | Source | Target | Details |\n"
        report += "|-------|--------|--------|--------|----------|\n"

        for result in self.results:
            status = "PASS" if result.passed else "FAIL"
            report += f"| {result.check_name} | {status} | {result.source_value} | {result.target_value} | {result.details} |\n"

        return report
```

## 5. Migration Orchestration

### 5.1 Migration Executor

```python
from dataclasses import dataclass
from typing import Dict, List, Optional, Callable
from datetime import datetime
from enum import Enum
import asyncio


class MigrationStatus(Enum):
    """Migration status."""
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    PAUSED = "paused"
    COMPLETED = "completed"
    FAILED = "failed"
    ROLLED_BACK = "rolled_back"


@dataclass
class MigrationProgress:
    """Migration progress tracking."""
    migration_id: str
    current_phase: str
    status: MigrationStatus
    started_at: datetime
    records_processed: int
    records_total: int
    errors: List[Dict]
    last_updated: datetime


class MigrationOrchestrator:
    """Orchestrate data migration execution."""

    def __init__(
        self,
        plan: DataMigrationPlan,
        extractor: DataExtractor,
        transformer: DataTransformer,
        loader: DataLoader,
        validator: DataValidator
    ):
        self.plan = plan
        self.extractor = extractor
        self.transformer = transformer
        self.loader = loader
        self.validator = validator

        self.progress = MigrationProgress(
            migration_id=plan.migration_id,
            current_phase="",
            status=MigrationStatus.PENDING,
            started_at=datetime.utcnow(),
            records_processed=0,
            records_total=0,
            errors=[],
            last_updated=datetime.utcnow()
        )

        self.checkpoints: List[Dict] = []
        self.on_progress_callbacks: List[Callable] = []

    def add_progress_callback(self, callback: Callable):
        """Add a progress callback."""
        self.on_progress_callbacks.append(callback)

    def _notify_progress(self):
        """Notify all progress callbacks."""
        for callback in self.on_progress_callbacks:
            callback(self.progress)

    def execute(self) -> Dict:
        """Execute the migration."""
        self.progress.status = MigrationStatus.IN_PROGRESS
        self.progress.started_at = datetime.utcnow()

        try:
            for phase in self.plan.phases:
                self.progress.current_phase = phase.name
                self._notify_progress()

                if phase.phase_id == "PREP":
                    self._execute_preparation()
                elif phase.phase_id == "INITIAL":
                    self._execute_initial_load()
                elif phase.phase_id == "CDC":
                    self._execute_cdc()
                elif phase.phase_id == "VALIDATE":
                    self._execute_validation()
                elif phase.phase_id == "CUTOVER":
                    self._execute_cutover()
                elif phase.phase_id == "CLEANUP":
                    self._execute_cleanup()

                # Save checkpoint
                self._save_checkpoint(phase.phase_id)

            self.progress.status = MigrationStatus.COMPLETED

        except Exception as e:
            self.progress.status = MigrationStatus.FAILED
            self.progress.errors.append({
                "phase": self.progress.current_phase,
                "error": str(e),
                "timestamp": datetime.utcnow().isoformat()
            })

        self.progress.last_updated = datetime.utcnow()
        self._notify_progress()

        return self.get_summary()

    def _execute_preparation(self):
        """Execute preparation phase."""
        # Connect to source
        self.extractor.connect()

        # Get total count
        self.progress.records_total = self.extractor.get_total_count()

        # Connect to target
        self.loader.connect()

    def _execute_initial_load(self):
        """Execute initial data load."""
        for batch in self.extractor.extract_batches():
            # Transform
            transformed = self.transformer.transform_batch(batch)

            # Load
            result = self.loader.load_batch(transformed)

            self.progress.records_processed += result.records_loaded

            if result.records_failed > 0:
                self.progress.errors.extend([
                    {"batch": batch.batch_id, "error": e}
                    for e in result.errors
                ])

            self._notify_progress()

    def _execute_cdc(self):
        """Execute CDC phase."""
        # Implementation would set up and monitor CDC
        pass

    def _execute_validation(self):
        """Execute validation phase."""
        report = self.validator.run_validation(
            level=self.plan.integrity_level,
            entities=[]  # Would be populated from plan
        )

        if not report.overall_passed:
            raise Exception(f"Validation failed: {report.confidence_score}")

    def _execute_cutover(self):
        """Execute cutover phase."""
        # Implementation would handle application cutover
        pass

    def _execute_cleanup(self):
        """Execute cleanup phase."""
        self.extractor.disconnect()
        self.loader.disconnect()

    def _save_checkpoint(self, phase_id: str):
        """Save migration checkpoint."""
        self.checkpoints.append({
            "phase": phase_id,
            "timestamp": datetime.utcnow().isoformat(),
            "records_processed": self.progress.records_processed,
            "status": self.progress.status.value
        })

    def resume_from_checkpoint(
        self,
        checkpoint_phase: str
    ) -> Dict:
        """Resume migration from a checkpoint."""
        # Find checkpoint
        checkpoint = None
        for cp in self.checkpoints:
            if cp["phase"] == checkpoint_phase:
                checkpoint = cp
                break

        if not checkpoint:
            raise ValueError(f"Checkpoint {checkpoint_phase} not found")

        # Resume from checkpoint
        self.progress.records_processed = checkpoint["records_processed"]

        # Find phase index
        phase_idx = 0
        for i, phase in enumerate(self.plan.phases):
            if phase.phase_id == checkpoint_phase:
                phase_idx = i + 1
                break

        # Execute remaining phases
        for phase in self.plan.phases[phase_idx:]:
            self.progress.current_phase = phase.name
            # Execute phase...

        return self.get_summary()

    def pause(self):
        """Pause the migration."""
        self.progress.status = MigrationStatus.PAUSED
        self._save_checkpoint(self.progress.current_phase)

    def rollback(self) -> Dict:
        """Rollback the migration."""
        self.progress.status = MigrationStatus.ROLLED_BACK

        # Implementation would reverse changes
        # based on checkpoints and plan

        return {
            "status": "rolled_back",
            "checkpoints_processed": len(self.checkpoints)
        }

    def get_summary(self) -> Dict:
        """Get migration summary."""
        duration = (
            self.progress.last_updated - self.progress.started_at
        ).total_seconds()

        return {
            "migration_id": self.plan.migration_id,
            "status": self.progress.status.value,
            "current_phase": self.progress.current_phase,
            "records_processed": self.progress.records_processed,
            "records_total": self.progress.records_total,
            "progress_percent": (
                self.progress.records_processed / self.progress.records_total * 100
                if self.progress.records_total > 0 else 0
            ),
            "duration_seconds": duration,
            "errors_count": len(self.progress.errors),
            "checkpoints": self.checkpoints
        }
```

## Troubleshooting

### Common Migration Issues

| Issue | Symptoms | Resolution |
|-------|----------|------------|
| Connection timeout | Extraction fails | Increase timeout, check network |
| Data truncation | Missing data in target | Check column sizes, encoding |
| Foreign key violations | Load failures | Migrate parent tables first |
| Checksum mismatch | Validation fails | Check transformations, encoding |
| Performance degradation | Slow migration | Increase batch size, parallelism |

### Migration Checklist

- [ ] Source data backed up
- [ ] Target schema created
- [ ] Network connectivity verified
- [ ] Disk space sufficient
- [ ] Monitoring configured
- [ ] Rollback procedure tested

## Related Documentation

- [15.1 Model Migration Guide](15.1_model_migration_guide.md)
- [15.3 System Integration Guide](15.3_system_integration_guide.md)
- [13.2 Disaster Recovery Guide](../13_operations_reliability/13.2_disaster_recovery_guide.md)

## Version History

| Version | Date | Author | Changes |
|---------|------|--------|---------|
| 1.0 | 2024-01-15 | Data Engineering Team | Initial release |
